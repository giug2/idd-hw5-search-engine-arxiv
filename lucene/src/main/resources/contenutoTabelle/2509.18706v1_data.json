{
    "S4.T1": {
        "source_file": "M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition",
        "caption": "TABLE I: Parameter Settings on the IEMOCAP and MELD Datasets.",
        "body": "Parameter Settings\nIEMOCAP\nMELD\n\n\nBatch size\n16\n16\n\n\nEpochs\n100\n100\n\n\nLearning rate\n1​e−51e^{-5}\n1​e−41e^{-4}\n\n\nDropout\n0.5\n0.5\n\n\ndS\\textit{d}_{S}\n768\n768\n\n\ndT\\textit{d}_{T}\n768\n768\n\n\ndv​o​c​a​b\\textit{d}_{vocab}\n30,522\n30,522\n\n\nd\n768\n768\n\n\nAttention layers\n12\n12\n\n\nAttention heads\n12\n12\n\n\nα\\alpha\n0.1\n0.1\n\n\nβ\\beta\n3\n3\n\n\nγ\\gamma\n0.01\n0.01\n\n\nλ\\lambda\n0.1\n0.1\n\n\nτ\\tau\n0.07\n0.07",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Parameter Settings</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">IEMOCAP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">MELD</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Batch size</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">16</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Epochs</td>\n<td class=\"ltx_td ltx_align_center\">100</td>\n<td class=\"ltx_td ltx_align_center\">100</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Learning rate</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1e^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-5}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1e^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-4}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Dropout</td>\n<td class=\"ltx_td ltx_align_center\">0.5</td>\n<td class=\"ltx_td ltx_align_center\">0.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"\\textit{d}_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m3\" intent=\":literal\"><semantics><msub><mtext class=\"ltx_mathvariant_italic\">d</mtext><mi>S</mi></msub><annotation encoding=\"application/x-tex\">\\textit{d}_{S}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">768</td>\n<td class=\"ltx_td ltx_align_center\">768</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"\\textit{d}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m4\" intent=\":literal\"><semantics><msub><mtext class=\"ltx_mathvariant_italic\">d</mtext><mi>T</mi></msub><annotation encoding=\"application/x-tex\">\\textit{d}_{T}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">768</td>\n<td class=\"ltx_td ltx_align_center\">768</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"\\textit{d}_{vocab}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m5\" intent=\":literal\"><semantics><msub><mtext class=\"ltx_mathvariant_italic\">d</mtext><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\textit{d}_{vocab}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">30,522</td>\n<td class=\"ltx_td ltx_align_center\">30,522</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_markedasmath ltx_font_italic\">d</span></td>\n<td class=\"ltx_td ltx_align_center\">768</td>\n<td class=\"ltx_td ltx_align_center\">768</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Attention layers</td>\n<td class=\"ltx_td ltx_align_center\">12</td>\n<td class=\"ltx_td ltx_align_center\">12</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Attention heads</td>\n<td class=\"ltx_td ltx_align_center\">12</td>\n<td class=\"ltx_td ltx_align_center\">12</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m7\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">0.1</td>\n<td class=\"ltx_td ltx_align_center\">0.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m8\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">3</td>\n<td class=\"ltx_td ltx_align_center\">3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m9\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">0.01</td>\n<td class=\"ltx_td ltx_align_center\">0.01</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m10\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">0.1</td>\n<td class=\"ltx_td ltx_align_center\">0.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m11\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.07</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "dstextitds",
            "datasets",
            "heads",
            "1​e−41e4",
            "attention",
            "rate",
            "batch",
            "learning",
            "dttextitdt",
            "iemocap",
            "τtau",
            "αalpha",
            "1​e−51e5",
            "parameter",
            "settings",
            "size",
            "γgamma",
            "βbeta",
            "dropout",
            "λlambda",
            "dv​o​c​a​btextitdvocab",
            "layers",
            "epochs",
            "meld"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Our method was implemented using Python 3.10.0 and Pytorch 1.11.0. The model was trained and evaluated on a system equipped with an Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz, 32GB RAM, and one NVIDIA Tesla V100 GPU. The detailed parameter settings are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S4.T1\" title=\"TABLE I &#8227; IV-B Implementation Details &#8227; IV Experimental Setup &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Multimodal speech emotion recognition (SER) has emerged as pivotal for improving human&#8211;machine interaction. Researchers are increasingly leveraging both speech and textual information obtained through automatic speech recognition (ASR) to comprehensively recognize emotional states from speakers. Although this approach reduces reliance on human-annotated text data, ASR errors possibly degrade emotion recognition performance.\nTo address this challenge, in our previous work, we introduced two auxiliary tasks, namely, ASR error detection and ASR error correction, and we proposed a novel multimodal fusion (MF) method for learning modality-specific and modality-invariant representations across different modalities.\nBuilding on this foundation, in this paper, we introduce two additional training strategies. First, we propose an adversarial network to enhance the diversity of modality-specific representations. Second, we introduce a label-based contrastive learning strategy to better capture emotional features.\nWe refer to our proposed method as M<sup class=\"ltx_sup\">4</sup>SER and validate its superiority over state-of-the-art methods through extensive experiments using IEMOCAP and MELD datasets.</p>\n\n",
                "matched_terms": [
                    "meld",
                    "iemocap",
                    "learning",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the other hand, how to effectively integrate speech and text modalities has become a key focus. Previous methods have included simple feature concatenation, recurrent neural networks (RNNs), convolutional neural networks (CNNs), and cross-modal attention mechanisms <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib1\" title=\"\">1</a>]</cite>. Although these methods have shown some effectiveness, they often face challenges arising from differences in representations across different modalities.\nIn recent multimodal tasks such as sentiment analysis <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib11\" title=\"\">11</a>]</cite>, researchers have proposed learning two distinct types of representation to enhance multimodal learning.\nThe first is a representation tailored to each modality, that is, a modality-specific representation (MSR).\nThe second is modality-invariant representation (MIR), aimed at mapping all modalities of the same utterance into a shared subspace. This representation captures commonalities in multimodal signals as well as the speaker&#8217;s shared motives, which collectively affect the emotional state of the utterance through distribution alignment. By combining these two types of representation, one can obtain a comprehensive view of multimodal data can be provided for downstream tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib12\" title=\"\">12</a>]</cite>.\nHowever, these methods focus on utterance-level representations, which, while effective for short and isolated utterances, often fail to capture fine-grained emotional dynamics, especially in long and complex interactions. Such coarse-grained representations fail to capture subtle temporal cues and modality-specific variations that are crucial for accurate emotion recognition.\nMoreover, mapping entire utterances to a shared or modality-specific space using similarity loss oversimplifies the underlying multimodal relationships and hinders effective cross-modal alignment.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "attention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed M<sup class=\"ltx_sup\">4</sup>SER outperforms state-of-the-arts on the IEMOCAP and MELD datasets. Extensive experiments demonstrate its superiority in SER tasks.</p>\n\n",
                "matched_terms": [
                    "meld",
                    "iemocap",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to factorization-based approaches which explicitly disentangle modality inputs into shared and private spaces, our M<sup class=\"ltx_sup\">4</sup>SER framework adopts a structured encoding&#8211;generation strategy to implicitly model MSRs and MIRs. Specifically, we utilize a stack of cross-modal encoder blocks to extract token-aware and speech-aware features (MSRs), and then apply a dedicated generator composed of hybrid-modal attention and masking mechanisms to derive the MIRs. In addition, unlike prior works that incorporate adversarial learning at a global or representation-level, our M<sup class=\"ltx_sup\">4</sup>SER employs a frame-level discriminator that operates on each temporal representation. A frame-level discriminator attempts to distinguish the modality origin of each temporal representation, while the MIR generator is optimized to deceive the discriminator by producing modality-agnostic outputs that are hard to classify as either text or speech. This fine-grained, per-frame adversarial constraint leads to more robust and aligned cross-modal representations. Moreover, M<sup class=\"ltx_sup\">4</sup>SER integrates this disentanglement framework into a multitask learning setup with ASR-aware auxiliary supervision, enabling more effective adaptation to noisy real-world speech&#8211;text scenarios.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "attention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further develop the modality discriminator <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m1\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> utilizing the modality-invariant representations produced by the MIR generator. This discriminator illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(f) is composed of two linear layers with a ReLU activation function in between, followed by a Sigmoid activation function. To enhance the MIR&#8217;s capability to remain modality-agnostic, we employ adversarial learning techniques.\nSpecifically, the discriminator <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> outputs a scalar value between 0 and 1 for each frame, where values close to 0 indicate the text modality, values close to 1 indicate the speech modality, and values near 0.5 represent an ambiguous modality.\nHere, <math alttext=\"D(H)\\in\\mathbb{R}^{m\\times 1},H\\in\\{H^{(spe)}_{T},H^{(spe)}_{S},H_{ST}^{\\rm(inv)}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>D</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow></msup></mrow><mo>,</mo><mrow><mi>H</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>inv</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">D(H)\\in\\mathbb{R}^{m\\times 1},H\\in\\{H^{(spe)}_{T},H^{(spe)}_{S},H_{ST}^{\\rm(inv)}\\}</annotation></semantics></math>.\nOn one hand, we aim for the discriminator that correctly classifies frames in the modality-specific representations <math alttext=\"H^{(spe)}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H^{(spe)}_{T}</annotation></semantics></math> and <math alttext=\"H^{(spe)}_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m5\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H^{(spe)}_{S}</annotation></semantics></math> as 0 and 1, respectively. On the other hand, to enhance the modality agnosticism of the modality-invariant representation <math alttext=\"H_{ST}^{\\rm(inv)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m6\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>inv</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{ST}^{\\rm(inv)}</annotation></semantics></math> produced by the MIR generator, we aim for a discriminator that outputs values close to 0.5, indicating an ambiguous modality. With this design of the MIR generator and discriminator, we define a generative adversarial training objective, mathematically represented as</p>\n\n",
                "matched_terms": [
                    "learning",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance the model&#8217;s capability to learn emotion features from multimodal data, we employ a label-based contrastive learning task to complement the cross-entropy loss, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(g). This task aids the model in extracting emotion-related features when the MF module integrates speech and text data. As depicted in the LCL task in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.F5\" title=\"Figure 5 &#8227; III-H Label-based Contrastive Learning (LCL) &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we categorize data in each batch into positive and negative samples on the basis of emotion labels. For instance, in a batch containing eight samples, we compute the set of positive samples for each sample, where those with the same label are considered positive samples (yellow squares), and those with different labels are considered negative samples (white squares). We then calculate the label-based contrastive loss (LCL) using Eq. <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:eq:lcl</span>, which promotes instances of a specific label to be more similar to each other than instances of other labels.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "batch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\langle\\cdot,\\cdot\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo rspace=\"0em\">,</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\cdot,\\cdot\\rangle</annotation></semantics></math> denotes cosine similarity and <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m2\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> is the temperature parameter. For the multimodal representations, let <math alttext=\"(H_{ST}^{\\rm(ap)})_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m3\" intent=\":literal\"><semantics><msub><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>ap</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mi>i</mi></msub><annotation encoding=\"application/x-tex\">(H_{ST}^{\\rm(ap)})_{i}</annotation></semantics></math>, <math alttext=\"(H_{ST}^{\\rm(ap)})_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m4\" intent=\":literal\"><semantics><msub><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>ap</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mi>p</mi></msub><annotation encoding=\"application/x-tex\">(H_{ST}^{\\rm(ap)})_{p}</annotation></semantics></math>, and <math alttext=\"(H_{ST}^{\\rm(ap)})_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m5\" intent=\":literal\"><semantics><msub><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>ap</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mi>a</mi></msub><annotation encoding=\"application/x-tex\">(H_{ST}^{\\rm(ap)})_{a}</annotation></semantics></math> represent the <math alttext=\"i^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m6\" intent=\":literal\"><semantics><msup><mi>i</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">i^{th}</annotation></semantics></math> sample, the <math alttext=\"p^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m7\" intent=\":literal\"><semantics><msup><mi>p</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">p^{th}</annotation></semantics></math> positive sample, and the <math alttext=\"a^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m8\" intent=\":literal\"><semantics><msup><mi>a</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">a^{th}</annotation></semantics></math> sample, respectively. We define <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m9\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math> as the index set of samples, <math alttext=\"P(i)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m10\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(i)</annotation></semantics></math> as the set of positive samples for the <math alttext=\"i^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m11\" intent=\":literal\"><semantics><msup><mi>i</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">i^{th}</annotation></semantics></math> sample, and <math alttext=\"A(i)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m12\" intent=\":literal\"><semantics><mrow><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">A(i)</annotation></semantics></math> as the set of all samples.</p>\n\n",
                "matched_terms": [
                    "parameter",
                    "τtau"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m6\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> is the hyperparameter for adjusting the weight between the main task and the auxiliary tasks, and <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m7\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> is the hyperparameter for adjusting the weight between the two auxiliary tasks, and <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m8\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>, <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m9\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> are the hyperparameters for balancing different training strategies.</p>\n\n",
                "matched_terms": [
                    "λlambda",
                    "βbeta",
                    "γgamma",
                    "αalpha"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the effectiveness\nof our proposed model, we carried out experiments on two public datasets:\nthe Interactive Emotional Dyadic Motion Capture (IEMOCAP) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib45\" title=\"\">45</a>]</cite> and the Multimodal Emotion Lines Dataset (MELD) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib46\" title=\"\">46</a>]</cite>. The statistics of the two datasets are shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S4.F6\" title=\"Figure 6 &#8227; IV-A Dataset and Evaluation Metrics &#8227; IV Experimental Setup &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "meld",
                    "iemocap",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The acoustic encoder was initialized using the <span class=\"ltx_text ltx_font_typewriter\">hubert-base-ls960<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">3</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/facebook/hubert-base-ls960\" title=\"\">https://huggingface.co/facebook/hubert-base-ls960</a></span></span></span></span> model, producing acoustic representations <math alttext=\"d_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">d_{S}</annotation></semantics></math> with a dimensionality of 768.\nThe text encoder employed the <span class=\"ltx_text ltx_font_typewriter\">bert-base-uncased<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">4</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/google-bert/bert-base-uncased\" title=\"\">https://huggingface.co/google-bert/bert-base-uncased</a></span></span></span></span> model for initialization, yielding token representations <math alttext=\"d_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">d_{T}</annotation></semantics></math> with a dimensionality of 768. The vocabulary size for word tokenization <math alttext=\"d_{vocab}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>d</mi><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">d_{vocab}</annotation></semantics></math> was set to 30,522. We set the hidden size <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> to 768, the number of attention layers to 12, and the number of attention heads to 12. Both the HuBERT and BERT models were fine-tuned during the training stage.\nThe transformer decoder in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c) adopted a single-layer transformer with a hidden size of 768. Additionally, we obtained corresponding ASR hypotheses using the Whisper ASR model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib48\" title=\"\">48</a>]</cite>. We used the <span class=\"ltx_text ltx_font_typewriter\">openai/whisper-medium.en<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">5</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/openai/whisper-medium.en\" title=\"\">https://huggingface.co/openai/whisper-medium.en</a></span></span></span></span> and <span class=\"ltx_text ltx_font_typewriter\">openai/whisper-large-v2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">6</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/openai/whisper-large-v2\" title=\"\">https://huggingface.co/openai/whisper-large-v2</a></span></span></span></span> models, achieving word error rates (WERs) of 20.48% and 37.87% on the IEMOCAP and MELD datasets, respectively.</p>\n\n",
                "matched_terms": [
                    "attention",
                    "iemocap",
                    "size",
                    "datasets",
                    "layers",
                    "heads",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used Adam <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib49\" title=\"\">49</a>]</cite> as the optimizer with a batch size of 16. On the basis of empirical observations and prior work <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib50\" title=\"\">50</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib51\" title=\"\">51</a>]</cite>, we kept the learning rate constant at <math alttext=\"1e^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-5}</annotation></semantics></math> for IEMOCAP and <math alttext=\"1e^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-4}</annotation></semantics></math> for MELD during training. For our multitask learning setup, we set <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m3\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> to 3 and <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m4\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> to 0.1. For our multistrategy learning setup, we set <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m5\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> to 0.01 and <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m6\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> to 0.1. The temperature parameter <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m7\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> was set to 0.07. To validate these choices, we additionally performed a one-at-a-time sensitivity analysis on the hyperparameters <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m8\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m9\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>, <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m10\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>, and <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m11\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math>, as presented in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.SS3\" title=\"V-C Sensitivity Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">V-C</span></a> and illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F7\" title=\"Figure 7 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "1​e−41e4",
                    "iemocap",
                    "rate",
                    "τtau",
                    "size",
                    "αalpha",
                    "γgamma",
                    "βbeta",
                    "λlambda",
                    "1​e−51e5",
                    "batch",
                    "parameter",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DIMMN</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib57\" title=\"\">57</a>]</cite> fuses multimodal data using multiview attention layers, temporal convolutional networks, gated recurrent units, and memory networks to model dynamic dependencies and contextual information.</p>\n\n",
                "matched_terms": [
                    "attention",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T2\" title=\"TABLE II &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> compare the performance of the M<sup class=\"ltx_sup\">4</sup>SER method with those of recent multimodal SER methods on the IEMOCAP and MELD datasets. Our proposed M<sup class=\"ltx_sup\">4</sup>SER achieves a WA of 79.2% and a UA of 80.1% on IEMOCAP, and an ACC of 66.5% and a W-F1 of 66.0% on MELD, outperforming other models and demonstrating the superiority of M<sup class=\"ltx_sup\">4</sup>SER. Specifically, on the IEMOCAP dataset, compared with the recent MAF-DCT, the M<sup class=\"ltx_sup\">4</sup>SER method shows a 0.7% improvement in WA and a 0.8% improvement in UA, reaching a new SOTA performance. These improvements stem from our M<sup class=\"ltx_sup\">4</sup>SER&#8217;s capability to mitigate the impact of ASR errors, capture modality-specific and modality-invariant representations across different modalities, enhance commonality between modalities through adversarial learning, and excel in emotion feature learning with the LCL loss.</p>\n\n",
                "matched_terms": [
                    "meld",
                    "iemocap",
                    "learning",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed M<sup class=\"ltx_sup\">4</sup>SER model is composed of four types of learning: multimodal learning, multirepresentation learning, multitask learning, and multistrategy learning. To determine the impact of different types of learning on the performance of the emotion recognition task, ablation experiments were conducted on the IEMOCAP and MELD datasets, as presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>.</p>\n\n",
                "matched_terms": [
                    "meld",
                    "iemocap",
                    "learning",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multimodalities.</span>\nTo assess the necessity of multimodality, we conduct single-modal comparison experiments involving text and speech modalities. These experiments operate under a baseline framework excluding the MF module, LCL loss, and GAN loss, precluding intermodal interaction.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(A), in the single-modal experiments, the performance of speech modality in the IEMOCAP dataset is better than that of text modality, whereas the reverse is observed in the MELD dataset. This divergence suggests that emotional information is more distinctly conveyed through speech in IEMOCAP, whereas MELD leans towards text for emotional expression.\nMoreover, the multimodal baseline model, which simply concatenates speech and text features for recognition (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> A(3)), significantly enhances performance compared with single-modal baselines, highlighting the essential role of multimodal information.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multistrategies.</span> To validate the effectiveness of the adversarial training strategy outlined in Alg. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#alg1\" title=\"Algorithm 1 &#8227; III-I Joint Training &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we remove the adversarial training strategy from M<sup class=\"ltx_sup\">4</sup>SER. The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(D)(1) show a decrease in performance on both datasets, demonstrating the critical role of this strategy in learning modality-invariant representations. The proposed modality discriminator effectively enhances the modality agnosticism of the refined representations from the generator.\nAdditionally, omitting the LCL strategy described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.SS8\" title=\"III-H Label-based Contrastive Learning (LCL) &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III-H</span></a> also results in similar performance degradation (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(D)(2)). We visualize the results before and after introducing this strategy to demonstrate its effectiveness, with specific analysis also detailed in the following t-SNE analysis section. Moreover, removing both of these strategies further diminishes emotion recognition performance, confirming that both learning strategies contribute significantly to performance improvement.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a one-at-a-time sensitivity analysis on four key hyperparameters: the auxiliary task weight <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, the AED/AEC balancing factor <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m2\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>, the GAN loss weight <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m3\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>, and the LCL loss weight <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m4\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math>. As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F7\" title=\"Figure 7 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, model performance is moderately sensitive to <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m5\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m6\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math>, with optimal results achieved at <math alttext=\"\\alpha=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.1</annotation></semantics></math> and <math alttext=\"\\lambda=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m8\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=0.1</annotation></semantics></math>. Excessively small values reduce the effectiveness of auxiliary guidance, while overly large values interfere with the main task.</p>\n\n",
                "matched_terms": [
                    "λlambda",
                    "βbeta",
                    "γgamma",
                    "αalpha"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the model exhibits relative robustness to variations in <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m1\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>, indicating stable synergy between the AED and AEC objectives. Conversely, performance is highly sensitive to <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m2\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>; large values result in unstable training due to adversarial objectives, while a small weight (e.g., <math alttext=\"\\gamma=0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=0.01</annotation></semantics></math>) ensures stable convergence and optimal results.</p>\n\n",
                "matched_terms": [
                    "γgamma",
                    "βbeta"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Among the single-modal models, HuBERT exhibits higher computational cost than BERT owing to the nature of speech encoders, but also delivers better recognition performance. In the multi-modal setting, each additional module introduces a moderate increase in parameter size and training cost, while consistently improving recognition accuracy.</p>\n\n",
                "matched_terms": [
                    "parameter",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">t-SNE Analysis.</span> To intuitively demonstrate the advantages of our proposed M<sup class=\"ltx_sup\">4</sup>SER model on IEMOCAP and MELD datasets, we utilize the t-distributed stochastic neighbor embedding (t-SNE) tool <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib61\" title=\"\">61</a>]</cite> to visualize the learned emotion features using all samples from session 3 of the IEMOCAP and test set of the MELD. We compare these features across the following models: the speech model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(a)), the text model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(b)), the proposed M<sup class=\"ltx_sup\">4</sup>SER model without label-based contrastive learning (LCL) (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(c)), and the proposed M<sup class=\"ltx_sup\">4</sup>SER model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(d)).</p>\n\n",
                "matched_terms": [
                    "meld",
                    "iemocap",
                    "learning",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the visualization results in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, we observe that the emotion label distributions for the two single-modal baselines on the IEMOCAP and MELD datasets show a significant overlap among the emotion categories, indicating that they are often confused with each other. In contrast, the emotion label distributions for the two multimodal models are more distinguishable, demonstrating the effectiveness of multimodal models in capturing richer emotional features.</p>\n\n",
                "matched_terms": [
                    "meld",
                    "iemocap",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, compared with the M<sup class=\"ltx_sup\">4</sup>SER model without the LCL strategy, our M<sup class=\"ltx_sup\">4</sup>SER model achieves better clustering for each emotion category, making them as distinct as possible. For instance, on the IEMOCAP dataset, the intra-class clustering of samples learned by the M<sup class=\"ltx_sup\">4</sup>SER model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(d)) is more pronounced than that learned by the M<sup class=\"ltx_sup\">4</sup>SER model without the LCL strategy (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(c)), especially with clearer boundaries for sad and angry samples. Additionally, the distances between the four types of emotion in the emotion vector space increase. Although distinguishing MELD samples is more challenging than distinguishing IEMOCAP samples, the overall trend remains similar.\nThis indicates that the proposed M<sup class=\"ltx_sup\">4</sup>SER model effectively leverages both modality-specific and modality-invariant representations to capture high-level shared feature representations across speech and text modalities for emotion recognition.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Representation Analysis.</span>\nWe find that our M<sup class=\"ltx_sup\">4</sup>SER method performs better when using ASR text than when using GT text, as presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T6\" title=\"TABLE VI &#8227; V-F Visualization Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a>. To investigate this finding, we visualize representation weights for two examples from IEMOCAP, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F11\" title=\"Figure 11 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. We display representations of each modality across the temporal level in two learning spaces, accumulating hidden features of <math alttext=\"H_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m2\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">H_{S}</annotation></semantics></math>, <math alttext=\"H_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m3\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">H_{T}</annotation></semantics></math>, <math alttext=\"\\hat{H}_{S}^{spe}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m4\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>H</mi><mo>^</mo></mover><mi>S</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\hat{H}_{S}^{spe}</annotation></semantics></math>, and <math alttext=\"H_{T}^{spe}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m5\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>T</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{T}^{spe}</annotation></semantics></math> into one dimension.\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F11\" title=\"Figure 11 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(a) illustrates that during single-modal learning with ASR text, representation weights in the text modality mainly focus on the word &#8220;oh&#8221;, whereas in the speech modality, representation weights concentrate towards the sentence&#8217;s end. After cross-modal learning, we observed a significant decrease in &#8220;oh&#8221; weight in the text modality. This change occurs as the model detects errors or inaccuracies in ASR transcription using AED and AEC modules, shifting representation weights to other words such as &#8220;that&#8217;s&#8221;. Owing to limited emotional cues in the text, the model relies more on speech features, especially anger-related cues such as intonation and volume, critical for accurate anger recognition. Conversely, with GT text, after cross-modal learning, attention in the text modality focuses on words conveying positive emotions such as &#8220;amusing&#8221;, whereas speech modality distribution remains largely unchanged. Without ASR error signals, the model leans towards these textual features, leading to a bias towards happiness in emotion recognition.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "learning",
                    "attention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Confusion Matrix Analysis.</span> To investigate the impact of different modules on class-wise prediction, we visualize the averaged confusion matrices over 5-folds on IEMOCAP in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F12\" title=\"Figure 12 &#8227; V-F Visualization Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>. Compared with the baseline, introducing MSR and MIR significantly enhances the prediction of &#8220;Happy&#8221; class, which often suffer from ambiguous acoustic-textual alignment.\nBy further incorporating GAN and LCL strategies, we observe overall improvements across most emotion classes, particularly in &#8220;Neutral&#8221;. However, the performance on the &#8220;Happy&#8221; class slightly drops, potentially owing to the acoustic-textual heterogeneity introduced by merging &#8220;Excited&#8221; into &#8220;Happy&#8221;, which possibly reduces the effectiveness of local contrastive learning in capturing class-specific discriminative features.\nFinally, integrating AED and AEC modules yields the most accurate and balanced predictions overall. We observe further improvements in &#8220;Neutral&#8221; and &#8220;Sad&#8221; classes, while the performance of &#8220;Happy&#8221; slightly recovers compared with the previous setting. These results highlight the effectiveness of multi-task learning in mitigating ASR-induced errors and enhancing emotional robustness across modalities.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "learning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the performance of M<sup class=\"ltx_sup\">4</sup>SER in real-world environments, we conduct cross-corpus emotion recognition experiments, which simulate the practical scenario of domain shift between different datasets. Following the experimental settings of previous works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib62\" title=\"\">62</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib63\" title=\"\">63</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib64\" title=\"\">64</a>]</cite>, we adopt a transfer evaluation strategy where one corpus is used entirely for training, and 30% of the target corpus is reserved for validation for parameter tuning, while the remaining 70% is used for final testing.\nIn addition, as the IEMOCAP dataset contains only four emotion classes (&#8220;Neutral&#8221;, &#8220;Happy&#8221;, &#8220;Angry&#8221;, &#8220;Sad&#8221;), we restrict MELD to the same set of overlapping emotions for a fair comparison and discard the rest (&#8220;Suprise&#8221;, &#8220;Fear&#8221;, &#8220;Disgust&#8221;). This ensures consistent label space across corpora during both training and evaluation.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "settings",
                    "datasets",
                    "parameter",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose M<sup class=\"ltx_sup\">4</sup>SER, a novel emotion recognition method that combines multimodal, multirepresentation, multitask, and multistrategy learning. M<sup class=\"ltx_sup\">4</sup>SER leverages an innovative multimodal fusion module to learn modality-specific and modality-invariant representations, capturing unique features of each modality and common features across modalities. We then introduce a modality discriminator to enhance modality diversity through adversarial learning. Additionally, we design two auxiliary tasks, AED and AEC, aimed at enhancing the semantic consistency within the text modality. Finally, we propose a label-based contrastive learning strategy to distinguish different emotional features. Results of experiments on the IEMOCAP and MELD datasets demonstrate that M<sup class=\"ltx_sup\">4</sup>SER surpasses previous baselines, proving its effectiveness. In the future, we plan to extend our approach to the visual modality and introduce disentangled representation learning to further enhance emotion recognition performance. We also plan to investigate whether AED and AEC modules remain necessary when using powerful LLM-based ASR systems.</p>\n\n",
                "matched_terms": [
                    "meld",
                    "iemocap",
                    "learning",
                    "datasets"
                ]
            }
        ]
    },
    "S5.T2": {
        "source_file": "M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition",
        "caption": "TABLE II: Performance Comparison of SOTA Multimodal Models on IEMOCAP (%). “S” and “T” represent Speech and Text modalities, respectively. “ASR” and “GT” denote ASR and ground truth transcripts, respectively. Bold indicates the best result, whereas underline signifies the second-best result.",
        "body": "Method\nYear\nModality\nWA\nUA\n\n\nSAWC [47]\n\n2022\nS+T(ASR)\n76.6\n76.8\n\n\nRMSER-AEA [9]\n\n2023\nS+T(ASR)\n76.4\n76.9\n\n\nSAMS [52]\n\n2023\nS+T(GT)\n76.6\n78.1\n\n\nMGAT [21]\n\n2023\nS+T(GT)\n78.5\n79.3\n\n\nMMER [14]\n\n2023\nS+T(GT)\n78.1\n79.8\n\n\nIMISA [53]\n\n2024\nS+T(GT)\n77.4\n77.9\n\n\nMAF-DCT [54]\n\n2024\nS+T(GT)\n78.5\n79.3\n\n\nFDRL [22]\n\n2024\nS+T(GT)\n78.3\n79.4\n\n\nMF-AED-AEC [13]\n\n2024\nS+T(ASR)\n78.1\n79.3\n\n\nM4SER\n2024\nS+T(ASR)\n79.2\n80.1",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Year</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Modality</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">WA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">UA</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">SAWC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib47\" title=\"\">47</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">2022</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">S+T(ASR)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">76.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">76.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">RMSER-AEA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">2023</td>\n<td class=\"ltx_td ltx_align_center\">S+T(ASR)</td>\n<td class=\"ltx_td ltx_align_center\">76.4</td>\n<td class=\"ltx_td ltx_align_center\">76.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">SAMS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib52\" title=\"\">52</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">2023</td>\n<td class=\"ltx_td ltx_align_center\">S+T(GT)</td>\n<td class=\"ltx_td ltx_align_center\">76.6</td>\n<td class=\"ltx_td ltx_align_center\">78.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MGAT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib21\" title=\"\">21</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">2023</td>\n<td class=\"ltx_td ltx_align_center\">S+T(GT)</td>\n<td class=\"ltx_td ltx_align_center\">78.5</td>\n<td class=\"ltx_td ltx_align_center\">79.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MMER <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib14\" title=\"\">14</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">2023</td>\n<td class=\"ltx_td ltx_align_center\">S+T(GT)</td>\n<td class=\"ltx_td ltx_align_center\">78.1</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_ulem_uline\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">79.8</span></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">IMISA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib53\" title=\"\">53</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">2024</td>\n<td class=\"ltx_td ltx_align_center\">S+T(GT)</td>\n<td class=\"ltx_td ltx_align_center\">77.4</td>\n<td class=\"ltx_td ltx_align_center\">77.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MAF-DCT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib54\" title=\"\">54</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">2024</td>\n<td class=\"ltx_td ltx_align_center\">S+T(GT)</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_ulem_uline\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">78.5</span></span></td>\n<td class=\"ltx_td ltx_align_center\">79.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">FDRL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib22\" title=\"\">22</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">2024</td>\n<td class=\"ltx_td ltx_align_center\">S+T(GT)</td>\n<td class=\"ltx_td ltx_align_center\">78.3</td>\n<td class=\"ltx_td ltx_align_center\">79.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MF-AED-AEC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib13\" title=\"\">13</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">2024</td>\n<td class=\"ltx_td ltx_align_center\">S+T(ASR)</td>\n<td class=\"ltx_td ltx_align_center\">78.1</td>\n<td class=\"ltx_td ltx_align_center\">79.3</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#EFEFEF;\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">M<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">4</span></sup>SER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">2024</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">S+T(ASR)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">79.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">80.1</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "respectively",
            "stgt",
            "speech",
            "whereas",
            "“gt”",
            "secondbest",
            "underline",
            "mfaedaec",
            "text",
            "transcripts",
            "signifies",
            "best",
            "imisa",
            "asr",
            "“asr”",
            "sawc",
            "multimodal",
            "truth",
            "bold",
            "mafdct",
            "indicates",
            "modality",
            "method",
            "performance",
            "year",
            "iemocap",
            "“s”",
            "comparison",
            "mmer",
            "sams",
            "sota",
            "“t”",
            "m4ser",
            "modalities",
            "rmseraea",
            "mgat",
            "stasr",
            "fdrl",
            "result",
            "represent",
            "ground",
            "models",
            "denote"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T2\" title=\"TABLE II &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> compare the performance of the M<sup class=\"ltx_sup\">4</sup>SER method with those of recent multimodal SER methods on the IEMOCAP and MELD datasets. Our proposed M<sup class=\"ltx_sup\">4</sup>SER achieves a WA of 79.2% and a UA of 80.1% on IEMOCAP, and an ACC of 66.5% and a W-F1 of 66.0% on MELD, outperforming other models and demonstrating the superiority of M<sup class=\"ltx_sup\">4</sup>SER. Specifically, on the IEMOCAP dataset, compared with the recent MAF-DCT, the M<sup class=\"ltx_sup\">4</sup>SER method shows a 0.7% improvement in WA and a 0.8% improvement in UA, reaching a new SOTA performance. These improvements stem from our M<sup class=\"ltx_sup\">4</sup>SER&#8217;s capability to mitigate the impact of ASR errors, capture modality-specific and modality-invariant representations across different modalities, enhance commonality between modalities through adversarial learning, and excel in emotion feature learning with the LCL loss.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Multimodal speech emotion recognition (SER) has emerged as pivotal for improving human&#8211;machine interaction. Researchers are increasingly leveraging both speech and textual information obtained through automatic speech recognition (ASR) to comprehensively recognize emotional states from speakers. Although this approach reduces reliance on human-annotated text data, ASR errors possibly degrade emotion recognition performance.\nTo address this challenge, in our previous work, we introduced two auxiliary tasks, namely, ASR error detection and ASR error correction, and we proposed a novel multimodal fusion (MF) method for learning modality-specific and modality-invariant representations across different modalities.\nBuilding on this foundation, in this paper, we introduce two additional training strategies. First, we propose an adversarial network to enhance the diversity of modality-specific representations. Second, we introduce a label-based contrastive learning strategy to better capture emotional features.\nWe refer to our proposed method as M<sup class=\"ltx_sup\">4</sup>SER and validate its superiority over state-of-the-art methods through extensive experiments using IEMOCAP and MELD datasets.</p>\n\n",
                "matched_terms": [
                    "text",
                    "method",
                    "performance",
                    "iemocap",
                    "speech",
                    "asr",
                    "multimodal",
                    "m4ser",
                    "modalities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal speech emotion recognition (SER) serves as a cornerstone in intelligent human&#8211;computer interaction systems, playing a crucial role in fields such as healthcare and intelligent customer service <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib2\" title=\"\">2</a>]</cite>. By analyzing physiological signals such as speech, language, facial expressions, and gestures, multimodal SER can identify and understand human emotional states, making it a key technology of widespread interest among researchers nowadays.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the rapid advancement of deep learning, SER has evolved into end-to-end (E2E) systems capable of directly processing speech signals and outputting emotion classification results.\nIn recent years, the success of self-supervised learning (SSL) has led to the development of a series of speech-based pretrained models such as wav2vec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib3\" title=\"\">3</a>]</cite>, HuBERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib4\" title=\"\">4</a>]</cite>, and WavLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib5\" title=\"\">5</a>]</cite>, achieving the current state-of-the-art (SOTA) performance in SER tasks.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech",
                    "sota",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent research has shown that single-modal approaches struggle to meet the increasing demands for SER owing to their inherent complexity. As a solution, multimodal information has emerged as a viable option because it offers diverse emotional cues. A common strategy involves integrating speech and text modalities to achieve multimodal SER. Text data can be obtained through manual transcription or automatic speech recognition (ASR), which have been widely adopted in recent studies to reduce reliance on extensive annotated datasets.\nAlthough text-based pretrained models such as BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib6\" title=\"\">6</a>]</cite> and RoBERTa <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib7\" title=\"\">7</a>]</cite> have excelled in multimodal SER tasks, the accuracy of ASR remains equally crucial for achieving precise emotion recognition (ER) results. To mitigate the impact of ASR errors, Santoso et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib8\" title=\"\">8</a>]</cite> have integrated self-attention mechanisms and word-level confidence measurements, particularly reducing the importance of words with high error probabilities, thereby enhancing SER performance. However, this approach heavily depends on the performance of the ASR system, thus limiting its generalizability. Lin and Wang <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> have proposed an auxiliary ASR error detection task aimed at determining the probabilities of erroneous words to adjust the trust levels assigned to each word in the ASR hypothesis, thereby improving multimodal SER performance. However, this method focuses solely on error detection within the ASR hypothesis without directly correcting these errors, indicating that it does not fully enhance the coherence of semantic information.</p>\n\n",
                "matched_terms": [
                    "text",
                    "method",
                    "performance",
                    "models",
                    "speech",
                    "asr",
                    "multimodal",
                    "modalities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the other hand, how to effectively integrate speech and text modalities has become a key focus. Previous methods have included simple feature concatenation, recurrent neural networks (RNNs), convolutional neural networks (CNNs), and cross-modal attention mechanisms <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib1\" title=\"\">1</a>]</cite>. Although these methods have shown some effectiveness, they often face challenges arising from differences in representations across different modalities.\nIn recent multimodal tasks such as sentiment analysis <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib11\" title=\"\">11</a>]</cite>, researchers have proposed learning two distinct types of representation to enhance multimodal learning.\nThe first is a representation tailored to each modality, that is, a modality-specific representation (MSR).\nThe second is modality-invariant representation (MIR), aimed at mapping all modalities of the same utterance into a shared subspace. This representation captures commonalities in multimodal signals as well as the speaker&#8217;s shared motives, which collectively affect the emotional state of the utterance through distribution alignment. By combining these two types of representation, one can obtain a comprehensive view of multimodal data can be provided for downstream tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib12\" title=\"\">12</a>]</cite>.\nHowever, these methods focus on utterance-level representations, which, while effective for short and isolated utterances, often fail to capture fine-grained emotional dynamics, especially in long and complex interactions. Such coarse-grained representations fail to capture subtle temporal cues and modality-specific variations that are crucial for accurate emotion recognition.\nMoreover, mapping entire utterances to a shared or modality-specific space using similarity loss oversimplifies the underlying multimodal relationships and hinders effective cross-modal alignment.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "multimodal",
                    "modalities",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the basis of the above observations, in our previous work <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib13\" title=\"\">13</a>]</cite>, we introduced two auxiliary tasks: ASR error detection (AED) and ASR error correction (AEC). Specifically, we introduced an AED module to identify the positions of ASR errors. Subsequently, we employed an AEC module to correct these errors, enhancing the semantic coherence of ASR hypotheses and reducing the impact of ASR errors on ER tasks.\nAdditionally, we designed a novel multimodal fusion (MF) module for learning frame-level MSRs and MIRs in a shared speech&#8211;text modality space.\nWe observed the following shortcomings in our previous work: 1) Despite introducing the AED and AEC modules, uncorrected ASR errors continue to affect the accuracy of SER. 2) Existing MF methods possibly fail to capture subtle differences in emotional features.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "multimodal",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these issues, we propose two learning strategies. First, we introduce adversarial learning to enhance the diversity and robustness of modality representations, thereby reducing the impact of ASR errors on SER tasks. Second, we introduce a contrastive learning strategy based on emotion labels to more accurately distinguish and capture feature differences between different emotion categories, thus improving the performance and robustness of the SER system.\nIn summary, our main contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "asr",
                    "performance",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce an adversarial modality discriminator to enhance the diversity of MSRs and improve the generalization of MIRs. This design alleviates modality mismatch and suppresses modality-specific noise introduced by ASR errors, which often cause misalignment between acoustic and textual features and degrade the robustness of multimodal fusion.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "multimodal",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a label-based contrastive learning strategy to enhance the discriminability of emotion representations across modalities. By encouraging emotion instances of the same category to cluster more tightly and separating those from different categories, this approach addresses the problem of emotion category confusion caused by overlapping feature distributions, especially in complex multimodal scenarios.</p>\n\n",
                "matched_terms": [
                    "modalities",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the basis of our previous work, we present M<sup class=\"ltx_sup\">4</sup>SER, an SER method that leverages multimodal, multirepresentation, multitask, and multistrategy learning. The difference between our M<sup class=\"ltx_sup\">4</sup>SER and previous SER methods is illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S1.F1\" title=\"Figure 1 &#8227; I Introduction &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. M<sup class=\"ltx_sup\">4</sup>SER mitigates the impact of ASR errors, improves modality-invariant representations, and captures commonalities between modalities to bridge their heterogeneity.</p>\n\n",
                "matched_terms": [
                    "method",
                    "asr",
                    "m4ser",
                    "multimodal",
                    "modalities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed M<sup class=\"ltx_sup\">4</sup>SER outperforms state-of-the-arts on the IEMOCAP and MELD datasets. Extensive experiments demonstrate its superiority in SER tasks.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remaining sections of this paper are organized as follows: In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2\" title=\"II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">II</span></a>, we review related work. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3\" title=\"III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>, we discuss the specific model and method of M<sup class=\"ltx_sup\">4</sup>SER. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S4\" title=\"IV Experimental Setup &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>, we outline the experimental setup used in this study, including datasets, evaluation metrics, and implementation details. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5\" title=\"V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>, we verify the effectiveness of the proposed model. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S6\" title=\"VI Conclusion &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a>, we present our conclusion and future work.</p>\n\n",
                "matched_terms": [
                    "method",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SER aims to identify the speaker&#8217;s emotional state from spoken utterances. Early SER models primarily rely on audio signals, extracting prosodic and acoustic features such as Mel-frequency cepstral coefficients, filter banks, or other handcrafted descriptors&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib14\" title=\"\">14</a>]</cite>. With the advent of deep learning, models based on RNNs, CNNs and Transformer architectures&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib18\" title=\"\">18</a>]</cite> have achieved significant improvements by modeling complex temporal and hierarchical patterns in speech. More recently, the rise of SSL has led to the development of speech-based pretrained models such as wav2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib3\" title=\"\">3</a>]</cite>, HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib4\" title=\"\">4</a>]</cite>, and WavLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib5\" title=\"\">5</a>]</cite>, which provide rich contextual representations and deliver SOTA performance on various SER benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib19\" title=\"\">19</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech",
                    "sota",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these limitations, multimodal SER often integrates both speech and text modalities, capturing both acoustic and semantic cues. A common setting involves using speech signals alongside textual transcripts (typically from manual annotations or ASR outputs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib24\" title=\"\">24</a>]</cite>.\nFan et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib21\" title=\"\">21</a>]</cite> proposed multi-granularity attention-based transformers (MGAT) to address emotional asynchrony and modality misalignment.\nSun et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib22\" title=\"\">22</a>]</cite> introduced a method integrating shared and private encoders, projecting each modality into a separate subspace to capture modality consistency and diversity while ensuring label consistency in the output.\nThese methods typically assume access to high-quality manual transcriptions for the text modality.</p>\n\n",
                "matched_terms": [
                    "text",
                    "transcripts",
                    "method",
                    "speech",
                    "asr",
                    "multimodal",
                    "mgat",
                    "modalities",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, growing efforts have focused on reducing the dependency on annotated text by directly leveraging ASR hypotheses as input <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib28\" title=\"\">28</a>]</cite>. Santoso et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib25\" title=\"\">25</a>]</cite> introduced confidence-aware self-attention mechanisms that downweight unreliable ASR tokens to mitigate error propagation. Lin and Wang&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> proposed a robust multimodal SER framework that adaptively fuses attention-weighted acoustic representations with ASR-derived text embeddings, compensating for recognition noise in the transcript.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Learning modality-specific representations (MSRs) and modality-invariant representations (MIRs) has proven effective for multimodal SER, as it enables models to capture both shared semantics across modalities and complementary modality-unique cues.\nHazarika et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib10\" title=\"\">10</a>]</cite> proposed MISA, which disentangles each modality into invariant and specific subspaces through factorization, facilitating effective fusion for emotion prediction.\nLiu et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib29\" title=\"\">29</a>]</cite> proposed a modality-robust MER framework that introduces a contrastive learning module to extract MIRs from full-modality inputs, and an imagination module that reconstructs missing modalities based on the MIRs, enhancing robustness under incomplete input conditions.\nTo explicitly reduce distribution gaps and mitigate feature redundancy, Yang et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib11\" title=\"\">11</a>]</cite> proposed FDMER, which extracts both MSRs and MIRs, combined with consistency&#8211;disparity constraints and a modality discriminator to encourage disentangled learning.\nLiu et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib30\" title=\"\">30</a>]</cite> presented NORM&#8209;TR, introducing a noise&#8209;resistant generic feature (NRGF) extractor trained with noise&#8209;aware adversarial objectives, followed by a multimodal transformer that fuses NRGFs with raw modality features for robust representation learning.</p>\n\n",
                "matched_terms": [
                    "models",
                    "modalities",
                    "multimodal",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to factorization-based approaches which explicitly disentangle modality inputs into shared and private spaces, our M<sup class=\"ltx_sup\">4</sup>SER framework adopts a structured encoding&#8211;generation strategy to implicitly model MSRs and MIRs. Specifically, we utilize a stack of cross-modal encoder blocks to extract token-aware and speech-aware features (MSRs), and then apply a dedicated generator composed of hybrid-modal attention and masking mechanisms to derive the MIRs. In addition, unlike prior works that incorporate adversarial learning at a global or representation-level, our M<sup class=\"ltx_sup\">4</sup>SER employs a frame-level discriminator that operates on each temporal representation. A frame-level discriminator attempts to distinguish the modality origin of each temporal representation, while the MIR generator is optimized to deceive the discriminator by producing modality-agnostic outputs that are hard to classify as either text or speech. This fine-grained, per-frame adversarial constraint leads to more robust and aligned cross-modal representations. Moreover, M<sup class=\"ltx_sup\">4</sup>SER integrates this disentanglement framework into a multitask learning setup with ASR-aware auxiliary supervision, enabling more effective adaptation to noisy real-world speech&#8211;text scenarios.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "m4ser",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ASR error correction (AEC) techniques remain effective in optimizing the ASR hypotheses. AEC has been jointly trained with ER tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib13\" title=\"\">13</a>]</cite> in multitask settings. Li et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib27\" title=\"\">27</a>]</cite> proposed an AEC method that improves transcription quality on low-resource out-of-domain data through cross-modal training and the incorporation of discrete speech units, and they validated its effectiveness in downstream ER tasks. Lin and Wang <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> introduced a robust ER method that leverages complementary semantic information from audio, adapts to ASR errors through an auxiliary task for ASR error detection, and fuses text and acoustic representations for ER.\nOn the basis of <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib32\" title=\"\">32</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib33\" title=\"\">33</a>]</cite>, we propose a partially autoregressive AEC method that uses a label predictor to restrict decoding to only desirable parts of the input sequence embeddings, thereby reducing inference latency.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "method",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The concept of adversarial network was first introduced using GAN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib34\" title=\"\">34</a>]</cite>, which rapidly attracted extensive research interest owing to their strong capability to generate high-quality novel samples on the basis of existing data.\nAs research progressed, the applications of GAN expanded to multimodal tasks.\nIn a multimodal SER task, Ren et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib35\" title=\"\">35</a>]</cite> combined speaker characteristics with single-modal features using an adversarial module to capture both the commonality and diversity of single-modal features, and they finally fused different modalities to generate refined utterance representations for emotion classification.</p>\n\n",
                "matched_terms": [
                    "modalities",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed M<sup class=\"ltx_sup\">4</sup>SER framework can be formalized as the function <math alttext=\"f(S,T)=(L,Z)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mi>L</mi><mo>,</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f(S,T)=(L,Z)</annotation></semantics></math>. Here, the speech modality <math alttext=\"S=(s_{1},s_{2},\\cdots,s_{m})\\in\\mathbb{R}^{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>s</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>s</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>m</mi></msup></mrow><annotation encoding=\"application/x-tex\">S=(s_{1},s_{2},\\cdots,s_{m})\\in\\mathbb{R}^{m}</annotation></semantics></math> consists of <span class=\"ltx_text ltx_font_italic\">m</span> frames extracted from an utterance, whereas the text modality <math alttext=\"T=(t_{1},t_{2},\\cdots,t_{n})\\in\\mathbb{R}^{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><msub><mi>t</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>t</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>n</mi></msup></mrow><annotation encoding=\"application/x-tex\">T=(t_{1},t_{2},\\cdots,t_{n})\\in\\mathbb{R}^{n}</annotation></semantics></math> comprises <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> tokens from the original ASR hypotheses of the utterance. All tokens are mapped to a predefined WordPiece vocabulary <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib40\" title=\"\">40</a>]</cite>. Additionally, within our multitask learning framework, the primary task is ER, yielding an emotion classification output <math alttext=\"L\\in\\{l_{1},l_{2},\\cdots,l_{e}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>l</mi><mn>1</mn></msub><mo>,</mo><msub><mi>l</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>l</mi><mi>e</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">L\\in\\{l_{1},l_{2},\\cdots,l_{e}\\}</annotation></semantics></math>, where <math alttext=\"e\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mi>e</mi><annotation encoding=\"application/x-tex\">e</annotation></semantics></math> represents the number of emotional categories. Concurrently, the auxiliary tasks include ASR error detection (AED) and ASR error correction (AEC). The output of these auxiliary tasks is <math alttext=\"Z\\in\\{Z_{1},Z_{2},\\cdots,Z_{k}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>Z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>Z</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>Z</mi><mi>k</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Z\\in\\{Z_{1},Z_{2},\\cdots,Z_{k}\\}</annotation></semantics></math>, representing all the ground truth (GT) sequences where the ASR transcriptions differ from the human-annotated transcriptions.\n<math alttext=\"Z_{k}=(z_{1,k},z_{2,k},\\cdots,z_{c,k})\\in\\mathbb{R}^{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><msub><mi>Z</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mn>1</mn><mo>,</mo><mi>k</mi></mrow></msub><mo>,</mo><msub><mi>z</mi><mrow><mn>2</mn><mo>,</mo><mi>k</mi></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>z</mi><mrow><mi>c</mi><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>c</mi></msup></mrow><annotation encoding=\"application/x-tex\">Z_{k}=(z_{1,k},z_{2,k},\\cdots,z_{c,k})\\in\\mathbb{R}^{c}</annotation></semantics></math>, denoting that the <math alttext=\"k^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\"><semantics><msup><mi>k</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">k^{th}</annotation></semantics></math> error position includes <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m10\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> tokens.</p>\n\n",
                "matched_terms": [
                    "text",
                    "ground",
                    "whereas",
                    "speech",
                    "asr",
                    "truth",
                    "m4ser",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contextual Speech Representations.</span> To obtain comprehensive contextual representations of acoustic features, we utilize a pretrained SSL model, HuBERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib4\" title=\"\">4</a>]</cite>, as our acoustic encoder. HuBERT integrates CNN layers with a transformer encoder to effectively capture both speech features and contextual information.\nWe denote the acoustic hidden representations of the speech modality inputs <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> generated by HuBERT as <math alttext=\"H_{S}=(h_{S}^{(1)},h_{S}^{(2)},\\cdots,h_{S}^{(m)})\\in\\mathbb{R}^{m\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>H</mi><mi>S</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msubsup><mi>h</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">H_{S}=(h_{S}^{(1)},h_{S}^{(2)},\\cdots,h_{S}^{(m)})\\in\\mathbb{R}^{m\\times d}</annotation></semantics></math>, where <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> is the dimension of hidden representations.</p>\n\n",
                "matched_terms": [
                    "denote",
                    "speech",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contextual Token Representations.</span> We use the pretrained language model BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib6\" title=\"\">6</a>]</cite> as our text encoder to obtain the token representations <math alttext=\"H_{T}=(h_{T}^{(1)},h_{T}^{(2)},\\cdots,h_{T}^{(n)})\\in\\mathbb{R}^{n\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>H</mi><mi>T</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msubsup><mi>h</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">H_{T}=(h_{T}^{(1)},h_{T}^{(2)},\\cdots,h_{T}^{(n)})\\in\\mathbb{R}^{n\\times d}</annotation></semantics></math> for the text modality inputs <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"{\\rm TE}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><mi>TE</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\rm TE}(\\cdot)</annotation></semantics></math> and <math alttext=\"{\\rm PE}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><mrow><mi>PE</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\rm PE}(\\cdot)</annotation></semantics></math> denote the token and position embeddings, respectively.</p>\n\n",
                "matched_terms": [
                    "denote",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first subtask we introduce is AED, which is designed to detect the positions of ASR errors.\nSimilarly to <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib31\" title=\"\">31</a>]</cite>, we align <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> and <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> by determining the longest common subsequence between them. The aligned tokens are labeled <span class=\"ltx_text ltx_font_italic\">KEEP</span> (<span class=\"ltx_text ltx_font_bold\">K</span>), whereas the remaining tokens are labeled <span class=\"ltx_text ltx_font_italic\">DELETE</span> (<span class=\"ltx_text ltx_font_bold\">D</span>) or <span class=\"ltx_text ltx_font_italic\">CHANGE</span> (<span class=\"ltx_text ltx_font_bold\">C</span>). A specific example is illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(b).\nThe label prediction layer is a straightforward fully connected (FC) layer with three classes.</p>\n\n",
                "matched_terms": [
                    "whereas",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More specifically, once the erroneous positions are identified, the decoder constructs a separate generation sequence for each token labeled as <span class=\"ltx_text ltx_font_bold\">C</span>. Each sequence begins with a special start token <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_italic\">BOS<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span> and generates the corrected text in an autoregressive manner.\nAt each decoding step, the decoder input for a given correction sequence consists of all previously generated tokens, each concatenated with the representation of the corresponding erroneous token.\nAll correction sequences are processed in parallel by a shared transformer decoder, which attends to the full ASR representation <math alttext=\"H_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m3\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">H_{T}</annotation></semantics></math> as memory.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> denotes either the speech or text modality and <math alttext=\"H_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m2\" intent=\":literal\"><semantics><msub><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">H_{ST}</annotation></semantics></math> represents the concatenation of <math alttext=\"H_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m3\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">H_{S}</annotation></semantics></math> and <math alttext=\"H_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m4\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">H_{T}</annotation></semantics></math>, which is then fed into a FC layer to map it back to the same dimension as <math alttext=\"H_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m5\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">H_{S}</annotation></semantics></math>. This process integrates both speech and text information into a unified representation. The resulting features are subsequently summed with <math alttext=\"H_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m6\" intent=\":literal\"><semantics><msub><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">H_{ST}</annotation></semantics></math>, culminating in the ultimate modality-invariant representation:</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where &#8220;<math alttext=\"\\sigma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p5.m1\" intent=\":literal\"><semantics><mi>&#963;</mi><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math>&#8221; denotes Sigmoid activation and &#8220;<math alttext=\"\\otimes\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p5.m2\" intent=\":literal\"><semantics><mo>&#8855;</mo><annotation encoding=\"application/x-tex\">\\otimes</annotation></semantics></math>&#8221; indicates element-wise multiplication.\nFinally, the modality-specific and modality-invariant representations are concatenated together to obtain the final multimodal fusion representation <math alttext=\"H_{ST}^{\\rm(fus)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p5.m3\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>fus</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{ST}^{\\rm(fus)}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "indicates",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further develop the modality discriminator <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m1\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> utilizing the modality-invariant representations produced by the MIR generator. This discriminator illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(f) is composed of two linear layers with a ReLU activation function in between, followed by a Sigmoid activation function. To enhance the MIR&#8217;s capability to remain modality-agnostic, we employ adversarial learning techniques.\nSpecifically, the discriminator <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> outputs a scalar value between 0 and 1 for each frame, where values close to 0 indicate the text modality, values close to 1 indicate the speech modality, and values near 0.5 represent an ambiguous modality.\nHere, <math alttext=\"D(H)\\in\\mathbb{R}^{m\\times 1},H\\in\\{H^{(spe)}_{T},H^{(spe)}_{S},H_{ST}^{\\rm(inv)}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>D</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow></msup></mrow><mo>,</mo><mrow><mi>H</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>inv</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">D(H)\\in\\mathbb{R}^{m\\times 1},H\\in\\{H^{(spe)}_{T},H^{(spe)}_{S},H_{ST}^{\\rm(inv)}\\}</annotation></semantics></math>.\nOn one hand, we aim for the discriminator that correctly classifies frames in the modality-specific representations <math alttext=\"H^{(spe)}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H^{(spe)}_{T}</annotation></semantics></math> and <math alttext=\"H^{(spe)}_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m5\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H^{(spe)}_{S}</annotation></semantics></math> as 0 and 1, respectively. On the other hand, to enhance the modality agnosticism of the modality-invariant representation <math alttext=\"H_{ST}^{\\rm(inv)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m6\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>inv</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{ST}^{\\rm(inv)}</annotation></semantics></math> produced by the MIR generator, we aim for a discriminator that outputs values close to 0.5, indicating an ambiguous modality. With this design of the MIR generator and discriminator, we define a generative adversarial training objective, mathematically represented as</p>\n\n",
                "matched_terms": [
                    "text",
                    "respectively",
                    "represent",
                    "speech",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance the model&#8217;s capability to learn emotion features from multimodal data, we employ a label-based contrastive learning task to complement the cross-entropy loss, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(g). This task aids the model in extracting emotion-related features when the MF module integrates speech and text data. As depicted in the LCL task in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.F5\" title=\"Figure 5 &#8227; III-H Label-based Contrastive Learning (LCL) &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we categorize data in each batch into positive and negative samples on the basis of emotion labels. For instance, in a batch containing eight samples, we compute the set of positive samples for each sample, where those with the same label are considered positive samples (yellow squares), and those with different labels are considered negative samples (white squares). We then calculate the label-based contrastive loss (LCL) using Eq. <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:eq:lcl</span>, which promotes instances of a specific label to be more similar to each other than instances of other labels.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\langle\\cdot,\\cdot\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo rspace=\"0em\">,</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\cdot,\\cdot\\rangle</annotation></semantics></math> denotes cosine similarity and <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m2\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> is the temperature parameter. For the multimodal representations, let <math alttext=\"(H_{ST}^{\\rm(ap)})_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m3\" intent=\":literal\"><semantics><msub><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>ap</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mi>i</mi></msub><annotation encoding=\"application/x-tex\">(H_{ST}^{\\rm(ap)})_{i}</annotation></semantics></math>, <math alttext=\"(H_{ST}^{\\rm(ap)})_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m4\" intent=\":literal\"><semantics><msub><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>ap</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mi>p</mi></msub><annotation encoding=\"application/x-tex\">(H_{ST}^{\\rm(ap)})_{p}</annotation></semantics></math>, and <math alttext=\"(H_{ST}^{\\rm(ap)})_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m5\" intent=\":literal\"><semantics><msub><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>ap</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mi>a</mi></msub><annotation encoding=\"application/x-tex\">(H_{ST}^{\\rm(ap)})_{a}</annotation></semantics></math> represent the <math alttext=\"i^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m6\" intent=\":literal\"><semantics><msup><mi>i</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">i^{th}</annotation></semantics></math> sample, the <math alttext=\"p^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m7\" intent=\":literal\"><semantics><msup><mi>p</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">p^{th}</annotation></semantics></math> positive sample, and the <math alttext=\"a^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m8\" intent=\":literal\"><semantics><msup><mi>a</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">a^{th}</annotation></semantics></math> sample, respectively. We define <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m9\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math> as the index set of samples, <math alttext=\"P(i)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m10\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(i)</annotation></semantics></math> as the set of positive samples for the <math alttext=\"i^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m11\" intent=\":literal\"><semantics><msup><mi>i</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">i^{th}</annotation></semantics></math> sample, and <math alttext=\"A(i)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m12\" intent=\":literal\"><semantics><mrow><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">A(i)</annotation></semantics></math> as the set of all samples.</p>\n\n",
                "matched_terms": [
                    "represent",
                    "multimodal",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the GAN training strategy <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib34\" title=\"\">34</a>]</cite>, we divide the backpropagation process into two steps. Firstly, we maximize <math alttext=\"\\mathcal{L}_{\\rm GAN}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>GAN</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm GAN}</annotation></semantics></math> to update the discriminator, during which the generator is detached from the optimization. According to Eq. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.Ex1\" title=\"III-G Modality Discriminator &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III-G</span></a>), on one hand, maximizing the first term <math alttext=\"L_{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>D</mi></msub><annotation encoding=\"application/x-tex\">L_{D}</annotation></semantics></math> of <math alttext=\"\\mathcal{L}_{\\rm GAN}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>GAN</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm GAN}</annotation></semantics></math> essentially trains the discriminator to correctly distinguish between text and audio features by making <math alttext=\"H_{S}^{\\rm(spe)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mi>spe</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{S}^{\\rm(spe)}</annotation></semantics></math> close to 1 and <math alttext=\"H_{T}^{\\rm(spe)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mi>spe</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{T}^{\\rm(spe)}</annotation></semantics></math> close to 0 <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Function <math alttext=\"\\log(x)+\\log(1-y)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>&#8722;</mo><mi>y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(x)+\\log(1-y)</annotation></semantics></math>, where <math alttext=\"x,y\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x,y\\in(0,1)</annotation></semantics></math> reaches its maximum when <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m3\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> approaches 1 and <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m4\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math> approaches 0.</span></span></span>. On the other hand, maximizing the second term <math alttext=\"L_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m6\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>G</mi></msub><annotation encoding=\"application/x-tex\">L_{G}</annotation></semantics></math> (i.e., minimizing <math alttext=\"-L_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m7\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><msub><mi>L</mi><mi>G</mi></msub></mrow><annotation encoding=\"application/x-tex\">-L_{G}</annotation></semantics></math>) will make <math alttext=\"H^{\\text{(inv)}}_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mtext>(inv)</mtext></msubsup><annotation encoding=\"application/x-tex\">H^{\\text{(inv)}}_{ST}</annotation></semantics></math> approach 0 or 1 <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Function <math alttext=\"\\log(x)+\\log(1-x)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>&#8722;</mo><mi>x</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(x)+\\log(1-x)</annotation></semantics></math>, where <math alttext=\"x\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m2\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x\\in(0,1)</annotation></semantics></math> reaches its maximum at <math alttext=\"x=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m3\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">x=0.5</annotation></semantics></math> and its minimum near <math alttext=\"x=0\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m4\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">x=0</annotation></semantics></math> and <math alttext=\"x=1\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m5\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">x=1</annotation></semantics></math>.</span></span></span>, indicating that the discriminator recognizes <math alttext=\"H^{\\text{(inv)}}_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m9\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mtext>(inv)</mtext></msubsup><annotation encoding=\"application/x-tex\">H^{\\text{(inv)}}_{ST}</annotation></semantics></math> as modality-specific, which can be either text or audio, contrary to our desired modality invariance. Secondly, we freeze the discriminator parameters and update the remaining parameters by minimizing <math alttext=\"L_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m10\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>G</mi></msub><annotation encoding=\"application/x-tex\">L_{G}</annotation></semantics></math>, which makes the output of the discriminator <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m11\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> for the modality-invariant features <math alttext=\"H^{\\text{(inv)}}_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m12\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mtext>(inv)</mtext></msubsup><annotation encoding=\"application/x-tex\">H^{\\text{(inv)}}_{ST}</annotation></semantics></math> approach 0.5, thereby blurring these features between audio and text modalities to achieve modality agnosticism. Additionally, <math alttext=\"\\mathcal{L}_{\\text{ER}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m13\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ER</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ER}}</annotation></semantics></math> optimizes the downstream emotion recognition model, <math alttext=\"\\mathcal{L}_{\\text{AED}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m14\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>AED</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{AED}}</annotation></semantics></math> and <math alttext=\"\\mathcal{L}_{\\text{AEC}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m15\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>AEC</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{AEC}}</annotation></semantics></math> optimize the auxiliary tasks of ASR error detection and ASR error correction models, respectively, and <math alttext=\"\\mathcal{L}_{\\text{LCL}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m16\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>LCL</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{LCL}}</annotation></semantics></math> implements the LCL strategy. The entire system is trained in an end-to-end manner and optimized by adjusting weight parameters.</p>\n\n",
                "matched_terms": [
                    "text",
                    "respectively",
                    "models",
                    "asr",
                    "modalities",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the inference stage, the AED and AEC modules are excluded. The remaining modules accept speech and ASR transcripts as input and output emotion classification results.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "speech",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the effectiveness\nof our proposed model, we carried out experiments on two public datasets:\nthe Interactive Emotional Dyadic Motion Capture (IEMOCAP) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib45\" title=\"\">45</a>]</cite> and the Multimodal Emotion Lines Dataset (MELD) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib46\" title=\"\">46</a>]</cite>. The statistics of the two datasets are shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S4.F6\" title=\"Figure 6 &#8227; IV-A Dataset and Evaluation Metrics &#8227; IV Experimental Setup &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">IEMOCAP</span> comprised roughly 12 hours of speech from ten speakers participating in five scripted sessions. Following prior work <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib47\" title=\"\">47</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> on IEMOCAP, we employed 5531 utterances that were categorized into four emotion categories: &#8220;Neutral&#8221; (1,708), &#8220;Angry&#8221; (1,103), &#8220;Happy&#8221; (including &#8220;Excited&#8221;) (595 + 1,041 = 1,636), and &#8220;Sad&#8221; (1,084).\nWe performed five-fold leave-one-session-out (LOSO) cross-validation to evaluate the emotion classification performance using weighted accuracy (WA) and unweighted accuracy (UA).</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The acoustic encoder was initialized using the <span class=\"ltx_text ltx_font_typewriter\">hubert-base-ls960<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">3</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/facebook/hubert-base-ls960\" title=\"\">https://huggingface.co/facebook/hubert-base-ls960</a></span></span></span></span> model, producing acoustic representations <math alttext=\"d_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">d_{S}</annotation></semantics></math> with a dimensionality of 768.\nThe text encoder employed the <span class=\"ltx_text ltx_font_typewriter\">bert-base-uncased<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">4</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/google-bert/bert-base-uncased\" title=\"\">https://huggingface.co/google-bert/bert-base-uncased</a></span></span></span></span> model for initialization, yielding token representations <math alttext=\"d_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">d_{T}</annotation></semantics></math> with a dimensionality of 768. The vocabulary size for word tokenization <math alttext=\"d_{vocab}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>d</mi><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">d_{vocab}</annotation></semantics></math> was set to 30,522. We set the hidden size <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> to 768, the number of attention layers to 12, and the number of attention heads to 12. Both the HuBERT and BERT models were fine-tuned during the training stage.\nThe transformer decoder in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c) adopted a single-layer transformer with a hidden size of 768. Additionally, we obtained corresponding ASR hypotheses using the Whisper ASR model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib48\" title=\"\">48</a>]</cite>. We used the <span class=\"ltx_text ltx_font_typewriter\">openai/whisper-medium.en<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">5</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/openai/whisper-medium.en\" title=\"\">https://huggingface.co/openai/whisper-medium.en</a></span></span></span></span> and <span class=\"ltx_text ltx_font_typewriter\">openai/whisper-large-v2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">6</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/openai/whisper-large-v2\" title=\"\">https://huggingface.co/openai/whisper-large-v2</a></span></span></span></span> models, achieving word error rates (WERs) of 20.48% and 37.87% on the IEMOCAP and MELD datasets, respectively.</p>\n\n",
                "matched_terms": [
                    "text",
                    "respectively",
                    "iemocap",
                    "asr",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the experiments, we compared various multimodal speech emotion recognition SOTA methods with our proposed method, M<sup class=\"ltx_sup\">4</sup>SER.\nThe comparison was conducted by the following methods on the IEMOCAP dataset:</p>\n\n",
                "matched_terms": [
                    "method",
                    "iemocap",
                    "speech",
                    "comparison",
                    "multimodal",
                    "sota",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SAWC</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib47\" title=\"\">47</a>]</cite> adjusts importance weights using confidence measures, reducing ASR error impact by emphasizing the corresponding speech segments.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sawc",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RMSER-AEA</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> uses complementary semantic information, adapting to ASR errors through an auxiliary task and fusing text and acoustic representations for SER.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr",
                    "rmseraea"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SAMS</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib52\" title=\"\">52</a>]</cite> leverages high-level emotion representations as supervisory signals to build a multi-spatial learning framework for each modality, enabling cross-modal semantic learning and fusion representation exploration.</p>\n\n",
                "matched_terms": [
                    "sams",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MGAT</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib21\" title=\"\">21</a>]</cite> tackles emotional asynchrony and modality misalignment problems by employing a multi-granularity attention mechanism.</p>\n\n",
                "matched_terms": [
                    "modality",
                    "mgat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MMER</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib14\" title=\"\">14</a>]</cite> employs early fusion and cross-modal self-attention between text and acoustic modalities, and addresses three novel auxiliary tasks to enhance SER. For fairness, we selected the result that did not introduce augmented text for comparison.</p>\n\n",
                "matched_terms": [
                    "text",
                    "result",
                    "comparison",
                    "mmer",
                    "modalities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MAF-DCT</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib54\" title=\"\">54</a>]</cite> introduces a dual approach utilizing SSL representation and spectral features for comprehensive speech feature extraction, along with a dual cross-modal transformer to handle interactions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mafdct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <span class=\"ltx_text ltx_font_bold\">MF-AED-AEC</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib13\" title=\"\">13</a>]</cite>, as in our previous work, we consider two auxiliary tasks to improve semantic coherence in ASR text and introduce a MF method to learn shared representations.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr",
                    "method",
                    "mfaedaec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SAMS and MF-AED-AEC methods are also utilized as baselines for the MELD dataset. Additionally, the following methods were included for comparison:</p>\n\n",
                "matched_terms": [
                    "mfaedaec",
                    "sams",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">HCAM</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib56\" title=\"\">56</a>]</cite> combines wav2vec audio and BERT text inputs with co-attention and recurrent neural networks for multimodal emotion recognition.</p>\n\n",
                "matched_terms": [
                    "text",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed M<sup class=\"ltx_sup\">4</sup>SER model is composed of four types of learning: multimodal learning, multirepresentation learning, multitask learning, and multistrategy learning. To determine the impact of different types of learning on the performance of the emotion recognition task, ablation experiments were conducted on the IEMOCAP and MELD datasets, as presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "iemocap",
                    "m4ser",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multimodalities.</span>\nTo assess the necessity of multimodality, we conduct single-modal comparison experiments involving text and speech modalities. These experiments operate under a baseline framework excluding the MF module, LCL loss, and GAN loss, precluding intermodal interaction.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(A), in the single-modal experiments, the performance of speech modality in the IEMOCAP dataset is better than that of text modality, whereas the reverse is observed in the MELD dataset. This divergence suggests that emotional information is more distinctly conveyed through speech in IEMOCAP, whereas MELD leans towards text for emotional expression.\nMoreover, the multimodal baseline model, which simply concatenates speech and text features for recognition (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> A(3)), significantly enhances performance compared with single-modal baselines, highlighting the essential role of multimodal information.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance",
                    "iemocap",
                    "whereas",
                    "speech",
                    "comparison",
                    "multimodal",
                    "modalities",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multirepresentations.</span> We study the importance of modality-invariant and modality-specific representations by discarding each type. Removing modality-specific representations from multimodal fusion significantly reduces downstream emotion recognition performance on both datasets, confirming their effectiveness in capturing and utilizing unique information from each modality. Similarly, omitting refined modality-invariant representations from multimodal fusion also significantly decreases emotion recognition performance on both datasets, demonstrating their importance in bridging modality gaps. Clearly, removing both representations further decreases performance.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "multimodal",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multitasks.</span> First, we discuss the impact of the AED module. Specifically, we remove the AED module and introduce only the AEC module as an auxiliary task. This requires the model to correct all errors in each utterance from scratch, rather than only the detected errors. Evidently, the absence of the AED module leads to a significant drop in emotion recognition performance, demonstrating the necessity of the AED module. Similarly, we observe that the AEC module also plays an important role. Moreover, we note that the WA results using only the AEC module (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(C)(1)) are even worse than the results with neither the AED nor AEC module (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(C)(3)) in both datasets. This phenomenon occurs because directly using the neural machine translation (NMT) model for AEC can even increase the WER <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib60\" title=\"\">60</a>]</cite>. Unlike NMT tasks, which typically require modifying almost all input tokens, AEC involves fewer but more challenging corrections. For instance, if the ASR model&#8217;s WER is 10%, then only about 10% of the input tokens require correction in the AEC model. However, these tokens are often difficult to recognize and correct because they have already been misrecognized by the ASR model. Therefore, we should consider the characteristics of ASR outputs and carefully design the AEC model, which is why we introduce both AED and AEC as auxiliary tasks.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multistrategies.</span> To validate the effectiveness of the adversarial training strategy outlined in Alg. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#alg1\" title=\"Algorithm 1 &#8227; III-I Joint Training &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we remove the adversarial training strategy from M<sup class=\"ltx_sup\">4</sup>SER. The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(D)(1) show a decrease in performance on both datasets, demonstrating the critical role of this strategy in learning modality-invariant representations. The proposed modality discriminator effectively enhances the modality agnosticism of the refined representations from the generator.\nAdditionally, omitting the LCL strategy described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.SS8\" title=\"III-H Label-based Contrastive Learning (LCL) &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III-H</span></a> also results in similar performance degradation (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(D)(2)). We visualize the results before and after introducing this strategy to demonstrate its effectiveness, with specific analysis also detailed in the following t-SNE analysis section. Moreover, removing both of these strategies further diminishes emotion recognition performance, confirming that both learning strategies contribute significantly to performance improvement.</p>\n\n",
                "matched_terms": [
                    "m4ser",
                    "performance",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:703.2pt;height:86pt;vertical-align:-40.5pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Type</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Modality</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Params (M)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Train Time (h)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Infer Time (ms/U)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">WA (%)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">UA (%)</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_2\">Single-modal</span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">HuBERT</span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">S</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">316.2</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">5.4</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">23.7 &#177; 0.1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">68.9</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt\">69.8</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\">BERT</span>\n<span class=\"ltx_td ltx_align_left ltx_border_r\">T(ASR)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">109.5</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">0.5</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">6.3 &#177; 0.1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">65.1</span>\n<span class=\"ltx_td ltx_align_center\">66.4</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_4\">Multi-modal</span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Baseline (HuBERT + BERT)</span>\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_4\">S+T(ASR)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">426.9</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">9.6</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">29.6 &#177; 0.1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">73.4</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">75.2</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\">&#8194;&#8202;&#8195;+ MSR &amp; MIR</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">481.4</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">12.0</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">44.8 &#177; 0.1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">76.6</span>\n<span class=\"ltx_td ltx_align_center\">77.4</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\">&#8194;&#8202;&#8195;&#8195;+ GAN &amp; LCL</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">481.6</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">20.8</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">44.5 &#177; 0.6</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">77.3</span>\n<span class=\"ltx_td ltx_align_center\">78.6</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">&#8195;&#8195;&#8195;&#8196;&#8202;+ AED &amp; AEC (M<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">4</span></sup>SER)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">512.9</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">21.5</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">44.7 &#177; 0.2</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">79.2</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">80.1</span></span></span>\n</span></span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "stasr",
                    "m4ser",
                    "multimodal",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the model exhibits relative robustness to variations in <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m1\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>, indicating stable synergy between the AED and AEC objectives. Conversely, performance is highly sensitive to <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m2\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>; large values result in unstable training due to adversarial objectives, while a small weight (e.g., <math alttext=\"\\gamma=0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=0.01</annotation></semantics></math>) ensures stable convergence and optimal results.</p>\n\n",
                "matched_terms": [
                    "result",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the trade-off between model performance and computational cost, we report the number of parameters, training time, and inference time for each model on the IEMOCAP dataset in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T5\" title=\"TABLE V &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>, with all experiments conducted on a single NVIDIA Tesla V100 GPU.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Among the single-modal models, HuBERT exhibits higher computational cost than BERT owing to the nature of speech encoders, but also delivers better recognition performance. In the multi-modal setting, each additional module introduces a moderate increase in parameter size and training cost, while consistently improving recognition accuracy.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "models",
                    "speech",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our full model M<sup class=\"ltx_sup\">4</sup>SER achieves the best performance with WA of 79.2% and UA of 80.1%. Although its training time increases to 21.5 hours owing to the inclusion of GAN and LCL strategies and AED and AEC subtasks, these components are used only during training. Consequently, the inference latency of M<sup class=\"ltx_sup\">4</sup>SER remains nearly identical to the baseline with only MSR and MIR. Considering the significant gains in performance, M<sup class=\"ltx_sup\">4</sup>SER demonstrates a favorable trade-off between computational cost and recognition accuracy, making it suitable for practical deployment.</p>\n\n",
                "matched_terms": [
                    "best",
                    "m4ser",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">t-SNE Analysis.</span> To intuitively demonstrate the advantages of our proposed M<sup class=\"ltx_sup\">4</sup>SER model on IEMOCAP and MELD datasets, we utilize the t-distributed stochastic neighbor embedding (t-SNE) tool <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib61\" title=\"\">61</a>]</cite> to visualize the learned emotion features using all samples from session 3 of the IEMOCAP and test set of the MELD. We compare these features across the following models: the speech model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(a)), the text model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(b)), the proposed M<sup class=\"ltx_sup\">4</sup>SER model without label-based contrastive learning (LCL) (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(c)), and the proposed M<sup class=\"ltx_sup\">4</sup>SER model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(d)).</p>\n\n",
                "matched_terms": [
                    "text",
                    "iemocap",
                    "speech",
                    "models",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the visualization results in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, we observe that the emotion label distributions for the two single-modal baselines on the IEMOCAP and MELD datasets show a significant overlap among the emotion categories, indicating that they are often confused with each other. In contrast, the emotion label distributions for the two multimodal models are more distinguishable, demonstrating the effectiveness of multimodal models in capturing richer emotional features.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "models",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, compared with the M<sup class=\"ltx_sup\">4</sup>SER model without the LCL strategy, our M<sup class=\"ltx_sup\">4</sup>SER model achieves better clustering for each emotion category, making them as distinct as possible. For instance, on the IEMOCAP dataset, the intra-class clustering of samples learned by the M<sup class=\"ltx_sup\">4</sup>SER model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(d)) is more pronounced than that learned by the M<sup class=\"ltx_sup\">4</sup>SER model without the LCL strategy (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(c)), especially with clearer boundaries for sad and angry samples. Additionally, the distances between the four types of emotion in the emotion vector space increase. Although distinguishing MELD samples is more challenging than distinguishing IEMOCAP samples, the overall trend remains similar.\nThis indicates that the proposed M<sup class=\"ltx_sup\">4</sup>SER model effectively leverages both modality-specific and modality-invariant representations to capture high-level shared feature representations across speech and text modalities for emotion recognition.</p>\n\n",
                "matched_terms": [
                    "text",
                    "iemocap",
                    "speech",
                    "m4ser",
                    "modalities",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further explore the effectiveness of adversarial learning in M<sup class=\"ltx_sup\">4</sup>SER, we visualize the distribution of modality-invariant and modality-specific representations before and after adversarial learning using the t-SNE tool, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F10\" title=\"Figure 10 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. It can be observed that after adversarial learning, different modality-specific representations become more separated with increasing intraclass clustering. This indicates that with the introduction of adversarial learning, M<sup class=\"ltx_sup\">4</sup>SER enhances the diversity of modality-specific representations, generating superior features for the downstream emotion recognition task.</p>\n\n",
                "matched_terms": [
                    "m4ser",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Representation Analysis.</span>\nWe find that our M<sup class=\"ltx_sup\">4</sup>SER method performs better when using ASR text than when using GT text, as presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T6\" title=\"TABLE VI &#8227; V-F Visualization Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a>. To investigate this finding, we visualize representation weights for two examples from IEMOCAP, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F11\" title=\"Figure 11 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. We display representations of each modality across the temporal level in two learning spaces, accumulating hidden features of <math alttext=\"H_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m2\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">H_{S}</annotation></semantics></math>, <math alttext=\"H_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m3\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">H_{T}</annotation></semantics></math>, <math alttext=\"\\hat{H}_{S}^{spe}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m4\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>H</mi><mo>^</mo></mover><mi>S</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\hat{H}_{S}^{spe}</annotation></semantics></math>, and <math alttext=\"H_{T}^{spe}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m5\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>T</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{T}^{spe}</annotation></semantics></math> into one dimension.\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F11\" title=\"Figure 11 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(a) illustrates that during single-modal learning with ASR text, representation weights in the text modality mainly focus on the word &#8220;oh&#8221;, whereas in the speech modality, representation weights concentrate towards the sentence&#8217;s end. After cross-modal learning, we observed a significant decrease in &#8220;oh&#8221; weight in the text modality. This change occurs as the model detects errors or inaccuracies in ASR transcription using AED and AEC modules, shifting representation weights to other words such as &#8220;that&#8217;s&#8221;. Owing to limited emotional cues in the text, the model relies more on speech features, especially anger-related cues such as intonation and volume, critical for accurate anger recognition. Conversely, with GT text, after cross-modal learning, attention in the text modality focuses on words conveying positive emotions such as &#8220;amusing&#8221;, whereas speech modality distribution remains largely unchanged. Without ASR error signals, the model leans towards these textual features, leading to a bias towards happiness in emotion recognition.</p>\n\n",
                "matched_terms": [
                    "text",
                    "method",
                    "iemocap",
                    "whereas",
                    "speech",
                    "asr",
                    "m4ser",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Similarly, Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F11\" title=\"Figure 11 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(b) shows inconsistency between GT text and speech features. GT text contains words (e.g., &#8220;don&#8217;t know&#8221;) indicating confusion and uncertainty, whereas speech features exhibit significant emotional fluctuations. This mismatch complicates feature alignment in cross-modal learning, resulting in erroneous anger classification. In contrast, ASR text shows higher consistency with speech features, highlighting emotional words (&#8220;freaking out&#8221;, &#8220;what the hell&#8221;, and &#8220;all of a sudden&#8221;) despite errors and simplifications. Representation weights confirm high consistency between ASR text and speech features across multiple regions, enhancing cross-modal learning accuracy and correct emotion identification as &#8220;Happy&#8221; (&#8220;Excited&#8221;). We demonstrate with dynamic time warping (DTW) that ASR transcriptions are more consistent with speech than GT transcriptions, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T7\" title=\"TABLE VII &#8227; V-F Visualization Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VII</span></a>.</p>\n\n",
                "matched_terms": [
                    "whereas",
                    "speech",
                    "asr",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, compared with ASR text, GT text exhibits two primary differences:</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Lack of ASR error signals:</span> GT text lacks ASR errors, missing additional cues beneficial for emotion recognition. This limitation possibly hinders capturing intense emotional signals when relying solely on GT text.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Differences in feature weight distribution:</span> the absence of ASR errors alters feature weight distribution in GT text modality compared with ASR text, potentially leading to inaccurate emotion classification.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Confusion Matrix Analysis.</span> To investigate the impact of different modules on class-wise prediction, we visualize the averaged confusion matrices over 5-folds on IEMOCAP in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F12\" title=\"Figure 12 &#8227; V-F Visualization Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>. Compared with the baseline, introducing MSR and MIR significantly enhances the prediction of &#8220;Happy&#8221; class, which often suffer from ambiguous acoustic-textual alignment.\nBy further incorporating GAN and LCL strategies, we observe overall improvements across most emotion classes, particularly in &#8220;Neutral&#8221;. However, the performance on the &#8220;Happy&#8221; class slightly drops, potentially owing to the acoustic-textual heterogeneity introduced by merging &#8220;Excited&#8221; into &#8220;Happy&#8221;, which possibly reduces the effectiveness of local contrastive learning in capturing class-specific discriminative features.\nFinally, integrating AED and AEC modules yields the most accurate and balanced predictions overall. We observe further improvements in &#8220;Neutral&#8221; and &#8220;Sad&#8221; classes, while the performance of &#8220;Happy&#8221; slightly recovers compared with the previous setting. These results highlight the effectiveness of multi-task learning in mitigating ASR-induced errors and enhancing emotional robustness across modalities.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "modalities",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the performance of M<sup class=\"ltx_sup\">4</sup>SER in real-world environments, we conduct cross-corpus emotion recognition experiments, which simulate the practical scenario of domain shift between different datasets. Following the experimental settings of previous works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib62\" title=\"\">62</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib63\" title=\"\">63</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib64\" title=\"\">64</a>]</cite>, we adopt a transfer evaluation strategy where one corpus is used entirely for training, and 30% of the target corpus is reserved for validation for parameter tuning, while the remaining 70% is used for final testing.\nIn addition, as the IEMOCAP dataset contains only four emotion classes (&#8220;Neutral&#8221;, &#8220;Happy&#8221;, &#8220;Angry&#8221;, &#8220;Sad&#8221;), we restrict MELD to the same set of overlapping emotions for a fair comparison and discard the rest (&#8220;Suprise&#8221;, &#8220;Fear&#8221;, &#8220;Disgust&#8221;). This ensures consistent label space across corpora during both training and evaluation.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "m4ser",
                    "comparison",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T8\" title=\"TABLE VIII &#8227; V-G Cross-corpus Generalization Ability &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VIII</span></a> presents the cross-corpus generalization results. Compared with the reimplemented SAMS and MF-AED-AEC baselines, our full model M<sup class=\"ltx_sup\">4</sup>SER achieves the best performance in both IE<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ME and ME<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>IE directions. Specifically, M<sup class=\"ltx_sup\">4</sup>SER outperforms the strong multimodal baseline by 4.3% in ACC and 3.1% in W-F1 under the IE<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ME setting, and by 5.1% WA and 4.8% UA in the ME<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>IE setting.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "best",
                    "sams",
                    "multimodal",
                    "m4ser",
                    "mfaedaec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further evaluate the adaptability of M<sup class=\"ltx_sup\">4</sup>SER under limited supervision, we conduct few-shot domain adaptation experiments by sampling 5, 10, 20, and 60 labeled instances per class from the target-domain validation set. The remaining validation data is still used for model selection, while the test set remains fixed across all settings. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T9\" title=\"TABLE IX &#8227; V-G Cross-corpus Generalization Ability &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IX</span></a>, M<sup class=\"ltx_sup\">4</sup>SER consistently outperforms SAMS and MF-AED-AEC across all shot settings. For example, with only 5 labeled samples per class, M<sup class=\"ltx_sup\">4</sup>SER achieves ACC of 57.9% in the IE<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ME setting, already surpassing MF-AED-AEC with 20 shots. As the number of target samples increases, our model scales well and reaches ACC of 62.1% and W-F1 61.5% at 60-shot. In the ME<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>IE setting, M<sup class=\"ltx_sup\">4</sup>SER reaches UA of 75.9%, demonstrating excellent cross-domain adaptability.</p>\n\n",
                "matched_terms": [
                    "sams",
                    "mfaedaec",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose M<sup class=\"ltx_sup\">4</sup>SER, a novel emotion recognition method that combines multimodal, multirepresentation, multitask, and multistrategy learning. M<sup class=\"ltx_sup\">4</sup>SER leverages an innovative multimodal fusion module to learn modality-specific and modality-invariant representations, capturing unique features of each modality and common features across modalities. We then introduce a modality discriminator to enhance modality diversity through adversarial learning. Additionally, we design two auxiliary tasks, AED and AEC, aimed at enhancing the semantic consistency within the text modality. Finally, we propose a label-based contrastive learning strategy to distinguish different emotional features. Results of experiments on the IEMOCAP and MELD datasets demonstrate that M<sup class=\"ltx_sup\">4</sup>SER surpasses previous baselines, proving its effectiveness. In the future, we plan to extend our approach to the visual modality and introduce disentangled representation learning to further enhance emotion recognition performance. We also plan to investigate whether AED and AEC modules remain necessary when using powerful LLM-based ASR systems.</p>\n\n",
                "matched_terms": [
                    "text",
                    "method",
                    "performance",
                    "iemocap",
                    "asr",
                    "multimodal",
                    "m4ser",
                    "modalities",
                    "modality"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition",
        "caption": "TABLE III: Performance Comparison of SOTA Multimodal Models on MELD (%).\n“V” represents Visual modality.\n∗ indicates the result of training with GT texts and testing with ASR texts. We reimplement the method indicated by ∘ on MELD and obtain the corresponding result.",
        "body": "Method\nYear\nModality\nACC\nW-F1\n\n\nFull [55]\n\n2022\nS+T(ASR)\n-\n61.4\n\n\nHCAM∗ [56]\n\n2023\nS+T(ASR)\n-\n50.2\n\n\nDIMMN [57]\n\n2023\nS+T(GT)+V\n60.6\n58.0\n\n\nMER-HAN [58]\n\n2023\nS+T(GT)\n62.9\n60.2\n\n\nMLCCT [59]\n\n2023\nS+T(GT)\n63.2\n62.4\n\n\nSAMS [52]\n\n2023\nS+T(GT)\n65.4\n62.6\n\n\nMF-AED-AEC∘ [13]\n\n2024\nS+T(ASR)\n65.5\n64.1\n\n\nM4SER\n2024\nS+T(ASR)\n66.5\n66.0",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Year</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Modality</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">ACC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">W-F1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Full <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib55\" title=\"\">55</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">2022</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">S+T(ASR)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">61.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">HCAM<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib56\" title=\"\">56</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">2023</td>\n<td class=\"ltx_td ltx_align_center\">S+T(ASR)</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">50.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">DIMMN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib57\" title=\"\">57</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">2023</td>\n<td class=\"ltx_td ltx_align_center\">S+T(GT)+V</td>\n<td class=\"ltx_td ltx_align_center\">60.6</td>\n<td class=\"ltx_td ltx_align_center\">58.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MER-HAN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib58\" title=\"\">58</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">2023</td>\n<td class=\"ltx_td ltx_align_center\">S+T(GT)</td>\n<td class=\"ltx_td ltx_align_center\">62.9</td>\n<td class=\"ltx_td ltx_align_center\">60.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MLCCT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib59\" title=\"\">59</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">2023</td>\n<td class=\"ltx_td ltx_align_center\">S+T(GT)</td>\n<td class=\"ltx_td ltx_align_center\">63.2</td>\n<td class=\"ltx_td ltx_align_center\">62.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">SAMS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib52\" title=\"\">52</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">2023</td>\n<td class=\"ltx_td ltx_align_center\">S+T(GT)</td>\n<td class=\"ltx_td ltx_align_center\">65.4</td>\n<td class=\"ltx_td ltx_align_center\">62.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MF-AED-AEC<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8728;</span></sup> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib13\" title=\"\">13</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">2024</td>\n<td class=\"ltx_td ltx_align_center\">S+T(ASR)</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_ulem_uline\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">65.5</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_ulem_uline\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">64.1</span></span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#EFEFEF;\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">M<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">4</span></sup>SER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">2024</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">S+T(ASR)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">66.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">66.0</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "texts",
            "stgt",
            "visual",
            "testing",
            "asr",
            "indicated",
            "multimodal",
            "wf1",
            "merhan",
            "full",
            "indicates",
            "modality",
            "method",
            "mlcct",
            "performance",
            "year",
            "comparison",
            "stgtv",
            "mfaedaec∘",
            "acc",
            "reimplement",
            "sams",
            "sota",
            "training",
            "m4ser",
            "iii",
            "hcam∗",
            "stasr",
            "result",
            "models",
            "“v”",
            "corresponding",
            "dimmn",
            "obtain",
            "represents",
            "meld"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T2\" title=\"TABLE II &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> compare the performance of the M<sup class=\"ltx_sup\">4</sup>SER method with those of recent multimodal SER methods on the IEMOCAP and MELD datasets. Our proposed M<sup class=\"ltx_sup\">4</sup>SER achieves a WA of 79.2% and a UA of 80.1% on IEMOCAP, and an ACC of 66.5% and a W-F1 of 66.0% on MELD, outperforming other models and demonstrating the superiority of M<sup class=\"ltx_sup\">4</sup>SER. Specifically, on the IEMOCAP dataset, compared with the recent MAF-DCT, the M<sup class=\"ltx_sup\">4</sup>SER method shows a 0.7% improvement in WA and a 0.8% improvement in UA, reaching a new SOTA performance. These improvements stem from our M<sup class=\"ltx_sup\">4</sup>SER&#8217;s capability to mitigate the impact of ASR errors, capture modality-specific and modality-invariant representations across different modalities, enhance commonality between modalities through adversarial learning, and excel in emotion feature learning with the LCL loss.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Multimodal speech emotion recognition (SER) has emerged as pivotal for improving human&#8211;machine interaction. Researchers are increasingly leveraging both speech and textual information obtained through automatic speech recognition (ASR) to comprehensively recognize emotional states from speakers. Although this approach reduces reliance on human-annotated text data, ASR errors possibly degrade emotion recognition performance.\nTo address this challenge, in our previous work, we introduced two auxiliary tasks, namely, ASR error detection and ASR error correction, and we proposed a novel multimodal fusion (MF) method for learning modality-specific and modality-invariant representations across different modalities.\nBuilding on this foundation, in this paper, we introduce two additional training strategies. First, we propose an adversarial network to enhance the diversity of modality-specific representations. Second, we introduce a label-based contrastive learning strategy to better capture emotional features.\nWe refer to our proposed method as M<sup class=\"ltx_sup\">4</sup>SER and validate its superiority over state-of-the-art methods through extensive experiments using IEMOCAP and MELD datasets.</p>\n\n",
                "matched_terms": [
                    "method",
                    "performance",
                    "asr",
                    "training",
                    "multimodal",
                    "m4ser",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the rapid advancement of deep learning, SER has evolved into end-to-end (E2E) systems capable of directly processing speech signals and outputting emotion classification results.\nIn recent years, the success of self-supervised learning (SSL) has led to the development of a series of speech-based pretrained models such as wav2vec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib3\" title=\"\">3</a>]</cite>, HuBERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib4\" title=\"\">4</a>]</cite>, and WavLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib5\" title=\"\">5</a>]</cite>, achieving the current state-of-the-art (SOTA) performance in SER tasks.</p>\n\n",
                "matched_terms": [
                    "models",
                    "sota",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent research has shown that single-modal approaches struggle to meet the increasing demands for SER owing to their inherent complexity. As a solution, multimodal information has emerged as a viable option because it offers diverse emotional cues. A common strategy involves integrating speech and text modalities to achieve multimodal SER. Text data can be obtained through manual transcription or automatic speech recognition (ASR), which have been widely adopted in recent studies to reduce reliance on extensive annotated datasets.\nAlthough text-based pretrained models such as BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib6\" title=\"\">6</a>]</cite> and RoBERTa <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib7\" title=\"\">7</a>]</cite> have excelled in multimodal SER tasks, the accuracy of ASR remains equally crucial for achieving precise emotion recognition (ER) results. To mitigate the impact of ASR errors, Santoso et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib8\" title=\"\">8</a>]</cite> have integrated self-attention mechanisms and word-level confidence measurements, particularly reducing the importance of words with high error probabilities, thereby enhancing SER performance. However, this approach heavily depends on the performance of the ASR system, thus limiting its generalizability. Lin and Wang <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> have proposed an auxiliary ASR error detection task aimed at determining the probabilities of erroneous words to adjust the trust levels assigned to each word in the ASR hypothesis, thereby improving multimodal SER performance. However, this method focuses solely on error detection within the ASR hypothesis without directly correcting these errors, indicating that it does not fully enhance the coherence of semantic information.</p>\n\n",
                "matched_terms": [
                    "method",
                    "performance",
                    "models",
                    "asr",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the other hand, how to effectively integrate speech and text modalities has become a key focus. Previous methods have included simple feature concatenation, recurrent neural networks (RNNs), convolutional neural networks (CNNs), and cross-modal attention mechanisms <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib1\" title=\"\">1</a>]</cite>. Although these methods have shown some effectiveness, they often face challenges arising from differences in representations across different modalities.\nIn recent multimodal tasks such as sentiment analysis <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib11\" title=\"\">11</a>]</cite>, researchers have proposed learning two distinct types of representation to enhance multimodal learning.\nThe first is a representation tailored to each modality, that is, a modality-specific representation (MSR).\nThe second is modality-invariant representation (MIR), aimed at mapping all modalities of the same utterance into a shared subspace. This representation captures commonalities in multimodal signals as well as the speaker&#8217;s shared motives, which collectively affect the emotional state of the utterance through distribution alignment. By combining these two types of representation, one can obtain a comprehensive view of multimodal data can be provided for downstream tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib12\" title=\"\">12</a>]</cite>.\nHowever, these methods focus on utterance-level representations, which, while effective for short and isolated utterances, often fail to capture fine-grained emotional dynamics, especially in long and complex interactions. Such coarse-grained representations fail to capture subtle temporal cues and modality-specific variations that are crucial for accurate emotion recognition.\nMoreover, mapping entire utterances to a shared or modality-specific space using similarity loss oversimplifies the underlying multimodal relationships and hinders effective cross-modal alignment.</p>\n\n",
                "matched_terms": [
                    "obtain",
                    "multimodal",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the basis of the above observations, in our previous work <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib13\" title=\"\">13</a>]</cite>, we introduced two auxiliary tasks: ASR error detection (AED) and ASR error correction (AEC). Specifically, we introduced an AED module to identify the positions of ASR errors. Subsequently, we employed an AEC module to correct these errors, enhancing the semantic coherence of ASR hypotheses and reducing the impact of ASR errors on ER tasks.\nAdditionally, we designed a novel multimodal fusion (MF) module for learning frame-level MSRs and MIRs in a shared speech&#8211;text modality space.\nWe observed the following shortcomings in our previous work: 1) Despite introducing the AED and AEC modules, uncorrected ASR errors continue to affect the accuracy of SER. 2) Existing MF methods possibly fail to capture subtle differences in emotional features.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "multimodal",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these issues, we propose two learning strategies. First, we introduce adversarial learning to enhance the diversity and robustness of modality representations, thereby reducing the impact of ASR errors on SER tasks. Second, we introduce a contrastive learning strategy based on emotion labels to more accurately distinguish and capture feature differences between different emotion categories, thus improving the performance and robustness of the SER system.\nIn summary, our main contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "asr",
                    "performance",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce an adversarial modality discriminator to enhance the diversity of MSRs and improve the generalization of MIRs. This design alleviates modality mismatch and suppresses modality-specific noise introduced by ASR errors, which often cause misalignment between acoustic and textual features and degrade the robustness of multimodal fusion.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "multimodal",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the basis of our previous work, we present M<sup class=\"ltx_sup\">4</sup>SER, an SER method that leverages multimodal, multirepresentation, multitask, and multistrategy learning. The difference between our M<sup class=\"ltx_sup\">4</sup>SER and previous SER methods is illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S1.F1\" title=\"Figure 1 &#8227; I Introduction &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. M<sup class=\"ltx_sup\">4</sup>SER mitigates the impact of ASR errors, improves modality-invariant representations, and captures commonalities between modalities to bridge their heterogeneity.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "method",
                    "m4ser",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed M<sup class=\"ltx_sup\">4</sup>SER outperforms state-of-the-arts on the IEMOCAP and MELD datasets. Extensive experiments demonstrate its superiority in SER tasks.</p>\n\n",
                "matched_terms": [
                    "meld",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remaining sections of this paper are organized as follows: In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2\" title=\"II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">II</span></a>, we review related work. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3\" title=\"III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>, we discuss the specific model and method of M<sup class=\"ltx_sup\">4</sup>SER. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S4\" title=\"IV Experimental Setup &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>, we outline the experimental setup used in this study, including datasets, evaluation metrics, and implementation details. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5\" title=\"V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>, we verify the effectiveness of the proposed model. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S6\" title=\"VI Conclusion &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a>, we present our conclusion and future work.</p>\n\n",
                "matched_terms": [
                    "iii",
                    "method",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SER aims to identify the speaker&#8217;s emotional state from spoken utterances. Early SER models primarily rely on audio signals, extracting prosodic and acoustic features such as Mel-frequency cepstral coefficients, filter banks, or other handcrafted descriptors&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib14\" title=\"\">14</a>]</cite>. With the advent of deep learning, models based on RNNs, CNNs and Transformer architectures&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib18\" title=\"\">18</a>]</cite> have achieved significant improvements by modeling complex temporal and hierarchical patterns in speech. More recently, the rise of SSL has led to the development of speech-based pretrained models such as wav2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib3\" title=\"\">3</a>]</cite>, HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib4\" title=\"\">4</a>]</cite>, and WavLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib5\" title=\"\">5</a>]</cite>, which provide rich contextual representations and deliver SOTA performance on various SER benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib19\" title=\"\">19</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "sota",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these limitations, multimodal SER often integrates both speech and text modalities, capturing both acoustic and semantic cues. A common setting involves using speech signals alongside textual transcripts (typically from manual annotations or ASR outputs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib24\" title=\"\">24</a>]</cite>.\nFan et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib21\" title=\"\">21</a>]</cite> proposed multi-granularity attention-based transformers (MGAT) to address emotional asynchrony and modality misalignment.\nSun et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib22\" title=\"\">22</a>]</cite> introduced a method integrating shared and private encoders, projecting each modality into a separate subspace to capture modality consistency and diversity while ensuring label consistency in the output.\nThese methods typically assume access to high-quality manual transcriptions for the text modality.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "method",
                    "multimodal",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, growing efforts have focused on reducing the dependency on annotated text by directly leveraging ASR hypotheses as input <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib28\" title=\"\">28</a>]</cite>. Santoso et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib25\" title=\"\">25</a>]</cite> introduced confidence-aware self-attention mechanisms that downweight unreliable ASR tokens to mitigate error propagation. Lin and Wang&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> proposed a robust multimodal SER framework that adaptively fuses attention-weighted acoustic representations with ASR-derived text embeddings, compensating for recognition noise in the transcript.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Learning modality-specific representations (MSRs) and modality-invariant representations (MIRs) has proven effective for multimodal SER, as it enables models to capture both shared semantics across modalities and complementary modality-unique cues.\nHazarika et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib10\" title=\"\">10</a>]</cite> proposed MISA, which disentangles each modality into invariant and specific subspaces through factorization, facilitating effective fusion for emotion prediction.\nLiu et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib29\" title=\"\">29</a>]</cite> proposed a modality-robust MER framework that introduces a contrastive learning module to extract MIRs from full-modality inputs, and an imagination module that reconstructs missing modalities based on the MIRs, enhancing robustness under incomplete input conditions.\nTo explicitly reduce distribution gaps and mitigate feature redundancy, Yang et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib11\" title=\"\">11</a>]</cite> proposed FDMER, which extracts both MSRs and MIRs, combined with consistency&#8211;disparity constraints and a modality discriminator to encourage disentangled learning.\nLiu et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib30\" title=\"\">30</a>]</cite> presented NORM&#8209;TR, introducing a noise&#8209;resistant generic feature (NRGF) extractor trained with noise&#8209;aware adversarial objectives, followed by a multimodal transformer that fuses NRGFs with raw modality features for robust representation learning.</p>\n\n",
                "matched_terms": [
                    "models",
                    "multimodal",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to factorization-based approaches which explicitly disentangle modality inputs into shared and private spaces, our M<sup class=\"ltx_sup\">4</sup>SER framework adopts a structured encoding&#8211;generation strategy to implicitly model MSRs and MIRs. Specifically, we utilize a stack of cross-modal encoder blocks to extract token-aware and speech-aware features (MSRs), and then apply a dedicated generator composed of hybrid-modal attention and masking mechanisms to derive the MIRs. In addition, unlike prior works that incorporate adversarial learning at a global or representation-level, our M<sup class=\"ltx_sup\">4</sup>SER employs a frame-level discriminator that operates on each temporal representation. A frame-level discriminator attempts to distinguish the modality origin of each temporal representation, while the MIR generator is optimized to deceive the discriminator by producing modality-agnostic outputs that are hard to classify as either text or speech. This fine-grained, per-frame adversarial constraint leads to more robust and aligned cross-modal representations. Moreover, M<sup class=\"ltx_sup\">4</sup>SER integrates this disentanglement framework into a multitask learning setup with ASR-aware auxiliary supervision, enabling more effective adaptation to noisy real-world speech&#8211;text scenarios.</p>\n\n",
                "matched_terms": [
                    "m4ser",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ASR error correction (AEC) techniques remain effective in optimizing the ASR hypotheses. AEC has been jointly trained with ER tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib13\" title=\"\">13</a>]</cite> in multitask settings. Li et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib27\" title=\"\">27</a>]</cite> proposed an AEC method that improves transcription quality on low-resource out-of-domain data through cross-modal training and the incorporation of discrete speech units, and they validated its effectiveness in downstream ER tasks. Lin and Wang <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> introduced a robust ER method that leverages complementary semantic information from audio, adapts to ASR errors through an auxiliary task for ASR error detection, and fuses text and acoustic representations for ER.\nOn the basis of <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib32\" title=\"\">32</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib33\" title=\"\">33</a>]</cite>, we propose a partially autoregressive AEC method that uses a label predictor to restrict decoding to only desirable parts of the input sequence embeddings, thereby reducing inference latency.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "method",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed M<sup class=\"ltx_sup\">4</sup>SER framework can be formalized as the function <math alttext=\"f(S,T)=(L,Z)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mi>L</mi><mo>,</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f(S,T)=(L,Z)</annotation></semantics></math>. Here, the speech modality <math alttext=\"S=(s_{1},s_{2},\\cdots,s_{m})\\in\\mathbb{R}^{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>s</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>s</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>m</mi></msup></mrow><annotation encoding=\"application/x-tex\">S=(s_{1},s_{2},\\cdots,s_{m})\\in\\mathbb{R}^{m}</annotation></semantics></math> consists of <span class=\"ltx_text ltx_font_italic\">m</span> frames extracted from an utterance, whereas the text modality <math alttext=\"T=(t_{1},t_{2},\\cdots,t_{n})\\in\\mathbb{R}^{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><msub><mi>t</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>t</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>n</mi></msup></mrow><annotation encoding=\"application/x-tex\">T=(t_{1},t_{2},\\cdots,t_{n})\\in\\mathbb{R}^{n}</annotation></semantics></math> comprises <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> tokens from the original ASR hypotheses of the utterance. All tokens are mapped to a predefined WordPiece vocabulary <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib40\" title=\"\">40</a>]</cite>. Additionally, within our multitask learning framework, the primary task is ER, yielding an emotion classification output <math alttext=\"L\\in\\{l_{1},l_{2},\\cdots,l_{e}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>l</mi><mn>1</mn></msub><mo>,</mo><msub><mi>l</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>l</mi><mi>e</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">L\\in\\{l_{1},l_{2},\\cdots,l_{e}\\}</annotation></semantics></math>, where <math alttext=\"e\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mi>e</mi><annotation encoding=\"application/x-tex\">e</annotation></semantics></math> represents the number of emotional categories. Concurrently, the auxiliary tasks include ASR error detection (AED) and ASR error correction (AEC). The output of these auxiliary tasks is <math alttext=\"Z\\in\\{Z_{1},Z_{2},\\cdots,Z_{k}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>Z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>Z</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>Z</mi><mi>k</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Z\\in\\{Z_{1},Z_{2},\\cdots,Z_{k}\\}</annotation></semantics></math>, representing all the ground truth (GT) sequences where the ASR transcriptions differ from the human-annotated transcriptions.\n<math alttext=\"Z_{k}=(z_{1,k},z_{2,k},\\cdots,z_{c,k})\\in\\mathbb{R}^{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><msub><mi>Z</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mn>1</mn><mo>,</mo><mi>k</mi></mrow></msub><mo>,</mo><msub><mi>z</mi><mrow><mn>2</mn><mo>,</mo><mi>k</mi></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>z</mi><mrow><mi>c</mi><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>c</mi></msup></mrow><annotation encoding=\"application/x-tex\">Z_{k}=(z_{1,k},z_{2,k},\\cdots,z_{c,k})\\in\\mathbb{R}^{c}</annotation></semantics></math>, denoting that the <math alttext=\"k^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\"><semantics><msup><mi>k</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">k^{th}</annotation></semantics></math> error position includes <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m10\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> tokens.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "represents",
                    "m4ser",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contextual Speech Representations.</span> To obtain comprehensive contextual representations of acoustic features, we utilize a pretrained SSL model, HuBERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib4\" title=\"\">4</a>]</cite>, as our acoustic encoder. HuBERT integrates CNN layers with a transformer encoder to effectively capture both speech features and contextual information.\nWe denote the acoustic hidden representations of the speech modality inputs <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> generated by HuBERT as <math alttext=\"H_{S}=(h_{S}^{(1)},h_{S}^{(2)},\\cdots,h_{S}^{(m)})\\in\\mathbb{R}^{m\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>H</mi><mi>S</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msubsup><mi>h</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">H_{S}=(h_{S}^{(1)},h_{S}^{(2)},\\cdots,h_{S}^{(m)})\\in\\mathbb{R}^{m\\times d}</annotation></semantics></math>, where <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> is the dimension of hidden representations.</p>\n\n",
                "matched_terms": [
                    "obtain",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contextual Token Representations.</span> We use the pretrained language model BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib6\" title=\"\">6</a>]</cite> as our text encoder to obtain the token representations <math alttext=\"H_{T}=(h_{T}^{(1)},h_{T}^{(2)},\\cdots,h_{T}^{(n)})\\in\\mathbb{R}^{n\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>H</mi><mi>T</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msubsup><mi>h</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">H_{T}=(h_{T}^{(1)},h_{T}^{(2)},\\cdots,h_{T}^{(n)})\\in\\mathbb{R}^{n\\times d}</annotation></semantics></math> for the text modality inputs <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "obtain",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More specifically, once the erroneous positions are identified, the decoder constructs a separate generation sequence for each token labeled as <span class=\"ltx_text ltx_font_bold\">C</span>. Each sequence begins with a special start token <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_italic\">BOS<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span> and generates the corrected text in an autoregressive manner.\nAt each decoding step, the decoder input for a given correction sequence consists of all previously generated tokens, each concatenated with the representation of the corresponding erroneous token.\nAll correction sequences are processed in parallel by a shared transformer decoder, which attends to the full ASR representation <math alttext=\"H_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m3\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">H_{T}</annotation></semantics></math> as memory.</p>\n\n",
                "matched_terms": [
                    "full",
                    "asr",
                    "corresponding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> denotes either the speech or text modality and <math alttext=\"H_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m2\" intent=\":literal\"><semantics><msub><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">H_{ST}</annotation></semantics></math> represents the concatenation of <math alttext=\"H_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m3\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">H_{S}</annotation></semantics></math> and <math alttext=\"H_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m4\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">H_{T}</annotation></semantics></math>, which is then fed into a FC layer to map it back to the same dimension as <math alttext=\"H_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m5\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">H_{S}</annotation></semantics></math>. This process integrates both speech and text information into a unified representation. The resulting features are subsequently summed with <math alttext=\"H_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m6\" intent=\":literal\"><semantics><msub><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">H_{ST}</annotation></semantics></math>, culminating in the ultimate modality-invariant representation:</p>\n\n",
                "matched_terms": [
                    "represents",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where &#8220;<math alttext=\"\\sigma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p5.m1\" intent=\":literal\"><semantics><mi>&#963;</mi><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math>&#8221; denotes Sigmoid activation and &#8220;<math alttext=\"\\otimes\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p5.m2\" intent=\":literal\"><semantics><mo>&#8855;</mo><annotation encoding=\"application/x-tex\">\\otimes</annotation></semantics></math>&#8221; indicates element-wise multiplication.\nFinally, the modality-specific and modality-invariant representations are concatenated together to obtain the final multimodal fusion representation <math alttext=\"H_{ST}^{\\rm(fus)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p5.m3\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>fus</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{ST}^{\\rm(fus)}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "indicates",
                    "obtain",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further develop the modality discriminator <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m1\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> utilizing the modality-invariant representations produced by the MIR generator. This discriminator illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(f) is composed of two linear layers with a ReLU activation function in between, followed by a Sigmoid activation function. To enhance the MIR&#8217;s capability to remain modality-agnostic, we employ adversarial learning techniques.\nSpecifically, the discriminator <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> outputs a scalar value between 0 and 1 for each frame, where values close to 0 indicate the text modality, values close to 1 indicate the speech modality, and values near 0.5 represent an ambiguous modality.\nHere, <math alttext=\"D(H)\\in\\mathbb{R}^{m\\times 1},H\\in\\{H^{(spe)}_{T},H^{(spe)}_{S},H_{ST}^{\\rm(inv)}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>D</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow></msup></mrow><mo>,</mo><mrow><mi>H</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>inv</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">D(H)\\in\\mathbb{R}^{m\\times 1},H\\in\\{H^{(spe)}_{T},H^{(spe)}_{S},H_{ST}^{\\rm(inv)}\\}</annotation></semantics></math>.\nOn one hand, we aim for the discriminator that correctly classifies frames in the modality-specific representations <math alttext=\"H^{(spe)}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H^{(spe)}_{T}</annotation></semantics></math> and <math alttext=\"H^{(spe)}_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m5\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H^{(spe)}_{S}</annotation></semantics></math> as 0 and 1, respectively. On the other hand, to enhance the modality agnosticism of the modality-invariant representation <math alttext=\"H_{ST}^{\\rm(inv)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m6\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>inv</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{ST}^{\\rm(inv)}</annotation></semantics></math> produced by the MIR generator, we aim for a discriminator that outputs values close to 0.5, indicating an ambiguous modality. With this design of the MIR generator and discriminator, we define a generative adversarial training objective, mathematically represented as</p>\n\n",
                "matched_terms": [
                    "training",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the training stage, the learning process is optimized using three loss functions that correspond to ER, AED, and AEC (i.e., <math alttext=\"\\mathcal{L}_{\\rm ER}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>ER</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm ER}</annotation></semantics></math>, <math alttext=\"\\mathcal{L}_{\\rm AED}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>AED</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm AED}</annotation></semantics></math>, and <math alttext=\"\\mathcal{L}_{\\rm AEC}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>AEC</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm AEC}</annotation></semantics></math>), and two training strategies (i.e., <math alttext=\"\\mathcal{L}_{\\rm GAN}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>GAN</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm GAN}</annotation></semantics></math> and <math alttext=\"\\mathcal{L}_{\\rm LCL}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>LCL</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm LCL}</annotation></semantics></math>). The specific optimization process of M<sup class=\"ltx_sup\">4</sup>SER is detailed in Alg. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#alg1\" title=\"Algorithm 1 &#8227; III-I Joint Training &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nThese five loss functions are linearly combined as the overall training objective of M<sup class=\"ltx_sup\">4</sup>SER:</p>\n\n",
                "matched_terms": [
                    "m4ser",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the GAN training strategy <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib34\" title=\"\">34</a>]</cite>, we divide the backpropagation process into two steps. Firstly, we maximize <math alttext=\"\\mathcal{L}_{\\rm GAN}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>GAN</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm GAN}</annotation></semantics></math> to update the discriminator, during which the generator is detached from the optimization. According to Eq. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.Ex1\" title=\"III-G Modality Discriminator &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III-G</span></a>), on one hand, maximizing the first term <math alttext=\"L_{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>D</mi></msub><annotation encoding=\"application/x-tex\">L_{D}</annotation></semantics></math> of <math alttext=\"\\mathcal{L}_{\\rm GAN}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>GAN</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm GAN}</annotation></semantics></math> essentially trains the discriminator to correctly distinguish between text and audio features by making <math alttext=\"H_{S}^{\\rm(spe)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mi>spe</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{S}^{\\rm(spe)}</annotation></semantics></math> close to 1 and <math alttext=\"H_{T}^{\\rm(spe)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mi>spe</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{T}^{\\rm(spe)}</annotation></semantics></math> close to 0 <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Function <math alttext=\"\\log(x)+\\log(1-y)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>&#8722;</mo><mi>y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(x)+\\log(1-y)</annotation></semantics></math>, where <math alttext=\"x,y\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x,y\\in(0,1)</annotation></semantics></math> reaches its maximum when <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m3\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> approaches 1 and <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m4\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math> approaches 0.</span></span></span>. On the other hand, maximizing the second term <math alttext=\"L_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m6\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>G</mi></msub><annotation encoding=\"application/x-tex\">L_{G}</annotation></semantics></math> (i.e., minimizing <math alttext=\"-L_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m7\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><msub><mi>L</mi><mi>G</mi></msub></mrow><annotation encoding=\"application/x-tex\">-L_{G}</annotation></semantics></math>) will make <math alttext=\"H^{\\text{(inv)}}_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mtext>(inv)</mtext></msubsup><annotation encoding=\"application/x-tex\">H^{\\text{(inv)}}_{ST}</annotation></semantics></math> approach 0 or 1 <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Function <math alttext=\"\\log(x)+\\log(1-x)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>&#8722;</mo><mi>x</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(x)+\\log(1-x)</annotation></semantics></math>, where <math alttext=\"x\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m2\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x\\in(0,1)</annotation></semantics></math> reaches its maximum at <math alttext=\"x=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m3\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">x=0.5</annotation></semantics></math> and its minimum near <math alttext=\"x=0\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m4\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">x=0</annotation></semantics></math> and <math alttext=\"x=1\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m5\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">x=1</annotation></semantics></math>.</span></span></span>, indicating that the discriminator recognizes <math alttext=\"H^{\\text{(inv)}}_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m9\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mtext>(inv)</mtext></msubsup><annotation encoding=\"application/x-tex\">H^{\\text{(inv)}}_{ST}</annotation></semantics></math> as modality-specific, which can be either text or audio, contrary to our desired modality invariance. Secondly, we freeze the discriminator parameters and update the remaining parameters by minimizing <math alttext=\"L_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m10\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>G</mi></msub><annotation encoding=\"application/x-tex\">L_{G}</annotation></semantics></math>, which makes the output of the discriminator <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m11\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> for the modality-invariant features <math alttext=\"H^{\\text{(inv)}}_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m12\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mtext>(inv)</mtext></msubsup><annotation encoding=\"application/x-tex\">H^{\\text{(inv)}}_{ST}</annotation></semantics></math> approach 0.5, thereby blurring these features between audio and text modalities to achieve modality agnosticism. Additionally, <math alttext=\"\\mathcal{L}_{\\text{ER}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m13\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ER</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ER}}</annotation></semantics></math> optimizes the downstream emotion recognition model, <math alttext=\"\\mathcal{L}_{\\text{AED}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m14\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>AED</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{AED}}</annotation></semantics></math> and <math alttext=\"\\mathcal{L}_{\\text{AEC}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m15\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>AEC</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{AEC}}</annotation></semantics></math> optimize the auxiliary tasks of ASR error detection and ASR error correction models, respectively, and <math alttext=\"\\mathcal{L}_{\\text{LCL}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m16\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>LCL</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{LCL}}</annotation></semantics></math> implements the LCL strategy. The entire system is trained in an end-to-end manner and optimized by adjusting weight parameters.</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr",
                    "training",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the effectiveness\nof our proposed model, we carried out experiments on two public datasets:\nthe Interactive Emotional Dyadic Motion Capture (IEMOCAP) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib45\" title=\"\">45</a>]</cite> and the Multimodal Emotion Lines Dataset (MELD) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib46\" title=\"\">46</a>]</cite>. The statistics of the two datasets are shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S4.F6\" title=\"Figure 6 &#8227; IV-A Dataset and Evaluation Metrics &#8227; IV Experimental Setup &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "meld",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MELD</span> included a total of 13708 samples extracted from the TV series Friends, divided into 9,989 for training, 1,109 for validation, and 2,610 for testing. The dataset contained seven labels: &#8220;Neutral&#8221; (6167), &#8220;Happy&#8221; (2198), &#8220;Fear&#8221; (350), &#8220;Sad&#8221; (985), &#8220;Disgust&#8221; (355), &#8220;Angry&#8221; (1541), and &#8220;Surprise&#8221; (1499).\nWe tuned hyperparameters on the validation set and reported final results on the test set using the best checkpoint, evaluated by accuracy (ACC) and weighted F1-score (W-F1).</p>\n\n",
                "matched_terms": [
                    "acc",
                    "training",
                    "testing",
                    "wf1",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The acoustic encoder was initialized using the <span class=\"ltx_text ltx_font_typewriter\">hubert-base-ls960<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">3</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/facebook/hubert-base-ls960\" title=\"\">https://huggingface.co/facebook/hubert-base-ls960</a></span></span></span></span> model, producing acoustic representations <math alttext=\"d_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">d_{S}</annotation></semantics></math> with a dimensionality of 768.\nThe text encoder employed the <span class=\"ltx_text ltx_font_typewriter\">bert-base-uncased<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">4</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/google-bert/bert-base-uncased\" title=\"\">https://huggingface.co/google-bert/bert-base-uncased</a></span></span></span></span> model for initialization, yielding token representations <math alttext=\"d_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">d_{T}</annotation></semantics></math> with a dimensionality of 768. The vocabulary size for word tokenization <math alttext=\"d_{vocab}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>d</mi><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">d_{vocab}</annotation></semantics></math> was set to 30,522. We set the hidden size <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> to 768, the number of attention layers to 12, and the number of attention heads to 12. Both the HuBERT and BERT models were fine-tuned during the training stage.\nThe transformer decoder in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c) adopted a single-layer transformer with a hidden size of 768. Additionally, we obtained corresponding ASR hypotheses using the Whisper ASR model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib48\" title=\"\">48</a>]</cite>. We used the <span class=\"ltx_text ltx_font_typewriter\">openai/whisper-medium.en<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">5</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/openai/whisper-medium.en\" title=\"\">https://huggingface.co/openai/whisper-medium.en</a></span></span></span></span> and <span class=\"ltx_text ltx_font_typewriter\">openai/whisper-large-v2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">6</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/openai/whisper-large-v2\" title=\"\">https://huggingface.co/openai/whisper-large-v2</a></span></span></span></span> models, achieving word error rates (WERs) of 20.48% and 37.87% on the IEMOCAP and MELD datasets, respectively.</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr",
                    "corresponding",
                    "training",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used Adam <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib49\" title=\"\">49</a>]</cite> as the optimizer with a batch size of 16. On the basis of empirical observations and prior work <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib50\" title=\"\">50</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib51\" title=\"\">51</a>]</cite>, we kept the learning rate constant at <math alttext=\"1e^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-5}</annotation></semantics></math> for IEMOCAP and <math alttext=\"1e^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-4}</annotation></semantics></math> for MELD during training. For our multitask learning setup, we set <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m3\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> to 3 and <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m4\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> to 0.1. For our multistrategy learning setup, we set <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m5\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> to 0.01 and <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m6\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> to 0.1. The temperature parameter <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m7\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> was set to 0.07. To validate these choices, we additionally performed a one-at-a-time sensitivity analysis on the hyperparameters <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m8\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m9\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>, <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m10\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>, and <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m11\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math>, as presented in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.SS3\" title=\"V-C Sensitivity Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">V-C</span></a> and illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F7\" title=\"Figure 7 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "meld",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the experiments, we compared various multimodal speech emotion recognition SOTA methods with our proposed method, M<sup class=\"ltx_sup\">4</sup>SER.\nThe comparison was conducted by the following methods on the IEMOCAP dataset:</p>\n\n",
                "matched_terms": [
                    "method",
                    "comparison",
                    "m4ser",
                    "multimodal",
                    "sota"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SAWC</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib47\" title=\"\">47</a>]</cite> adjusts importance weights using confidence measures, reducing ASR error impact by emphasizing the corresponding speech segments.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "corresponding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SAMS</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib52\" title=\"\">52</a>]</cite> leverages high-level emotion representations as supervisory signals to build a multi-spatial learning framework for each modality, enabling cross-modal semantic learning and fusion representation exploration.</p>\n\n",
                "matched_terms": [
                    "sams",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MMER</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib14\" title=\"\">14</a>]</cite> employs early fusion and cross-modal self-attention between text and acoustic modalities, and addresses three novel auxiliary tasks to enhance SER. For fairness, we selected the result that did not introduce augmented text for comparison.</p>\n\n",
                "matched_terms": [
                    "result",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <span class=\"ltx_text ltx_font_bold\">MF-AED-AEC</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib13\" title=\"\">13</a>]</cite>, as in our previous work, we consider two auxiliary tasks to improve semantic coherence in ASR text and introduce a MF method to learn shared representations.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SAMS and MF-AED-AEC methods are also utilized as baselines for the MELD dataset. Additionally, the following methods were included for comparison:</p>\n\n",
                "matched_terms": [
                    "sams",
                    "comparison",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Full</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib55\" title=\"\">55</a>]</cite> uses contextual cross-modal transformers and graph convolutional networks for enhanced emotion representations and modality fusion.</p>\n\n",
                "matched_terms": [
                    "full",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DIMMN</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib57\" title=\"\">57</a>]</cite> fuses multimodal data using multiview attention layers, temporal convolutional networks, gated recurrent units, and memory networks to model dynamic dependencies and contextual information.</p>\n\n",
                "matched_terms": [
                    "dimmn",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MLCCT</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib59\" title=\"\">59</a>]</cite> involves feature extraction, interaction, and fusion using SSL embedding models, Bi-LSTM, cross-modal transformers, and self-attention blocks.</p>\n\n",
                "matched_terms": [
                    "models",
                    "mlcct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed M<sup class=\"ltx_sup\">4</sup>SER model is composed of four types of learning: multimodal learning, multirepresentation learning, multitask learning, and multistrategy learning. To determine the impact of different types of learning on the performance of the emotion recognition task, ablation experiments were conducted on the IEMOCAP and MELD datasets, as presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "meld",
                    "m4ser",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multimodalities.</span>\nTo assess the necessity of multimodality, we conduct single-modal comparison experiments involving text and speech modalities. These experiments operate under a baseline framework excluding the MF module, LCL loss, and GAN loss, precluding intermodal interaction.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(A), in the single-modal experiments, the performance of speech modality in the IEMOCAP dataset is better than that of text modality, whereas the reverse is observed in the MELD dataset. This divergence suggests that emotional information is more distinctly conveyed through speech in IEMOCAP, whereas MELD leans towards text for emotional expression.\nMoreover, the multimodal baseline model, which simply concatenates speech and text features for recognition (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> A(3)), significantly enhances performance compared with single-modal baselines, highlighting the essential role of multimodal information.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "comparison",
                    "multimodal",
                    "meld",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multirepresentations.</span> We study the importance of modality-invariant and modality-specific representations by discarding each type. Removing modality-specific representations from multimodal fusion significantly reduces downstream emotion recognition performance on both datasets, confirming their effectiveness in capturing and utilizing unique information from each modality. Similarly, omitting refined modality-invariant representations from multimodal fusion also significantly decreases emotion recognition performance on both datasets, demonstrating their importance in bridging modality gaps. Clearly, removing both representations further decreases performance.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "multimodal",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multitasks.</span> First, we discuss the impact of the AED module. Specifically, we remove the AED module and introduce only the AEC module as an auxiliary task. This requires the model to correct all errors in each utterance from scratch, rather than only the detected errors. Evidently, the absence of the AED module leads to a significant drop in emotion recognition performance, demonstrating the necessity of the AED module. Similarly, we observe that the AEC module also plays an important role. Moreover, we note that the WA results using only the AEC module (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(C)(1)) are even worse than the results with neither the AED nor AEC module (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(C)(3)) in both datasets. This phenomenon occurs because directly using the neural machine translation (NMT) model for AEC can even increase the WER <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib60\" title=\"\">60</a>]</cite>. Unlike NMT tasks, which typically require modifying almost all input tokens, AEC involves fewer but more challenging corrections. For instance, if the ASR model&#8217;s WER is 10%, then only about 10% of the input tokens require correction in the AEC model. However, these tokens are often difficult to recognize and correct because they have already been misrecognized by the ASR model. Therefore, we should consider the characteristics of ASR outputs and carefully design the AEC model, which is why we introduce both AED and AEC as auxiliary tasks.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multistrategies.</span> To validate the effectiveness of the adversarial training strategy outlined in Alg. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#alg1\" title=\"Algorithm 1 &#8227; III-I Joint Training &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we remove the adversarial training strategy from M<sup class=\"ltx_sup\">4</sup>SER. The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(D)(1) show a decrease in performance on both datasets, demonstrating the critical role of this strategy in learning modality-invariant representations. The proposed modality discriminator effectively enhances the modality agnosticism of the refined representations from the generator.\nAdditionally, omitting the LCL strategy described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.SS8\" title=\"III-H Label-based Contrastive Learning (LCL) &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III-H</span></a> also results in similar performance degradation (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(D)(2)). We visualize the results before and after introducing this strategy to demonstrate its effectiveness, with specific analysis also detailed in the following t-SNE analysis section. Moreover, removing both of these strategies further diminishes emotion recognition performance, confirming that both learning strategies contribute significantly to performance improvement.</p>\n\n",
                "matched_terms": [
                    "m4ser",
                    "training",
                    "performance",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:703.2pt;height:86pt;vertical-align:-40.5pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Type</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Modality</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Params (M)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Train Time (h)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Infer Time (ms/U)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">WA (%)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">UA (%)</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_2\">Single-modal</span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">HuBERT</span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">S</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">316.2</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">5.4</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">23.7 &#177; 0.1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">68.9</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt\">69.8</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\">BERT</span>\n<span class=\"ltx_td ltx_align_left ltx_border_r\">T(ASR)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">109.5</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">0.5</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">6.3 &#177; 0.1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">65.1</span>\n<span class=\"ltx_td ltx_align_center\">66.4</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_4\">Multi-modal</span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Baseline (HuBERT + BERT)</span>\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_4\">S+T(ASR)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">426.9</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">9.6</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">29.6 &#177; 0.1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">73.4</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">75.2</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\">&#8194;&#8202;&#8195;+ MSR &amp; MIR</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">481.4</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">12.0</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">44.8 &#177; 0.1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">76.6</span>\n<span class=\"ltx_td ltx_align_center\">77.4</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\">&#8194;&#8202;&#8195;&#8195;+ GAN &amp; LCL</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">481.6</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">20.8</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">44.5 &#177; 0.6</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">77.3</span>\n<span class=\"ltx_td ltx_align_center\">78.6</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">&#8195;&#8195;&#8195;&#8196;&#8202;+ AED &amp; AEC (M<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">4</span></sup>SER)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">512.9</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">21.5</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">44.7 &#177; 0.2</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">79.2</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">80.1</span></span></span>\n</span></span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "stasr",
                    "m4ser",
                    "multimodal",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the model exhibits relative robustness to variations in <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m1\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>, indicating stable synergy between the AED and AEC objectives. Conversely, performance is highly sensitive to <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m2\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>; large values result in unstable training due to adversarial objectives, while a small weight (e.g., <math alttext=\"\\gamma=0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=0.01</annotation></semantics></math>) ensures stable convergence and optimal results.</p>\n\n",
                "matched_terms": [
                    "result",
                    "training",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the trade-off between model performance and computational cost, we report the number of parameters, training time, and inference time for each model on the IEMOCAP dataset in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T5\" title=\"TABLE V &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>, with all experiments conducted on a single NVIDIA Tesla V100 GPU.</p>\n\n",
                "matched_terms": [
                    "training",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Among the single-modal models, HuBERT exhibits higher computational cost than BERT owing to the nature of speech encoders, but also delivers better recognition performance. In the multi-modal setting, each additional module introduces a moderate increase in parameter size and training cost, while consistently improving recognition accuracy.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "models",
                    "training",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our full model M<sup class=\"ltx_sup\">4</sup>SER achieves the best performance with WA of 79.2% and UA of 80.1%. Although its training time increases to 21.5 hours owing to the inclusion of GAN and LCL strategies and AED and AEC subtasks, these components are used only during training. Consequently, the inference latency of M<sup class=\"ltx_sup\">4</sup>SER remains nearly identical to the baseline with only MSR and MIR. Considering the significant gains in performance, M<sup class=\"ltx_sup\">4</sup>SER demonstrates a favorable trade-off between computational cost and recognition accuracy, making it suitable for practical deployment.</p>\n\n",
                "matched_terms": [
                    "full",
                    "m4ser",
                    "training",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">t-SNE Analysis.</span> To intuitively demonstrate the advantages of our proposed M<sup class=\"ltx_sup\">4</sup>SER model on IEMOCAP and MELD datasets, we utilize the t-distributed stochastic neighbor embedding (t-SNE) tool <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib61\" title=\"\">61</a>]</cite> to visualize the learned emotion features using all samples from session 3 of the IEMOCAP and test set of the MELD. We compare these features across the following models: the speech model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(a)), the text model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(b)), the proposed M<sup class=\"ltx_sup\">4</sup>SER model without label-based contrastive learning (LCL) (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(c)), and the proposed M<sup class=\"ltx_sup\">4</sup>SER model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(d)).</p>\n\n",
                "matched_terms": [
                    "meld",
                    "models",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the visualization results in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, we observe that the emotion label distributions for the two single-modal baselines on the IEMOCAP and MELD datasets show a significant overlap among the emotion categories, indicating that they are often confused with each other. In contrast, the emotion label distributions for the two multimodal models are more distinguishable, demonstrating the effectiveness of multimodal models in capturing richer emotional features.</p>\n\n",
                "matched_terms": [
                    "meld",
                    "models",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, compared with the M<sup class=\"ltx_sup\">4</sup>SER model without the LCL strategy, our M<sup class=\"ltx_sup\">4</sup>SER model achieves better clustering for each emotion category, making them as distinct as possible. For instance, on the IEMOCAP dataset, the intra-class clustering of samples learned by the M<sup class=\"ltx_sup\">4</sup>SER model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(d)) is more pronounced than that learned by the M<sup class=\"ltx_sup\">4</sup>SER model without the LCL strategy (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(c)), especially with clearer boundaries for sad and angry samples. Additionally, the distances between the four types of emotion in the emotion vector space increase. Although distinguishing MELD samples is more challenging than distinguishing IEMOCAP samples, the overall trend remains similar.\nThis indicates that the proposed M<sup class=\"ltx_sup\">4</sup>SER model effectively leverages both modality-specific and modality-invariant representations to capture high-level shared feature representations across speech and text modalities for emotion recognition.</p>\n\n",
                "matched_terms": [
                    "meld",
                    "m4ser",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further explore the effectiveness of adversarial learning in M<sup class=\"ltx_sup\">4</sup>SER, we visualize the distribution of modality-invariant and modality-specific representations before and after adversarial learning using the t-SNE tool, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F10\" title=\"Figure 10 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. It can be observed that after adversarial learning, different modality-specific representations become more separated with increasing intraclass clustering. This indicates that with the introduction of adversarial learning, M<sup class=\"ltx_sup\">4</sup>SER enhances the diversity of modality-specific representations, generating superior features for the downstream emotion recognition task.</p>\n\n",
                "matched_terms": [
                    "m4ser",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Representation Analysis.</span>\nWe find that our M<sup class=\"ltx_sup\">4</sup>SER method performs better when using ASR text than when using GT text, as presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T6\" title=\"TABLE VI &#8227; V-F Visualization Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a>. To investigate this finding, we visualize representation weights for two examples from IEMOCAP, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F11\" title=\"Figure 11 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. We display representations of each modality across the temporal level in two learning spaces, accumulating hidden features of <math alttext=\"H_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m2\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">H_{S}</annotation></semantics></math>, <math alttext=\"H_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m3\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">H_{T}</annotation></semantics></math>, <math alttext=\"\\hat{H}_{S}^{spe}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m4\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>H</mi><mo>^</mo></mover><mi>S</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\hat{H}_{S}^{spe}</annotation></semantics></math>, and <math alttext=\"H_{T}^{spe}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m5\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>T</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{T}^{spe}</annotation></semantics></math> into one dimension.\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F11\" title=\"Figure 11 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(a) illustrates that during single-modal learning with ASR text, representation weights in the text modality mainly focus on the word &#8220;oh&#8221;, whereas in the speech modality, representation weights concentrate towards the sentence&#8217;s end. After cross-modal learning, we observed a significant decrease in &#8220;oh&#8221; weight in the text modality. This change occurs as the model detects errors or inaccuracies in ASR transcription using AED and AEC modules, shifting representation weights to other words such as &#8220;that&#8217;s&#8221;. Owing to limited emotional cues in the text, the model relies more on speech features, especially anger-related cues such as intonation and volume, critical for accurate anger recognition. Conversely, with GT text, after cross-modal learning, attention in the text modality focuses on words conveying positive emotions such as &#8220;amusing&#8221;, whereas speech modality distribution remains largely unchanged. Without ASR error signals, the model leans towards these textual features, leading to a bias towards happiness in emotion recognition.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "method",
                    "m4ser",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Differences in feature weight distribution:</span> the absence of ASR errors alters feature weight distribution in GT text modality compared with ASR text, potentially leading to inaccurate emotion classification.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the performance of M<sup class=\"ltx_sup\">4</sup>SER in real-world environments, we conduct cross-corpus emotion recognition experiments, which simulate the practical scenario of domain shift between different datasets. Following the experimental settings of previous works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib62\" title=\"\">62</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib63\" title=\"\">63</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib64\" title=\"\">64</a>]</cite>, we adopt a transfer evaluation strategy where one corpus is used entirely for training, and 30% of the target corpus is reserved for validation for parameter tuning, while the remaining 70% is used for final testing.\nIn addition, as the IEMOCAP dataset contains only four emotion classes (&#8220;Neutral&#8221;, &#8220;Happy&#8221;, &#8220;Angry&#8221;, &#8220;Sad&#8221;), we restrict MELD to the same set of overlapping emotions for a fair comparison and discard the rest (&#8220;Suprise&#8221;, &#8220;Fear&#8221;, &#8220;Disgust&#8221;). This ensures consistent label space across corpora during both training and evaluation.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "comparison",
                    "m4ser",
                    "training",
                    "testing",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T8\" title=\"TABLE VIII &#8227; V-G Cross-corpus Generalization Ability &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VIII</span></a> presents the cross-corpus generalization results. Compared with the reimplemented SAMS and MF-AED-AEC baselines, our full model M<sup class=\"ltx_sup\">4</sup>SER achieves the best performance in both IE<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ME and ME<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>IE directions. Specifically, M<sup class=\"ltx_sup\">4</sup>SER outperforms the strong multimodal baseline by 4.3% in ACC and 3.1% in W-F1 under the IE<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ME setting, and by 5.1% WA and 4.8% UA in the ME<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>IE setting.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "acc",
                    "sams",
                    "multimodal",
                    "m4ser",
                    "wf1",
                    "full"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further evaluate the adaptability of M<sup class=\"ltx_sup\">4</sup>SER under limited supervision, we conduct few-shot domain adaptation experiments by sampling 5, 10, 20, and 60 labeled instances per class from the target-domain validation set. The remaining validation data is still used for model selection, while the test set remains fixed across all settings. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T9\" title=\"TABLE IX &#8227; V-G Cross-corpus Generalization Ability &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IX</span></a>, M<sup class=\"ltx_sup\">4</sup>SER consistently outperforms SAMS and MF-AED-AEC across all shot settings. For example, with only 5 labeled samples per class, M<sup class=\"ltx_sup\">4</sup>SER achieves ACC of 57.9% in the IE<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ME setting, already surpassing MF-AED-AEC with 20 shots. As the number of target samples increases, our model scales well and reaches ACC of 62.1% and W-F1 61.5% at 60-shot. In the ME<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>IE setting, M<sup class=\"ltx_sup\">4</sup>SER reaches UA of 75.9%, demonstrating excellent cross-domain adaptability.</p>\n\n",
                "matched_terms": [
                    "wf1",
                    "acc",
                    "sams",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose M<sup class=\"ltx_sup\">4</sup>SER, a novel emotion recognition method that combines multimodal, multirepresentation, multitask, and multistrategy learning. M<sup class=\"ltx_sup\">4</sup>SER leverages an innovative multimodal fusion module to learn modality-specific and modality-invariant representations, capturing unique features of each modality and common features across modalities. We then introduce a modality discriminator to enhance modality diversity through adversarial learning. Additionally, we design two auxiliary tasks, AED and AEC, aimed at enhancing the semantic consistency within the text modality. Finally, we propose a label-based contrastive learning strategy to distinguish different emotional features. Results of experiments on the IEMOCAP and MELD datasets demonstrate that M<sup class=\"ltx_sup\">4</sup>SER surpasses previous baselines, proving its effectiveness. In the future, we plan to extend our approach to the visual modality and introduce disentangled representation learning to further enhance emotion recognition performance. We also plan to investigate whether AED and AEC modules remain necessary when using powerful LLM-based ASR systems.</p>\n\n",
                "matched_terms": [
                    "method",
                    "performance",
                    "visual",
                    "asr",
                    "multimodal",
                    "m4ser",
                    "meld",
                    "modality"
                ]
            }
        ]
    },
    "S5.T4": {
        "source_file": "M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition",
        "caption": "TABLE IV: Ablation study on IEMOCAP and MELD datasets (%). “w/o” means “without”. “Concat” denotes “concatenation” operation.",
        "body": "Method\nIEMOCAP\nMELD\n\n\nWA\nUA\nACC\nW-F1\n\n\nM4SER (Full)\n79.2\n80.1\n66.5\n66.0\n\n\nA. Impact of Multimodalities\n\n\n(1) only Speech Modality\n68.9\n69.8\n51.7\n45.1\n\n\n(2) only Text Modality\n65.1\n66.4\n62.2\n58.6\n\n\n(3) Speech & Text + Concat\n73.4\n75.2\n63.9\n60.8\n\n\nB. Impact of Multirepresentations\n\n\n(1) w/o Modality-Specific (MSR)\n77.3\n78.4\n65.0\n62.4\n\n\n(2) w/o Modality-Invariant (MIR)\n78.9\n79.5\n65.6\n64.5\n\n\n(3) w/o MSR & MIR\n76.5\n77.9\n64.8\n61.7\n\n\nC. Impact of Multitasks\n\n\n(1) w/o ASR Error Detection\n76.9\n78.3\n64.3\n61.7\n\n\n(2) w/o ASR Error Correction\n78.0\n79.2\n65.9\n64.1\n\n\n(3) w/o AED & AEC\n77.3\n78.6\n65.1\n62.6\n\n\nD. Impact of Multistrategies\n\n\n(1) w/o GAN\n78.7\n79.8\n65.8\n65.3\n\n\n(2) w/o LCL\n78.2\n79.6\n65.9\n64.8\n\n\n(3) w/o GAN & LCL\n78.1\n79.3\n65.5\n64.1",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">IEMOCAP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">MELD</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">WA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">UA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">ACC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">W-F1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">M<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">4</span></sup>SER (Full)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">79.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">80.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">66.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">66.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"5\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">A. Impact of Multimodalities</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">(1) only Speech Modality</td>\n<td class=\"ltx_td ltx_align_center\">68.9</td>\n<td class=\"ltx_td ltx_align_center\">69.8</td>\n<td class=\"ltx_td ltx_align_center\">51.7</td>\n<td class=\"ltx_td ltx_align_center\">45.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">(2) only Text Modality</td>\n<td class=\"ltx_td ltx_align_center\">65.1</td>\n<td class=\"ltx_td ltx_align_center\">66.4</td>\n<td class=\"ltx_td ltx_align_center\">62.2</td>\n<td class=\"ltx_td ltx_align_center\">58.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">(3) Speech &amp; Text + Concat</td>\n<td class=\"ltx_td ltx_align_center\">73.4</td>\n<td class=\"ltx_td ltx_align_center\">75.2</td>\n<td class=\"ltx_td ltx_align_center\">63.9</td>\n<td class=\"ltx_td ltx_align_center\">60.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"5\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">B. Impact of Multirepresentations</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">(1) w/o Modality-Specific (MSR)</td>\n<td class=\"ltx_td ltx_align_center\">77.3</td>\n<td class=\"ltx_td ltx_align_center\">78.4</td>\n<td class=\"ltx_td ltx_align_center\">65.0</td>\n<td class=\"ltx_td ltx_align_center\">62.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">(2) w/o Modality-Invariant (MIR)</td>\n<td class=\"ltx_td ltx_align_center\">78.9</td>\n<td class=\"ltx_td ltx_align_center\">79.5</td>\n<td class=\"ltx_td ltx_align_center\">65.6</td>\n<td class=\"ltx_td ltx_align_center\">64.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">(3) w/o MSR &amp; MIR</td>\n<td class=\"ltx_td ltx_align_center\">76.5</td>\n<td class=\"ltx_td ltx_align_center\">77.9</td>\n<td class=\"ltx_td ltx_align_center\">64.8</td>\n<td class=\"ltx_td ltx_align_center\">61.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"5\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">C. Impact of Multitasks</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">(1) w/o ASR Error Detection</td>\n<td class=\"ltx_td ltx_align_center\">76.9</td>\n<td class=\"ltx_td ltx_align_center\">78.3</td>\n<td class=\"ltx_td ltx_align_center\">64.3</td>\n<td class=\"ltx_td ltx_align_center\">61.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">(2) w/o ASR Error Correction</td>\n<td class=\"ltx_td ltx_align_center\">78.0</td>\n<td class=\"ltx_td ltx_align_center\">79.2</td>\n<td class=\"ltx_td ltx_align_center\">65.9</td>\n<td class=\"ltx_td ltx_align_center\">64.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">(3) w/o AED &amp; AEC</td>\n<td class=\"ltx_td ltx_align_center\">77.3</td>\n<td class=\"ltx_td ltx_align_center\">78.6</td>\n<td class=\"ltx_td ltx_align_center\">65.1</td>\n<td class=\"ltx_td ltx_align_center\">62.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"5\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">D. Impact of Multistrategies</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">(1) w/o GAN</td>\n<td class=\"ltx_td ltx_align_center\">78.7</td>\n<td class=\"ltx_td ltx_align_center\">79.8</td>\n<td class=\"ltx_td ltx_align_center\">65.8</td>\n<td class=\"ltx_td ltx_align_center\">65.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">(2) w/o LCL</td>\n<td class=\"ltx_td ltx_align_center\">78.2</td>\n<td class=\"ltx_td ltx_align_center\">79.6</td>\n<td class=\"ltx_td ltx_align_center\">65.9</td>\n<td class=\"ltx_td ltx_align_center\">64.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">(3) w/o GAN &amp; LCL</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">78.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">79.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">65.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">64.1</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "speech",
            "study",
            "datasets",
            "multirepresentations",
            "“without”",
            "operation",
            "text",
            "modalityspecific",
            "denotes",
            "error",
            "“wo”",
            "only",
            "asr",
            "modalityinvariant",
            "aed",
            "wf1",
            "correction",
            "modality",
            "“concat”",
            "method",
            "aec",
            "ablation",
            "means",
            "detection",
            "iemocap",
            "msr",
            "mir",
            "acc",
            "multitasks",
            "lcl",
            "m4ser",
            "concat",
            "“concatenation”",
            "multistrategies",
            "multimodalities",
            "impact",
            "full",
            "meld",
            "gan"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Our proposed M<sup class=\"ltx_sup\">4</sup>SER model is composed of four types of learning: multimodal learning, multirepresentation learning, multitask learning, and multistrategy learning. To determine the impact of different types of learning on the performance of the emotion recognition task, ablation experiments were conducted on the IEMOCAP and MELD datasets, as presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multimodalities.</span>\nTo assess the necessity of multimodality, we conduct single-modal comparison experiments involving text and speech modalities. These experiments operate under a baseline framework excluding the MF module, LCL loss, and GAN loss, precluding intermodal interaction.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(A), in the single-modal experiments, the performance of speech modality in the IEMOCAP dataset is better than that of text modality, whereas the reverse is observed in the MELD dataset. This divergence suggests that emotional information is more distinctly conveyed through speech in IEMOCAP, whereas MELD leans towards text for emotional expression.\nMoreover, the multimodal baseline model, which simply concatenates speech and text features for recognition (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> A(3)), significantly enhances performance compared with single-modal baselines, highlighting the essential role of multimodal information.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multitasks.</span> First, we discuss the impact of the AED module. Specifically, we remove the AED module and introduce only the AEC module as an auxiliary task. This requires the model to correct all errors in each utterance from scratch, rather than only the detected errors. Evidently, the absence of the AED module leads to a significant drop in emotion recognition performance, demonstrating the necessity of the AED module. Similarly, we observe that the AEC module also plays an important role. Moreover, we note that the WA results using only the AEC module (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(C)(1)) are even worse than the results with neither the AED nor AEC module (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(C)(3)) in both datasets. This phenomenon occurs because directly using the neural machine translation (NMT) model for AEC can even increase the WER <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib60\" title=\"\">60</a>]</cite>. Unlike NMT tasks, which typically require modifying almost all input tokens, AEC involves fewer but more challenging corrections. For instance, if the ASR model&#8217;s WER is 10%, then only about 10% of the input tokens require correction in the AEC model. However, these tokens are often difficult to recognize and correct because they have already been misrecognized by the ASR model. Therefore, we should consider the characteristics of ASR outputs and carefully design the AEC model, which is why we introduce both AED and AEC as auxiliary tasks.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multistrategies.</span> To validate the effectiveness of the adversarial training strategy outlined in Alg. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#alg1\" title=\"Algorithm 1 &#8227; III-I Joint Training &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we remove the adversarial training strategy from M<sup class=\"ltx_sup\">4</sup>SER. The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(D)(1) show a decrease in performance on both datasets, demonstrating the critical role of this strategy in learning modality-invariant representations. The proposed modality discriminator effectively enhances the modality agnosticism of the refined representations from the generator.\nAdditionally, omitting the LCL strategy described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.SS8\" title=\"III-H Label-based Contrastive Learning (LCL) &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III-H</span></a> also results in similar performance degradation (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(D)(2)). We visualize the results before and after introducing this strategy to demonstrate its effectiveness, with specific analysis also detailed in the following t-SNE analysis section. Moreover, removing both of these strategies further diminishes emotion recognition performance, confirming that both learning strategies contribute significantly to performance improvement.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Multimodal speech emotion recognition (SER) has emerged as pivotal for improving human&#8211;machine interaction. Researchers are increasingly leveraging both speech and textual information obtained through automatic speech recognition (ASR) to comprehensively recognize emotional states from speakers. Although this approach reduces reliance on human-annotated text data, ASR errors possibly degrade emotion recognition performance.\nTo address this challenge, in our previous work, we introduced two auxiliary tasks, namely, ASR error detection and ASR error correction, and we proposed a novel multimodal fusion (MF) method for learning modality-specific and modality-invariant representations across different modalities.\nBuilding on this foundation, in this paper, we introduce two additional training strategies. First, we propose an adversarial network to enhance the diversity of modality-specific representations. Second, we introduce a label-based contrastive learning strategy to better capture emotional features.\nWe refer to our proposed method as M<sup class=\"ltx_sup\">4</sup>SER and validate its superiority over state-of-the-art methods through extensive experiments using IEMOCAP and MELD datasets.</p>\n\n",
                "matched_terms": [
                    "text",
                    "meld",
                    "modalityspecific",
                    "method",
                    "error",
                    "detection",
                    "iemocap",
                    "speech",
                    "asr",
                    "datasets",
                    "m4ser",
                    "correction",
                    "modalityinvariant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent research has shown that single-modal approaches struggle to meet the increasing demands for SER owing to their inherent complexity. As a solution, multimodal information has emerged as a viable option because it offers diverse emotional cues. A common strategy involves integrating speech and text modalities to achieve multimodal SER. Text data can be obtained through manual transcription or automatic speech recognition (ASR), which have been widely adopted in recent studies to reduce reliance on extensive annotated datasets.\nAlthough text-based pretrained models such as BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib6\" title=\"\">6</a>]</cite> and RoBERTa <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib7\" title=\"\">7</a>]</cite> have excelled in multimodal SER tasks, the accuracy of ASR remains equally crucial for achieving precise emotion recognition (ER) results. To mitigate the impact of ASR errors, Santoso et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib8\" title=\"\">8</a>]</cite> have integrated self-attention mechanisms and word-level confidence measurements, particularly reducing the importance of words with high error probabilities, thereby enhancing SER performance. However, this approach heavily depends on the performance of the ASR system, thus limiting its generalizability. Lin and Wang <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> have proposed an auxiliary ASR error detection task aimed at determining the probabilities of erroneous words to adjust the trust levels assigned to each word in the ASR hypothesis, thereby improving multimodal SER performance. However, this method focuses solely on error detection within the ASR hypothesis without directly correcting these errors, indicating that it does not fully enhance the coherence of semantic information.</p>\n\n",
                "matched_terms": [
                    "text",
                    "method",
                    "error",
                    "detection",
                    "speech",
                    "asr",
                    "datasets",
                    "impact"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the other hand, how to effectively integrate speech and text modalities has become a key focus. Previous methods have included simple feature concatenation, recurrent neural networks (RNNs), convolutional neural networks (CNNs), and cross-modal attention mechanisms <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib1\" title=\"\">1</a>]</cite>. Although these methods have shown some effectiveness, they often face challenges arising from differences in representations across different modalities.\nIn recent multimodal tasks such as sentiment analysis <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib11\" title=\"\">11</a>]</cite>, researchers have proposed learning two distinct types of representation to enhance multimodal learning.\nThe first is a representation tailored to each modality, that is, a modality-specific representation (MSR).\nThe second is modality-invariant representation (MIR), aimed at mapping all modalities of the same utterance into a shared subspace. This representation captures commonalities in multimodal signals as well as the speaker&#8217;s shared motives, which collectively affect the emotional state of the utterance through distribution alignment. By combining these two types of representation, one can obtain a comprehensive view of multimodal data can be provided for downstream tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib12\" title=\"\">12</a>]</cite>.\nHowever, these methods focus on utterance-level representations, which, while effective for short and isolated utterances, often fail to capture fine-grained emotional dynamics, especially in long and complex interactions. Such coarse-grained representations fail to capture subtle temporal cues and modality-specific variations that are crucial for accurate emotion recognition.\nMoreover, mapping entire utterances to a shared or modality-specific space using similarity loss oversimplifies the underlying multimodal relationships and hinders effective cross-modal alignment.</p>\n\n",
                "matched_terms": [
                    "text",
                    "modalityspecific",
                    "speech",
                    "msr",
                    "mir",
                    "modalityinvariant",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the basis of the above observations, in our previous work <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib13\" title=\"\">13</a>]</cite>, we introduced two auxiliary tasks: ASR error detection (AED) and ASR error correction (AEC). Specifically, we introduced an AED module to identify the positions of ASR errors. Subsequently, we employed an AEC module to correct these errors, enhancing the semantic coherence of ASR hypotheses and reducing the impact of ASR errors on ER tasks.\nAdditionally, we designed a novel multimodal fusion (MF) module for learning frame-level MSRs and MIRs in a shared speech&#8211;text modality space.\nWe observed the following shortcomings in our previous work: 1) Despite introducing the AED and AEC modules, uncorrected ASR errors continue to affect the accuracy of SER. 2) Existing MF methods possibly fail to capture subtle differences in emotional features.</p>\n\n",
                "matched_terms": [
                    "aec",
                    "error",
                    "detection",
                    "asr",
                    "aed",
                    "impact",
                    "correction",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these issues, we propose two learning strategies. First, we introduce adversarial learning to enhance the diversity and robustness of modality representations, thereby reducing the impact of ASR errors on SER tasks. Second, we introduce a contrastive learning strategy based on emotion labels to more accurately distinguish and capture feature differences between different emotion categories, thus improving the performance and robustness of the SER system.\nIn summary, our main contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "impact",
                    "asr",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce an adversarial modality discriminator to enhance the diversity of MSRs and improve the generalization of MIRs. This design alleviates modality mismatch and suppresses modality-specific noise introduced by ASR errors, which often cause misalignment between acoustic and textual features and degrade the robustness of multimodal fusion.</p>\n\n",
                "matched_terms": [
                    "modalityspecific",
                    "asr",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the basis of our previous work, we present M<sup class=\"ltx_sup\">4</sup>SER, an SER method that leverages multimodal, multirepresentation, multitask, and multistrategy learning. The difference between our M<sup class=\"ltx_sup\">4</sup>SER and previous SER methods is illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S1.F1\" title=\"Figure 1 &#8227; I Introduction &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. M<sup class=\"ltx_sup\">4</sup>SER mitigates the impact of ASR errors, improves modality-invariant representations, and captures commonalities between modalities to bridge their heterogeneity.</p>\n\n",
                "matched_terms": [
                    "method",
                    "asr",
                    "m4ser",
                    "impact",
                    "modalityinvariant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed M<sup class=\"ltx_sup\">4</sup>SER outperforms state-of-the-arts on the IEMOCAP and MELD datasets. Extensive experiments demonstrate its superiority in SER tasks.</p>\n\n",
                "matched_terms": [
                    "meld",
                    "iemocap",
                    "m4ser",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remaining sections of this paper are organized as follows: In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2\" title=\"II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">II</span></a>, we review related work. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3\" title=\"III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>, we discuss the specific model and method of M<sup class=\"ltx_sup\">4</sup>SER. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S4\" title=\"IV Experimental Setup &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>, we outline the experimental setup used in this study, including datasets, evaluation metrics, and implementation details. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5\" title=\"V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>, we verify the effectiveness of the proposed model. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S6\" title=\"VI Conclusion &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a>, we present our conclusion and future work.</p>\n\n",
                "matched_terms": [
                    "study",
                    "m4ser",
                    "datasets",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these limitations, multimodal SER often integrates both speech and text modalities, capturing both acoustic and semantic cues. A common setting involves using speech signals alongside textual transcripts (typically from manual annotations or ASR outputs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib24\" title=\"\">24</a>]</cite>.\nFan et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib21\" title=\"\">21</a>]</cite> proposed multi-granularity attention-based transformers (MGAT) to address emotional asynchrony and modality misalignment.\nSun et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib22\" title=\"\">22</a>]</cite> introduced a method integrating shared and private encoders, projecting each modality into a separate subspace to capture modality consistency and diversity while ensuring label consistency in the output.\nThese methods typically assume access to high-quality manual transcriptions for the text modality.</p>\n\n",
                "matched_terms": [
                    "text",
                    "method",
                    "speech",
                    "asr",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, growing efforts have focused on reducing the dependency on annotated text by directly leveraging ASR hypotheses as input <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib28\" title=\"\">28</a>]</cite>. Santoso et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib25\" title=\"\">25</a>]</cite> introduced confidence-aware self-attention mechanisms that downweight unreliable ASR tokens to mitigate error propagation. Lin and Wang&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> proposed a robust multimodal SER framework that adaptively fuses attention-weighted acoustic representations with ASR-derived text embeddings, compensating for recognition noise in the transcript.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Learning modality-specific representations (MSRs) and modality-invariant representations (MIRs) has proven effective for multimodal SER, as it enables models to capture both shared semantics across modalities and complementary modality-unique cues.\nHazarika et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib10\" title=\"\">10</a>]</cite> proposed MISA, which disentangles each modality into invariant and specific subspaces through factorization, facilitating effective fusion for emotion prediction.\nLiu et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib29\" title=\"\">29</a>]</cite> proposed a modality-robust MER framework that introduces a contrastive learning module to extract MIRs from full-modality inputs, and an imagination module that reconstructs missing modalities based on the MIRs, enhancing robustness under incomplete input conditions.\nTo explicitly reduce distribution gaps and mitigate feature redundancy, Yang et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib11\" title=\"\">11</a>]</cite> proposed FDMER, which extracts both MSRs and MIRs, combined with consistency&#8211;disparity constraints and a modality discriminator to encourage disentangled learning.\nLiu et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib30\" title=\"\">30</a>]</cite> presented NORM&#8209;TR, introducing a noise&#8209;resistant generic feature (NRGF) extractor trained with noise&#8209;aware adversarial objectives, followed by a multimodal transformer that fuses NRGFs with raw modality features for robust representation learning.</p>\n\n",
                "matched_terms": [
                    "modalityspecific",
                    "modalityinvariant",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to factorization-based approaches which explicitly disentangle modality inputs into shared and private spaces, our M<sup class=\"ltx_sup\">4</sup>SER framework adopts a structured encoding&#8211;generation strategy to implicitly model MSRs and MIRs. Specifically, we utilize a stack of cross-modal encoder blocks to extract token-aware and speech-aware features (MSRs), and then apply a dedicated generator composed of hybrid-modal attention and masking mechanisms to derive the MIRs. In addition, unlike prior works that incorporate adversarial learning at a global or representation-level, our M<sup class=\"ltx_sup\">4</sup>SER employs a frame-level discriminator that operates on each temporal representation. A frame-level discriminator attempts to distinguish the modality origin of each temporal representation, while the MIR generator is optimized to deceive the discriminator by producing modality-agnostic outputs that are hard to classify as either text or speech. This fine-grained, per-frame adversarial constraint leads to more robust and aligned cross-modal representations. Moreover, M<sup class=\"ltx_sup\">4</sup>SER integrates this disentanglement framework into a multitask learning setup with ASR-aware auxiliary supervision, enabling more effective adaptation to noisy real-world speech&#8211;text scenarios.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "mir",
                    "m4ser",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ASR error correction (AEC) techniques remain effective in optimizing the ASR hypotheses. AEC has been jointly trained with ER tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib13\" title=\"\">13</a>]</cite> in multitask settings. Li et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib27\" title=\"\">27</a>]</cite> proposed an AEC method that improves transcription quality on low-resource out-of-domain data through cross-modal training and the incorporation of discrete speech units, and they validated its effectiveness in downstream ER tasks. Lin and Wang <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> introduced a robust ER method that leverages complementary semantic information from audio, adapts to ASR errors through an auxiliary task for ASR error detection, and fuses text and acoustic representations for ER.\nOn the basis of <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib32\" title=\"\">32</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib33\" title=\"\">33</a>]</cite>, we propose a partially autoregressive AEC method that uses a label predictor to restrict decoding to only desirable parts of the input sequence embeddings, thereby reducing inference latency.</p>\n\n",
                "matched_terms": [
                    "text",
                    "method",
                    "aec",
                    "error",
                    "detection",
                    "only",
                    "speech",
                    "asr",
                    "correction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed M<sup class=\"ltx_sup\">4</sup>SER framework can be formalized as the function <math alttext=\"f(S,T)=(L,Z)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mi>L</mi><mo>,</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f(S,T)=(L,Z)</annotation></semantics></math>. Here, the speech modality <math alttext=\"S=(s_{1},s_{2},\\cdots,s_{m})\\in\\mathbb{R}^{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>s</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>s</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>m</mi></msup></mrow><annotation encoding=\"application/x-tex\">S=(s_{1},s_{2},\\cdots,s_{m})\\in\\mathbb{R}^{m}</annotation></semantics></math> consists of <span class=\"ltx_text ltx_font_italic\">m</span> frames extracted from an utterance, whereas the text modality <math alttext=\"T=(t_{1},t_{2},\\cdots,t_{n})\\in\\mathbb{R}^{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><msub><mi>t</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>t</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>n</mi></msup></mrow><annotation encoding=\"application/x-tex\">T=(t_{1},t_{2},\\cdots,t_{n})\\in\\mathbb{R}^{n}</annotation></semantics></math> comprises <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> tokens from the original ASR hypotheses of the utterance. All tokens are mapped to a predefined WordPiece vocabulary <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib40\" title=\"\">40</a>]</cite>. Additionally, within our multitask learning framework, the primary task is ER, yielding an emotion classification output <math alttext=\"L\\in\\{l_{1},l_{2},\\cdots,l_{e}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>l</mi><mn>1</mn></msub><mo>,</mo><msub><mi>l</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>l</mi><mi>e</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">L\\in\\{l_{1},l_{2},\\cdots,l_{e}\\}</annotation></semantics></math>, where <math alttext=\"e\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mi>e</mi><annotation encoding=\"application/x-tex\">e</annotation></semantics></math> represents the number of emotional categories. Concurrently, the auxiliary tasks include ASR error detection (AED) and ASR error correction (AEC). The output of these auxiliary tasks is <math alttext=\"Z\\in\\{Z_{1},Z_{2},\\cdots,Z_{k}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>Z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>Z</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>Z</mi><mi>k</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Z\\in\\{Z_{1},Z_{2},\\cdots,Z_{k}\\}</annotation></semantics></math>, representing all the ground truth (GT) sequences where the ASR transcriptions differ from the human-annotated transcriptions.\n<math alttext=\"Z_{k}=(z_{1,k},z_{2,k},\\cdots,z_{c,k})\\in\\mathbb{R}^{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><msub><mi>Z</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mn>1</mn><mo>,</mo><mi>k</mi></mrow></msub><mo>,</mo><msub><mi>z</mi><mrow><mn>2</mn><mo>,</mo><mi>k</mi></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>z</mi><mrow><mi>c</mi><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>c</mi></msup></mrow><annotation encoding=\"application/x-tex\">Z_{k}=(z_{1,k},z_{2,k},\\cdots,z_{c,k})\\in\\mathbb{R}^{c}</annotation></semantics></math>, denoting that the <math alttext=\"k^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\"><semantics><msup><mi>k</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">k^{th}</annotation></semantics></math> error position includes <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m10\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> tokens.</p>\n\n",
                "matched_terms": [
                    "text",
                    "aec",
                    "error",
                    "detection",
                    "speech",
                    "asr",
                    "aed",
                    "m4ser",
                    "correction",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overall architecture of the M<sup class=\"ltx_sup\">4</sup>SER model is\ncomposed of five modules, namely, the embedding module, AED module, AEC module, MF module, and ER module, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. In the following section, we provide a detailed explanation of each module.</p>\n\n",
                "matched_terms": [
                    "aec",
                    "m4ser",
                    "aed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contextual Speech Representations.</span> To obtain comprehensive contextual representations of acoustic features, we utilize a pretrained SSL model, HuBERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib4\" title=\"\">4</a>]</cite>, as our acoustic encoder. HuBERT integrates CNN layers with a transformer encoder to effectively capture both speech features and contextual information.\nWe denote the acoustic hidden representations of the speech modality inputs <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> generated by HuBERT as <math alttext=\"H_{S}=(h_{S}^{(1)},h_{S}^{(2)},\\cdots,h_{S}^{(m)})\\in\\mathbb{R}^{m\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>H</mi><mi>S</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msubsup><mi>h</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">H_{S}=(h_{S}^{(1)},h_{S}^{(2)},\\cdots,h_{S}^{(m)})\\in\\mathbb{R}^{m\\times d}</annotation></semantics></math>, where <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> is the dimension of hidden representations.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contextual Token Representations.</span> We use the pretrained language model BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib6\" title=\"\">6</a>]</cite> as our text encoder to obtain the token representations <math alttext=\"H_{T}=(h_{T}^{(1)},h_{T}^{(2)},\\cdots,h_{T}^{(n)})\\in\\mathbb{R}^{n\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>H</mi><mi>T</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msubsup><mi>h</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">H_{T}=(h_{T}^{(1)},h_{T}^{(2)},\\cdots,h_{T}^{(n)})\\in\\mathbb{R}^{n\\times d}</annotation></semantics></math> for the text modality inputs <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first subtask we introduce is AED, which is designed to detect the positions of ASR errors.\nSimilarly to <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib31\" title=\"\">31</a>]</cite>, we align <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> and <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> by determining the longest common subsequence between them. The aligned tokens are labeled <span class=\"ltx_text ltx_font_italic\">KEEP</span> (<span class=\"ltx_text ltx_font_bold\">K</span>), whereas the remaining tokens are labeled <span class=\"ltx_text ltx_font_italic\">DELETE</span> (<span class=\"ltx_text ltx_font_bold\">D</span>) or <span class=\"ltx_text ltx_font_italic\">CHANGE</span> (<span class=\"ltx_text ltx_font_bold\">C</span>). A specific example is illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(b).\nThe label prediction layer is a straightforward fully connected (FC) layer with three classes.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "aed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a second subtask called AEC, which aims to correct the errors determined by AED.\nUnlike conventional autoregressive decoders that start decoding from scratch, our decoder operates in parallel to the tokens predicted as <span class=\"ltx_text ltx_font_bold\">C</span>.</p>\n\n",
                "matched_terms": [
                    "aec",
                    "aed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More specifically, once the erroneous positions are identified, the decoder constructs a separate generation sequence for each token labeled as <span class=\"ltx_text ltx_font_bold\">C</span>. Each sequence begins with a special start token <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_italic\">BOS<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span> and generates the corrected text in an autoregressive manner.\nAt each decoding step, the decoder input for a given correction sequence consists of all previously generated tokens, each concatenated with the representation of the corresponding erroneous token.\nAll correction sequences are processed in parallel by a shared transformer decoder, which attends to the full ASR representation <math alttext=\"H_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m3\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">H_{T}</annotation></semantics></math> as memory.</p>\n\n",
                "matched_terms": [
                    "text",
                    "full",
                    "asr",
                    "correction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where &#8220;<math alttext=\"\\parallel\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m8\" intent=\":literal\"><semantics><mo>&#8741;</mo><annotation encoding=\"application/x-tex\">\\parallel</annotation></semantics></math>&#8221; denotes a concatenation operation along the feature dimension.\nHere, <math alttext=\"h_{T}^{(k)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m9\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">h_{T}^{(k)}</annotation></semantics></math> is the representation of the <math alttext=\"k^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m10\" intent=\":literal\"><semantics><msup><mi>k</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">k^{th}</annotation></semantics></math> erroneous token, repeated at each decoding position <math alttext=\"1{:}t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m11\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">1{:}t</annotation></semantics></math>.\nThen, a generic transformer decoder <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib41\" title=\"\">41</a>]</cite> is applied to obtain the decoder layer output, where the query is <math alttext=\"H_{1:t,k}^{(z)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m12\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi>t</mi><mo>,</mo><mi>k</mi></mrow></mrow><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{1:t,k}^{(z)}</annotation></semantics></math> and both the key and value are <math alttext=\"H_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m13\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">H_{T}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "operation",
                    "denotes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the basis of <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib13\" title=\"\">13</a>]</cite>, our MF module is composed of three cross-modal encoder (CME) blocks and one MIR generator. The objective is to facilitate the learning of modality-specific and modality-invariant representations.\nIn this section, we provide an in-depth explanation of the operation of each CME block and the MIR generator.</p>\n\n",
                "matched_terms": [
                    "operation",
                    "modalityspecific",
                    "modalityinvariant",
                    "mir"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">MIR generator</span> utilizes a hybrid-modal attention (HMA) module to extract the shared information from each modality-specific representation that pertains to both modalities, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.F4\" title=\"Figure 4 &#8227; III-E Multimodal Fusion (MF) Module &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(a):</p>\n\n",
                "matched_terms": [
                    "mir",
                    "modalityspecific"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> denotes either the speech or text modality and <math alttext=\"H_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m2\" intent=\":literal\"><semantics><msub><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">H_{ST}</annotation></semantics></math> represents the concatenation of <math alttext=\"H_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m3\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">H_{S}</annotation></semantics></math> and <math alttext=\"H_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m4\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">H_{T}</annotation></semantics></math>, which is then fed into a FC layer to map it back to the same dimension as <math alttext=\"H_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m5\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">H_{S}</annotation></semantics></math>. This process integrates both speech and text information into a unified representation. The resulting features are subsequently summed with <math alttext=\"H_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m6\" intent=\":literal\"><semantics><msub><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">H_{ST}</annotation></semantics></math>, culminating in the ultimate modality-invariant representation:</p>\n\n",
                "matched_terms": [
                    "text",
                    "denotes",
                    "speech",
                    "modalityinvariant",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance the feature&#8217;s modality invariance, a parallel convolutional network is employed to learn a mask that filters out modality-specific information:</p>\n\n",
                "matched_terms": [
                    "modalityspecific",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where &#8220;<math alttext=\"\\sigma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p5.m1\" intent=\":literal\"><semantics><mi>&#963;</mi><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math>&#8221; denotes Sigmoid activation and &#8220;<math alttext=\"\\otimes\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p5.m2\" intent=\":literal\"><semantics><mo>&#8855;</mo><annotation encoding=\"application/x-tex\">\\otimes</annotation></semantics></math>&#8221; indicates element-wise multiplication.\nFinally, the modality-specific and modality-invariant representations are concatenated together to obtain the final multimodal fusion representation <math alttext=\"H_{ST}^{\\rm(fus)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p5.m3\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>fus</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{ST}^{\\rm(fus)}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "modalityspecific",
                    "modalityinvariant",
                    "denotes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further develop the modality discriminator <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m1\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> utilizing the modality-invariant representations produced by the MIR generator. This discriminator illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(f) is composed of two linear layers with a ReLU activation function in between, followed by a Sigmoid activation function. To enhance the MIR&#8217;s capability to remain modality-agnostic, we employ adversarial learning techniques.\nSpecifically, the discriminator <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> outputs a scalar value between 0 and 1 for each frame, where values close to 0 indicate the text modality, values close to 1 indicate the speech modality, and values near 0.5 represent an ambiguous modality.\nHere, <math alttext=\"D(H)\\in\\mathbb{R}^{m\\times 1},H\\in\\{H^{(spe)}_{T},H^{(spe)}_{S},H_{ST}^{\\rm(inv)}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>D</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow></msup></mrow><mo>,</mo><mrow><mi>H</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>inv</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">D(H)\\in\\mathbb{R}^{m\\times 1},H\\in\\{H^{(spe)}_{T},H^{(spe)}_{S},H_{ST}^{\\rm(inv)}\\}</annotation></semantics></math>.\nOn one hand, we aim for the discriminator that correctly classifies frames in the modality-specific representations <math alttext=\"H^{(spe)}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H^{(spe)}_{T}</annotation></semantics></math> and <math alttext=\"H^{(spe)}_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m5\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H^{(spe)}_{S}</annotation></semantics></math> as 0 and 1, respectively. On the other hand, to enhance the modality agnosticism of the modality-invariant representation <math alttext=\"H_{ST}^{\\rm(inv)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m6\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>inv</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{ST}^{\\rm(inv)}</annotation></semantics></math> produced by the MIR generator, we aim for a discriminator that outputs values close to 0.5, indicating an ambiguous modality. With this design of the MIR generator and discriminator, we define a generative adversarial training objective, mathematically represented as</p>\n\n",
                "matched_terms": [
                    "text",
                    "modalityspecific",
                    "speech",
                    "mir",
                    "modalityinvariant",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"H_{ST}^{\\rm(inv)}=G(H_{S}^{\\rm(spe)},H_{T}^{\\rm(spe)},H_{ST})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m7\" intent=\":literal\"><semantics><mrow><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>inv</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>=</mo><mrow><mi>G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mi>spe</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mi>spe</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msub><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">H_{ST}^{\\rm(inv)}=G(H_{S}^{\\rm(spe)},H_{T}^{\\rm(spe)},H_{ST})</annotation></semantics></math> is the output of the MIR generator as shown in Eq. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.E13\" title=\"In III-E Multimodal Fusion (MF) Module &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>) and <math alttext=\"\\mathbb{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m8\" intent=\":literal\"><semantics><mi>&#120124;</mi><annotation encoding=\"application/x-tex\">\\mathbb{E}</annotation></semantics></math> denotes the expectation over all temporal frames in a batch.</p>\n\n",
                "matched_terms": [
                    "mir",
                    "denotes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance the model&#8217;s capability to learn emotion features from multimodal data, we employ a label-based contrastive learning task to complement the cross-entropy loss, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(g). This task aids the model in extracting emotion-related features when the MF module integrates speech and text data. As depicted in the LCL task in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.F5\" title=\"Figure 5 &#8227; III-H Label-based Contrastive Learning (LCL) &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we categorize data in each batch into positive and negative samples on the basis of emotion labels. For instance, in a batch containing eight samples, we compute the set of positive samples for each sample, where those with the same label are considered positive samples (yellow squares), and those with different labels are considered negative samples (white squares). We then calculate the label-based contrastive loss (LCL) using Eq. <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:eq:lcl</span>, which promotes instances of a specific label to be more similar to each other than instances of other labels.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "lcl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the training stage, the learning process is optimized using three loss functions that correspond to ER, AED, and AEC (i.e., <math alttext=\"\\mathcal{L}_{\\rm ER}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>ER</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm ER}</annotation></semantics></math>, <math alttext=\"\\mathcal{L}_{\\rm AED}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>AED</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm AED}</annotation></semantics></math>, and <math alttext=\"\\mathcal{L}_{\\rm AEC}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>AEC</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm AEC}</annotation></semantics></math>), and two training strategies (i.e., <math alttext=\"\\mathcal{L}_{\\rm GAN}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>GAN</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm GAN}</annotation></semantics></math> and <math alttext=\"\\mathcal{L}_{\\rm LCL}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>LCL</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm LCL}</annotation></semantics></math>). The specific optimization process of M<sup class=\"ltx_sup\">4</sup>SER is detailed in Alg. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#alg1\" title=\"Algorithm 1 &#8227; III-I Joint Training &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nThese five loss functions are linearly combined as the overall training objective of M<sup class=\"ltx_sup\">4</sup>SER:</p>\n\n",
                "matched_terms": [
                    "aec",
                    "aed",
                    "lcl",
                    "m4ser",
                    "gan"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the GAN training strategy <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib34\" title=\"\">34</a>]</cite>, we divide the backpropagation process into two steps. Firstly, we maximize <math alttext=\"\\mathcal{L}_{\\rm GAN}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>GAN</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm GAN}</annotation></semantics></math> to update the discriminator, during which the generator is detached from the optimization. According to Eq. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.Ex1\" title=\"III-G Modality Discriminator &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III-G</span></a>), on one hand, maximizing the first term <math alttext=\"L_{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>D</mi></msub><annotation encoding=\"application/x-tex\">L_{D}</annotation></semantics></math> of <math alttext=\"\\mathcal{L}_{\\rm GAN}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>GAN</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm GAN}</annotation></semantics></math> essentially trains the discriminator to correctly distinguish between text and audio features by making <math alttext=\"H_{S}^{\\rm(spe)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mi>spe</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{S}^{\\rm(spe)}</annotation></semantics></math> close to 1 and <math alttext=\"H_{T}^{\\rm(spe)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mi>spe</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{T}^{\\rm(spe)}</annotation></semantics></math> close to 0 <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Function <math alttext=\"\\log(x)+\\log(1-y)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>&#8722;</mo><mi>y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(x)+\\log(1-y)</annotation></semantics></math>, where <math alttext=\"x,y\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x,y\\in(0,1)</annotation></semantics></math> reaches its maximum when <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m3\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> approaches 1 and <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m4\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math> approaches 0.</span></span></span>. On the other hand, maximizing the second term <math alttext=\"L_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m6\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>G</mi></msub><annotation encoding=\"application/x-tex\">L_{G}</annotation></semantics></math> (i.e., minimizing <math alttext=\"-L_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m7\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><msub><mi>L</mi><mi>G</mi></msub></mrow><annotation encoding=\"application/x-tex\">-L_{G}</annotation></semantics></math>) will make <math alttext=\"H^{\\text{(inv)}}_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mtext>(inv)</mtext></msubsup><annotation encoding=\"application/x-tex\">H^{\\text{(inv)}}_{ST}</annotation></semantics></math> approach 0 or 1 <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Function <math alttext=\"\\log(x)+\\log(1-x)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>&#8722;</mo><mi>x</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(x)+\\log(1-x)</annotation></semantics></math>, where <math alttext=\"x\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m2\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x\\in(0,1)</annotation></semantics></math> reaches its maximum at <math alttext=\"x=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m3\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">x=0.5</annotation></semantics></math> and its minimum near <math alttext=\"x=0\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m4\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">x=0</annotation></semantics></math> and <math alttext=\"x=1\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m5\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">x=1</annotation></semantics></math>.</span></span></span>, indicating that the discriminator recognizes <math alttext=\"H^{\\text{(inv)}}_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m9\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mtext>(inv)</mtext></msubsup><annotation encoding=\"application/x-tex\">H^{\\text{(inv)}}_{ST}</annotation></semantics></math> as modality-specific, which can be either text or audio, contrary to our desired modality invariance. Secondly, we freeze the discriminator parameters and update the remaining parameters by minimizing <math alttext=\"L_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m10\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>G</mi></msub><annotation encoding=\"application/x-tex\">L_{G}</annotation></semantics></math>, which makes the output of the discriminator <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m11\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> for the modality-invariant features <math alttext=\"H^{\\text{(inv)}}_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m12\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mtext>(inv)</mtext></msubsup><annotation encoding=\"application/x-tex\">H^{\\text{(inv)}}_{ST}</annotation></semantics></math> approach 0.5, thereby blurring these features between audio and text modalities to achieve modality agnosticism. Additionally, <math alttext=\"\\mathcal{L}_{\\text{ER}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m13\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ER</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ER}}</annotation></semantics></math> optimizes the downstream emotion recognition model, <math alttext=\"\\mathcal{L}_{\\text{AED}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m14\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>AED</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{AED}}</annotation></semantics></math> and <math alttext=\"\\mathcal{L}_{\\text{AEC}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m15\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>AEC</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{AEC}}</annotation></semantics></math> optimize the auxiliary tasks of ASR error detection and ASR error correction models, respectively, and <math alttext=\"\\mathcal{L}_{\\text{LCL}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m16\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>LCL</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{LCL}}</annotation></semantics></math> implements the LCL strategy. The entire system is trained in an end-to-end manner and optimized by adjusting weight parameters.</p>\n\n",
                "matched_terms": [
                    "text",
                    "modalityspecific",
                    "error",
                    "detection",
                    "asr",
                    "gan",
                    "lcl",
                    "correction",
                    "modalityinvariant",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the inference stage, the AED and AEC modules are excluded. The remaining modules accept speech and ASR transcripts as input and output emotion classification results.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "aec",
                    "asr",
                    "aed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the effectiveness\nof our proposed model, we carried out experiments on two public datasets:\nthe Interactive Emotional Dyadic Motion Capture (IEMOCAP) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib45\" title=\"\">45</a>]</cite> and the Multimodal Emotion Lines Dataset (MELD) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib46\" title=\"\">46</a>]</cite>. The statistics of the two datasets are shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S4.F6\" title=\"Figure 6 &#8227; IV-A Dataset and Evaluation Metrics &#8227; IV Experimental Setup &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "meld",
                    "iemocap",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">IEMOCAP</span> comprised roughly 12 hours of speech from ten speakers participating in five scripted sessions. Following prior work <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib47\" title=\"\">47</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> on IEMOCAP, we employed 5531 utterances that were categorized into four emotion categories: &#8220;Neutral&#8221; (1,708), &#8220;Angry&#8221; (1,103), &#8220;Happy&#8221; (including &#8220;Excited&#8221;) (595 + 1,041 = 1,636), and &#8220;Sad&#8221; (1,084).\nWe performed five-fold leave-one-session-out (LOSO) cross-validation to evaluate the emotion classification performance using weighted accuracy (WA) and unweighted accuracy (UA).</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MELD</span> included a total of 13708 samples extracted from the TV series Friends, divided into 9,989 for training, 1,109 for validation, and 2,610 for testing. The dataset contained seven labels: &#8220;Neutral&#8221; (6167), &#8220;Happy&#8221; (2198), &#8220;Fear&#8221; (350), &#8220;Sad&#8221; (985), &#8220;Disgust&#8221; (355), &#8220;Angry&#8221; (1541), and &#8220;Surprise&#8221; (1499).\nWe tuned hyperparameters on the validation set and reported final results on the test set using the best checkpoint, evaluated by accuracy (ACC) and weighted F1-score (W-F1).</p>\n\n",
                "matched_terms": [
                    "wf1",
                    "acc",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The acoustic encoder was initialized using the <span class=\"ltx_text ltx_font_typewriter\">hubert-base-ls960<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">3</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/facebook/hubert-base-ls960\" title=\"\">https://huggingface.co/facebook/hubert-base-ls960</a></span></span></span></span> model, producing acoustic representations <math alttext=\"d_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">d_{S}</annotation></semantics></math> with a dimensionality of 768.\nThe text encoder employed the <span class=\"ltx_text ltx_font_typewriter\">bert-base-uncased<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">4</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/google-bert/bert-base-uncased\" title=\"\">https://huggingface.co/google-bert/bert-base-uncased</a></span></span></span></span> model for initialization, yielding token representations <math alttext=\"d_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">d_{T}</annotation></semantics></math> with a dimensionality of 768. The vocabulary size for word tokenization <math alttext=\"d_{vocab}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>d</mi><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">d_{vocab}</annotation></semantics></math> was set to 30,522. We set the hidden size <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> to 768, the number of attention layers to 12, and the number of attention heads to 12. Both the HuBERT and BERT models were fine-tuned during the training stage.\nThe transformer decoder in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c) adopted a single-layer transformer with a hidden size of 768. Additionally, we obtained corresponding ASR hypotheses using the Whisper ASR model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib48\" title=\"\">48</a>]</cite>. We used the <span class=\"ltx_text ltx_font_typewriter\">openai/whisper-medium.en<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">5</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/openai/whisper-medium.en\" title=\"\">https://huggingface.co/openai/whisper-medium.en</a></span></span></span></span> and <span class=\"ltx_text ltx_font_typewriter\">openai/whisper-large-v2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">6</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/openai/whisper-large-v2\" title=\"\">https://huggingface.co/openai/whisper-large-v2</a></span></span></span></span> models, achieving word error rates (WERs) of 20.48% and 37.87% on the IEMOCAP and MELD datasets, respectively.</p>\n\n",
                "matched_terms": [
                    "text",
                    "error",
                    "iemocap",
                    "asr",
                    "datasets",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used Adam <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib49\" title=\"\">49</a>]</cite> as the optimizer with a batch size of 16. On the basis of empirical observations and prior work <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib50\" title=\"\">50</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib51\" title=\"\">51</a>]</cite>, we kept the learning rate constant at <math alttext=\"1e^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-5}</annotation></semantics></math> for IEMOCAP and <math alttext=\"1e^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-4}</annotation></semantics></math> for MELD during training. For our multitask learning setup, we set <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m3\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> to 3 and <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m4\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> to 0.1. For our multistrategy learning setup, we set <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m5\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> to 0.01 and <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m6\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> to 0.1. The temperature parameter <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m7\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> was set to 0.07. To validate these choices, we additionally performed a one-at-a-time sensitivity analysis on the hyperparameters <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m8\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m9\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>, <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m10\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>, and <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m11\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math>, as presented in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.SS3\" title=\"V-C Sensitivity Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">V-C</span></a> and illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F7\" title=\"Figure 7 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the experiments, we compared various multimodal speech emotion recognition SOTA methods with our proposed method, M<sup class=\"ltx_sup\">4</sup>SER.\nThe comparison was conducted by the following methods on the IEMOCAP dataset:</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "speech",
                    "method",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SAWC</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib47\" title=\"\">47</a>]</cite> adjusts importance weights using confidence measures, reducing ASR error impact by emphasizing the corresponding speech segments.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "asr",
                    "impact",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RMSER-AEA</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> uses complementary semantic information, adapting to ASR errors through an auxiliary task and fusing text and acoustic representations for SER.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <span class=\"ltx_text ltx_font_bold\">MF-AED-AEC</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib13\" title=\"\">13</a>]</cite>, as in our previous work, we consider two auxiliary tasks to improve semantic coherence in ASR text and introduce a MF method to learn shared representations.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Full</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib55\" title=\"\">55</a>]</cite> uses contextual cross-modal transformers and graph convolutional networks for enhanced emotion representations and modality fusion.</p>\n\n",
                "matched_terms": [
                    "full",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T2\" title=\"TABLE II &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> compare the performance of the M<sup class=\"ltx_sup\">4</sup>SER method with those of recent multimodal SER methods on the IEMOCAP and MELD datasets. Our proposed M<sup class=\"ltx_sup\">4</sup>SER achieves a WA of 79.2% and a UA of 80.1% on IEMOCAP, and an ACC of 66.5% and a W-F1 of 66.0% on MELD, outperforming other models and demonstrating the superiority of M<sup class=\"ltx_sup\">4</sup>SER. Specifically, on the IEMOCAP dataset, compared with the recent MAF-DCT, the M<sup class=\"ltx_sup\">4</sup>SER method shows a 0.7% improvement in WA and a 0.8% improvement in UA, reaching a new SOTA performance. These improvements stem from our M<sup class=\"ltx_sup\">4</sup>SER&#8217;s capability to mitigate the impact of ASR errors, capture modality-specific and modality-invariant representations across different modalities, enhance commonality between modalities through adversarial learning, and excel in emotion feature learning with the LCL loss.</p>\n\n",
                "matched_terms": [
                    "meld",
                    "modalityspecific",
                    "method",
                    "iemocap",
                    "asr",
                    "datasets",
                    "impact",
                    "acc",
                    "lcl",
                    "m4ser",
                    "wf1",
                    "modalityinvariant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multirepresentations.</span> We study the importance of modality-invariant and modality-specific representations by discarding each type. Removing modality-specific representations from multimodal fusion significantly reduces downstream emotion recognition performance on both datasets, confirming their effectiveness in capturing and utilizing unique information from each modality. Similarly, omitting refined modality-invariant representations from multimodal fusion also significantly decreases emotion recognition performance on both datasets, demonstrating their importance in bridging modality gaps. Clearly, removing both representations further decreases performance.</p>\n\n",
                "matched_terms": [
                    "modalityspecific",
                    "study",
                    "datasets",
                    "multirepresentations",
                    "impact",
                    "modalityinvariant",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:703.2pt;height:86pt;vertical-align:-40.5pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Type</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Modality</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Params (M)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Train Time (h)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Infer Time (ms/U)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">WA (%)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">UA (%)</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_2\">Single-modal</span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">HuBERT</span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">S</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">316.2</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">5.4</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">23.7 &#177; 0.1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">68.9</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt\">69.8</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\">BERT</span>\n<span class=\"ltx_td ltx_align_left ltx_border_r\">T(ASR)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">109.5</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">0.5</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">6.3 &#177; 0.1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">65.1</span>\n<span class=\"ltx_td ltx_align_center\">66.4</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_4\">Multi-modal</span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Baseline (HuBERT + BERT)</span>\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_4\">S+T(ASR)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">426.9</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">9.6</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">29.6 &#177; 0.1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">73.4</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">75.2</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\">&#8194;&#8202;&#8195;+ MSR &amp; MIR</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">481.4</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">12.0</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">44.8 &#177; 0.1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">76.6</span>\n<span class=\"ltx_td ltx_align_center\">77.4</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\">&#8194;&#8202;&#8195;&#8195;+ GAN &amp; LCL</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">481.6</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">20.8</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">44.5 &#177; 0.6</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">77.3</span>\n<span class=\"ltx_td ltx_align_center\">78.6</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">&#8195;&#8195;&#8195;&#8196;&#8202;+ AED &amp; AEC (M<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">4</span></sup>SER)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">512.9</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">21.5</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">44.7 &#177; 0.2</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">79.2</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">80.1</span></span></span>\n</span></span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "aec",
                    "msr",
                    "aed",
                    "mir",
                    "gan",
                    "lcl",
                    "m4ser",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a one-at-a-time sensitivity analysis on four key hyperparameters: the auxiliary task weight <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, the AED/AEC balancing factor <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m2\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>, the GAN loss weight <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m3\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>, and the LCL loss weight <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m4\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math>. As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F7\" title=\"Figure 7 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, model performance is moderately sensitive to <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m5\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m6\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math>, with optimal results achieved at <math alttext=\"\\alpha=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.1</annotation></semantics></math> and <math alttext=\"\\lambda=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m8\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=0.1</annotation></semantics></math>. Excessively small values reduce the effectiveness of auxiliary guidance, while overly large values interfere with the main task.</p>\n\n",
                "matched_terms": [
                    "lcl",
                    "gan"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the model exhibits relative robustness to variations in <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m1\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>, indicating stable synergy between the AED and AEC objectives. Conversely, performance is highly sensitive to <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m2\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>; large values result in unstable training due to adversarial objectives, while a small weight (e.g., <math alttext=\"\\gamma=0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=0.01</annotation></semantics></math>) ensures stable convergence and optimal results.</p>\n\n",
                "matched_terms": [
                    "aec",
                    "aed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F8\" title=\"Figure 8 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows that the multitask objectives\n(<math alttext=\"\\mathcal{L}_{\\rm ER}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>ER</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm ER}</annotation></semantics></math>, <math alttext=\"\\mathcal{L}_{\\rm AED}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>AED</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm AED}</annotation></semantics></math>, <math alttext=\"\\mathcal{L}_{\\rm AEC}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>AEC</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm AEC}</annotation></semantics></math>)\nrapidly decrease and stabilize, confirming that the auxiliary tasks provide\nuseful supervision for ER. The multistrategy objectives also\nexhibit stable convergence: <math alttext=\"\\mathcal{L}_{\\rm LCL}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>LCL</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm LCL}</annotation></semantics></math> quickly decreases under label\nsupervision, while for the adversarial component, <math alttext=\"\\mathcal{L}_{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>D</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{D}</annotation></semantics></math> rises from\nnegative values to near zero as the discriminator learns, <math alttext=\"\\mathcal{L}_{\\rm GAN}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>GAN</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm GAN}</annotation></semantics></math>\nsteadily increases and plateaus, and <math alttext=\"\\mathcal{L}_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m7\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>G</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{G}</annotation></semantics></math> exhibits fluctuations, reflecting the adversarial interplay,\nand eventually converges as <math alttext=\"D(H_{ST}^{\\mathrm{(inv)}})\\to 0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m8\" intent=\":literal\"><semantics><mrow><mrow><mi>D</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>inv</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">&#8594;</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">D(H_{ST}^{\\mathrm{(inv)}})\\to 0.5</annotation></semantics></math>, which corresponds to about 1.386 in the <math alttext=\"\\mathcal{L}_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m9\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>G</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{G}</annotation></semantics></math> term of Eq.&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.Ex1\" title=\"III-G Modality Discriminator &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III-G</span></a>),\nindicating that an adversarial equilibrium is achieved. Overall, both multitask and multistrategy objectives\nare optimized in a stable manner, validating the effectiveness of the M<sup class=\"ltx_sup\">4</sup>SER.</p>\n\n",
                "matched_terms": [
                    "aec",
                    "aed",
                    "lcl",
                    "m4ser",
                    "gan"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our full model M<sup class=\"ltx_sup\">4</sup>SER achieves the best performance with WA of 79.2% and UA of 80.1%. Although its training time increases to 21.5 hours owing to the inclusion of GAN and LCL strategies and AED and AEC subtasks, these components are used only during training. Consequently, the inference latency of M<sup class=\"ltx_sup\">4</sup>SER remains nearly identical to the baseline with only MSR and MIR. Considering the significant gains in performance, M<sup class=\"ltx_sup\">4</sup>SER demonstrates a favorable trade-off between computational cost and recognition accuracy, making it suitable for practical deployment.</p>\n\n",
                "matched_terms": [
                    "aec",
                    "only",
                    "msr",
                    "aed",
                    "mir",
                    "lcl",
                    "m4ser",
                    "full",
                    "gan"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">t-SNE Analysis.</span> To intuitively demonstrate the advantages of our proposed M<sup class=\"ltx_sup\">4</sup>SER model on IEMOCAP and MELD datasets, we utilize the t-distributed stochastic neighbor embedding (t-SNE) tool <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib61\" title=\"\">61</a>]</cite> to visualize the learned emotion features using all samples from session 3 of the IEMOCAP and test set of the MELD. We compare these features across the following models: the speech model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(a)), the text model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(b)), the proposed M<sup class=\"ltx_sup\">4</sup>SER model without label-based contrastive learning (LCL) (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(c)), and the proposed M<sup class=\"ltx_sup\">4</sup>SER model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(d)).</p>\n\n",
                "matched_terms": [
                    "text",
                    "iemocap",
                    "speech",
                    "datasets",
                    "lcl",
                    "m4ser",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the visualization results in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, we observe that the emotion label distributions for the two single-modal baselines on the IEMOCAP and MELD datasets show a significant overlap among the emotion categories, indicating that they are often confused with each other. In contrast, the emotion label distributions for the two multimodal models are more distinguishable, demonstrating the effectiveness of multimodal models in capturing richer emotional features.</p>\n\n",
                "matched_terms": [
                    "meld",
                    "iemocap",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, compared with the M<sup class=\"ltx_sup\">4</sup>SER model without the LCL strategy, our M<sup class=\"ltx_sup\">4</sup>SER model achieves better clustering for each emotion category, making them as distinct as possible. For instance, on the IEMOCAP dataset, the intra-class clustering of samples learned by the M<sup class=\"ltx_sup\">4</sup>SER model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(d)) is more pronounced than that learned by the M<sup class=\"ltx_sup\">4</sup>SER model without the LCL strategy (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(c)), especially with clearer boundaries for sad and angry samples. Additionally, the distances between the four types of emotion in the emotion vector space increase. Although distinguishing MELD samples is more challenging than distinguishing IEMOCAP samples, the overall trend remains similar.\nThis indicates that the proposed M<sup class=\"ltx_sup\">4</sup>SER model effectively leverages both modality-specific and modality-invariant representations to capture high-level shared feature representations across speech and text modalities for emotion recognition.</p>\n\n",
                "matched_terms": [
                    "text",
                    "meld",
                    "modalityspecific",
                    "iemocap",
                    "speech",
                    "lcl",
                    "m4ser",
                    "modalityinvariant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further explore the effectiveness of adversarial learning in M<sup class=\"ltx_sup\">4</sup>SER, we visualize the distribution of modality-invariant and modality-specific representations before and after adversarial learning using the t-SNE tool, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F10\" title=\"Figure 10 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. It can be observed that after adversarial learning, different modality-specific representations become more separated with increasing intraclass clustering. This indicates that with the introduction of adversarial learning, M<sup class=\"ltx_sup\">4</sup>SER enhances the diversity of modality-specific representations, generating superior features for the downstream emotion recognition task.</p>\n\n",
                "matched_terms": [
                    "modalityspecific",
                    "modalityinvariant",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Representation Analysis.</span>\nWe find that our M<sup class=\"ltx_sup\">4</sup>SER method performs better when using ASR text than when using GT text, as presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T6\" title=\"TABLE VI &#8227; V-F Visualization Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a>. To investigate this finding, we visualize representation weights for two examples from IEMOCAP, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F11\" title=\"Figure 11 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. We display representations of each modality across the temporal level in two learning spaces, accumulating hidden features of <math alttext=\"H_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m2\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">H_{S}</annotation></semantics></math>, <math alttext=\"H_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m3\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">H_{T}</annotation></semantics></math>, <math alttext=\"\\hat{H}_{S}^{spe}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m4\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>H</mi><mo>^</mo></mover><mi>S</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\hat{H}_{S}^{spe}</annotation></semantics></math>, and <math alttext=\"H_{T}^{spe}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m5\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>T</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{T}^{spe}</annotation></semantics></math> into one dimension.\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F11\" title=\"Figure 11 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(a) illustrates that during single-modal learning with ASR text, representation weights in the text modality mainly focus on the word &#8220;oh&#8221;, whereas in the speech modality, representation weights concentrate towards the sentence&#8217;s end. After cross-modal learning, we observed a significant decrease in &#8220;oh&#8221; weight in the text modality. This change occurs as the model detects errors or inaccuracies in ASR transcription using AED and AEC modules, shifting representation weights to other words such as &#8220;that&#8217;s&#8221;. Owing to limited emotional cues in the text, the model relies more on speech features, especially anger-related cues such as intonation and volume, critical for accurate anger recognition. Conversely, with GT text, after cross-modal learning, attention in the text modality focuses on words conveying positive emotions such as &#8220;amusing&#8221;, whereas speech modality distribution remains largely unchanged. Without ASR error signals, the model leans towards these textual features, leading to a bias towards happiness in emotion recognition.</p>\n\n",
                "matched_terms": [
                    "text",
                    "method",
                    "aec",
                    "error",
                    "iemocap",
                    "speech",
                    "asr",
                    "aed",
                    "m4ser",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Similarly, Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F11\" title=\"Figure 11 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(b) shows inconsistency between GT text and speech features. GT text contains words (e.g., &#8220;don&#8217;t know&#8221;) indicating confusion and uncertainty, whereas speech features exhibit significant emotional fluctuations. This mismatch complicates feature alignment in cross-modal learning, resulting in erroneous anger classification. In contrast, ASR text shows higher consistency with speech features, highlighting emotional words (&#8220;freaking out&#8221;, &#8220;what the hell&#8221;, and &#8220;all of a sudden&#8221;) despite errors and simplifications. Representation weights confirm high consistency between ASR text and speech features across multiple regions, enhancing cross-modal learning accuracy and correct emotion identification as &#8220;Happy&#8221; (&#8220;Excited&#8221;). We demonstrate with dynamic time warping (DTW) that ASR transcriptions are more consistent with speech than GT transcriptions, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T7\" title=\"TABLE VII &#8227; V-F Visualization Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VII</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, compared with ASR text, GT text exhibits two primary differences:</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Lack of ASR error signals:</span> GT text lacks ASR errors, missing additional cues beneficial for emotion recognition. This limitation possibly hinders capturing intense emotional signals when relying solely on GT text.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Differences in feature weight distribution:</span> the absence of ASR errors alters feature weight distribution in GT text modality compared with ASR text, potentially leading to inaccurate emotion classification.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Confusion Matrix Analysis.</span> To investigate the impact of different modules on class-wise prediction, we visualize the averaged confusion matrices over 5-folds on IEMOCAP in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F12\" title=\"Figure 12 &#8227; V-F Visualization Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>. Compared with the baseline, introducing MSR and MIR significantly enhances the prediction of &#8220;Happy&#8221; class, which often suffer from ambiguous acoustic-textual alignment.\nBy further incorporating GAN and LCL strategies, we observe overall improvements across most emotion classes, particularly in &#8220;Neutral&#8221;. However, the performance on the &#8220;Happy&#8221; class slightly drops, potentially owing to the acoustic-textual heterogeneity introduced by merging &#8220;Excited&#8221; into &#8220;Happy&#8221;, which possibly reduces the effectiveness of local contrastive learning in capturing class-specific discriminative features.\nFinally, integrating AED and AEC modules yields the most accurate and balanced predictions overall. We observe further improvements in &#8220;Neutral&#8221; and &#8220;Sad&#8221; classes, while the performance of &#8220;Happy&#8221; slightly recovers compared with the previous setting. These results highlight the effectiveness of multi-task learning in mitigating ASR-induced errors and enhancing emotional robustness across modalities.</p>\n\n",
                "matched_terms": [
                    "aec",
                    "iemocap",
                    "msr",
                    "aed",
                    "mir",
                    "lcl",
                    "impact",
                    "gan"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the performance of M<sup class=\"ltx_sup\">4</sup>SER in real-world environments, we conduct cross-corpus emotion recognition experiments, which simulate the practical scenario of domain shift between different datasets. Following the experimental settings of previous works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib62\" title=\"\">62</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib63\" title=\"\">63</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib64\" title=\"\">64</a>]</cite>, we adopt a transfer evaluation strategy where one corpus is used entirely for training, and 30% of the target corpus is reserved for validation for parameter tuning, while the remaining 70% is used for final testing.\nIn addition, as the IEMOCAP dataset contains only four emotion classes (&#8220;Neutral&#8221;, &#8220;Happy&#8221;, &#8220;Angry&#8221;, &#8220;Sad&#8221;), we restrict MELD to the same set of overlapping emotions for a fair comparison and discard the rest (&#8220;Suprise&#8221;, &#8220;Fear&#8221;, &#8220;Disgust&#8221;). This ensures consistent label space across corpora during both training and evaluation.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "only",
                    "datasets",
                    "m4ser",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T8\" title=\"TABLE VIII &#8227; V-G Cross-corpus Generalization Ability &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VIII</span></a> presents the cross-corpus generalization results. Compared with the reimplemented SAMS and MF-AED-AEC baselines, our full model M<sup class=\"ltx_sup\">4</sup>SER achieves the best performance in both IE<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ME and ME<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>IE directions. Specifically, M<sup class=\"ltx_sup\">4</sup>SER outperforms the strong multimodal baseline by 4.3% in ACC and 3.1% in W-F1 under the IE<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ME setting, and by 5.1% WA and 4.8% UA in the ME<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>IE setting.</p>\n\n",
                "matched_terms": [
                    "wf1",
                    "full",
                    "acc",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further evaluate the adaptability of M<sup class=\"ltx_sup\">4</sup>SER under limited supervision, we conduct few-shot domain adaptation experiments by sampling 5, 10, 20, and 60 labeled instances per class from the target-domain validation set. The remaining validation data is still used for model selection, while the test set remains fixed across all settings. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T9\" title=\"TABLE IX &#8227; V-G Cross-corpus Generalization Ability &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IX</span></a>, M<sup class=\"ltx_sup\">4</sup>SER consistently outperforms SAMS and MF-AED-AEC across all shot settings. For example, with only 5 labeled samples per class, M<sup class=\"ltx_sup\">4</sup>SER achieves ACC of 57.9% in the IE<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ME setting, already surpassing MF-AED-AEC with 20 shots. As the number of target samples increases, our model scales well and reaches ACC of 62.1% and W-F1 61.5% at 60-shot. In the ME<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>IE setting, M<sup class=\"ltx_sup\">4</sup>SER reaches UA of 75.9%, demonstrating excellent cross-domain adaptability.</p>\n\n",
                "matched_terms": [
                    "wf1",
                    "only",
                    "acc",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results demonstrate that M<sup class=\"ltx_sup\">4</sup>SER not only generalizes robustly across corpora with minimal tuning, but also effectively adapts to new domains under few-shot conditions.</p>\n\n",
                "matched_terms": [
                    "only",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose M<sup class=\"ltx_sup\">4</sup>SER, a novel emotion recognition method that combines multimodal, multirepresentation, multitask, and multistrategy learning. M<sup class=\"ltx_sup\">4</sup>SER leverages an innovative multimodal fusion module to learn modality-specific and modality-invariant representations, capturing unique features of each modality and common features across modalities. We then introduce a modality discriminator to enhance modality diversity through adversarial learning. Additionally, we design two auxiliary tasks, AED and AEC, aimed at enhancing the semantic consistency within the text modality. Finally, we propose a label-based contrastive learning strategy to distinguish different emotional features. Results of experiments on the IEMOCAP and MELD datasets demonstrate that M<sup class=\"ltx_sup\">4</sup>SER surpasses previous baselines, proving its effectiveness. In the future, we plan to extend our approach to the visual modality and introduce disentangled representation learning to further enhance emotion recognition performance. We also plan to investigate whether AED and AEC modules remain necessary when using powerful LLM-based ASR systems.</p>\n\n",
                "matched_terms": [
                    "text",
                    "meld",
                    "modalityspecific",
                    "method",
                    "aec",
                    "iemocap",
                    "asr",
                    "datasets",
                    "aed",
                    "m4ser",
                    "modalityinvariant",
                    "modality"
                ]
            }
        ]
    },
    "S5.T6": {
        "source_file": "M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition",
        "caption": "TABLE VI: Performance comparison of our method on IEMOCAP using ASR and GT texts (%). When using GT text, the AED and AEC modules are excluded.",
        "body": "Method\nModality\nWA\nUA\n\n\nM4SER\nS+T(GT)\n78.4\n79.9\n\n\nM4SER\nS+T(ASR)\n79.2\n80.1",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Modality</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">WA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">UA</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">M<sup class=\"ltx_sup\">4</sup>SER</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">S+T(GT)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">78.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">79.9</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#EFEFEF;\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">M<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">4</span></sup>SER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">S+T(ASR)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">79.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">80.1</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "text",
            "stasr",
            "method",
            "aec",
            "performance",
            "texts",
            "stgt",
            "iemocap",
            "asr",
            "when",
            "comparison",
            "aed",
            "excluded",
            "modules",
            "m4ser",
            "our",
            "modality"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Representation Analysis.</span>\nWe find that our M<sup class=\"ltx_sup\">4</sup>SER method performs better when using ASR text than when using GT text, as presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T6\" title=\"TABLE VI &#8227; V-F Visualization Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a>. To investigate this finding, we visualize representation weights for two examples from IEMOCAP, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F11\" title=\"Figure 11 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. We display representations of each modality across the temporal level in two learning spaces, accumulating hidden features of <math alttext=\"H_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m2\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">H_{S}</annotation></semantics></math>, <math alttext=\"H_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m3\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">H_{T}</annotation></semantics></math>, <math alttext=\"\\hat{H}_{S}^{spe}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m4\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>H</mi><mo>^</mo></mover><mi>S</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\hat{H}_{S}^{spe}</annotation></semantics></math>, and <math alttext=\"H_{T}^{spe}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m5\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>T</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{T}^{spe}</annotation></semantics></math> into one dimension.\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F11\" title=\"Figure 11 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(a) illustrates that during single-modal learning with ASR text, representation weights in the text modality mainly focus on the word &#8220;oh&#8221;, whereas in the speech modality, representation weights concentrate towards the sentence&#8217;s end. After cross-modal learning, we observed a significant decrease in &#8220;oh&#8221; weight in the text modality. This change occurs as the model detects errors or inaccuracies in ASR transcription using AED and AEC modules, shifting representation weights to other words such as &#8220;that&#8217;s&#8221;. Owing to limited emotional cues in the text, the model relies more on speech features, especially anger-related cues such as intonation and volume, critical for accurate anger recognition. Conversely, with GT text, after cross-modal learning, attention in the text modality focuses on words conveying positive emotions such as &#8220;amusing&#8221;, whereas speech modality distribution remains largely unchanged. Without ASR error signals, the model leans towards these textual features, leading to a bias towards happiness in emotion recognition.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Multimodal speech emotion recognition (SER) has emerged as pivotal for improving human&#8211;machine interaction. Researchers are increasingly leveraging both speech and textual information obtained through automatic speech recognition (ASR) to comprehensively recognize emotional states from speakers. Although this approach reduces reliance on human-annotated text data, ASR errors possibly degrade emotion recognition performance.\nTo address this challenge, in our previous work, we introduced two auxiliary tasks, namely, ASR error detection and ASR error correction, and we proposed a novel multimodal fusion (MF) method for learning modality-specific and modality-invariant representations across different modalities.\nBuilding on this foundation, in this paper, we introduce two additional training strategies. First, we propose an adversarial network to enhance the diversity of modality-specific representations. Second, we introduce a label-based contrastive learning strategy to better capture emotional features.\nWe refer to our proposed method as M<sup class=\"ltx_sup\">4</sup>SER and validate its superiority over state-of-the-art methods through extensive experiments using IEMOCAP and MELD datasets.</p>\n\n",
                "matched_terms": [
                    "text",
                    "method",
                    "performance",
                    "iemocap",
                    "asr",
                    "m4ser",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent research has shown that single-modal approaches struggle to meet the increasing demands for SER owing to their inherent complexity. As a solution, multimodal information has emerged as a viable option because it offers diverse emotional cues. A common strategy involves integrating speech and text modalities to achieve multimodal SER. Text data can be obtained through manual transcription or automatic speech recognition (ASR), which have been widely adopted in recent studies to reduce reliance on extensive annotated datasets.\nAlthough text-based pretrained models such as BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib6\" title=\"\">6</a>]</cite> and RoBERTa <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib7\" title=\"\">7</a>]</cite> have excelled in multimodal SER tasks, the accuracy of ASR remains equally crucial for achieving precise emotion recognition (ER) results. To mitigate the impact of ASR errors, Santoso et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib8\" title=\"\">8</a>]</cite> have integrated self-attention mechanisms and word-level confidence measurements, particularly reducing the importance of words with high error probabilities, thereby enhancing SER performance. However, this approach heavily depends on the performance of the ASR system, thus limiting its generalizability. Lin and Wang <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> have proposed an auxiliary ASR error detection task aimed at determining the probabilities of erroneous words to adjust the trust levels assigned to each word in the ASR hypothesis, thereby improving multimodal SER performance. However, this method focuses solely on error detection within the ASR hypothesis without directly correcting these errors, indicating that it does not fully enhance the coherence of semantic information.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr",
                    "method",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the other hand, how to effectively integrate speech and text modalities has become a key focus. Previous methods have included simple feature concatenation, recurrent neural networks (RNNs), convolutional neural networks (CNNs), and cross-modal attention mechanisms <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib1\" title=\"\">1</a>]</cite>. Although these methods have shown some effectiveness, they often face challenges arising from differences in representations across different modalities.\nIn recent multimodal tasks such as sentiment analysis <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib11\" title=\"\">11</a>]</cite>, researchers have proposed learning two distinct types of representation to enhance multimodal learning.\nThe first is a representation tailored to each modality, that is, a modality-specific representation (MSR).\nThe second is modality-invariant representation (MIR), aimed at mapping all modalities of the same utterance into a shared subspace. This representation captures commonalities in multimodal signals as well as the speaker&#8217;s shared motives, which collectively affect the emotional state of the utterance through distribution alignment. By combining these two types of representation, one can obtain a comprehensive view of multimodal data can be provided for downstream tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib12\" title=\"\">12</a>]</cite>.\nHowever, these methods focus on utterance-level representations, which, while effective for short and isolated utterances, often fail to capture fine-grained emotional dynamics, especially in long and complex interactions. Such coarse-grained representations fail to capture subtle temporal cues and modality-specific variations that are crucial for accurate emotion recognition.\nMoreover, mapping entire utterances to a shared or modality-specific space using similarity loss oversimplifies the underlying multimodal relationships and hinders effective cross-modal alignment.</p>\n\n",
                "matched_terms": [
                    "text",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the basis of the above observations, in our previous work <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib13\" title=\"\">13</a>]</cite>, we introduced two auxiliary tasks: ASR error detection (AED) and ASR error correction (AEC). Specifically, we introduced an AED module to identify the positions of ASR errors. Subsequently, we employed an AEC module to correct these errors, enhancing the semantic coherence of ASR hypotheses and reducing the impact of ASR errors on ER tasks.\nAdditionally, we designed a novel multimodal fusion (MF) module for learning frame-level MSRs and MIRs in a shared speech&#8211;text modality space.\nWe observed the following shortcomings in our previous work: 1) Despite introducing the AED and AEC modules, uncorrected ASR errors continue to affect the accuracy of SER. 2) Existing MF methods possibly fail to capture subtle differences in emotional features.</p>\n\n",
                "matched_terms": [
                    "aec",
                    "asr",
                    "aed",
                    "modules",
                    "our",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these issues, we propose two learning strategies. First, we introduce adversarial learning to enhance the diversity and robustness of modality representations, thereby reducing the impact of ASR errors on SER tasks. Second, we introduce a contrastive learning strategy based on emotion labels to more accurately distinguish and capture feature differences between different emotion categories, thus improving the performance and robustness of the SER system.\nIn summary, our main contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "our",
                    "asr",
                    "performance",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce an adversarial modality discriminator to enhance the diversity of MSRs and improve the generalization of MIRs. This design alleviates modality mismatch and suppresses modality-specific noise introduced by ASR errors, which often cause misalignment between acoustic and textual features and degrade the robustness of multimodal fusion.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the basis of our previous work, we present M<sup class=\"ltx_sup\">4</sup>SER, an SER method that leverages multimodal, multirepresentation, multitask, and multistrategy learning. The difference between our M<sup class=\"ltx_sup\">4</sup>SER and previous SER methods is illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S1.F1\" title=\"Figure 1 &#8227; I Introduction &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. M<sup class=\"ltx_sup\">4</sup>SER mitigates the impact of ASR errors, improves modality-invariant representations, and captures commonalities between modalities to bridge their heterogeneity.</p>\n\n",
                "matched_terms": [
                    "our",
                    "asr",
                    "method",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed M<sup class=\"ltx_sup\">4</sup>SER outperforms state-of-the-arts on the IEMOCAP and MELD datasets. Extensive experiments demonstrate its superiority in SER tasks.</p>\n\n",
                "matched_terms": [
                    "our",
                    "iemocap",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remaining sections of this paper are organized as follows: In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2\" title=\"II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">II</span></a>, we review related work. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3\" title=\"III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>, we discuss the specific model and method of M<sup class=\"ltx_sup\">4</sup>SER. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S4\" title=\"IV Experimental Setup &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>, we outline the experimental setup used in this study, including datasets, evaluation metrics, and implementation details. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5\" title=\"V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>, we verify the effectiveness of the proposed model. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S6\" title=\"VI Conclusion &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a>, we present our conclusion and future work.</p>\n\n",
                "matched_terms": [
                    "our",
                    "method",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these limitations, multimodal SER often integrates both speech and text modalities, capturing both acoustic and semantic cues. A common setting involves using speech signals alongside textual transcripts (typically from manual annotations or ASR outputs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib24\" title=\"\">24</a>]</cite>.\nFan et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib21\" title=\"\">21</a>]</cite> proposed multi-granularity attention-based transformers (MGAT) to address emotional asynchrony and modality misalignment.\nSun et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib22\" title=\"\">22</a>]</cite> introduced a method integrating shared and private encoders, projecting each modality into a separate subspace to capture modality consistency and diversity while ensuring label consistency in the output.\nThese methods typically assume access to high-quality manual transcriptions for the text modality.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr",
                    "method",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, growing efforts have focused on reducing the dependency on annotated text by directly leveraging ASR hypotheses as input <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib28\" title=\"\">28</a>]</cite>. Santoso et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib25\" title=\"\">25</a>]</cite> introduced confidence-aware self-attention mechanisms that downweight unreliable ASR tokens to mitigate error propagation. Lin and Wang&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> proposed a robust multimodal SER framework that adaptively fuses attention-weighted acoustic representations with ASR-derived text embeddings, compensating for recognition noise in the transcript.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to factorization-based approaches which explicitly disentangle modality inputs into shared and private spaces, our M<sup class=\"ltx_sup\">4</sup>SER framework adopts a structured encoding&#8211;generation strategy to implicitly model MSRs and MIRs. Specifically, we utilize a stack of cross-modal encoder blocks to extract token-aware and speech-aware features (MSRs), and then apply a dedicated generator composed of hybrid-modal attention and masking mechanisms to derive the MIRs. In addition, unlike prior works that incorporate adversarial learning at a global or representation-level, our M<sup class=\"ltx_sup\">4</sup>SER employs a frame-level discriminator that operates on each temporal representation. A frame-level discriminator attempts to distinguish the modality origin of each temporal representation, while the MIR generator is optimized to deceive the discriminator by producing modality-agnostic outputs that are hard to classify as either text or speech. This fine-grained, per-frame adversarial constraint leads to more robust and aligned cross-modal representations. Moreover, M<sup class=\"ltx_sup\">4</sup>SER integrates this disentanglement framework into a multitask learning setup with ASR-aware auxiliary supervision, enabling more effective adaptation to noisy real-world speech&#8211;text scenarios.</p>\n\n",
                "matched_terms": [
                    "text",
                    "modality",
                    "m4ser",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ASR error correction (AEC) techniques remain effective in optimizing the ASR hypotheses. AEC has been jointly trained with ER tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib13\" title=\"\">13</a>]</cite> in multitask settings. Li et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib27\" title=\"\">27</a>]</cite> proposed an AEC method that improves transcription quality on low-resource out-of-domain data through cross-modal training and the incorporation of discrete speech units, and they validated its effectiveness in downstream ER tasks. Lin and Wang <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> introduced a robust ER method that leverages complementary semantic information from audio, adapts to ASR errors through an auxiliary task for ASR error detection, and fuses text and acoustic representations for ER.\nOn the basis of <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib32\" title=\"\">32</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib33\" title=\"\">33</a>]</cite>, we propose a partially autoregressive AEC method that uses a label predictor to restrict decoding to only desirable parts of the input sequence embeddings, thereby reducing inference latency.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr",
                    "method",
                    "aec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed M<sup class=\"ltx_sup\">4</sup>SER framework can be formalized as the function <math alttext=\"f(S,T)=(L,Z)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mi>L</mi><mo>,</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f(S,T)=(L,Z)</annotation></semantics></math>. Here, the speech modality <math alttext=\"S=(s_{1},s_{2},\\cdots,s_{m})\\in\\mathbb{R}^{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>s</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>s</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>m</mi></msup></mrow><annotation encoding=\"application/x-tex\">S=(s_{1},s_{2},\\cdots,s_{m})\\in\\mathbb{R}^{m}</annotation></semantics></math> consists of <span class=\"ltx_text ltx_font_italic\">m</span> frames extracted from an utterance, whereas the text modality <math alttext=\"T=(t_{1},t_{2},\\cdots,t_{n})\\in\\mathbb{R}^{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><msub><mi>t</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>t</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>n</mi></msup></mrow><annotation encoding=\"application/x-tex\">T=(t_{1},t_{2},\\cdots,t_{n})\\in\\mathbb{R}^{n}</annotation></semantics></math> comprises <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> tokens from the original ASR hypotheses of the utterance. All tokens are mapped to a predefined WordPiece vocabulary <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib40\" title=\"\">40</a>]</cite>. Additionally, within our multitask learning framework, the primary task is ER, yielding an emotion classification output <math alttext=\"L\\in\\{l_{1},l_{2},\\cdots,l_{e}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>l</mi><mn>1</mn></msub><mo>,</mo><msub><mi>l</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>l</mi><mi>e</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">L\\in\\{l_{1},l_{2},\\cdots,l_{e}\\}</annotation></semantics></math>, where <math alttext=\"e\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mi>e</mi><annotation encoding=\"application/x-tex\">e</annotation></semantics></math> represents the number of emotional categories. Concurrently, the auxiliary tasks include ASR error detection (AED) and ASR error correction (AEC). The output of these auxiliary tasks is <math alttext=\"Z\\in\\{Z_{1},Z_{2},\\cdots,Z_{k}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>Z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>Z</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>Z</mi><mi>k</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Z\\in\\{Z_{1},Z_{2},\\cdots,Z_{k}\\}</annotation></semantics></math>, representing all the ground truth (GT) sequences where the ASR transcriptions differ from the human-annotated transcriptions.\n<math alttext=\"Z_{k}=(z_{1,k},z_{2,k},\\cdots,z_{c,k})\\in\\mathbb{R}^{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><msub><mi>Z</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mn>1</mn><mo>,</mo><mi>k</mi></mrow></msub><mo>,</mo><msub><mi>z</mi><mrow><mn>2</mn><mo>,</mo><mi>k</mi></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>z</mi><mrow><mi>c</mi><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>c</mi></msup></mrow><annotation encoding=\"application/x-tex\">Z_{k}=(z_{1,k},z_{2,k},\\cdots,z_{c,k})\\in\\mathbb{R}^{c}</annotation></semantics></math>, denoting that the <math alttext=\"k^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\"><semantics><msup><mi>k</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">k^{th}</annotation></semantics></math> error position includes <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m10\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> tokens.</p>\n\n",
                "matched_terms": [
                    "text",
                    "aec",
                    "asr",
                    "aed",
                    "m4ser",
                    "our",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overall architecture of the M<sup class=\"ltx_sup\">4</sup>SER model is\ncomposed of five modules, namely, the embedding module, AED module, AEC module, MF module, and ER module, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. In the following section, we provide a detailed explanation of each module.</p>\n\n",
                "matched_terms": [
                    "m4ser",
                    "aec",
                    "modules",
                    "aed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contextual Speech Representations.</span> To obtain comprehensive contextual representations of acoustic features, we utilize a pretrained SSL model, HuBERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib4\" title=\"\">4</a>]</cite>, as our acoustic encoder. HuBERT integrates CNN layers with a transformer encoder to effectively capture both speech features and contextual information.\nWe denote the acoustic hidden representations of the speech modality inputs <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> generated by HuBERT as <math alttext=\"H_{S}=(h_{S}^{(1)},h_{S}^{(2)},\\cdots,h_{S}^{(m)})\\in\\mathbb{R}^{m\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>H</mi><mi>S</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msubsup><mi>h</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">H_{S}=(h_{S}^{(1)},h_{S}^{(2)},\\cdots,h_{S}^{(m)})\\in\\mathbb{R}^{m\\times d}</annotation></semantics></math>, where <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> is the dimension of hidden representations.</p>\n\n",
                "matched_terms": [
                    "our",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contextual Token Representations.</span> We use the pretrained language model BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib6\" title=\"\">6</a>]</cite> as our text encoder to obtain the token representations <math alttext=\"H_{T}=(h_{T}^{(1)},h_{T}^{(2)},\\cdots,h_{T}^{(n)})\\in\\mathbb{R}^{n\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>H</mi><mi>T</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msubsup><mi>h</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">H_{T}=(h_{T}^{(1)},h_{T}^{(2)},\\cdots,h_{T}^{(n)})\\in\\mathbb{R}^{n\\times d}</annotation></semantics></math> for the text modality inputs <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "modality",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first subtask we introduce is AED, which is designed to detect the positions of ASR errors.\nSimilarly to <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib31\" title=\"\">31</a>]</cite>, we align <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> and <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> by determining the longest common subsequence between them. The aligned tokens are labeled <span class=\"ltx_text ltx_font_italic\">KEEP</span> (<span class=\"ltx_text ltx_font_bold\">K</span>), whereas the remaining tokens are labeled <span class=\"ltx_text ltx_font_italic\">DELETE</span> (<span class=\"ltx_text ltx_font_bold\">D</span>) or <span class=\"ltx_text ltx_font_italic\">CHANGE</span> (<span class=\"ltx_text ltx_font_bold\">C</span>). A specific example is illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(b).\nThe label prediction layer is a straightforward fully connected (FC) layer with three classes.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "aed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a second subtask called AEC, which aims to correct the errors determined by AED.\nUnlike conventional autoregressive decoders that start decoding from scratch, our decoder operates in parallel to the tokens predicted as <span class=\"ltx_text ltx_font_bold\">C</span>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "aec",
                    "aed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More specifically, once the erroneous positions are identified, the decoder constructs a separate generation sequence for each token labeled as <span class=\"ltx_text ltx_font_bold\">C</span>. Each sequence begins with a special start token <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_italic\">BOS<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span> and generates the corrected text in an autoregressive manner.\nAt each decoding step, the decoder input for a given correction sequence consists of all previously generated tokens, each concatenated with the representation of the corresponding erroneous token.\nAll correction sequences are processed in parallel by a shared transformer decoder, which attends to the full ASR representation <math alttext=\"H_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m3\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">H_{T}</annotation></semantics></math> as memory.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> denotes either the speech or text modality and <math alttext=\"H_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m2\" intent=\":literal\"><semantics><msub><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">H_{ST}</annotation></semantics></math> represents the concatenation of <math alttext=\"H_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m3\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">H_{S}</annotation></semantics></math> and <math alttext=\"H_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m4\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">H_{T}</annotation></semantics></math>, which is then fed into a FC layer to map it back to the same dimension as <math alttext=\"H_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m5\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">H_{S}</annotation></semantics></math>. This process integrates both speech and text information into a unified representation. The resulting features are subsequently summed with <math alttext=\"H_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m6\" intent=\":literal\"><semantics><msub><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">H_{ST}</annotation></semantics></math>, culminating in the ultimate modality-invariant representation:</p>\n\n",
                "matched_terms": [
                    "text",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further develop the modality discriminator <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m1\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> utilizing the modality-invariant representations produced by the MIR generator. This discriminator illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(f) is composed of two linear layers with a ReLU activation function in between, followed by a Sigmoid activation function. To enhance the MIR&#8217;s capability to remain modality-agnostic, we employ adversarial learning techniques.\nSpecifically, the discriminator <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> outputs a scalar value between 0 and 1 for each frame, where values close to 0 indicate the text modality, values close to 1 indicate the speech modality, and values near 0.5 represent an ambiguous modality.\nHere, <math alttext=\"D(H)\\in\\mathbb{R}^{m\\times 1},H\\in\\{H^{(spe)}_{T},H^{(spe)}_{S},H_{ST}^{\\rm(inv)}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>D</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow></msup></mrow><mo>,</mo><mrow><mi>H</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>inv</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">D(H)\\in\\mathbb{R}^{m\\times 1},H\\in\\{H^{(spe)}_{T},H^{(spe)}_{S},H_{ST}^{\\rm(inv)}\\}</annotation></semantics></math>.\nOn one hand, we aim for the discriminator that correctly classifies frames in the modality-specific representations <math alttext=\"H^{(spe)}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H^{(spe)}_{T}</annotation></semantics></math> and <math alttext=\"H^{(spe)}_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m5\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H^{(spe)}_{S}</annotation></semantics></math> as 0 and 1, respectively. On the other hand, to enhance the modality agnosticism of the modality-invariant representation <math alttext=\"H_{ST}^{\\rm(inv)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m6\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>inv</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{ST}^{\\rm(inv)}</annotation></semantics></math> produced by the MIR generator, we aim for a discriminator that outputs values close to 0.5, indicating an ambiguous modality. With this design of the MIR generator and discriminator, we define a generative adversarial training objective, mathematically represented as</p>\n\n",
                "matched_terms": [
                    "text",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance the model&#8217;s capability to learn emotion features from multimodal data, we employ a label-based contrastive learning task to complement the cross-entropy loss, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(g). This task aids the model in extracting emotion-related features when the MF module integrates speech and text data. As depicted in the LCL task in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.F5\" title=\"Figure 5 &#8227; III-H Label-based Contrastive Learning (LCL) &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we categorize data in each batch into positive and negative samples on the basis of emotion labels. For instance, in a batch containing eight samples, we compute the set of positive samples for each sample, where those with the same label are considered positive samples (yellow squares), and those with different labels are considered negative samples (white squares). We then calculate the label-based contrastive loss (LCL) using Eq. <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:eq:lcl</span>, which promotes instances of a specific label to be more similar to each other than instances of other labels.</p>\n\n",
                "matched_terms": [
                    "text",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the training stage, the learning process is optimized using three loss functions that correspond to ER, AED, and AEC (i.e., <math alttext=\"\\mathcal{L}_{\\rm ER}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>ER</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm ER}</annotation></semantics></math>, <math alttext=\"\\mathcal{L}_{\\rm AED}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>AED</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm AED}</annotation></semantics></math>, and <math alttext=\"\\mathcal{L}_{\\rm AEC}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>AEC</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm AEC}</annotation></semantics></math>), and two training strategies (i.e., <math alttext=\"\\mathcal{L}_{\\rm GAN}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>GAN</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm GAN}</annotation></semantics></math> and <math alttext=\"\\mathcal{L}_{\\rm LCL}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>LCL</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm LCL}</annotation></semantics></math>). The specific optimization process of M<sup class=\"ltx_sup\">4</sup>SER is detailed in Alg. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#alg1\" title=\"Algorithm 1 &#8227; III-I Joint Training &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nThese five loss functions are linearly combined as the overall training objective of M<sup class=\"ltx_sup\">4</sup>SER:</p>\n\n",
                "matched_terms": [
                    "aec",
                    "m4ser",
                    "aed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the GAN training strategy <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib34\" title=\"\">34</a>]</cite>, we divide the backpropagation process into two steps. Firstly, we maximize <math alttext=\"\\mathcal{L}_{\\rm GAN}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>GAN</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm GAN}</annotation></semantics></math> to update the discriminator, during which the generator is detached from the optimization. According to Eq. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.Ex1\" title=\"III-G Modality Discriminator &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III-G</span></a>), on one hand, maximizing the first term <math alttext=\"L_{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>D</mi></msub><annotation encoding=\"application/x-tex\">L_{D}</annotation></semantics></math> of <math alttext=\"\\mathcal{L}_{\\rm GAN}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>GAN</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm GAN}</annotation></semantics></math> essentially trains the discriminator to correctly distinguish between text and audio features by making <math alttext=\"H_{S}^{\\rm(spe)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mi>spe</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{S}^{\\rm(spe)}</annotation></semantics></math> close to 1 and <math alttext=\"H_{T}^{\\rm(spe)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mi>spe</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{T}^{\\rm(spe)}</annotation></semantics></math> close to 0 <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Function <math alttext=\"\\log(x)+\\log(1-y)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>&#8722;</mo><mi>y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(x)+\\log(1-y)</annotation></semantics></math>, where <math alttext=\"x,y\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x,y\\in(0,1)</annotation></semantics></math> reaches its maximum when <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m3\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> approaches 1 and <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m4\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math> approaches 0.</span></span></span>. On the other hand, maximizing the second term <math alttext=\"L_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m6\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>G</mi></msub><annotation encoding=\"application/x-tex\">L_{G}</annotation></semantics></math> (i.e., minimizing <math alttext=\"-L_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m7\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><msub><mi>L</mi><mi>G</mi></msub></mrow><annotation encoding=\"application/x-tex\">-L_{G}</annotation></semantics></math>) will make <math alttext=\"H^{\\text{(inv)}}_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mtext>(inv)</mtext></msubsup><annotation encoding=\"application/x-tex\">H^{\\text{(inv)}}_{ST}</annotation></semantics></math> approach 0 or 1 <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Function <math alttext=\"\\log(x)+\\log(1-x)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>&#8722;</mo><mi>x</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(x)+\\log(1-x)</annotation></semantics></math>, where <math alttext=\"x\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m2\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x\\in(0,1)</annotation></semantics></math> reaches its maximum at <math alttext=\"x=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m3\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">x=0.5</annotation></semantics></math> and its minimum near <math alttext=\"x=0\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m4\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">x=0</annotation></semantics></math> and <math alttext=\"x=1\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m5\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">x=1</annotation></semantics></math>.</span></span></span>, indicating that the discriminator recognizes <math alttext=\"H^{\\text{(inv)}}_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m9\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mtext>(inv)</mtext></msubsup><annotation encoding=\"application/x-tex\">H^{\\text{(inv)}}_{ST}</annotation></semantics></math> as modality-specific, which can be either text or audio, contrary to our desired modality invariance. Secondly, we freeze the discriminator parameters and update the remaining parameters by minimizing <math alttext=\"L_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m10\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>G</mi></msub><annotation encoding=\"application/x-tex\">L_{G}</annotation></semantics></math>, which makes the output of the discriminator <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m11\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> for the modality-invariant features <math alttext=\"H^{\\text{(inv)}}_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m12\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mtext>(inv)</mtext></msubsup><annotation encoding=\"application/x-tex\">H^{\\text{(inv)}}_{ST}</annotation></semantics></math> approach 0.5, thereby blurring these features between audio and text modalities to achieve modality agnosticism. Additionally, <math alttext=\"\\mathcal{L}_{\\text{ER}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m13\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ER</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ER}}</annotation></semantics></math> optimizes the downstream emotion recognition model, <math alttext=\"\\mathcal{L}_{\\text{AED}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m14\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>AED</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{AED}}</annotation></semantics></math> and <math alttext=\"\\mathcal{L}_{\\text{AEC}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m15\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>AEC</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{AEC}}</annotation></semantics></math> optimize the auxiliary tasks of ASR error detection and ASR error correction models, respectively, and <math alttext=\"\\mathcal{L}_{\\text{LCL}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m16\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>LCL</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{LCL}}</annotation></semantics></math> implements the LCL strategy. The entire system is trained in an end-to-end manner and optimized by adjusting weight parameters.</p>\n\n",
                "matched_terms": [
                    "text",
                    "our",
                    "asr",
                    "when",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the inference stage, the AED and AEC modules are excluded. The remaining modules accept speech and ASR transcripts as input and output emotion classification results.</p>\n\n",
                "matched_terms": [
                    "aec",
                    "asr",
                    "excluded",
                    "aed",
                    "modules"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the effectiveness\nof our proposed model, we carried out experiments on two public datasets:\nthe Interactive Emotional Dyadic Motion Capture (IEMOCAP) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib45\" title=\"\">45</a>]</cite> and the Multimodal Emotion Lines Dataset (MELD) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib46\" title=\"\">46</a>]</cite>. The statistics of the two datasets are shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S4.F6\" title=\"Figure 6 &#8227; IV-A Dataset and Evaluation Metrics &#8227; IV Experimental Setup &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "iemocap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">IEMOCAP</span> comprised roughly 12 hours of speech from ten speakers participating in five scripted sessions. Following prior work <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib47\" title=\"\">47</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> on IEMOCAP, we employed 5531 utterances that were categorized into four emotion categories: &#8220;Neutral&#8221; (1,708), &#8220;Angry&#8221; (1,103), &#8220;Happy&#8221; (including &#8220;Excited&#8221;) (595 + 1,041 = 1,636), and &#8220;Sad&#8221; (1,084).\nWe performed five-fold leave-one-session-out (LOSO) cross-validation to evaluate the emotion classification performance using weighted accuracy (WA) and unweighted accuracy (UA).</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our method was implemented using Python 3.10.0 and Pytorch 1.11.0. The model was trained and evaluated on a system equipped with an Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz, 32GB RAM, and one NVIDIA Tesla V100 GPU. The detailed parameter settings are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S4.T1\" title=\"TABLE I &#8227; IV-B Implementation Details &#8227; IV Experimental Setup &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The acoustic encoder was initialized using the <span class=\"ltx_text ltx_font_typewriter\">hubert-base-ls960<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">3</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/facebook/hubert-base-ls960\" title=\"\">https://huggingface.co/facebook/hubert-base-ls960</a></span></span></span></span> model, producing acoustic representations <math alttext=\"d_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">d_{S}</annotation></semantics></math> with a dimensionality of 768.\nThe text encoder employed the <span class=\"ltx_text ltx_font_typewriter\">bert-base-uncased<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">4</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/google-bert/bert-base-uncased\" title=\"\">https://huggingface.co/google-bert/bert-base-uncased</a></span></span></span></span> model for initialization, yielding token representations <math alttext=\"d_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">d_{T}</annotation></semantics></math> with a dimensionality of 768. The vocabulary size for word tokenization <math alttext=\"d_{vocab}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>d</mi><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">d_{vocab}</annotation></semantics></math> was set to 30,522. We set the hidden size <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> to 768, the number of attention layers to 12, and the number of attention heads to 12. Both the HuBERT and BERT models were fine-tuned during the training stage.\nThe transformer decoder in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c) adopted a single-layer transformer with a hidden size of 768. Additionally, we obtained corresponding ASR hypotheses using the Whisper ASR model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib48\" title=\"\">48</a>]</cite>. We used the <span class=\"ltx_text ltx_font_typewriter\">openai/whisper-medium.en<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">5</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/openai/whisper-medium.en\" title=\"\">https://huggingface.co/openai/whisper-medium.en</a></span></span></span></span> and <span class=\"ltx_text ltx_font_typewriter\">openai/whisper-large-v2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">6</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/openai/whisper-large-v2\" title=\"\">https://huggingface.co/openai/whisper-large-v2</a></span></span></span></span> models, achieving word error rates (WERs) of 20.48% and 37.87% on the IEMOCAP and MELD datasets, respectively.</p>\n\n",
                "matched_terms": [
                    "text",
                    "iemocap",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used Adam <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib49\" title=\"\">49</a>]</cite> as the optimizer with a batch size of 16. On the basis of empirical observations and prior work <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib50\" title=\"\">50</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib51\" title=\"\">51</a>]</cite>, we kept the learning rate constant at <math alttext=\"1e^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-5}</annotation></semantics></math> for IEMOCAP and <math alttext=\"1e^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-4}</annotation></semantics></math> for MELD during training. For our multitask learning setup, we set <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m3\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> to 3 and <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m4\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> to 0.1. For our multistrategy learning setup, we set <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m5\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> to 0.01 and <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m6\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> to 0.1. The temperature parameter <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m7\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> was set to 0.07. To validate these choices, we additionally performed a one-at-a-time sensitivity analysis on the hyperparameters <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m8\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m9\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>, <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m10\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>, and <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m11\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math>, as presented in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.SS3\" title=\"V-C Sensitivity Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">V-C</span></a> and illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F7\" title=\"Figure 7 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "iemocap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the experiments, we compared various multimodal speech emotion recognition SOTA methods with our proposed method, M<sup class=\"ltx_sup\">4</sup>SER.\nThe comparison was conducted by the following methods on the IEMOCAP dataset:</p>\n\n",
                "matched_terms": [
                    "method",
                    "iemocap",
                    "comparison",
                    "m4ser",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RMSER-AEA</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> uses complementary semantic information, adapting to ASR errors through an auxiliary task and fusing text and acoustic representations for SER.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MMER</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib14\" title=\"\">14</a>]</cite> employs early fusion and cross-modal self-attention between text and acoustic modalities, and addresses three novel auxiliary tasks to enhance SER. For fairness, we selected the result that did not introduce augmented text for comparison.</p>\n\n",
                "matched_terms": [
                    "text",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <span class=\"ltx_text ltx_font_bold\">MF-AED-AEC</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib13\" title=\"\">13</a>]</cite>, as in our previous work, we consider two auxiliary tasks to improve semantic coherence in ASR text and introduce a MF method to learn shared representations.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr",
                    "method",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T2\" title=\"TABLE II &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> compare the performance of the M<sup class=\"ltx_sup\">4</sup>SER method with those of recent multimodal SER methods on the IEMOCAP and MELD datasets. Our proposed M<sup class=\"ltx_sup\">4</sup>SER achieves a WA of 79.2% and a UA of 80.1% on IEMOCAP, and an ACC of 66.5% and a W-F1 of 66.0% on MELD, outperforming other models and demonstrating the superiority of M<sup class=\"ltx_sup\">4</sup>SER. Specifically, on the IEMOCAP dataset, compared with the recent MAF-DCT, the M<sup class=\"ltx_sup\">4</sup>SER method shows a 0.7% improvement in WA and a 0.8% improvement in UA, reaching a new SOTA performance. These improvements stem from our M<sup class=\"ltx_sup\">4</sup>SER&#8217;s capability to mitigate the impact of ASR errors, capture modality-specific and modality-invariant representations across different modalities, enhance commonality between modalities through adversarial learning, and excel in emotion feature learning with the LCL loss.</p>\n\n",
                "matched_terms": [
                    "method",
                    "performance",
                    "iemocap",
                    "asr",
                    "m4ser",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed M<sup class=\"ltx_sup\">4</sup>SER model is composed of four types of learning: multimodal learning, multirepresentation learning, multitask learning, and multistrategy learning. To determine the impact of different types of learning on the performance of the emotion recognition task, ablation experiments were conducted on the IEMOCAP and MELD datasets, as presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "iemocap",
                    "m4ser",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multimodalities.</span>\nTo assess the necessity of multimodality, we conduct single-modal comparison experiments involving text and speech modalities. These experiments operate under a baseline framework excluding the MF module, LCL loss, and GAN loss, precluding intermodal interaction.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(A), in the single-modal experiments, the performance of speech modality in the IEMOCAP dataset is better than that of text modality, whereas the reverse is observed in the MELD dataset. This divergence suggests that emotional information is more distinctly conveyed through speech in IEMOCAP, whereas MELD leans towards text for emotional expression.\nMoreover, the multimodal baseline model, which simply concatenates speech and text features for recognition (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> A(3)), significantly enhances performance compared with single-modal baselines, highlighting the essential role of multimodal information.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance",
                    "iemocap",
                    "comparison",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multirepresentations.</span> We study the importance of modality-invariant and modality-specific representations by discarding each type. Removing modality-specific representations from multimodal fusion significantly reduces downstream emotion recognition performance on both datasets, confirming their effectiveness in capturing and utilizing unique information from each modality. Similarly, omitting refined modality-invariant representations from multimodal fusion also significantly decreases emotion recognition performance on both datasets, demonstrating their importance in bridging modality gaps. Clearly, removing both representations further decreases performance.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multitasks.</span> First, we discuss the impact of the AED module. Specifically, we remove the AED module and introduce only the AEC module as an auxiliary task. This requires the model to correct all errors in each utterance from scratch, rather than only the detected errors. Evidently, the absence of the AED module leads to a significant drop in emotion recognition performance, demonstrating the necessity of the AED module. Similarly, we observe that the AEC module also plays an important role. Moreover, we note that the WA results using only the AEC module (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(C)(1)) are even worse than the results with neither the AED nor AEC module (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(C)(3)) in both datasets. This phenomenon occurs because directly using the neural machine translation (NMT) model for AEC can even increase the WER <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib60\" title=\"\">60</a>]</cite>. Unlike NMT tasks, which typically require modifying almost all input tokens, AEC involves fewer but more challenging corrections. For instance, if the ASR model&#8217;s WER is 10%, then only about 10% of the input tokens require correction in the AEC model. However, these tokens are often difficult to recognize and correct because they have already been misrecognized by the ASR model. Therefore, we should consider the characteristics of ASR outputs and carefully design the AEC model, which is why we introduce both AED and AEC as auxiliary tasks.</p>\n\n",
                "matched_terms": [
                    "aed",
                    "asr",
                    "aec",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multistrategies.</span> To validate the effectiveness of the adversarial training strategy outlined in Alg. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#alg1\" title=\"Algorithm 1 &#8227; III-I Joint Training &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we remove the adversarial training strategy from M<sup class=\"ltx_sup\">4</sup>SER. The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(D)(1) show a decrease in performance on both datasets, demonstrating the critical role of this strategy in learning modality-invariant representations. The proposed modality discriminator effectively enhances the modality agnosticism of the refined representations from the generator.\nAdditionally, omitting the LCL strategy described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.SS8\" title=\"III-H Label-based Contrastive Learning (LCL) &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III-H</span></a> also results in similar performance degradation (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(D)(2)). We visualize the results before and after introducing this strategy to demonstrate its effectiveness, with specific analysis also detailed in the following t-SNE analysis section. Moreover, removing both of these strategies further diminishes emotion recognition performance, confirming that both learning strategies contribute significantly to performance improvement.</p>\n\n",
                "matched_terms": [
                    "m4ser",
                    "performance",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:703.2pt;height:86pt;vertical-align:-40.5pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Type</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Modality</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Params (M)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Train Time (h)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Infer Time (ms/U)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">WA (%)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">UA (%)</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_2\">Single-modal</span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">HuBERT</span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">S</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">316.2</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">5.4</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">23.7 &#177; 0.1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">68.9</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt\">69.8</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\">BERT</span>\n<span class=\"ltx_td ltx_align_left ltx_border_r\">T(ASR)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">109.5</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">0.5</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">6.3 &#177; 0.1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">65.1</span>\n<span class=\"ltx_td ltx_align_center\">66.4</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_4\">Multi-modal</span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Baseline (HuBERT + BERT)</span>\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_4\">S+T(ASR)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">426.9</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">9.6</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">29.6 &#177; 0.1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">73.4</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">75.2</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\">&#8194;&#8202;&#8195;+ MSR &amp; MIR</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">481.4</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">12.0</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">44.8 &#177; 0.1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">76.6</span>\n<span class=\"ltx_td ltx_align_center\">77.4</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\">&#8194;&#8202;&#8195;&#8195;+ GAN &amp; LCL</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">481.6</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">20.8</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">44.5 &#177; 0.6</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">77.3</span>\n<span class=\"ltx_td ltx_align_center\">78.6</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">&#8195;&#8195;&#8195;&#8196;&#8202;+ AED &amp; AEC (M<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">4</span></sup>SER)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">512.9</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">21.5</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">44.7 &#177; 0.2</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">79.2</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">80.1</span></span></span>\n</span></span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "stasr",
                    "aec",
                    "aed",
                    "m4ser",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the model exhibits relative robustness to variations in <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m1\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>, indicating stable synergy between the AED and AEC objectives. Conversely, performance is highly sensitive to <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m2\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>; large values result in unstable training due to adversarial objectives, while a small weight (e.g., <math alttext=\"\\gamma=0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=0.01</annotation></semantics></math>) ensures stable convergence and optimal results.</p>\n\n",
                "matched_terms": [
                    "aed",
                    "aec",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F8\" title=\"Figure 8 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows that the multitask objectives\n(<math alttext=\"\\mathcal{L}_{\\rm ER}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>ER</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm ER}</annotation></semantics></math>, <math alttext=\"\\mathcal{L}_{\\rm AED}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>AED</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm AED}</annotation></semantics></math>, <math alttext=\"\\mathcal{L}_{\\rm AEC}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>AEC</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm AEC}</annotation></semantics></math>)\nrapidly decrease and stabilize, confirming that the auxiliary tasks provide\nuseful supervision for ER. The multistrategy objectives also\nexhibit stable convergence: <math alttext=\"\\mathcal{L}_{\\rm LCL}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>LCL</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm LCL}</annotation></semantics></math> quickly decreases under label\nsupervision, while for the adversarial component, <math alttext=\"\\mathcal{L}_{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>D</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{D}</annotation></semantics></math> rises from\nnegative values to near zero as the discriminator learns, <math alttext=\"\\mathcal{L}_{\\rm GAN}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>GAN</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm GAN}</annotation></semantics></math>\nsteadily increases and plateaus, and <math alttext=\"\\mathcal{L}_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m7\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>G</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{G}</annotation></semantics></math> exhibits fluctuations, reflecting the adversarial interplay,\nand eventually converges as <math alttext=\"D(H_{ST}^{\\mathrm{(inv)}})\\to 0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m8\" intent=\":literal\"><semantics><mrow><mrow><mi>D</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>inv</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">&#8594;</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">D(H_{ST}^{\\mathrm{(inv)}})\\to 0.5</annotation></semantics></math>, which corresponds to about 1.386 in the <math alttext=\"\\mathcal{L}_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m9\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>G</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{G}</annotation></semantics></math> term of Eq.&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.Ex1\" title=\"III-G Modality Discriminator &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III-G</span></a>),\nindicating that an adversarial equilibrium is achieved. Overall, both multitask and multistrategy objectives\nare optimized in a stable manner, validating the effectiveness of the M<sup class=\"ltx_sup\">4</sup>SER.</p>\n\n",
                "matched_terms": [
                    "aec",
                    "m4ser",
                    "aed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the trade-off between model performance and computational cost, we report the number of parameters, training time, and inference time for each model on the IEMOCAP dataset in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T5\" title=\"TABLE V &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>, with all experiments conducted on a single NVIDIA Tesla V100 GPU.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our full model M<sup class=\"ltx_sup\">4</sup>SER achieves the best performance with WA of 79.2% and UA of 80.1%. Although its training time increases to 21.5 hours owing to the inclusion of GAN and LCL strategies and AED and AEC subtasks, these components are used only during training. Consequently, the inference latency of M<sup class=\"ltx_sup\">4</sup>SER remains nearly identical to the baseline with only MSR and MIR. Considering the significant gains in performance, M<sup class=\"ltx_sup\">4</sup>SER demonstrates a favorable trade-off between computational cost and recognition accuracy, making it suitable for practical deployment.</p>\n\n",
                "matched_terms": [
                    "aec",
                    "performance",
                    "aed",
                    "m4ser",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">t-SNE Analysis.</span> To intuitively demonstrate the advantages of our proposed M<sup class=\"ltx_sup\">4</sup>SER model on IEMOCAP and MELD datasets, we utilize the t-distributed stochastic neighbor embedding (t-SNE) tool <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib61\" title=\"\">61</a>]</cite> to visualize the learned emotion features using all samples from session 3 of the IEMOCAP and test set of the MELD. We compare these features across the following models: the speech model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(a)), the text model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(b)), the proposed M<sup class=\"ltx_sup\">4</sup>SER model without label-based contrastive learning (LCL) (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(c)), and the proposed M<sup class=\"ltx_sup\">4</sup>SER model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(d)).</p>\n\n",
                "matched_terms": [
                    "text",
                    "iemocap",
                    "m4ser",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, compared with the M<sup class=\"ltx_sup\">4</sup>SER model without the LCL strategy, our M<sup class=\"ltx_sup\">4</sup>SER model achieves better clustering for each emotion category, making them as distinct as possible. For instance, on the IEMOCAP dataset, the intra-class clustering of samples learned by the M<sup class=\"ltx_sup\">4</sup>SER model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(d)) is more pronounced than that learned by the M<sup class=\"ltx_sup\">4</sup>SER model without the LCL strategy (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(c)), especially with clearer boundaries for sad and angry samples. Additionally, the distances between the four types of emotion in the emotion vector space increase. Although distinguishing MELD samples is more challenging than distinguishing IEMOCAP samples, the overall trend remains similar.\nThis indicates that the proposed M<sup class=\"ltx_sup\">4</sup>SER model effectively leverages both modality-specific and modality-invariant representations to capture high-level shared feature representations across speech and text modalities for emotion recognition.</p>\n\n",
                "matched_terms": [
                    "text",
                    "iemocap",
                    "m4ser",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Similarly, Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F11\" title=\"Figure 11 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(b) shows inconsistency between GT text and speech features. GT text contains words (e.g., &#8220;don&#8217;t know&#8221;) indicating confusion and uncertainty, whereas speech features exhibit significant emotional fluctuations. This mismatch complicates feature alignment in cross-modal learning, resulting in erroneous anger classification. In contrast, ASR text shows higher consistency with speech features, highlighting emotional words (&#8220;freaking out&#8221;, &#8220;what the hell&#8221;, and &#8220;all of a sudden&#8221;) despite errors and simplifications. Representation weights confirm high consistency between ASR text and speech features across multiple regions, enhancing cross-modal learning accuracy and correct emotion identification as &#8220;Happy&#8221; (&#8220;Excited&#8221;). We demonstrate with dynamic time warping (DTW) that ASR transcriptions are more consistent with speech than GT transcriptions, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T7\" title=\"TABLE VII &#8227; V-F Visualization Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VII</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, compared with ASR text, GT text exhibits two primary differences:</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Lack of ASR error signals:</span> GT text lacks ASR errors, missing additional cues beneficial for emotion recognition. This limitation possibly hinders capturing intense emotional signals when relying solely on GT text.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Differences in feature weight distribution:</span> the absence of ASR errors alters feature weight distribution in GT text modality compared with ASR text, potentially leading to inaccurate emotion classification.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Confusion Matrix Analysis.</span> To investigate the impact of different modules on class-wise prediction, we visualize the averaged confusion matrices over 5-folds on IEMOCAP in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F12\" title=\"Figure 12 &#8227; V-F Visualization Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>. Compared with the baseline, introducing MSR and MIR significantly enhances the prediction of &#8220;Happy&#8221; class, which often suffer from ambiguous acoustic-textual alignment.\nBy further incorporating GAN and LCL strategies, we observe overall improvements across most emotion classes, particularly in &#8220;Neutral&#8221;. However, the performance on the &#8220;Happy&#8221; class slightly drops, potentially owing to the acoustic-textual heterogeneity introduced by merging &#8220;Excited&#8221; into &#8220;Happy&#8221;, which possibly reduces the effectiveness of local contrastive learning in capturing class-specific discriminative features.\nFinally, integrating AED and AEC modules yields the most accurate and balanced predictions overall. We observe further improvements in &#8220;Neutral&#8221; and &#8220;Sad&#8221; classes, while the performance of &#8220;Happy&#8221; slightly recovers compared with the previous setting. These results highlight the effectiveness of multi-task learning in mitigating ASR-induced errors and enhancing emotional robustness across modalities.</p>\n\n",
                "matched_terms": [
                    "aec",
                    "performance",
                    "iemocap",
                    "aed",
                    "modules"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the performance of M<sup class=\"ltx_sup\">4</sup>SER in real-world environments, we conduct cross-corpus emotion recognition experiments, which simulate the practical scenario of domain shift between different datasets. Following the experimental settings of previous works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib62\" title=\"\">62</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib63\" title=\"\">63</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib64\" title=\"\">64</a>]</cite>, we adopt a transfer evaluation strategy where one corpus is used entirely for training, and 30% of the target corpus is reserved for validation for parameter tuning, while the remaining 70% is used for final testing.\nIn addition, as the IEMOCAP dataset contains only four emotion classes (&#8220;Neutral&#8221;, &#8220;Happy&#8221;, &#8220;Angry&#8221;, &#8220;Sad&#8221;), we restrict MELD to the same set of overlapping emotions for a fair comparison and discard the rest (&#8220;Suprise&#8221;, &#8220;Fear&#8221;, &#8220;Disgust&#8221;). This ensures consistent label space across corpora during both training and evaluation.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "m4ser",
                    "comparison",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T8\" title=\"TABLE VIII &#8227; V-G Cross-corpus Generalization Ability &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VIII</span></a> presents the cross-corpus generalization results. Compared with the reimplemented SAMS and MF-AED-AEC baselines, our full model M<sup class=\"ltx_sup\">4</sup>SER achieves the best performance in both IE<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ME and ME<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>IE directions. Specifically, M<sup class=\"ltx_sup\">4</sup>SER outperforms the strong multimodal baseline by 4.3% in ACC and 3.1% in W-F1 under the IE<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ME setting, and by 5.1% WA and 4.8% UA in the ME<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>IE setting.</p>\n\n",
                "matched_terms": [
                    "our",
                    "m4ser",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further evaluate the adaptability of M<sup class=\"ltx_sup\">4</sup>SER under limited supervision, we conduct few-shot domain adaptation experiments by sampling 5, 10, 20, and 60 labeled instances per class from the target-domain validation set. The remaining validation data is still used for model selection, while the test set remains fixed across all settings. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T9\" title=\"TABLE IX &#8227; V-G Cross-corpus Generalization Ability &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IX</span></a>, M<sup class=\"ltx_sup\">4</sup>SER consistently outperforms SAMS and MF-AED-AEC across all shot settings. For example, with only 5 labeled samples per class, M<sup class=\"ltx_sup\">4</sup>SER achieves ACC of 57.9% in the IE<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ME setting, already surpassing MF-AED-AEC with 20 shots. As the number of target samples increases, our model scales well and reaches ACC of 62.1% and W-F1 61.5% at 60-shot. In the ME<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>IE setting, M<sup class=\"ltx_sup\">4</sup>SER reaches UA of 75.9%, demonstrating excellent cross-domain adaptability.</p>\n\n",
                "matched_terms": [
                    "our",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose M<sup class=\"ltx_sup\">4</sup>SER, a novel emotion recognition method that combines multimodal, multirepresentation, multitask, and multistrategy learning. M<sup class=\"ltx_sup\">4</sup>SER leverages an innovative multimodal fusion module to learn modality-specific and modality-invariant representations, capturing unique features of each modality and common features across modalities. We then introduce a modality discriminator to enhance modality diversity through adversarial learning. Additionally, we design two auxiliary tasks, AED and AEC, aimed at enhancing the semantic consistency within the text modality. Finally, we propose a label-based contrastive learning strategy to distinguish different emotional features. Results of experiments on the IEMOCAP and MELD datasets demonstrate that M<sup class=\"ltx_sup\">4</sup>SER surpasses previous baselines, proving its effectiveness. In the future, we plan to extend our approach to the visual modality and introduce disentangled representation learning to further enhance emotion recognition performance. We also plan to investigate whether AED and AEC modules remain necessary when using powerful LLM-based ASR systems.</p>\n\n",
                "matched_terms": [
                    "text",
                    "method",
                    "aec",
                    "performance",
                    "our",
                    "iemocap",
                    "asr",
                    "aed",
                    "modules",
                    "m4ser",
                    "when",
                    "modality"
                ]
            }
        ]
    },
    "S5.T7": {
        "source_file": "M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition",
        "caption": "TABLE VII: Consistency Analysis between Cross-modal Speech and Text Representations Using ASR and GT Texts.",
        "body": "Metric\nExample (a)\nExample (b)\n\n\nASR\nGT\nASR\nGT\n\n\nDTW ↓\\downarrow\n\n0.52\n1.89\n1.46\n1.57",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Metric</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Example (a)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Example (b)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ASR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">GT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ASR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">GT</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_tt\">DTW <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T7.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">0.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">1.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">1.46</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">1.57</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "crossmodal",
            "text",
            "representations",
            "consistency",
            "dtw",
            "texts",
            "↓downarrow",
            "speech",
            "asr",
            "analysis",
            "example",
            "between",
            "metric",
            "vii"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Similarly, Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F11\" title=\"Figure 11 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(b) shows inconsistency between GT text and speech features. GT text contains words (e.g., &#8220;don&#8217;t know&#8221;) indicating confusion and uncertainty, whereas speech features exhibit significant emotional fluctuations. This mismatch complicates feature alignment in cross-modal learning, resulting in erroneous anger classification. In contrast, ASR text shows higher consistency with speech features, highlighting emotional words (&#8220;freaking out&#8221;, &#8220;what the hell&#8221;, and &#8220;all of a sudden&#8221;) despite errors and simplifications. Representation weights confirm high consistency between ASR text and speech features across multiple regions, enhancing cross-modal learning accuracy and correct emotion identification as &#8220;Happy&#8221; (&#8220;Excited&#8221;). We demonstrate with dynamic time warping (DTW) that ASR transcriptions are more consistent with speech than GT transcriptions, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T7\" title=\"TABLE VII &#8227; V-F Visualization Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VII</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Multimodal speech emotion recognition (SER) has emerged as pivotal for improving human&#8211;machine interaction. Researchers are increasingly leveraging both speech and textual information obtained through automatic speech recognition (ASR) to comprehensively recognize emotional states from speakers. Although this approach reduces reliance on human-annotated text data, ASR errors possibly degrade emotion recognition performance.\nTo address this challenge, in our previous work, we introduced two auxiliary tasks, namely, ASR error detection and ASR error correction, and we proposed a novel multimodal fusion (MF) method for learning modality-specific and modality-invariant representations across different modalities.\nBuilding on this foundation, in this paper, we introduce two additional training strategies. First, we propose an adversarial network to enhance the diversity of modality-specific representations. Second, we introduce a label-based contrastive learning strategy to better capture emotional features.\nWe refer to our proposed method as M<sup class=\"ltx_sup\">4</sup>SER and validate its superiority over state-of-the-art methods through extensive experiments using IEMOCAP and MELD datasets.</p>\n\n",
                "matched_terms": [
                    "text",
                    "representations",
                    "speech",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent research has shown that single-modal approaches struggle to meet the increasing demands for SER owing to their inherent complexity. As a solution, multimodal information has emerged as a viable option because it offers diverse emotional cues. A common strategy involves integrating speech and text modalities to achieve multimodal SER. Text data can be obtained through manual transcription or automatic speech recognition (ASR), which have been widely adopted in recent studies to reduce reliance on extensive annotated datasets.\nAlthough text-based pretrained models such as BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib6\" title=\"\">6</a>]</cite> and RoBERTa <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib7\" title=\"\">7</a>]</cite> have excelled in multimodal SER tasks, the accuracy of ASR remains equally crucial for achieving precise emotion recognition (ER) results. To mitigate the impact of ASR errors, Santoso et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib8\" title=\"\">8</a>]</cite> have integrated self-attention mechanisms and word-level confidence measurements, particularly reducing the importance of words with high error probabilities, thereby enhancing SER performance. However, this approach heavily depends on the performance of the ASR system, thus limiting its generalizability. Lin and Wang <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> have proposed an auxiliary ASR error detection task aimed at determining the probabilities of erroneous words to adjust the trust levels assigned to each word in the ASR hypothesis, thereby improving multimodal SER performance. However, this method focuses solely on error detection within the ASR hypothesis without directly correcting these errors, indicating that it does not fully enhance the coherence of semantic information.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the other hand, how to effectively integrate speech and text modalities has become a key focus. Previous methods have included simple feature concatenation, recurrent neural networks (RNNs), convolutional neural networks (CNNs), and cross-modal attention mechanisms <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib1\" title=\"\">1</a>]</cite>. Although these methods have shown some effectiveness, they often face challenges arising from differences in representations across different modalities.\nIn recent multimodal tasks such as sentiment analysis <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib11\" title=\"\">11</a>]</cite>, researchers have proposed learning two distinct types of representation to enhance multimodal learning.\nThe first is a representation tailored to each modality, that is, a modality-specific representation (MSR).\nThe second is modality-invariant representation (MIR), aimed at mapping all modalities of the same utterance into a shared subspace. This representation captures commonalities in multimodal signals as well as the speaker&#8217;s shared motives, which collectively affect the emotional state of the utterance through distribution alignment. By combining these two types of representation, one can obtain a comprehensive view of multimodal data can be provided for downstream tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib12\" title=\"\">12</a>]</cite>.\nHowever, these methods focus on utterance-level representations, which, while effective for short and isolated utterances, often fail to capture fine-grained emotional dynamics, especially in long and complex interactions. Such coarse-grained representations fail to capture subtle temporal cues and modality-specific variations that are crucial for accurate emotion recognition.\nMoreover, mapping entire utterances to a shared or modality-specific space using similarity loss oversimplifies the underlying multimodal relationships and hinders effective cross-modal alignment.</p>\n\n",
                "matched_terms": [
                    "crossmodal",
                    "text",
                    "speech",
                    "analysis",
                    "representations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these issues, we propose two learning strategies. First, we introduce adversarial learning to enhance the diversity and robustness of modality representations, thereby reducing the impact of ASR errors on SER tasks. Second, we introduce a contrastive learning strategy based on emotion labels to more accurately distinguish and capture feature differences between different emotion categories, thus improving the performance and robustness of the SER system.\nIn summary, our main contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "between",
                    "representations",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce an adversarial modality discriminator to enhance the diversity of MSRs and improve the generalization of MIRs. This design alleviates modality mismatch and suppresses modality-specific noise introduced by ASR errors, which often cause misalignment between acoustic and textual features and degrade the robustness of multimodal fusion.</p>\n\n",
                "matched_terms": [
                    "between",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the basis of our previous work, we present M<sup class=\"ltx_sup\">4</sup>SER, an SER method that leverages multimodal, multirepresentation, multitask, and multistrategy learning. The difference between our M<sup class=\"ltx_sup\">4</sup>SER and previous SER methods is illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S1.F1\" title=\"Figure 1 &#8227; I Introduction &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. M<sup class=\"ltx_sup\">4</sup>SER mitigates the impact of ASR errors, improves modality-invariant representations, and captures commonalities between modalities to bridge their heterogeneity.</p>\n\n",
                "matched_terms": [
                    "between",
                    "representations",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SER aims to identify the speaker&#8217;s emotional state from spoken utterances. Early SER models primarily rely on audio signals, extracting prosodic and acoustic features such as Mel-frequency cepstral coefficients, filter banks, or other handcrafted descriptors&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib14\" title=\"\">14</a>]</cite>. With the advent of deep learning, models based on RNNs, CNNs and Transformer architectures&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib18\" title=\"\">18</a>]</cite> have achieved significant improvements by modeling complex temporal and hierarchical patterns in speech. More recently, the rise of SSL has led to the development of speech-based pretrained models such as wav2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib3\" title=\"\">3</a>]</cite>, HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib4\" title=\"\">4</a>]</cite>, and WavLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib5\" title=\"\">5</a>]</cite>, which provide rich contextual representations and deliver SOTA performance on various SER benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib19\" title=\"\">19</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "representations",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these limitations, multimodal SER often integrates both speech and text modalities, capturing both acoustic and semantic cues. A common setting involves using speech signals alongside textual transcripts (typically from manual annotations or ASR outputs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib24\" title=\"\">24</a>]</cite>.\nFan et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib21\" title=\"\">21</a>]</cite> proposed multi-granularity attention-based transformers (MGAT) to address emotional asynchrony and modality misalignment.\nSun et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib22\" title=\"\">22</a>]</cite> introduced a method integrating shared and private encoders, projecting each modality into a separate subspace to capture modality consistency and diversity while ensuring label consistency in the output.\nThese methods typically assume access to high-quality manual transcriptions for the text modality.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr",
                    "consistency",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, growing efforts have focused on reducing the dependency on annotated text by directly leveraging ASR hypotheses as input <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib28\" title=\"\">28</a>]</cite>. Santoso et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib25\" title=\"\">25</a>]</cite> introduced confidence-aware self-attention mechanisms that downweight unreliable ASR tokens to mitigate error propagation. Lin and Wang&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> proposed a robust multimodal SER framework that adaptively fuses attention-weighted acoustic representations with ASR-derived text embeddings, compensating for recognition noise in the transcript.</p>\n\n",
                "matched_terms": [
                    "text",
                    "representations",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to factorization-based approaches which explicitly disentangle modality inputs into shared and private spaces, our M<sup class=\"ltx_sup\">4</sup>SER framework adopts a structured encoding&#8211;generation strategy to implicitly model MSRs and MIRs. Specifically, we utilize a stack of cross-modal encoder blocks to extract token-aware and speech-aware features (MSRs), and then apply a dedicated generator composed of hybrid-modal attention and masking mechanisms to derive the MIRs. In addition, unlike prior works that incorporate adversarial learning at a global or representation-level, our M<sup class=\"ltx_sup\">4</sup>SER employs a frame-level discriminator that operates on each temporal representation. A frame-level discriminator attempts to distinguish the modality origin of each temporal representation, while the MIR generator is optimized to deceive the discriminator by producing modality-agnostic outputs that are hard to classify as either text or speech. This fine-grained, per-frame adversarial constraint leads to more robust and aligned cross-modal representations. Moreover, M<sup class=\"ltx_sup\">4</sup>SER integrates this disentanglement framework into a multitask learning setup with ASR-aware auxiliary supervision, enabling more effective adaptation to noisy real-world speech&#8211;text scenarios.</p>\n\n",
                "matched_terms": [
                    "crossmodal",
                    "representations",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ASR error correction (AEC) techniques remain effective in optimizing the ASR hypotheses. AEC has been jointly trained with ER tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib13\" title=\"\">13</a>]</cite> in multitask settings. Li et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib27\" title=\"\">27</a>]</cite> proposed an AEC method that improves transcription quality on low-resource out-of-domain data through cross-modal training and the incorporation of discrete speech units, and they validated its effectiveness in downstream ER tasks. Lin and Wang <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> introduced a robust ER method that leverages complementary semantic information from audio, adapts to ASR errors through an auxiliary task for ASR error detection, and fuses text and acoustic representations for ER.\nOn the basis of <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib32\" title=\"\">32</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib33\" title=\"\">33</a>]</cite>, we propose a partially autoregressive AEC method that uses a label predictor to restrict decoding to only desirable parts of the input sequence embeddings, thereby reducing inference latency.</p>\n\n",
                "matched_terms": [
                    "crossmodal",
                    "text",
                    "speech",
                    "asr",
                    "representations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent years, to fully utilize supervised information, some researchers have proposed supervised contrastive learning (SCL) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib36\" title=\"\">36</a>]</cite>, which leverages label information to construct positive pairs, making the distance between samples of the same class closer than that between samples of different classes.\nZhang et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib37\" title=\"\">37</a>]</cite> introduced label embeddings to better understand the language. Li et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib38\" title=\"\">38</a>]</cite> unified the target setting on a hypersphere and forced the data representations to be close to these targets. Zhu et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib39\" title=\"\">39</a>]</cite> regarded classifier weights as prototypes in the representation space and incorporated them into the contrastive loss.</p>\n\n",
                "matched_terms": [
                    "between",
                    "representations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed M<sup class=\"ltx_sup\">4</sup>SER framework can be formalized as the function <math alttext=\"f(S,T)=(L,Z)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mi>L</mi><mo>,</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f(S,T)=(L,Z)</annotation></semantics></math>. Here, the speech modality <math alttext=\"S=(s_{1},s_{2},\\cdots,s_{m})\\in\\mathbb{R}^{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>s</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>s</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>m</mi></msup></mrow><annotation encoding=\"application/x-tex\">S=(s_{1},s_{2},\\cdots,s_{m})\\in\\mathbb{R}^{m}</annotation></semantics></math> consists of <span class=\"ltx_text ltx_font_italic\">m</span> frames extracted from an utterance, whereas the text modality <math alttext=\"T=(t_{1},t_{2},\\cdots,t_{n})\\in\\mathbb{R}^{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><msub><mi>t</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>t</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>n</mi></msup></mrow><annotation encoding=\"application/x-tex\">T=(t_{1},t_{2},\\cdots,t_{n})\\in\\mathbb{R}^{n}</annotation></semantics></math> comprises <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> tokens from the original ASR hypotheses of the utterance. All tokens are mapped to a predefined WordPiece vocabulary <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib40\" title=\"\">40</a>]</cite>. Additionally, within our multitask learning framework, the primary task is ER, yielding an emotion classification output <math alttext=\"L\\in\\{l_{1},l_{2},\\cdots,l_{e}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>l</mi><mn>1</mn></msub><mo>,</mo><msub><mi>l</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>l</mi><mi>e</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">L\\in\\{l_{1},l_{2},\\cdots,l_{e}\\}</annotation></semantics></math>, where <math alttext=\"e\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mi>e</mi><annotation encoding=\"application/x-tex\">e</annotation></semantics></math> represents the number of emotional categories. Concurrently, the auxiliary tasks include ASR error detection (AED) and ASR error correction (AEC). The output of these auxiliary tasks is <math alttext=\"Z\\in\\{Z_{1},Z_{2},\\cdots,Z_{k}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>Z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>Z</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>Z</mi><mi>k</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Z\\in\\{Z_{1},Z_{2},\\cdots,Z_{k}\\}</annotation></semantics></math>, representing all the ground truth (GT) sequences where the ASR transcriptions differ from the human-annotated transcriptions.\n<math alttext=\"Z_{k}=(z_{1,k},z_{2,k},\\cdots,z_{c,k})\\in\\mathbb{R}^{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><msub><mi>Z</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mn>1</mn><mo>,</mo><mi>k</mi></mrow></msub><mo>,</mo><msub><mi>z</mi><mrow><mn>2</mn><mo>,</mo><mi>k</mi></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>z</mi><mrow><mi>c</mi><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>c</mi></msup></mrow><annotation encoding=\"application/x-tex\">Z_{k}=(z_{1,k},z_{2,k},\\cdots,z_{c,k})\\in\\mathbb{R}^{c}</annotation></semantics></math>, denoting that the <math alttext=\"k^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\"><semantics><msup><mi>k</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">k^{th}</annotation></semantics></math> error position includes <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m10\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> tokens.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contextual Speech Representations.</span> To obtain comprehensive contextual representations of acoustic features, we utilize a pretrained SSL model, HuBERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib4\" title=\"\">4</a>]</cite>, as our acoustic encoder. HuBERT integrates CNN layers with a transformer encoder to effectively capture both speech features and contextual information.\nWe denote the acoustic hidden representations of the speech modality inputs <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> generated by HuBERT as <math alttext=\"H_{S}=(h_{S}^{(1)},h_{S}^{(2)},\\cdots,h_{S}^{(m)})\\in\\mathbb{R}^{m\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>H</mi><mi>S</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msubsup><mi>h</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">H_{S}=(h_{S}^{(1)},h_{S}^{(2)},\\cdots,h_{S}^{(m)})\\in\\mathbb{R}^{m\\times d}</annotation></semantics></math>, where <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> is the dimension of hidden representations.</p>\n\n",
                "matched_terms": [
                    "representations",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contextual Token Representations.</span> We use the pretrained language model BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib6\" title=\"\">6</a>]</cite> as our text encoder to obtain the token representations <math alttext=\"H_{T}=(h_{T}^{(1)},h_{T}^{(2)},\\cdots,h_{T}^{(n)})\\in\\mathbb{R}^{n\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>H</mi><mi>T</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msubsup><mi>h</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">H_{T}=(h_{T}^{(1)},h_{T}^{(2)},\\cdots,h_{T}^{(n)})\\in\\mathbb{R}^{n\\times d}</annotation></semantics></math> for the text modality inputs <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "representations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first subtask we introduce is AED, which is designed to detect the positions of ASR errors.\nSimilarly to <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib31\" title=\"\">31</a>]</cite>, we align <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> and <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> by determining the longest common subsequence between them. The aligned tokens are labeled <span class=\"ltx_text ltx_font_italic\">KEEP</span> (<span class=\"ltx_text ltx_font_bold\">K</span>), whereas the remaining tokens are labeled <span class=\"ltx_text ltx_font_italic\">DELETE</span> (<span class=\"ltx_text ltx_font_bold\">D</span>) or <span class=\"ltx_text ltx_font_italic\">CHANGE</span> (<span class=\"ltx_text ltx_font_bold\">C</span>). A specific example is illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(b).\nThe label prediction layer is a straightforward fully connected (FC) layer with three classes.</p>\n\n",
                "matched_terms": [
                    "example",
                    "between",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More specifically, once the erroneous positions are identified, the decoder constructs a separate generation sequence for each token labeled as <span class=\"ltx_text ltx_font_bold\">C</span>. Each sequence begins with a special start token <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_italic\">BOS<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span> and generates the corrected text in an autoregressive manner.\nAt each decoding step, the decoder input for a given correction sequence consists of all previously generated tokens, each concatenated with the representation of the corresponding erroneous token.\nAll correction sequences are processed in parallel by a shared transformer decoder, which attends to the full ASR representation <math alttext=\"H_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m3\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">H_{T}</annotation></semantics></math> as memory.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the basis of <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib13\" title=\"\">13</a>]</cite>, our MF module is composed of three cross-modal encoder (CME) blocks and one MIR generator. The objective is to facilitate the learning of modality-specific and modality-invariant representations.\nIn this section, we provide an in-depth explanation of the operation of each CME block and the MIR generator.</p>\n\n",
                "matched_terms": [
                    "crossmodal",
                    "representations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CME</span> is structured akin to a standard transformer layer, featuring an <span class=\"ltx_text ltx_font_italic\">h</span>-head cross-attention module <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib42\" title=\"\">42</a>]</cite>, residual connections, and FC layers, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.F3\" title=\"Figure 3 &#8227; III-E Multimodal Fusion (MF) Module &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nTo acquire token-aware speech representations, we first feed <math alttext=\"H_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m1\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">H_{T}</annotation></semantics></math> as the query and <math alttext=\"H_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m2\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">H_{S}</annotation></semantics></math> as the key and value into a CME block:</p>\n\n",
                "matched_terms": [
                    "representations",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> denotes either the speech or text modality and <math alttext=\"H_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m2\" intent=\":literal\"><semantics><msub><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">H_{ST}</annotation></semantics></math> represents the concatenation of <math alttext=\"H_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m3\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">H_{S}</annotation></semantics></math> and <math alttext=\"H_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m4\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">H_{T}</annotation></semantics></math>, which is then fed into a FC layer to map it back to the same dimension as <math alttext=\"H_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m5\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">H_{S}</annotation></semantics></math>. This process integrates both speech and text information into a unified representation. The resulting features are subsequently summed with <math alttext=\"H_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m6\" intent=\":literal\"><semantics><msub><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">H_{ST}</annotation></semantics></math>, culminating in the ultimate modality-invariant representation:</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further develop the modality discriminator <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m1\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> utilizing the modality-invariant representations produced by the MIR generator. This discriminator illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(f) is composed of two linear layers with a ReLU activation function in between, followed by a Sigmoid activation function. To enhance the MIR&#8217;s capability to remain modality-agnostic, we employ adversarial learning techniques.\nSpecifically, the discriminator <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> outputs a scalar value between 0 and 1 for each frame, where values close to 0 indicate the text modality, values close to 1 indicate the speech modality, and values near 0.5 represent an ambiguous modality.\nHere, <math alttext=\"D(H)\\in\\mathbb{R}^{m\\times 1},H\\in\\{H^{(spe)}_{T},H^{(spe)}_{S},H_{ST}^{\\rm(inv)}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>D</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow></msup></mrow><mo>,</mo><mrow><mi>H</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>inv</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">D(H)\\in\\mathbb{R}^{m\\times 1},H\\in\\{H^{(spe)}_{T},H^{(spe)}_{S},H_{ST}^{\\rm(inv)}\\}</annotation></semantics></math>.\nOn one hand, we aim for the discriminator that correctly classifies frames in the modality-specific representations <math alttext=\"H^{(spe)}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H^{(spe)}_{T}</annotation></semantics></math> and <math alttext=\"H^{(spe)}_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m5\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H^{(spe)}_{S}</annotation></semantics></math> as 0 and 1, respectively. On the other hand, to enhance the modality agnosticism of the modality-invariant representation <math alttext=\"H_{ST}^{\\rm(inv)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m6\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>inv</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{ST}^{\\rm(inv)}</annotation></semantics></math> produced by the MIR generator, we aim for a discriminator that outputs values close to 0.5, indicating an ambiguous modality. With this design of the MIR generator and discriminator, we define a generative adversarial training objective, mathematically represented as</p>\n\n",
                "matched_terms": [
                    "text",
                    "representations",
                    "speech",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance the model&#8217;s capability to learn emotion features from multimodal data, we employ a label-based contrastive learning task to complement the cross-entropy loss, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(g). This task aids the model in extracting emotion-related features when the MF module integrates speech and text data. As depicted in the LCL task in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.F5\" title=\"Figure 5 &#8227; III-H Label-based Contrastive Learning (LCL) &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we categorize data in each batch into positive and negative samples on the basis of emotion labels. For instance, in a batch containing eight samples, we compute the set of positive samples for each sample, where those with the same label are considered positive samples (yellow squares), and those with different labels are considered negative samples (white squares). We then calculate the label-based contrastive loss (LCL) using Eq. <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:eq:lcl</span>, which promotes instances of a specific label to be more similar to each other than instances of other labels.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the GAN training strategy <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib34\" title=\"\">34</a>]</cite>, we divide the backpropagation process into two steps. Firstly, we maximize <math alttext=\"\\mathcal{L}_{\\rm GAN}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>GAN</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm GAN}</annotation></semantics></math> to update the discriminator, during which the generator is detached from the optimization. According to Eq. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.Ex1\" title=\"III-G Modality Discriminator &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III-G</span></a>), on one hand, maximizing the first term <math alttext=\"L_{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>D</mi></msub><annotation encoding=\"application/x-tex\">L_{D}</annotation></semantics></math> of <math alttext=\"\\mathcal{L}_{\\rm GAN}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>GAN</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm GAN}</annotation></semantics></math> essentially trains the discriminator to correctly distinguish between text and audio features by making <math alttext=\"H_{S}^{\\rm(spe)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mi>spe</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{S}^{\\rm(spe)}</annotation></semantics></math> close to 1 and <math alttext=\"H_{T}^{\\rm(spe)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mi>spe</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{T}^{\\rm(spe)}</annotation></semantics></math> close to 0 <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Function <math alttext=\"\\log(x)+\\log(1-y)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>&#8722;</mo><mi>y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(x)+\\log(1-y)</annotation></semantics></math>, where <math alttext=\"x,y\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x,y\\in(0,1)</annotation></semantics></math> reaches its maximum when <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m3\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> approaches 1 and <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m4\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math> approaches 0.</span></span></span>. On the other hand, maximizing the second term <math alttext=\"L_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m6\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>G</mi></msub><annotation encoding=\"application/x-tex\">L_{G}</annotation></semantics></math> (i.e., minimizing <math alttext=\"-L_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m7\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><msub><mi>L</mi><mi>G</mi></msub></mrow><annotation encoding=\"application/x-tex\">-L_{G}</annotation></semantics></math>) will make <math alttext=\"H^{\\text{(inv)}}_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mtext>(inv)</mtext></msubsup><annotation encoding=\"application/x-tex\">H^{\\text{(inv)}}_{ST}</annotation></semantics></math> approach 0 or 1 <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Function <math alttext=\"\\log(x)+\\log(1-x)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>&#8722;</mo><mi>x</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(x)+\\log(1-x)</annotation></semantics></math>, where <math alttext=\"x\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m2\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x\\in(0,1)</annotation></semantics></math> reaches its maximum at <math alttext=\"x=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m3\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">x=0.5</annotation></semantics></math> and its minimum near <math alttext=\"x=0\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m4\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">x=0</annotation></semantics></math> and <math alttext=\"x=1\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m5\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">x=1</annotation></semantics></math>.</span></span></span>, indicating that the discriminator recognizes <math alttext=\"H^{\\text{(inv)}}_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m9\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mtext>(inv)</mtext></msubsup><annotation encoding=\"application/x-tex\">H^{\\text{(inv)}}_{ST}</annotation></semantics></math> as modality-specific, which can be either text or audio, contrary to our desired modality invariance. Secondly, we freeze the discriminator parameters and update the remaining parameters by minimizing <math alttext=\"L_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m10\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>G</mi></msub><annotation encoding=\"application/x-tex\">L_{G}</annotation></semantics></math>, which makes the output of the discriminator <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m11\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> for the modality-invariant features <math alttext=\"H^{\\text{(inv)}}_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m12\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mtext>(inv)</mtext></msubsup><annotation encoding=\"application/x-tex\">H^{\\text{(inv)}}_{ST}</annotation></semantics></math> approach 0.5, thereby blurring these features between audio and text modalities to achieve modality agnosticism. Additionally, <math alttext=\"\\mathcal{L}_{\\text{ER}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m13\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ER</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ER}}</annotation></semantics></math> optimizes the downstream emotion recognition model, <math alttext=\"\\mathcal{L}_{\\text{AED}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m14\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>AED</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{AED}}</annotation></semantics></math> and <math alttext=\"\\mathcal{L}_{\\text{AEC}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m15\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>AEC</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{AEC}}</annotation></semantics></math> optimize the auxiliary tasks of ASR error detection and ASR error correction models, respectively, and <math alttext=\"\\mathcal{L}_{\\text{LCL}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m16\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>LCL</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{LCL}}</annotation></semantics></math> implements the LCL strategy. The entire system is trained in an end-to-end manner and optimized by adjusting weight parameters.</p>\n\n",
                "matched_terms": [
                    "text",
                    "between",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the inference stage, the AED and AEC modules are excluded. The remaining modules accept speech and ASR transcripts as input and output emotion classification results.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The acoustic encoder was initialized using the <span class=\"ltx_text ltx_font_typewriter\">hubert-base-ls960<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">3</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/facebook/hubert-base-ls960\" title=\"\">https://huggingface.co/facebook/hubert-base-ls960</a></span></span></span></span> model, producing acoustic representations <math alttext=\"d_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">d_{S}</annotation></semantics></math> with a dimensionality of 768.\nThe text encoder employed the <span class=\"ltx_text ltx_font_typewriter\">bert-base-uncased<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">4</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/google-bert/bert-base-uncased\" title=\"\">https://huggingface.co/google-bert/bert-base-uncased</a></span></span></span></span> model for initialization, yielding token representations <math alttext=\"d_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">d_{T}</annotation></semantics></math> with a dimensionality of 768. The vocabulary size for word tokenization <math alttext=\"d_{vocab}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>d</mi><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">d_{vocab}</annotation></semantics></math> was set to 30,522. We set the hidden size <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> to 768, the number of attention layers to 12, and the number of attention heads to 12. Both the HuBERT and BERT models were fine-tuned during the training stage.\nThe transformer decoder in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c) adopted a single-layer transformer with a hidden size of 768. Additionally, we obtained corresponding ASR hypotheses using the Whisper ASR model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib48\" title=\"\">48</a>]</cite>. We used the <span class=\"ltx_text ltx_font_typewriter\">openai/whisper-medium.en<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">5</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/openai/whisper-medium.en\" title=\"\">https://huggingface.co/openai/whisper-medium.en</a></span></span></span></span> and <span class=\"ltx_text ltx_font_typewriter\">openai/whisper-large-v2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">6</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/openai/whisper-large-v2\" title=\"\">https://huggingface.co/openai/whisper-large-v2</a></span></span></span></span> models, achieving word error rates (WERs) of 20.48% and 37.87% on the IEMOCAP and MELD datasets, respectively.</p>\n\n",
                "matched_terms": [
                    "text",
                    "representations",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SAWC</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib47\" title=\"\">47</a>]</cite> adjusts importance weights using confidence measures, reducing ASR error impact by emphasizing the corresponding speech segments.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RMSER-AEA</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> uses complementary semantic information, adapting to ASR errors through an auxiliary task and fusing text and acoustic representations for SER.</p>\n\n",
                "matched_terms": [
                    "text",
                    "representations",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SAMS</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib52\" title=\"\">52</a>]</cite> leverages high-level emotion representations as supervisory signals to build a multi-spatial learning framework for each modality, enabling cross-modal semantic learning and fusion representation exploration.</p>\n\n",
                "matched_terms": [
                    "crossmodal",
                    "representations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MMER</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib14\" title=\"\">14</a>]</cite> employs early fusion and cross-modal self-attention between text and acoustic modalities, and addresses three novel auxiliary tasks to enhance SER. For fairness, we selected the result that did not introduce augmented text for comparison.</p>\n\n",
                "matched_terms": [
                    "crossmodal",
                    "between",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MAF-DCT</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib54\" title=\"\">54</a>]</cite> introduces a dual approach utilizing SSL representation and spectral features for comprehensive speech feature extraction, along with a dual cross-modal transformer to handle interactions.</p>\n\n",
                "matched_terms": [
                    "crossmodal",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <span class=\"ltx_text ltx_font_bold\">MF-AED-AEC</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib13\" title=\"\">13</a>]</cite>, as in our previous work, we consider two auxiliary tasks to improve semantic coherence in ASR text and introduce a MF method to learn shared representations.</p>\n\n",
                "matched_terms": [
                    "text",
                    "representations",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Full</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib55\" title=\"\">55</a>]</cite> uses contextual cross-modal transformers and graph convolutional networks for enhanced emotion representations and modality fusion.</p>\n\n",
                "matched_terms": [
                    "crossmodal",
                    "representations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T2\" title=\"TABLE II &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> compare the performance of the M<sup class=\"ltx_sup\">4</sup>SER method with those of recent multimodal SER methods on the IEMOCAP and MELD datasets. Our proposed M<sup class=\"ltx_sup\">4</sup>SER achieves a WA of 79.2% and a UA of 80.1% on IEMOCAP, and an ACC of 66.5% and a W-F1 of 66.0% on MELD, outperforming other models and demonstrating the superiority of M<sup class=\"ltx_sup\">4</sup>SER. Specifically, on the IEMOCAP dataset, compared with the recent MAF-DCT, the M<sup class=\"ltx_sup\">4</sup>SER method shows a 0.7% improvement in WA and a 0.8% improvement in UA, reaching a new SOTA performance. These improvements stem from our M<sup class=\"ltx_sup\">4</sup>SER&#8217;s capability to mitigate the impact of ASR errors, capture modality-specific and modality-invariant representations across different modalities, enhance commonality between modalities through adversarial learning, and excel in emotion feature learning with the LCL loss.</p>\n\n",
                "matched_terms": [
                    "between",
                    "representations",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multimodalities.</span>\nTo assess the necessity of multimodality, we conduct single-modal comparison experiments involving text and speech modalities. These experiments operate under a baseline framework excluding the MF module, LCL loss, and GAN loss, precluding intermodal interaction.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(A), in the single-modal experiments, the performance of speech modality in the IEMOCAP dataset is better than that of text modality, whereas the reverse is observed in the MELD dataset. This divergence suggests that emotional information is more distinctly conveyed through speech in IEMOCAP, whereas MELD leans towards text for emotional expression.\nMoreover, the multimodal baseline model, which simply concatenates speech and text features for recognition (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> A(3)), significantly enhances performance compared with single-modal baselines, highlighting the essential role of multimodal information.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multistrategies.</span> To validate the effectiveness of the adversarial training strategy outlined in Alg. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#alg1\" title=\"Algorithm 1 &#8227; III-I Joint Training &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we remove the adversarial training strategy from M<sup class=\"ltx_sup\">4</sup>SER. The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(D)(1) show a decrease in performance on both datasets, demonstrating the critical role of this strategy in learning modality-invariant representations. The proposed modality discriminator effectively enhances the modality agnosticism of the refined representations from the generator.\nAdditionally, omitting the LCL strategy described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.SS8\" title=\"III-H Label-based Contrastive Learning (LCL) &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III-H</span></a> also results in similar performance degradation (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(D)(2)). We visualize the results before and after introducing this strategy to demonstrate its effectiveness, with specific analysis also detailed in the following t-SNE analysis section. Moreover, removing both of these strategies further diminishes emotion recognition performance, confirming that both learning strategies contribute significantly to performance improvement.</p>\n\n",
                "matched_terms": [
                    "representations",
                    "analysis"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">t-SNE Analysis.</span> To intuitively demonstrate the advantages of our proposed M<sup class=\"ltx_sup\">4</sup>SER model on IEMOCAP and MELD datasets, we utilize the t-distributed stochastic neighbor embedding (t-SNE) tool <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib61\" title=\"\">61</a>]</cite> to visualize the learned emotion features using all samples from session 3 of the IEMOCAP and test set of the MELD. We compare these features across the following models: the speech model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(a)), the text model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(b)), the proposed M<sup class=\"ltx_sup\">4</sup>SER model without label-based contrastive learning (LCL) (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(c)), and the proposed M<sup class=\"ltx_sup\">4</sup>SER model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(d)).</p>\n\n",
                "matched_terms": [
                    "text",
                    "analysis",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, compared with the M<sup class=\"ltx_sup\">4</sup>SER model without the LCL strategy, our M<sup class=\"ltx_sup\">4</sup>SER model achieves better clustering for each emotion category, making them as distinct as possible. For instance, on the IEMOCAP dataset, the intra-class clustering of samples learned by the M<sup class=\"ltx_sup\">4</sup>SER model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(d)) is more pronounced than that learned by the M<sup class=\"ltx_sup\">4</sup>SER model without the LCL strategy (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(c)), especially with clearer boundaries for sad and angry samples. Additionally, the distances between the four types of emotion in the emotion vector space increase. Although distinguishing MELD samples is more challenging than distinguishing IEMOCAP samples, the overall trend remains similar.\nThis indicates that the proposed M<sup class=\"ltx_sup\">4</sup>SER model effectively leverages both modality-specific and modality-invariant representations to capture high-level shared feature representations across speech and text modalities for emotion recognition.</p>\n\n",
                "matched_terms": [
                    "text",
                    "representations",
                    "speech",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Representation Analysis.</span>\nWe find that our M<sup class=\"ltx_sup\">4</sup>SER method performs better when using ASR text than when using GT text, as presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T6\" title=\"TABLE VI &#8227; V-F Visualization Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a>. To investigate this finding, we visualize representation weights for two examples from IEMOCAP, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F11\" title=\"Figure 11 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. We display representations of each modality across the temporal level in two learning spaces, accumulating hidden features of <math alttext=\"H_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m2\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">H_{S}</annotation></semantics></math>, <math alttext=\"H_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m3\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">H_{T}</annotation></semantics></math>, <math alttext=\"\\hat{H}_{S}^{spe}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m4\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>H</mi><mo>^</mo></mover><mi>S</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\hat{H}_{S}^{spe}</annotation></semantics></math>, and <math alttext=\"H_{T}^{spe}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m5\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>T</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{T}^{spe}</annotation></semantics></math> into one dimension.\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F11\" title=\"Figure 11 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(a) illustrates that during single-modal learning with ASR text, representation weights in the text modality mainly focus on the word &#8220;oh&#8221;, whereas in the speech modality, representation weights concentrate towards the sentence&#8217;s end. After cross-modal learning, we observed a significant decrease in &#8220;oh&#8221; weight in the text modality. This change occurs as the model detects errors or inaccuracies in ASR transcription using AED and AEC modules, shifting representation weights to other words such as &#8220;that&#8217;s&#8221;. Owing to limited emotional cues in the text, the model relies more on speech features, especially anger-related cues such as intonation and volume, critical for accurate anger recognition. Conversely, with GT text, after cross-modal learning, attention in the text modality focuses on words conveying positive emotions such as &#8220;amusing&#8221;, whereas speech modality distribution remains largely unchanged. Without ASR error signals, the model leans towards these textual features, leading to a bias towards happiness in emotion recognition.</p>\n\n",
                "matched_terms": [
                    "crossmodal",
                    "text",
                    "speech",
                    "asr",
                    "analysis",
                    "representations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, compared with ASR text, GT text exhibits two primary differences:</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Lack of ASR error signals:</span> GT text lacks ASR errors, missing additional cues beneficial for emotion recognition. This limitation possibly hinders capturing intense emotional signals when relying solely on GT text.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Differences in feature weight distribution:</span> the absence of ASR errors alters feature weight distribution in GT text modality compared with ASR text, potentially leading to inaccurate emotion classification.</p>\n\n",
                "matched_terms": [
                    "text",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose M<sup class=\"ltx_sup\">4</sup>SER, a novel emotion recognition method that combines multimodal, multirepresentation, multitask, and multistrategy learning. M<sup class=\"ltx_sup\">4</sup>SER leverages an innovative multimodal fusion module to learn modality-specific and modality-invariant representations, capturing unique features of each modality and common features across modalities. We then introduce a modality discriminator to enhance modality diversity through adversarial learning. Additionally, we design two auxiliary tasks, AED and AEC, aimed at enhancing the semantic consistency within the text modality. Finally, we propose a label-based contrastive learning strategy to distinguish different emotional features. Results of experiments on the IEMOCAP and MELD datasets demonstrate that M<sup class=\"ltx_sup\">4</sup>SER surpasses previous baselines, proving its effectiveness. In the future, we plan to extend our approach to the visual modality and introduce disentangled representation learning to further enhance emotion recognition performance. We also plan to investigate whether AED and AEC modules remain necessary when using powerful LLM-based ASR systems.</p>\n\n",
                "matched_terms": [
                    "text",
                    "representations",
                    "consistency",
                    "asr"
                ]
            }
        ]
    },
    "S5.T8": {
        "source_file": "M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition",
        "caption": "TABLE VIII: Analysis of cross-corpus generalization ability on a 4-class emotion classification task (%). We reimplement the method indicated by ∘ and obtain the corresponding result. IE →\\rightarrow ME indicates that the IEMOCAP dataset is used for training and MELD is used for testing. Conversely, ME →\\rightarrow IE means the model is trained on MELD and evaluated on IEMOCAP.",
        "body": "Methods\nIE →\\rightarrow ME\nME →\\rightarrow IE\n\n\nACC\nW-F1\nWA\nUA\n\n\nSAMS∘ [52]\n\n17.0\n11.6\n36.3\n33.9\n\n\nMF-AED-AEC∘ [13]\n\n56.0\n54.2\n62.5\n60.6\n\n\nBaseline (HuBERT + BERT)\n53.0\n52.4\n58.9\n57.4\n\n\n   + MSR & MIR\n54.7\n53.2\n60.8\n59.9\n\n\n    + GAN & LCL\n55.6\n53.9\n61.9\n60.7\n\n\n       + AED & AEC (M4SER)\n57.3\n55.5\n64.0\n62.2\n\n\nΔBaseline\\Delta_{\\text{Baseline}}\n4.3↑\\uparrow\n\n3.1↑\\uparrow\n\n5.1↑\\uparrow\n\n4.8↑\\uparrow",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Methods</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">IE <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T8.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> ME</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">ME <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T8.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> IE</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">ACC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">W-F1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">WA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">UA</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">SAMS<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8728;</span></sup> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib52\" title=\"\">52</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">17.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">11.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">36.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">33.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MF-AED-AEC<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8728;</span></sup> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib13\" title=\"\">13</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">56.0</td>\n<td class=\"ltx_td ltx_align_center\">54.2</td>\n<td class=\"ltx_td ltx_align_center\">62.5</td>\n<td class=\"ltx_td ltx_align_center\">60.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Baseline (HuBERT + BERT)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">53.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">52.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">58.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">57.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">&#8194;&#8202;&#8195;+ MSR &amp; MIR</td>\n<td class=\"ltx_td ltx_align_center\">54.7</td>\n<td class=\"ltx_td ltx_align_center\">53.2</td>\n<td class=\"ltx_td ltx_align_center\">60.8</td>\n<td class=\"ltx_td ltx_align_center\">59.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">&#8194;&#8202;&#8195;&#8195;+ GAN &amp; LCL</td>\n<td class=\"ltx_td ltx_align_center\">55.6</td>\n<td class=\"ltx_td ltx_align_center\">53.9</td>\n<td class=\"ltx_td ltx_align_center\">61.9</td>\n<td class=\"ltx_td ltx_align_center\">60.7</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#EFEFEF;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#EFEFEF;\">&#8194;&#8202;<span class=\"ltx_text ltx_font_bold\">&#8195;&#8195;&#8195;&#8196;&#8202;+ AED &amp; AEC (M<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">4</span></sup>SER)</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">57.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">55.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">64.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">62.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><math alttext=\"\\Delta_{\\text{Baseline}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T8.m12\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#916;</mi><mtext>Baseline</mtext></msub><annotation encoding=\"application/x-tex\">\\Delta_{\\text{Baseline}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">4.3<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T8.m13\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">3.1<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T8.m14\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">5.1<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T8.m15\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">4.8<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T8.m16\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "ability",
            "51↑uparrow",
            "emotion",
            "classification",
            "testing",
            "4class",
            "used",
            "sams∘",
            "viii",
            "31↑uparrow",
            "indicated",
            "methods",
            "aed",
            "δbaselinedeltatextbaseline",
            "bert",
            "wf1",
            "43↑uparrow",
            "indicates",
            "48↑uparrow",
            "method",
            "aec",
            "means",
            "iemocap",
            "evaluated",
            "conversely",
            "mfaedaec∘",
            "msr",
            "trained",
            "crosscorpus",
            "analysis",
            "reimplement",
            "acc",
            "mir",
            "training",
            "lcl",
            "m4ser",
            "dataset",
            "result",
            "hubert",
            "baseline",
            "task",
            "→rightarrow",
            "model",
            "corresponding",
            "obtain",
            "generalization",
            "meld",
            "gan"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T8\" title=\"TABLE VIII &#8227; V-G Cross-corpus Generalization Ability &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VIII</span></a> presents the cross-corpus generalization results. Compared with the reimplemented SAMS and MF-AED-AEC baselines, our full model M<sup class=\"ltx_sup\">4</sup>SER achieves the best performance in both IE<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ME and ME<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>IE directions. Specifically, M<sup class=\"ltx_sup\">4</sup>SER outperforms the strong multimodal baseline by 4.3% in ACC and 3.1% in W-F1 under the IE<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ME setting, and by 5.1% WA and 4.8% UA in the ME<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>IE setting.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Multimodal speech emotion recognition (SER) has emerged as pivotal for improving human&#8211;machine interaction. Researchers are increasingly leveraging both speech and textual information obtained through automatic speech recognition (ASR) to comprehensively recognize emotional states from speakers. Although this approach reduces reliance on human-annotated text data, ASR errors possibly degrade emotion recognition performance.\nTo address this challenge, in our previous work, we introduced two auxiliary tasks, namely, ASR error detection and ASR error correction, and we proposed a novel multimodal fusion (MF) method for learning modality-specific and modality-invariant representations across different modalities.\nBuilding on this foundation, in this paper, we introduce two additional training strategies. First, we propose an adversarial network to enhance the diversity of modality-specific representations. Second, we introduce a label-based contrastive learning strategy to better capture emotional features.\nWe refer to our proposed method as M<sup class=\"ltx_sup\">4</sup>SER and validate its superiority over state-of-the-art methods through extensive experiments using IEMOCAP and MELD datasets.</p>\n\n",
                "matched_terms": [
                    "method",
                    "iemocap",
                    "methods",
                    "emotion",
                    "training",
                    "m4ser",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the rapid advancement of deep learning, SER has evolved into end-to-end (E2E) systems capable of directly processing speech signals and outputting emotion classification results.\nIn recent years, the success of self-supervised learning (SSL) has led to the development of a series of speech-based pretrained models such as wav2vec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib3\" title=\"\">3</a>]</cite>, HuBERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib4\" title=\"\">4</a>]</cite>, and WavLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib5\" title=\"\">5</a>]</cite>, achieving the current state-of-the-art (SOTA) performance in SER tasks.</p>\n\n",
                "matched_terms": [
                    "hubert",
                    "emotion",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent research has shown that single-modal approaches struggle to meet the increasing demands for SER owing to their inherent complexity. As a solution, multimodal information has emerged as a viable option because it offers diverse emotional cues. A common strategy involves integrating speech and text modalities to achieve multimodal SER. Text data can be obtained through manual transcription or automatic speech recognition (ASR), which have been widely adopted in recent studies to reduce reliance on extensive annotated datasets.\nAlthough text-based pretrained models such as BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib6\" title=\"\">6</a>]</cite> and RoBERTa <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib7\" title=\"\">7</a>]</cite> have excelled in multimodal SER tasks, the accuracy of ASR remains equally crucial for achieving precise emotion recognition (ER) results. To mitigate the impact of ASR errors, Santoso et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib8\" title=\"\">8</a>]</cite> have integrated self-attention mechanisms and word-level confidence measurements, particularly reducing the importance of words with high error probabilities, thereby enhancing SER performance. However, this approach heavily depends on the performance of the ASR system, thus limiting its generalizability. Lin and Wang <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> have proposed an auxiliary ASR error detection task aimed at determining the probabilities of erroneous words to adjust the trust levels assigned to each word in the ASR hypothesis, thereby improving multimodal SER performance. However, this method focuses solely on error detection within the ASR hypothesis without directly correcting these errors, indicating that it does not fully enhance the coherence of semantic information.</p>\n\n",
                "matched_terms": [
                    "task",
                    "bert",
                    "emotion",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the other hand, how to effectively integrate speech and text modalities has become a key focus. Previous methods have included simple feature concatenation, recurrent neural networks (RNNs), convolutional neural networks (CNNs), and cross-modal attention mechanisms <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib1\" title=\"\">1</a>]</cite>. Although these methods have shown some effectiveness, they often face challenges arising from differences in representations across different modalities.\nIn recent multimodal tasks such as sentiment analysis <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib11\" title=\"\">11</a>]</cite>, researchers have proposed learning two distinct types of representation to enhance multimodal learning.\nThe first is a representation tailored to each modality, that is, a modality-specific representation (MSR).\nThe second is modality-invariant representation (MIR), aimed at mapping all modalities of the same utterance into a shared subspace. This representation captures commonalities in multimodal signals as well as the speaker&#8217;s shared motives, which collectively affect the emotional state of the utterance through distribution alignment. By combining these two types of representation, one can obtain a comprehensive view of multimodal data can be provided for downstream tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib12\" title=\"\">12</a>]</cite>.\nHowever, these methods focus on utterance-level representations, which, while effective for short and isolated utterances, often fail to capture fine-grained emotional dynamics, especially in long and complex interactions. Such coarse-grained representations fail to capture subtle temporal cues and modality-specific variations that are crucial for accurate emotion recognition.\nMoreover, mapping entire utterances to a shared or modality-specific space using similarity loss oversimplifies the underlying multimodal relationships and hinders effective cross-modal alignment.</p>\n\n",
                "matched_terms": [
                    "methods",
                    "msr",
                    "mir",
                    "analysis",
                    "emotion",
                    "obtain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the basis of the above observations, in our previous work <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib13\" title=\"\">13</a>]</cite>, we introduced two auxiliary tasks: ASR error detection (AED) and ASR error correction (AEC). Specifically, we introduced an AED module to identify the positions of ASR errors. Subsequently, we employed an AEC module to correct these errors, enhancing the semantic coherence of ASR hypotheses and reducing the impact of ASR errors on ER tasks.\nAdditionally, we designed a novel multimodal fusion (MF) module for learning frame-level MSRs and MIRs in a shared speech&#8211;text modality space.\nWe observed the following shortcomings in our previous work: 1) Despite introducing the AED and AEC modules, uncorrected ASR errors continue to affect the accuracy of SER. 2) Existing MF methods possibly fail to capture subtle differences in emotional features.</p>\n\n",
                "matched_terms": [
                    "methods",
                    "aec",
                    "aed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the basis of our previous work, we present M<sup class=\"ltx_sup\">4</sup>SER, an SER method that leverages multimodal, multirepresentation, multitask, and multistrategy learning. The difference between our M<sup class=\"ltx_sup\">4</sup>SER and previous SER methods is illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S1.F1\" title=\"Figure 1 &#8227; I Introduction &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. M<sup class=\"ltx_sup\">4</sup>SER mitigates the impact of ASR errors, improves modality-invariant representations, and captures commonalities between modalities to bridge their heterogeneity.</p>\n\n",
                "matched_terms": [
                    "methods",
                    "method",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed M<sup class=\"ltx_sup\">4</sup>SER outperforms state-of-the-arts on the IEMOCAP and MELD datasets. Extensive experiments demonstrate its superiority in SER tasks.</p>\n\n",
                "matched_terms": [
                    "meld",
                    "iemocap",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remaining sections of this paper are organized as follows: In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2\" title=\"II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">II</span></a>, we review related work. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3\" title=\"III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>, we discuss the specific model and method of M<sup class=\"ltx_sup\">4</sup>SER. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S4\" title=\"IV Experimental Setup &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>, we outline the experimental setup used in this study, including datasets, evaluation metrics, and implementation details. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5\" title=\"V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>, we verify the effectiveness of the proposed model. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S6\" title=\"VI Conclusion &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a>, we present our conclusion and future work.</p>\n\n",
                "matched_terms": [
                    "used",
                    "model",
                    "method",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these limitations, multimodal SER often integrates both speech and text modalities, capturing both acoustic and semantic cues. A common setting involves using speech signals alongside textual transcripts (typically from manual annotations or ASR outputs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib24\" title=\"\">24</a>]</cite>.\nFan et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib21\" title=\"\">21</a>]</cite> proposed multi-granularity attention-based transformers (MGAT) to address emotional asynchrony and modality misalignment.\nSun et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib22\" title=\"\">22</a>]</cite> introduced a method integrating shared and private encoders, projecting each modality into a separate subspace to capture modality consistency and diversity while ensuring label consistency in the output.\nThese methods typically assume access to high-quality manual transcriptions for the text modality.</p>\n\n",
                "matched_terms": [
                    "methods",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Learning modality-specific representations (MSRs) and modality-invariant representations (MIRs) has proven effective for multimodal SER, as it enables models to capture both shared semantics across modalities and complementary modality-unique cues.\nHazarika et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib10\" title=\"\">10</a>]</cite> proposed MISA, which disentangles each modality into invariant and specific subspaces through factorization, facilitating effective fusion for emotion prediction.\nLiu et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib29\" title=\"\">29</a>]</cite> proposed a modality-robust MER framework that introduces a contrastive learning module to extract MIRs from full-modality inputs, and an imagination module that reconstructs missing modalities based on the MIRs, enhancing robustness under incomplete input conditions.\nTo explicitly reduce distribution gaps and mitigate feature redundancy, Yang et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib11\" title=\"\">11</a>]</cite> proposed FDMER, which extracts both MSRs and MIRs, combined with consistency&#8211;disparity constraints and a modality discriminator to encourage disentangled learning.\nLiu et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib30\" title=\"\">30</a>]</cite> presented NORM&#8209;TR, introducing a noise&#8209;resistant generic feature (NRGF) extractor trained with noise&#8209;aware adversarial objectives, followed by a multimodal transformer that fuses NRGFs with raw modality features for robust representation learning.</p>\n\n",
                "matched_terms": [
                    "trained",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to factorization-based approaches which explicitly disentangle modality inputs into shared and private spaces, our M<sup class=\"ltx_sup\">4</sup>SER framework adopts a structured encoding&#8211;generation strategy to implicitly model MSRs and MIRs. Specifically, we utilize a stack of cross-modal encoder blocks to extract token-aware and speech-aware features (MSRs), and then apply a dedicated generator composed of hybrid-modal attention and masking mechanisms to derive the MIRs. In addition, unlike prior works that incorporate adversarial learning at a global or representation-level, our M<sup class=\"ltx_sup\">4</sup>SER employs a frame-level discriminator that operates on each temporal representation. A frame-level discriminator attempts to distinguish the modality origin of each temporal representation, while the MIR generator is optimized to deceive the discriminator by producing modality-agnostic outputs that are hard to classify as either text or speech. This fine-grained, per-frame adversarial constraint leads to more robust and aligned cross-modal representations. Moreover, M<sup class=\"ltx_sup\">4</sup>SER integrates this disentanglement framework into a multitask learning setup with ASR-aware auxiliary supervision, enabling more effective adaptation to noisy real-world speech&#8211;text scenarios.</p>\n\n",
                "matched_terms": [
                    "mir",
                    "model",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ASR error correction (AEC) techniques remain effective in optimizing the ASR hypotheses. AEC has been jointly trained with ER tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib13\" title=\"\">13</a>]</cite> in multitask settings. Li et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib27\" title=\"\">27</a>]</cite> proposed an AEC method that improves transcription quality on low-resource out-of-domain data through cross-modal training and the incorporation of discrete speech units, and they validated its effectiveness in downstream ER tasks. Lin and Wang <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> introduced a robust ER method that leverages complementary semantic information from audio, adapts to ASR errors through an auxiliary task for ASR error detection, and fuses text and acoustic representations for ER.\nOn the basis of <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib32\" title=\"\">32</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib33\" title=\"\">33</a>]</cite>, we propose a partially autoregressive AEC method that uses a label predictor to restrict decoding to only desirable parts of the input sequence embeddings, thereby reducing inference latency.</p>\n\n",
                "matched_terms": [
                    "method",
                    "aec",
                    "task",
                    "trained",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The concept of adversarial network was first introduced using GAN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib34\" title=\"\">34</a>]</cite>, which rapidly attracted extensive research interest owing to their strong capability to generate high-quality novel samples on the basis of existing data.\nAs research progressed, the applications of GAN expanded to multimodal tasks.\nIn a multimodal SER task, Ren et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib35\" title=\"\">35</a>]</cite> combined speaker characteristics with single-modal features using an adversarial module to capture both the commonality and diversity of single-modal features, and they finally fused different modalities to generate refined utterance representations for emotion classification.</p>\n\n",
                "matched_terms": [
                    "task",
                    "emotion",
                    "classification",
                    "gan"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed M<sup class=\"ltx_sup\">4</sup>SER framework can be formalized as the function <math alttext=\"f(S,T)=(L,Z)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mi>L</mi><mo>,</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f(S,T)=(L,Z)</annotation></semantics></math>. Here, the speech modality <math alttext=\"S=(s_{1},s_{2},\\cdots,s_{m})\\in\\mathbb{R}^{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>s</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>s</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>m</mi></msup></mrow><annotation encoding=\"application/x-tex\">S=(s_{1},s_{2},\\cdots,s_{m})\\in\\mathbb{R}^{m}</annotation></semantics></math> consists of <span class=\"ltx_text ltx_font_italic\">m</span> frames extracted from an utterance, whereas the text modality <math alttext=\"T=(t_{1},t_{2},\\cdots,t_{n})\\in\\mathbb{R}^{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><msub><mi>t</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>t</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>n</mi></msup></mrow><annotation encoding=\"application/x-tex\">T=(t_{1},t_{2},\\cdots,t_{n})\\in\\mathbb{R}^{n}</annotation></semantics></math> comprises <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> tokens from the original ASR hypotheses of the utterance. All tokens are mapped to a predefined WordPiece vocabulary <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib40\" title=\"\">40</a>]</cite>. Additionally, within our multitask learning framework, the primary task is ER, yielding an emotion classification output <math alttext=\"L\\in\\{l_{1},l_{2},\\cdots,l_{e}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>l</mi><mn>1</mn></msub><mo>,</mo><msub><mi>l</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>l</mi><mi>e</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">L\\in\\{l_{1},l_{2},\\cdots,l_{e}\\}</annotation></semantics></math>, where <math alttext=\"e\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mi>e</mi><annotation encoding=\"application/x-tex\">e</annotation></semantics></math> represents the number of emotional categories. Concurrently, the auxiliary tasks include ASR error detection (AED) and ASR error correction (AEC). The output of these auxiliary tasks is <math alttext=\"Z\\in\\{Z_{1},Z_{2},\\cdots,Z_{k}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>Z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>Z</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>Z</mi><mi>k</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Z\\in\\{Z_{1},Z_{2},\\cdots,Z_{k}\\}</annotation></semantics></math>, representing all the ground truth (GT) sequences where the ASR transcriptions differ from the human-annotated transcriptions.\n<math alttext=\"Z_{k}=(z_{1,k},z_{2,k},\\cdots,z_{c,k})\\in\\mathbb{R}^{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><msub><mi>Z</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mn>1</mn><mo>,</mo><mi>k</mi></mrow></msub><mo>,</mo><msub><mi>z</mi><mrow><mn>2</mn><mo>,</mo><mi>k</mi></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>z</mi><mrow><mi>c</mi><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>c</mi></msup></mrow><annotation encoding=\"application/x-tex\">Z_{k}=(z_{1,k},z_{2,k},\\cdots,z_{c,k})\\in\\mathbb{R}^{c}</annotation></semantics></math>, denoting that the <math alttext=\"k^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\"><semantics><msup><mi>k</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">k^{th}</annotation></semantics></math> error position includes <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m10\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> tokens.</p>\n\n",
                "matched_terms": [
                    "aec",
                    "task",
                    "aed",
                    "emotion",
                    "m4ser",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overall architecture of the M<sup class=\"ltx_sup\">4</sup>SER model is\ncomposed of five modules, namely, the embedding module, AED module, AEC module, MF module, and ER module, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. In the following section, we provide a detailed explanation of each module.</p>\n\n",
                "matched_terms": [
                    "model",
                    "aec",
                    "m4ser",
                    "aed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contextual Speech Representations.</span> To obtain comprehensive contextual representations of acoustic features, we utilize a pretrained SSL model, HuBERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib4\" title=\"\">4</a>]</cite>, as our acoustic encoder. HuBERT integrates CNN layers with a transformer encoder to effectively capture both speech features and contextual information.\nWe denote the acoustic hidden representations of the speech modality inputs <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> generated by HuBERT as <math alttext=\"H_{S}=(h_{S}^{(1)},h_{S}^{(2)},\\cdots,h_{S}^{(m)})\\in\\mathbb{R}^{m\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>H</mi><mi>S</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msubsup><mi>h</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">H_{S}=(h_{S}^{(1)},h_{S}^{(2)},\\cdots,h_{S}^{(m)})\\in\\mathbb{R}^{m\\times d}</annotation></semantics></math>, where <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> is the dimension of hidden representations.</p>\n\n",
                "matched_terms": [
                    "obtain",
                    "model",
                    "hubert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contextual Token Representations.</span> We use the pretrained language model BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib6\" title=\"\">6</a>]</cite> as our text encoder to obtain the token representations <math alttext=\"H_{T}=(h_{T}^{(1)},h_{T}^{(2)},\\cdots,h_{T}^{(n)})\\in\\mathbb{R}^{n\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>H</mi><mi>T</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msubsup><mi>h</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">H_{T}=(h_{T}^{(1)},h_{T}^{(2)},\\cdots,h_{T}^{(n)})\\in\\mathbb{R}^{n\\times d}</annotation></semantics></math> for the text modality inputs <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "obtain",
                    "model",
                    "bert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a second subtask called AEC, which aims to correct the errors determined by AED.\nUnlike conventional autoregressive decoders that start decoding from scratch, our decoder operates in parallel to the tokens predicted as <span class=\"ltx_text ltx_font_bold\">C</span>.</p>\n\n",
                "matched_terms": [
                    "aec",
                    "aed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where &#8220;<math alttext=\"\\sigma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p5.m1\" intent=\":literal\"><semantics><mi>&#963;</mi><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math>&#8221; denotes Sigmoid activation and &#8220;<math alttext=\"\\otimes\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p5.m2\" intent=\":literal\"><semantics><mo>&#8855;</mo><annotation encoding=\"application/x-tex\">\\otimes</annotation></semantics></math>&#8221; indicates element-wise multiplication.\nFinally, the modality-specific and modality-invariant representations are concatenated together to obtain the final multimodal fusion representation <math alttext=\"H_{ST}^{\\rm(fus)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p5.m3\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>fus</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{ST}^{\\rm(fus)}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "obtain",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Emotion classification is performed by applying the temporal average pooling layer to the output feature <math alttext=\"H_{ST}^{\\rm(fus)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p1.m1\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>fus</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{ST}^{\\rm(fus)}</annotation></semantics></math> of the MF module, followed by an FC layer and a SoftMax activation function.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"y_{\\rm emo}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m1\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>emo</mi></msub><annotation encoding=\"application/x-tex\">y_{\\rm emo}</annotation></semantics></math> is the predicted emotion classification and <math alttext=\"e\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m2\" intent=\":literal\"><semantics><mi>e</mi><annotation encoding=\"application/x-tex\">e</annotation></semantics></math> is the number of emotion categories. The corresponding loss function can be defined as</p>\n\n",
                "matched_terms": [
                    "corresponding",
                    "emotion",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further develop the modality discriminator <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m1\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> utilizing the modality-invariant representations produced by the MIR generator. This discriminator illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(f) is composed of two linear layers with a ReLU activation function in between, followed by a Sigmoid activation function. To enhance the MIR&#8217;s capability to remain modality-agnostic, we employ adversarial learning techniques.\nSpecifically, the discriminator <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> outputs a scalar value between 0 and 1 for each frame, where values close to 0 indicate the text modality, values close to 1 indicate the speech modality, and values near 0.5 represent an ambiguous modality.\nHere, <math alttext=\"D(H)\\in\\mathbb{R}^{m\\times 1},H\\in\\{H^{(spe)}_{T},H^{(spe)}_{S},H_{ST}^{\\rm(inv)}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>D</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow></msup></mrow><mo>,</mo><mrow><mi>H</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>inv</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">D(H)\\in\\mathbb{R}^{m\\times 1},H\\in\\{H^{(spe)}_{T},H^{(spe)}_{S},H_{ST}^{\\rm(inv)}\\}</annotation></semantics></math>.\nOn one hand, we aim for the discriminator that correctly classifies frames in the modality-specific representations <math alttext=\"H^{(spe)}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H^{(spe)}_{T}</annotation></semantics></math> and <math alttext=\"H^{(spe)}_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m5\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H^{(spe)}_{S}</annotation></semantics></math> as 0 and 1, respectively. On the other hand, to enhance the modality agnosticism of the modality-invariant representation <math alttext=\"H_{ST}^{\\rm(inv)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m6\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>inv</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{ST}^{\\rm(inv)}</annotation></semantics></math> produced by the MIR generator, we aim for a discriminator that outputs values close to 0.5, indicating an ambiguous modality. With this design of the MIR generator and discriminator, we define a generative adversarial training objective, mathematically represented as</p>\n\n",
                "matched_terms": [
                    "mir",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance the model&#8217;s capability to learn emotion features from multimodal data, we employ a label-based contrastive learning task to complement the cross-entropy loss, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(g). This task aids the model in extracting emotion-related features when the MF module integrates speech and text data. As depicted in the LCL task in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.F5\" title=\"Figure 5 &#8227; III-H Label-based Contrastive Learning (LCL) &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we categorize data in each batch into positive and negative samples on the basis of emotion labels. For instance, in a batch containing eight samples, we compute the set of positive samples for each sample, where those with the same label are considered positive samples (yellow squares), and those with different labels are considered negative samples (white squares). We then calculate the label-based contrastive loss (LCL) using Eq. <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:eq:lcl</span>, which promotes instances of a specific label to be more similar to each other than instances of other labels.</p>\n\n",
                "matched_terms": [
                    "model",
                    "task",
                    "lcl",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the training stage, the learning process is optimized using three loss functions that correspond to ER, AED, and AEC (i.e., <math alttext=\"\\mathcal{L}_{\\rm ER}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>ER</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm ER}</annotation></semantics></math>, <math alttext=\"\\mathcal{L}_{\\rm AED}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>AED</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm AED}</annotation></semantics></math>, and <math alttext=\"\\mathcal{L}_{\\rm AEC}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>AEC</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm AEC}</annotation></semantics></math>), and two training strategies (i.e., <math alttext=\"\\mathcal{L}_{\\rm GAN}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>GAN</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm GAN}</annotation></semantics></math> and <math alttext=\"\\mathcal{L}_{\\rm LCL}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>LCL</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm LCL}</annotation></semantics></math>). The specific optimization process of M<sup class=\"ltx_sup\">4</sup>SER is detailed in Alg. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#alg1\" title=\"Algorithm 1 &#8227; III-I Joint Training &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nThese five loss functions are linearly combined as the overall training objective of M<sup class=\"ltx_sup\">4</sup>SER:</p>\n\n",
                "matched_terms": [
                    "aec",
                    "aed",
                    "lcl",
                    "m4ser",
                    "training",
                    "gan"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m6\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> is the hyperparameter for adjusting the weight between the main task and the auxiliary tasks, and <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m7\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> is the hyperparameter for adjusting the weight between the two auxiliary tasks, and <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m8\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>, <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m9\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> are the hyperparameters for balancing different training strategies.</p>\n\n",
                "matched_terms": [
                    "task",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the GAN training strategy <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib34\" title=\"\">34</a>]</cite>, we divide the backpropagation process into two steps. Firstly, we maximize <math alttext=\"\\mathcal{L}_{\\rm GAN}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>GAN</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm GAN}</annotation></semantics></math> to update the discriminator, during which the generator is detached from the optimization. According to Eq. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.Ex1\" title=\"III-G Modality Discriminator &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III-G</span></a>), on one hand, maximizing the first term <math alttext=\"L_{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>D</mi></msub><annotation encoding=\"application/x-tex\">L_{D}</annotation></semantics></math> of <math alttext=\"\\mathcal{L}_{\\rm GAN}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>GAN</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm GAN}</annotation></semantics></math> essentially trains the discriminator to correctly distinguish between text and audio features by making <math alttext=\"H_{S}^{\\rm(spe)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mi>spe</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{S}^{\\rm(spe)}</annotation></semantics></math> close to 1 and <math alttext=\"H_{T}^{\\rm(spe)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mi>spe</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{T}^{\\rm(spe)}</annotation></semantics></math> close to 0 <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Function <math alttext=\"\\log(x)+\\log(1-y)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>&#8722;</mo><mi>y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(x)+\\log(1-y)</annotation></semantics></math>, where <math alttext=\"x,y\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x,y\\in(0,1)</annotation></semantics></math> reaches its maximum when <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m3\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> approaches 1 and <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m4\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math> approaches 0.</span></span></span>. On the other hand, maximizing the second term <math alttext=\"L_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m6\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>G</mi></msub><annotation encoding=\"application/x-tex\">L_{G}</annotation></semantics></math> (i.e., minimizing <math alttext=\"-L_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m7\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><msub><mi>L</mi><mi>G</mi></msub></mrow><annotation encoding=\"application/x-tex\">-L_{G}</annotation></semantics></math>) will make <math alttext=\"H^{\\text{(inv)}}_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mtext>(inv)</mtext></msubsup><annotation encoding=\"application/x-tex\">H^{\\text{(inv)}}_{ST}</annotation></semantics></math> approach 0 or 1 <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Function <math alttext=\"\\log(x)+\\log(1-x)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>&#8722;</mo><mi>x</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(x)+\\log(1-x)</annotation></semantics></math>, where <math alttext=\"x\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m2\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x\\in(0,1)</annotation></semantics></math> reaches its maximum at <math alttext=\"x=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m3\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">x=0.5</annotation></semantics></math> and its minimum near <math alttext=\"x=0\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m4\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">x=0</annotation></semantics></math> and <math alttext=\"x=1\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m5\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">x=1</annotation></semantics></math>.</span></span></span>, indicating that the discriminator recognizes <math alttext=\"H^{\\text{(inv)}}_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m9\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mtext>(inv)</mtext></msubsup><annotation encoding=\"application/x-tex\">H^{\\text{(inv)}}_{ST}</annotation></semantics></math> as modality-specific, which can be either text or audio, contrary to our desired modality invariance. Secondly, we freeze the discriminator parameters and update the remaining parameters by minimizing <math alttext=\"L_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m10\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>G</mi></msub><annotation encoding=\"application/x-tex\">L_{G}</annotation></semantics></math>, which makes the output of the discriminator <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m11\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> for the modality-invariant features <math alttext=\"H^{\\text{(inv)}}_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m12\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mtext>(inv)</mtext></msubsup><annotation encoding=\"application/x-tex\">H^{\\text{(inv)}}_{ST}</annotation></semantics></math> approach 0.5, thereby blurring these features between audio and text modalities to achieve modality agnosticism. Additionally, <math alttext=\"\\mathcal{L}_{\\text{ER}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m13\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ER</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ER}}</annotation></semantics></math> optimizes the downstream emotion recognition model, <math alttext=\"\\mathcal{L}_{\\text{AED}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m14\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>AED</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{AED}}</annotation></semantics></math> and <math alttext=\"\\mathcal{L}_{\\text{AEC}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m15\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>AEC</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{AEC}}</annotation></semantics></math> optimize the auxiliary tasks of ASR error detection and ASR error correction models, respectively, and <math alttext=\"\\mathcal{L}_{\\text{LCL}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m16\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>LCL</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{LCL}}</annotation></semantics></math> implements the LCL strategy. The entire system is trained in an end-to-end manner and optimized by adjusting weight parameters.</p>\n\n",
                "matched_terms": [
                    "trained",
                    "lcl",
                    "emotion",
                    "training",
                    "model",
                    "gan"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the inference stage, the AED and AEC modules are excluded. The remaining modules accept speech and ASR transcripts as input and output emotion classification results.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "aec",
                    "emotion",
                    "aed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the effectiveness\nof our proposed model, we carried out experiments on two public datasets:\nthe Interactive Emotional Dyadic Motion Capture (IEMOCAP) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib45\" title=\"\">45</a>]</cite> and the Multimodal Emotion Lines Dataset (MELD) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib46\" title=\"\">46</a>]</cite>. The statistics of the two datasets are shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S4.F6\" title=\"Figure 6 &#8227; IV-A Dataset and Evaluation Metrics &#8227; IV Experimental Setup &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "emotion",
                    "dataset",
                    "model",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">IEMOCAP</span> comprised roughly 12 hours of speech from ten speakers participating in five scripted sessions. Following prior work <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib47\" title=\"\">47</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> on IEMOCAP, we employed 5531 utterances that were categorized into four emotion categories: &#8220;Neutral&#8221; (1,708), &#8220;Angry&#8221; (1,103), &#8220;Happy&#8221; (including &#8220;Excited&#8221;) (595 + 1,041 = 1,636), and &#8220;Sad&#8221; (1,084).\nWe performed five-fold leave-one-session-out (LOSO) cross-validation to evaluate the emotion classification performance using weighted accuracy (WA) and unweighted accuracy (UA).</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "emotion",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MELD</span> included a total of 13708 samples extracted from the TV series Friends, divided into 9,989 for training, 1,109 for validation, and 2,610 for testing. The dataset contained seven labels: &#8220;Neutral&#8221; (6167), &#8220;Happy&#8221; (2198), &#8220;Fear&#8221; (350), &#8220;Sad&#8221; (985), &#8220;Disgust&#8221; (355), &#8220;Angry&#8221; (1541), and &#8220;Surprise&#8221; (1499).\nWe tuned hyperparameters on the validation set and reported final results on the test set using the best checkpoint, evaluated by accuracy (ACC) and weighted F1-score (W-F1).</p>\n\n",
                "matched_terms": [
                    "meld",
                    "evaluated",
                    "acc",
                    "training",
                    "testing",
                    "wf1",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our method was implemented using Python 3.10.0 and Pytorch 1.11.0. The model was trained and evaluated on a system equipped with an Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz, 32GB RAM, and one NVIDIA Tesla V100 GPU. The detailed parameter settings are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S4.T1\" title=\"TABLE I &#8227; IV-B Implementation Details &#8227; IV Experimental Setup &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "trained",
                    "model",
                    "evaluated",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The acoustic encoder was initialized using the <span class=\"ltx_text ltx_font_typewriter\">hubert-base-ls960<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">3</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/facebook/hubert-base-ls960\" title=\"\">https://huggingface.co/facebook/hubert-base-ls960</a></span></span></span></span> model, producing acoustic representations <math alttext=\"d_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">d_{S}</annotation></semantics></math> with a dimensionality of 768.\nThe text encoder employed the <span class=\"ltx_text ltx_font_typewriter\">bert-base-uncased<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">4</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/google-bert/bert-base-uncased\" title=\"\">https://huggingface.co/google-bert/bert-base-uncased</a></span></span></span></span> model for initialization, yielding token representations <math alttext=\"d_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">d_{T}</annotation></semantics></math> with a dimensionality of 768. The vocabulary size for word tokenization <math alttext=\"d_{vocab}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>d</mi><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">d_{vocab}</annotation></semantics></math> was set to 30,522. We set the hidden size <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> to 768, the number of attention layers to 12, and the number of attention heads to 12. Both the HuBERT and BERT models were fine-tuned during the training stage.\nThe transformer decoder in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c) adopted a single-layer transformer with a hidden size of 768. Additionally, we obtained corresponding ASR hypotheses using the Whisper ASR model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib48\" title=\"\">48</a>]</cite>. We used the <span class=\"ltx_text ltx_font_typewriter\">openai/whisper-medium.en<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">5</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/openai/whisper-medium.en\" title=\"\">https://huggingface.co/openai/whisper-medium.en</a></span></span></span></span> and <span class=\"ltx_text ltx_font_typewriter\">openai/whisper-large-v2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">6</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/openai/whisper-large-v2\" title=\"\">https://huggingface.co/openai/whisper-large-v2</a></span></span></span></span> models, achieving word error rates (WERs) of 20.48% and 37.87% on the IEMOCAP and MELD datasets, respectively.</p>\n\n",
                "matched_terms": [
                    "hubert",
                    "iemocap",
                    "bert",
                    "corresponding",
                    "training",
                    "used",
                    "model",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used Adam <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib49\" title=\"\">49</a>]</cite> as the optimizer with a batch size of 16. On the basis of empirical observations and prior work <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib50\" title=\"\">50</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib51\" title=\"\">51</a>]</cite>, we kept the learning rate constant at <math alttext=\"1e^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-5}</annotation></semantics></math> for IEMOCAP and <math alttext=\"1e^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-4}</annotation></semantics></math> for MELD during training. For our multitask learning setup, we set <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m3\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> to 3 and <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m4\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> to 0.1. For our multistrategy learning setup, we set <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m5\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> to 0.01 and <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m6\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> to 0.1. The temperature parameter <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m7\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> was set to 0.07. To validate these choices, we additionally performed a one-at-a-time sensitivity analysis on the hyperparameters <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m8\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m9\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>, <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m10\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>, and <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m11\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math>, as presented in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.SS3\" title=\"V-C Sensitivity Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">V-C</span></a> and illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F7\" title=\"Figure 7 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "analysis",
                    "training",
                    "used",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the experiments, we compared various multimodal speech emotion recognition SOTA methods with our proposed method, M<sup class=\"ltx_sup\">4</sup>SER.\nThe comparison was conducted by the following methods on the IEMOCAP dataset:</p>\n\n",
                "matched_terms": [
                    "method",
                    "iemocap",
                    "methods",
                    "emotion",
                    "m4ser",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SAMS and MF-AED-AEC methods are also utilized as baselines for the MELD dataset. Additionally, the following methods were included for comparison:</p>\n\n",
                "matched_terms": [
                    "meld",
                    "methods",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">HCAM</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib56\" title=\"\">56</a>]</cite> combines wav2vec audio and BERT text inputs with co-attention and recurrent neural networks for multimodal emotion recognition.</p>\n\n",
                "matched_terms": [
                    "bert",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T2\" title=\"TABLE II &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> compare the performance of the M<sup class=\"ltx_sup\">4</sup>SER method with those of recent multimodal SER methods on the IEMOCAP and MELD datasets. Our proposed M<sup class=\"ltx_sup\">4</sup>SER achieves a WA of 79.2% and a UA of 80.1% on IEMOCAP, and an ACC of 66.5% and a W-F1 of 66.0% on MELD, outperforming other models and demonstrating the superiority of M<sup class=\"ltx_sup\">4</sup>SER. Specifically, on the IEMOCAP dataset, compared with the recent MAF-DCT, the M<sup class=\"ltx_sup\">4</sup>SER method shows a 0.7% improvement in WA and a 0.8% improvement in UA, reaching a new SOTA performance. These improvements stem from our M<sup class=\"ltx_sup\">4</sup>SER&#8217;s capability to mitigate the impact of ASR errors, capture modality-specific and modality-invariant representations across different modalities, enhance commonality between modalities through adversarial learning, and excel in emotion feature learning with the LCL loss.</p>\n\n",
                "matched_terms": [
                    "meld",
                    "method",
                    "iemocap",
                    "methods",
                    "acc",
                    "emotion",
                    "lcl",
                    "m4ser",
                    "wf1",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed M<sup class=\"ltx_sup\">4</sup>SER model is composed of four types of learning: multimodal learning, multirepresentation learning, multitask learning, and multistrategy learning. To determine the impact of different types of learning on the performance of the emotion recognition task, ablation experiments were conducted on the IEMOCAP and MELD datasets, as presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "task",
                    "emotion",
                    "m4ser",
                    "model",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multimodalities.</span>\nTo assess the necessity of multimodality, we conduct single-modal comparison experiments involving text and speech modalities. These experiments operate under a baseline framework excluding the MF module, LCL loss, and GAN loss, precluding intermodal interaction.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(A), in the single-modal experiments, the performance of speech modality in the IEMOCAP dataset is better than that of text modality, whereas the reverse is observed in the MELD dataset. This divergence suggests that emotional information is more distinctly conveyed through speech in IEMOCAP, whereas MELD leans towards text for emotional expression.\nMoreover, the multimodal baseline model, which simply concatenates speech and text features for recognition (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> A(3)), significantly enhances performance compared with single-modal baselines, highlighting the essential role of multimodal information.</p>\n\n",
                "matched_terms": [
                    "meld",
                    "baseline",
                    "iemocap",
                    "lcl",
                    "model",
                    "dataset",
                    "gan"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multitasks.</span> First, we discuss the impact of the AED module. Specifically, we remove the AED module and introduce only the AEC module as an auxiliary task. This requires the model to correct all errors in each utterance from scratch, rather than only the detected errors. Evidently, the absence of the AED module leads to a significant drop in emotion recognition performance, demonstrating the necessity of the AED module. Similarly, we observe that the AEC module also plays an important role. Moreover, we note that the WA results using only the AEC module (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(C)(1)) are even worse than the results with neither the AED nor AEC module (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(C)(3)) in both datasets. This phenomenon occurs because directly using the neural machine translation (NMT) model for AEC can even increase the WER <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib60\" title=\"\">60</a>]</cite>. Unlike NMT tasks, which typically require modifying almost all input tokens, AEC involves fewer but more challenging corrections. For instance, if the ASR model&#8217;s WER is 10%, then only about 10% of the input tokens require correction in the AEC model. However, these tokens are often difficult to recognize and correct because they have already been misrecognized by the ASR model. Therefore, we should consider the characteristics of ASR outputs and carefully design the AEC model, which is why we introduce both AED and AEC as auxiliary tasks.</p>\n\n",
                "matched_terms": [
                    "aec",
                    "task",
                    "aed",
                    "emotion",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multistrategies.</span> To validate the effectiveness of the adversarial training strategy outlined in Alg. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#alg1\" title=\"Algorithm 1 &#8227; III-I Joint Training &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we remove the adversarial training strategy from M<sup class=\"ltx_sup\">4</sup>SER. The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(D)(1) show a decrease in performance on both datasets, demonstrating the critical role of this strategy in learning modality-invariant representations. The proposed modality discriminator effectively enhances the modality agnosticism of the refined representations from the generator.\nAdditionally, omitting the LCL strategy described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.SS8\" title=\"III-H Label-based Contrastive Learning (LCL) &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III-H</span></a> also results in similar performance degradation (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(D)(2)). We visualize the results before and after introducing this strategy to demonstrate its effectiveness, with specific analysis also detailed in the following t-SNE analysis section. Moreover, removing both of these strategies further diminishes emotion recognition performance, confirming that both learning strategies contribute significantly to performance improvement.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "lcl",
                    "m4ser",
                    "emotion",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:703.2pt;height:86pt;vertical-align:-40.5pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Type</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Modality</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Params (M)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Train Time (h)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Infer Time (ms/U)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">WA (%)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">UA (%)</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_2\">Single-modal</span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">HuBERT</span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">S</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">316.2</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">5.4</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">23.7 &#177; 0.1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">68.9</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt\">69.8</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\">BERT</span>\n<span class=\"ltx_td ltx_align_left ltx_border_r\">T(ASR)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">109.5</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">0.5</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">6.3 &#177; 0.1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">65.1</span>\n<span class=\"ltx_td ltx_align_center\">66.4</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_4\">Multi-modal</span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Baseline (HuBERT + BERT)</span>\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_4\">S+T(ASR)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">426.9</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">9.6</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">29.6 &#177; 0.1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">73.4</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">75.2</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\">&#8194;&#8202;&#8195;+ MSR &amp; MIR</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">481.4</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">12.0</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">44.8 &#177; 0.1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">76.6</span>\n<span class=\"ltx_td ltx_align_center\">77.4</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\">&#8194;&#8202;&#8195;&#8195;+ GAN &amp; LCL</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">481.6</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">20.8</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">44.5 &#177; 0.6</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\">77.3</span>\n<span class=\"ltx_td ltx_align_center\">78.6</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">&#8195;&#8195;&#8195;&#8196;&#8202;+ AED &amp; AEC (M<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">4</span></sup>SER)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">512.9</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">21.5</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">44.7 &#177; 0.2</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">79.2</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#EFEFEF;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EFEFEF;\">80.1</span></span></span>\n</span></span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "aec",
                    "hubert",
                    "baseline",
                    "msr",
                    "aed",
                    "mir",
                    "bert",
                    "lcl",
                    "m4ser",
                    "model",
                    "gan"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a one-at-a-time sensitivity analysis on four key hyperparameters: the auxiliary task weight <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, the AED/AEC balancing factor <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m2\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>, the GAN loss weight <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m3\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>, and the LCL loss weight <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m4\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math>. As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F7\" title=\"Figure 7 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, model performance is moderately sensitive to <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m5\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m6\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math>, with optimal results achieved at <math alttext=\"\\alpha=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.1</annotation></semantics></math> and <math alttext=\"\\lambda=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m8\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=0.1</annotation></semantics></math>. Excessively small values reduce the effectiveness of auxiliary guidance, while overly large values interfere with the main task.</p>\n\n",
                "matched_terms": [
                    "task",
                    "analysis",
                    "lcl",
                    "model",
                    "gan"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the model exhibits relative robustness to variations in <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m1\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>, indicating stable synergy between the AED and AEC objectives. Conversely, performance is highly sensitive to <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m2\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>; large values result in unstable training due to adversarial objectives, while a small weight (e.g., <math alttext=\"\\gamma=0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=0.01</annotation></semantics></math>) ensures stable convergence and optimal results.</p>\n\n",
                "matched_terms": [
                    "result",
                    "aec",
                    "conversely",
                    "aed",
                    "training",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F8\" title=\"Figure 8 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows that the multitask objectives\n(<math alttext=\"\\mathcal{L}_{\\rm ER}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>ER</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm ER}</annotation></semantics></math>, <math alttext=\"\\mathcal{L}_{\\rm AED}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>AED</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm AED}</annotation></semantics></math>, <math alttext=\"\\mathcal{L}_{\\rm AEC}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>AEC</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm AEC}</annotation></semantics></math>)\nrapidly decrease and stabilize, confirming that the auxiliary tasks provide\nuseful supervision for ER. The multistrategy objectives also\nexhibit stable convergence: <math alttext=\"\\mathcal{L}_{\\rm LCL}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>LCL</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm LCL}</annotation></semantics></math> quickly decreases under label\nsupervision, while for the adversarial component, <math alttext=\"\\mathcal{L}_{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>D</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{D}</annotation></semantics></math> rises from\nnegative values to near zero as the discriminator learns, <math alttext=\"\\mathcal{L}_{\\rm GAN}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>GAN</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm GAN}</annotation></semantics></math>\nsteadily increases and plateaus, and <math alttext=\"\\mathcal{L}_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m7\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>G</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{G}</annotation></semantics></math> exhibits fluctuations, reflecting the adversarial interplay,\nand eventually converges as <math alttext=\"D(H_{ST}^{\\mathrm{(inv)}})\\to 0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m8\" intent=\":literal\"><semantics><mrow><mrow><mi>D</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>inv</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">&#8594;</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">D(H_{ST}^{\\mathrm{(inv)}})\\to 0.5</annotation></semantics></math>, which corresponds to about 1.386 in the <math alttext=\"\\mathcal{L}_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m9\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>G</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{G}</annotation></semantics></math> term of Eq.&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.Ex1\" title=\"III-G Modality Discriminator &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III-G</span></a>),\nindicating that an adversarial equilibrium is achieved. Overall, both multitask and multistrategy objectives\nare optimized in a stable manner, validating the effectiveness of the M<sup class=\"ltx_sup\">4</sup>SER.</p>\n\n",
                "matched_terms": [
                    "aec",
                    "aed",
                    "lcl",
                    "m4ser",
                    "gan"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the trade-off between model performance and computational cost, we report the number of parameters, training time, and inference time for each model on the IEMOCAP dataset in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T5\" title=\"TABLE V &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>, with all experiments conducted on a single NVIDIA Tesla V100 GPU.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "iemocap",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Among the single-modal models, HuBERT exhibits higher computational cost than BERT owing to the nature of speech encoders, but also delivers better recognition performance. In the multi-modal setting, each additional module introduces a moderate increase in parameter size and training cost, while consistently improving recognition accuracy.</p>\n\n",
                "matched_terms": [
                    "bert",
                    "hubert",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our full model M<sup class=\"ltx_sup\">4</sup>SER achieves the best performance with WA of 79.2% and UA of 80.1%. Although its training time increases to 21.5 hours owing to the inclusion of GAN and LCL strategies and AED and AEC subtasks, these components are used only during training. Consequently, the inference latency of M<sup class=\"ltx_sup\">4</sup>SER remains nearly identical to the baseline with only MSR and MIR. Considering the significant gains in performance, M<sup class=\"ltx_sup\">4</sup>SER demonstrates a favorable trade-off between computational cost and recognition accuracy, making it suitable for practical deployment.</p>\n\n",
                "matched_terms": [
                    "aec",
                    "baseline",
                    "msr",
                    "aed",
                    "mir",
                    "lcl",
                    "training",
                    "m4ser",
                    "used",
                    "model",
                    "gan"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">t-SNE Analysis.</span> To intuitively demonstrate the advantages of our proposed M<sup class=\"ltx_sup\">4</sup>SER model on IEMOCAP and MELD datasets, we utilize the t-distributed stochastic neighbor embedding (t-SNE) tool <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib61\" title=\"\">61</a>]</cite> to visualize the learned emotion features using all samples from session 3 of the IEMOCAP and test set of the MELD. We compare these features across the following models: the speech model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(a)), the text model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(b)), the proposed M<sup class=\"ltx_sup\">4</sup>SER model without label-based contrastive learning (LCL) (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(c)), and the proposed M<sup class=\"ltx_sup\">4</sup>SER model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(d)).</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "analysis",
                    "lcl",
                    "emotion",
                    "m4ser",
                    "model",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the visualization results in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, we observe that the emotion label distributions for the two single-modal baselines on the IEMOCAP and MELD datasets show a significant overlap among the emotion categories, indicating that they are often confused with each other. In contrast, the emotion label distributions for the two multimodal models are more distinguishable, demonstrating the effectiveness of multimodal models in capturing richer emotional features.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "emotion",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, compared with the M<sup class=\"ltx_sup\">4</sup>SER model without the LCL strategy, our M<sup class=\"ltx_sup\">4</sup>SER model achieves better clustering for each emotion category, making them as distinct as possible. For instance, on the IEMOCAP dataset, the intra-class clustering of samples learned by the M<sup class=\"ltx_sup\">4</sup>SER model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(d)) is more pronounced than that learned by the M<sup class=\"ltx_sup\">4</sup>SER model without the LCL strategy (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(c)), especially with clearer boundaries for sad and angry samples. Additionally, the distances between the four types of emotion in the emotion vector space increase. Although distinguishing MELD samples is more challenging than distinguishing IEMOCAP samples, the overall trend remains similar.\nThis indicates that the proposed M<sup class=\"ltx_sup\">4</sup>SER model effectively leverages both modality-specific and modality-invariant representations to capture high-level shared feature representations across speech and text modalities for emotion recognition.</p>\n\n",
                "matched_terms": [
                    "meld",
                    "iemocap",
                    "lcl",
                    "m4ser",
                    "emotion",
                    "dataset",
                    "model",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further explore the effectiveness of adversarial learning in M<sup class=\"ltx_sup\">4</sup>SER, we visualize the distribution of modality-invariant and modality-specific representations before and after adversarial learning using the t-SNE tool, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F10\" title=\"Figure 10 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. It can be observed that after adversarial learning, different modality-specific representations become more separated with increasing intraclass clustering. This indicates that with the introduction of adversarial learning, M<sup class=\"ltx_sup\">4</sup>SER enhances the diversity of modality-specific representations, generating superior features for the downstream emotion recognition task.</p>\n\n",
                "matched_terms": [
                    "m4ser",
                    "task",
                    "emotion",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Representation Analysis.</span>\nWe find that our M<sup class=\"ltx_sup\">4</sup>SER method performs better when using ASR text than when using GT text, as presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T6\" title=\"TABLE VI &#8227; V-F Visualization Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a>. To investigate this finding, we visualize representation weights for two examples from IEMOCAP, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F11\" title=\"Figure 11 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. We display representations of each modality across the temporal level in two learning spaces, accumulating hidden features of <math alttext=\"H_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m2\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">H_{S}</annotation></semantics></math>, <math alttext=\"H_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m3\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">H_{T}</annotation></semantics></math>, <math alttext=\"\\hat{H}_{S}^{spe}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m4\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>H</mi><mo>^</mo></mover><mi>S</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\hat{H}_{S}^{spe}</annotation></semantics></math>, and <math alttext=\"H_{T}^{spe}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m5\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>T</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{T}^{spe}</annotation></semantics></math> into one dimension.\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F11\" title=\"Figure 11 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(a) illustrates that during single-modal learning with ASR text, representation weights in the text modality mainly focus on the word &#8220;oh&#8221;, whereas in the speech modality, representation weights concentrate towards the sentence&#8217;s end. After cross-modal learning, we observed a significant decrease in &#8220;oh&#8221; weight in the text modality. This change occurs as the model detects errors or inaccuracies in ASR transcription using AED and AEC modules, shifting representation weights to other words such as &#8220;that&#8217;s&#8221;. Owing to limited emotional cues in the text, the model relies more on speech features, especially anger-related cues such as intonation and volume, critical for accurate anger recognition. Conversely, with GT text, after cross-modal learning, attention in the text modality focuses on words conveying positive emotions such as &#8220;amusing&#8221;, whereas speech modality distribution remains largely unchanged. Without ASR error signals, the model leans towards these textual features, leading to a bias towards happiness in emotion recognition.</p>\n\n",
                "matched_terms": [
                    "method",
                    "aec",
                    "iemocap",
                    "conversely",
                    "aed",
                    "analysis",
                    "emotion",
                    "m4ser",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Similarly, Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F11\" title=\"Figure 11 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(b) shows inconsistency between GT text and speech features. GT text contains words (e.g., &#8220;don&#8217;t know&#8221;) indicating confusion and uncertainty, whereas speech features exhibit significant emotional fluctuations. This mismatch complicates feature alignment in cross-modal learning, resulting in erroneous anger classification. In contrast, ASR text shows higher consistency with speech features, highlighting emotional words (&#8220;freaking out&#8221;, &#8220;what the hell&#8221;, and &#8220;all of a sudden&#8221;) despite errors and simplifications. Representation weights confirm high consistency between ASR text and speech features across multiple regions, enhancing cross-modal learning accuracy and correct emotion identification as &#8220;Happy&#8221; (&#8220;Excited&#8221;). We demonstrate with dynamic time warping (DTW) that ASR transcriptions are more consistent with speech than GT transcriptions, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T7\" title=\"TABLE VII &#8227; V-F Visualization Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VII</span></a>.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Differences in feature weight distribution:</span> the absence of ASR errors alters feature weight distribution in GT text modality compared with ASR text, potentially leading to inaccurate emotion classification.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Confusion Matrix Analysis.</span> To investigate the impact of different modules on class-wise prediction, we visualize the averaged confusion matrices over 5-folds on IEMOCAP in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F12\" title=\"Figure 12 &#8227; V-F Visualization Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>. Compared with the baseline, introducing MSR and MIR significantly enhances the prediction of &#8220;Happy&#8221; class, which often suffer from ambiguous acoustic-textual alignment.\nBy further incorporating GAN and LCL strategies, we observe overall improvements across most emotion classes, particularly in &#8220;Neutral&#8221;. However, the performance on the &#8220;Happy&#8221; class slightly drops, potentially owing to the acoustic-textual heterogeneity introduced by merging &#8220;Excited&#8221; into &#8220;Happy&#8221;, which possibly reduces the effectiveness of local contrastive learning in capturing class-specific discriminative features.\nFinally, integrating AED and AEC modules yields the most accurate and balanced predictions overall. We observe further improvements in &#8220;Neutral&#8221; and &#8220;Sad&#8221; classes, while the performance of &#8220;Happy&#8221; slightly recovers compared with the previous setting. These results highlight the effectiveness of multi-task learning in mitigating ASR-induced errors and enhancing emotional robustness across modalities.</p>\n\n",
                "matched_terms": [
                    "aec",
                    "baseline",
                    "iemocap",
                    "msr",
                    "aed",
                    "mir",
                    "analysis",
                    "emotion",
                    "lcl",
                    "gan"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the performance of M<sup class=\"ltx_sup\">4</sup>SER in real-world environments, we conduct cross-corpus emotion recognition experiments, which simulate the practical scenario of domain shift between different datasets. Following the experimental settings of previous works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib62\" title=\"\">62</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib63\" title=\"\">63</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib64\" title=\"\">64</a>]</cite>, we adopt a transfer evaluation strategy where one corpus is used entirely for training, and 30% of the target corpus is reserved for validation for parameter tuning, while the remaining 70% is used for final testing.\nIn addition, as the IEMOCAP dataset contains only four emotion classes (&#8220;Neutral&#8221;, &#8220;Happy&#8221;, &#8220;Angry&#8221;, &#8220;Sad&#8221;), we restrict MELD to the same set of overlapping emotions for a fair comparison and discard the rest (&#8220;Suprise&#8221;, &#8220;Fear&#8221;, &#8220;Disgust&#8221;). This ensures consistent label space across corpora during both training and evaluation.</p>\n\n",
                "matched_terms": [
                    "meld",
                    "iemocap",
                    "crosscorpus",
                    "emotion",
                    "training",
                    "testing",
                    "m4ser",
                    "used",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further evaluate the adaptability of M<sup class=\"ltx_sup\">4</sup>SER under limited supervision, we conduct few-shot domain adaptation experiments by sampling 5, 10, 20, and 60 labeled instances per class from the target-domain validation set. The remaining validation data is still used for model selection, while the test set remains fixed across all settings. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T9\" title=\"TABLE IX &#8227; V-G Cross-corpus Generalization Ability &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IX</span></a>, M<sup class=\"ltx_sup\">4</sup>SER consistently outperforms SAMS and MF-AED-AEC across all shot settings. For example, with only 5 labeled samples per class, M<sup class=\"ltx_sup\">4</sup>SER achieves ACC of 57.9% in the IE<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ME setting, already surpassing MF-AED-AEC with 20 shots. As the number of target samples increases, our model scales well and reaches ACC of 62.1% and W-F1 61.5% at 60-shot. In the ME<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>IE setting, M<sup class=\"ltx_sup\">4</sup>SER reaches UA of 75.9%, demonstrating excellent cross-domain adaptability.</p>\n\n",
                "matched_terms": [
                    "acc",
                    "m4ser",
                    "wf1",
                    "used",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose M<sup class=\"ltx_sup\">4</sup>SER, a novel emotion recognition method that combines multimodal, multirepresentation, multitask, and multistrategy learning. M<sup class=\"ltx_sup\">4</sup>SER leverages an innovative multimodal fusion module to learn modality-specific and modality-invariant representations, capturing unique features of each modality and common features across modalities. We then introduce a modality discriminator to enhance modality diversity through adversarial learning. Additionally, we design two auxiliary tasks, AED and AEC, aimed at enhancing the semantic consistency within the text modality. Finally, we propose a label-based contrastive learning strategy to distinguish different emotional features. Results of experiments on the IEMOCAP and MELD datasets demonstrate that M<sup class=\"ltx_sup\">4</sup>SER surpasses previous baselines, proving its effectiveness. In the future, we plan to extend our approach to the visual modality and introduce disentangled representation learning to further enhance emotion recognition performance. We also plan to investigate whether AED and AEC modules remain necessary when using powerful LLM-based ASR systems.</p>\n\n",
                "matched_terms": [
                    "method",
                    "aec",
                    "iemocap",
                    "aed",
                    "emotion",
                    "m4ser",
                    "meld"
                ]
            }
        ]
    },
    "S5.T9": {
        "source_file": "M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition",
        "caption": "TABLE IX: Analysis of few-shot domain adaptation ability for cross-corpus 4-class emotion recognition (%). We reimplement the method indicated by ∘ and obtain the corresponding result. IE →\\rightarrow ME and ME →\\rightarrow IE denote training on IEMOCAP and MELD, respectively, with the other used for testing.",
        "body": "Methods\n# Shots\nIE →\\rightarrow ME\nME →\\rightarrow IE\n\n\nACC\nW-F1\nWA\nUA\n\n\nSAMS∘ [52]\n\n5\n51.6\n49.0\n39.0\n37.2\n\n\n10\n52.5\n49.2\n38.2\n38.3\n\n\n20\n52.3\n52.3\n39.9\n35.0\n\n\n60\n59.9\n56.8\n44.4\n45.3\n\n\nMF-AED-AEC∘ [13]\n\n5\n56.3\n54.8\n64.4\n63.3\n\n\n10\n57.8\n55.7\n65.8\n65.6\n\n\n20\n57.5\n56.7\n67.6\n66.6\n\n\n60\n60.8\n60.3\n72.5\n73.7\n\n\nM4SER\n5\n57.9\n56.0\n66.4\n64.4\n\n\n10\n58.7\n58.0\n66.7\n67.7\n\n\n20\n59.4\n58.8\n69.0\n68.7\n\n\n60\n62.1\n61.5\n74.0\n75.9",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Methods</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\"># Shots</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">IE <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T9.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> ME</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">ME <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T9.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> IE</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">ACC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">W-F1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">WA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">UA</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"4\">SAMS<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8728;</span></sup> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib52\" title=\"\">52</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">51.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">49.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">39.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">37.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">10</td>\n<td class=\"ltx_td ltx_align_center\">52.5</td>\n<td class=\"ltx_td ltx_align_center\">49.2</td>\n<td class=\"ltx_td ltx_align_center\">38.2</td>\n<td class=\"ltx_td ltx_align_center\">38.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">20</td>\n<td class=\"ltx_td ltx_align_center\">52.3</td>\n<td class=\"ltx_td ltx_align_center\">52.3</td>\n<td class=\"ltx_td ltx_align_center\">39.9</td>\n<td class=\"ltx_td ltx_align_center\">35.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">60</td>\n<td class=\"ltx_td ltx_align_center\">59.9</td>\n<td class=\"ltx_td ltx_align_center\">56.8</td>\n<td class=\"ltx_td ltx_align_center\">44.4</td>\n<td class=\"ltx_td ltx_align_center\">45.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"4\">MF-AED-AEC<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8728;</span></sup> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib13\" title=\"\">13</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">56.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">54.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">64.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">63.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">10</td>\n<td class=\"ltx_td ltx_align_center\">57.8</td>\n<td class=\"ltx_td ltx_align_center\">55.7</td>\n<td class=\"ltx_td ltx_align_center\">65.8</td>\n<td class=\"ltx_td ltx_align_center\">65.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">20</td>\n<td class=\"ltx_td ltx_align_center\">57.5</td>\n<td class=\"ltx_td ltx_align_center\">56.7</td>\n<td class=\"ltx_td ltx_align_center\">67.6</td>\n<td class=\"ltx_td ltx_align_center\">66.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">60</td>\n<td class=\"ltx_td ltx_align_center\">60.8</td>\n<td class=\"ltx_td ltx_align_center\">60.3</td>\n<td class=\"ltx_td ltx_align_center\">72.5</td>\n<td class=\"ltx_td ltx_align_center\">73.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text ltx_font_bold\">M<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">4</span></sup>SER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">57.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">56.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">66.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">64.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">10</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">58.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">58.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">66.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">67.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">20</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">59.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">58.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">69.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">68.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">62.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">61.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">74.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">75.9</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "recognition",
            "ability",
            "respectively",
            "emotion",
            "testing",
            "4class",
            "used",
            "sams∘",
            "indicated",
            "domain",
            "methods",
            "adaptation",
            "wf1",
            "fewshot",
            "method",
            "iemocap",
            "mfaedaec∘",
            "crosscorpus",
            "analysis",
            "reimplement",
            "acc",
            "training",
            "m4ser",
            "result",
            "shots",
            "→rightarrow",
            "denote",
            "corresponding",
            "other",
            "obtain",
            "meld"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To further evaluate the adaptability of M<sup class=\"ltx_sup\">4</sup>SER under limited supervision, we conduct few-shot domain adaptation experiments by sampling 5, 10, 20, and 60 labeled instances per class from the target-domain validation set. The remaining validation data is still used for model selection, while the test set remains fixed across all settings. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T9\" title=\"TABLE IX &#8227; V-G Cross-corpus Generalization Ability &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IX</span></a>, M<sup class=\"ltx_sup\">4</sup>SER consistently outperforms SAMS and MF-AED-AEC across all shot settings. For example, with only 5 labeled samples per class, M<sup class=\"ltx_sup\">4</sup>SER achieves ACC of 57.9% in the IE<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ME setting, already surpassing MF-AED-AEC with 20 shots. As the number of target samples increases, our model scales well and reaches ACC of 62.1% and W-F1 61.5% at 60-shot. In the ME<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>IE setting, M<sup class=\"ltx_sup\">4</sup>SER reaches UA of 75.9%, demonstrating excellent cross-domain adaptability.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Multimodal speech emotion recognition (SER) has emerged as pivotal for improving human&#8211;machine interaction. Researchers are increasingly leveraging both speech and textual information obtained through automatic speech recognition (ASR) to comprehensively recognize emotional states from speakers. Although this approach reduces reliance on human-annotated text data, ASR errors possibly degrade emotion recognition performance.\nTo address this challenge, in our previous work, we introduced two auxiliary tasks, namely, ASR error detection and ASR error correction, and we proposed a novel multimodal fusion (MF) method for learning modality-specific and modality-invariant representations across different modalities.\nBuilding on this foundation, in this paper, we introduce two additional training strategies. First, we propose an adversarial network to enhance the diversity of modality-specific representations. Second, we introduce a label-based contrastive learning strategy to better capture emotional features.\nWe refer to our proposed method as M<sup class=\"ltx_sup\">4</sup>SER and validate its superiority over state-of-the-art methods through extensive experiments using IEMOCAP and MELD datasets.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "method",
                    "iemocap",
                    "methods",
                    "emotion",
                    "training",
                    "m4ser",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal speech emotion recognition (SER) serves as a cornerstone in intelligent human&#8211;computer interaction systems, playing a crucial role in fields such as healthcare and intelligent customer service <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib2\" title=\"\">2</a>]</cite>. By analyzing physiological signals such as speech, language, facial expressions, and gestures, multimodal SER can identify and understand human emotional states, making it a key technology of widespread interest among researchers nowadays.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent research has shown that single-modal approaches struggle to meet the increasing demands for SER owing to their inherent complexity. As a solution, multimodal information has emerged as a viable option because it offers diverse emotional cues. A common strategy involves integrating speech and text modalities to achieve multimodal SER. Text data can be obtained through manual transcription or automatic speech recognition (ASR), which have been widely adopted in recent studies to reduce reliance on extensive annotated datasets.\nAlthough text-based pretrained models such as BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib6\" title=\"\">6</a>]</cite> and RoBERTa <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib7\" title=\"\">7</a>]</cite> have excelled in multimodal SER tasks, the accuracy of ASR remains equally crucial for achieving precise emotion recognition (ER) results. To mitigate the impact of ASR errors, Santoso et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib8\" title=\"\">8</a>]</cite> have integrated self-attention mechanisms and word-level confidence measurements, particularly reducing the importance of words with high error probabilities, thereby enhancing SER performance. However, this approach heavily depends on the performance of the ASR system, thus limiting its generalizability. Lin and Wang <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> have proposed an auxiliary ASR error detection task aimed at determining the probabilities of erroneous words to adjust the trust levels assigned to each word in the ASR hypothesis, thereby improving multimodal SER performance. However, this method focuses solely on error detection within the ASR hypothesis without directly correcting these errors, indicating that it does not fully enhance the coherence of semantic information.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "method",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the other hand, how to effectively integrate speech and text modalities has become a key focus. Previous methods have included simple feature concatenation, recurrent neural networks (RNNs), convolutional neural networks (CNNs), and cross-modal attention mechanisms <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib1\" title=\"\">1</a>]</cite>. Although these methods have shown some effectiveness, they often face challenges arising from differences in representations across different modalities.\nIn recent multimodal tasks such as sentiment analysis <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib11\" title=\"\">11</a>]</cite>, researchers have proposed learning two distinct types of representation to enhance multimodal learning.\nThe first is a representation tailored to each modality, that is, a modality-specific representation (MSR).\nThe second is modality-invariant representation (MIR), aimed at mapping all modalities of the same utterance into a shared subspace. This representation captures commonalities in multimodal signals as well as the speaker&#8217;s shared motives, which collectively affect the emotional state of the utterance through distribution alignment. By combining these two types of representation, one can obtain a comprehensive view of multimodal data can be provided for downstream tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib12\" title=\"\">12</a>]</cite>.\nHowever, these methods focus on utterance-level representations, which, while effective for short and isolated utterances, often fail to capture fine-grained emotional dynamics, especially in long and complex interactions. Such coarse-grained representations fail to capture subtle temporal cues and modality-specific variations that are crucial for accurate emotion recognition.\nMoreover, mapping entire utterances to a shared or modality-specific space using similarity loss oversimplifies the underlying multimodal relationships and hinders effective cross-modal alignment.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "methods",
                    "analysis",
                    "emotion",
                    "other",
                    "obtain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the basis of our previous work, we present M<sup class=\"ltx_sup\">4</sup>SER, an SER method that leverages multimodal, multirepresentation, multitask, and multistrategy learning. The difference between our M<sup class=\"ltx_sup\">4</sup>SER and previous SER methods is illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S1.F1\" title=\"Figure 1 &#8227; I Introduction &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. M<sup class=\"ltx_sup\">4</sup>SER mitigates the impact of ASR errors, improves modality-invariant representations, and captures commonalities between modalities to bridge their heterogeneity.</p>\n\n",
                "matched_terms": [
                    "methods",
                    "method",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed M<sup class=\"ltx_sup\">4</sup>SER outperforms state-of-the-arts on the IEMOCAP and MELD datasets. Extensive experiments demonstrate its superiority in SER tasks.</p>\n\n",
                "matched_terms": [
                    "meld",
                    "iemocap",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remaining sections of this paper are organized as follows: In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2\" title=\"II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">II</span></a>, we review related work. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3\" title=\"III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>, we discuss the specific model and method of M<sup class=\"ltx_sup\">4</sup>SER. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S4\" title=\"IV Experimental Setup &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>, we outline the experimental setup used in this study, including datasets, evaluation metrics, and implementation details. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5\" title=\"V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>, we verify the effectiveness of the proposed model. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S6\" title=\"VI Conclusion &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a>, we present our conclusion and future work.</p>\n\n",
                "matched_terms": [
                    "used",
                    "method",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these limitations, multimodal SER often integrates both speech and text modalities, capturing both acoustic and semantic cues. A common setting involves using speech signals alongside textual transcripts (typically from manual annotations or ASR outputs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib24\" title=\"\">24</a>]</cite>.\nFan et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib21\" title=\"\">21</a>]</cite> proposed multi-granularity attention-based transformers (MGAT) to address emotional asynchrony and modality misalignment.\nSun et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib22\" title=\"\">22</a>]</cite> introduced a method integrating shared and private encoders, projecting each modality into a separate subspace to capture modality consistency and diversity while ensuring label consistency in the output.\nThese methods typically assume access to high-quality manual transcriptions for the text modality.</p>\n\n",
                "matched_terms": [
                    "methods",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to factorization-based approaches which explicitly disentangle modality inputs into shared and private spaces, our M<sup class=\"ltx_sup\">4</sup>SER framework adopts a structured encoding&#8211;generation strategy to implicitly model MSRs and MIRs. Specifically, we utilize a stack of cross-modal encoder blocks to extract token-aware and speech-aware features (MSRs), and then apply a dedicated generator composed of hybrid-modal attention and masking mechanisms to derive the MIRs. In addition, unlike prior works that incorporate adversarial learning at a global or representation-level, our M<sup class=\"ltx_sup\">4</sup>SER employs a frame-level discriminator that operates on each temporal representation. A frame-level discriminator attempts to distinguish the modality origin of each temporal representation, while the MIR generator is optimized to deceive the discriminator by producing modality-agnostic outputs that are hard to classify as either text or speech. This fine-grained, per-frame adversarial constraint leads to more robust and aligned cross-modal representations. Moreover, M<sup class=\"ltx_sup\">4</sup>SER integrates this disentanglement framework into a multitask learning setup with ASR-aware auxiliary supervision, enabling more effective adaptation to noisy real-world speech&#8211;text scenarios.</p>\n\n",
                "matched_terms": [
                    "adaptation",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ASR error correction (AEC) techniques remain effective in optimizing the ASR hypotheses. AEC has been jointly trained with ER tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib13\" title=\"\">13</a>]</cite> in multitask settings. Li et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib27\" title=\"\">27</a>]</cite> proposed an AEC method that improves transcription quality on low-resource out-of-domain data through cross-modal training and the incorporation of discrete speech units, and they validated its effectiveness in downstream ER tasks. Lin and Wang <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> introduced a robust ER method that leverages complementary semantic information from audio, adapts to ASR errors through an auxiliary task for ASR error detection, and fuses text and acoustic representations for ER.\nOn the basis of <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib32\" title=\"\">32</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib33\" title=\"\">33</a>]</cite>, we propose a partially autoregressive AEC method that uses a label predictor to restrict decoding to only desirable parts of the input sequence embeddings, thereby reducing inference latency.</p>\n\n",
                "matched_terms": [
                    "method",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed M<sup class=\"ltx_sup\">4</sup>SER framework can be formalized as the function <math alttext=\"f(S,T)=(L,Z)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mi>L</mi><mo>,</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f(S,T)=(L,Z)</annotation></semantics></math>. Here, the speech modality <math alttext=\"S=(s_{1},s_{2},\\cdots,s_{m})\\in\\mathbb{R}^{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>s</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>s</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>m</mi></msup></mrow><annotation encoding=\"application/x-tex\">S=(s_{1},s_{2},\\cdots,s_{m})\\in\\mathbb{R}^{m}</annotation></semantics></math> consists of <span class=\"ltx_text ltx_font_italic\">m</span> frames extracted from an utterance, whereas the text modality <math alttext=\"T=(t_{1},t_{2},\\cdots,t_{n})\\in\\mathbb{R}^{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><msub><mi>t</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>t</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>n</mi></msup></mrow><annotation encoding=\"application/x-tex\">T=(t_{1},t_{2},\\cdots,t_{n})\\in\\mathbb{R}^{n}</annotation></semantics></math> comprises <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> tokens from the original ASR hypotheses of the utterance. All tokens are mapped to a predefined WordPiece vocabulary <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib40\" title=\"\">40</a>]</cite>. Additionally, within our multitask learning framework, the primary task is ER, yielding an emotion classification output <math alttext=\"L\\in\\{l_{1},l_{2},\\cdots,l_{e}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>l</mi><mn>1</mn></msub><mo>,</mo><msub><mi>l</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>l</mi><mi>e</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">L\\in\\{l_{1},l_{2},\\cdots,l_{e}\\}</annotation></semantics></math>, where <math alttext=\"e\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mi>e</mi><annotation encoding=\"application/x-tex\">e</annotation></semantics></math> represents the number of emotional categories. Concurrently, the auxiliary tasks include ASR error detection (AED) and ASR error correction (AEC). The output of these auxiliary tasks is <math alttext=\"Z\\in\\{Z_{1},Z_{2},\\cdots,Z_{k}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>Z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>Z</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>Z</mi><mi>k</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Z\\in\\{Z_{1},Z_{2},\\cdots,Z_{k}\\}</annotation></semantics></math>, representing all the ground truth (GT) sequences where the ASR transcriptions differ from the human-annotated transcriptions.\n<math alttext=\"Z_{k}=(z_{1,k},z_{2,k},\\cdots,z_{c,k})\\in\\mathbb{R}^{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><msub><mi>Z</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mn>1</mn><mo>,</mo><mi>k</mi></mrow></msub><mo>,</mo><msub><mi>z</mi><mrow><mn>2</mn><mo>,</mo><mi>k</mi></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>z</mi><mrow><mi>c</mi><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>c</mi></msup></mrow><annotation encoding=\"application/x-tex\">Z_{k}=(z_{1,k},z_{2,k},\\cdots,z_{c,k})\\in\\mathbb{R}^{c}</annotation></semantics></math>, denoting that the <math alttext=\"k^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\"><semantics><msup><mi>k</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">k^{th}</annotation></semantics></math> error position includes <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m10\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> tokens.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contextual Speech Representations.</span> To obtain comprehensive contextual representations of acoustic features, we utilize a pretrained SSL model, HuBERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib4\" title=\"\">4</a>]</cite>, as our acoustic encoder. HuBERT integrates CNN layers with a transformer encoder to effectively capture both speech features and contextual information.\nWe denote the acoustic hidden representations of the speech modality inputs <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> generated by HuBERT as <math alttext=\"H_{S}=(h_{S}^{(1)},h_{S}^{(2)},\\cdots,h_{S}^{(m)})\\in\\mathbb{R}^{m\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>H</mi><mi>S</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msubsup><mi>h</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">H_{S}=(h_{S}^{(1)},h_{S}^{(2)},\\cdots,h_{S}^{(m)})\\in\\mathbb{R}^{m\\times d}</annotation></semantics></math>, where <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> is the dimension of hidden representations.</p>\n\n",
                "matched_terms": [
                    "denote",
                    "obtain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"{\\rm TE}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><mi>TE</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\rm TE}(\\cdot)</annotation></semantics></math> and <math alttext=\"{\\rm PE}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><mrow><mi>PE</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\rm PE}(\\cdot)</annotation></semantics></math> denote the token and position embeddings, respectively.</p>\n\n",
                "matched_terms": [
                    "denote",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"y_{\\rm emo}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m1\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>emo</mi></msub><annotation encoding=\"application/x-tex\">y_{\\rm emo}</annotation></semantics></math> is the predicted emotion classification and <math alttext=\"e\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m2\" intent=\":literal\"><semantics><mi>e</mi><annotation encoding=\"application/x-tex\">e</annotation></semantics></math> is the number of emotion categories. The corresponding loss function can be defined as</p>\n\n",
                "matched_terms": [
                    "corresponding",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further develop the modality discriminator <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m1\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> utilizing the modality-invariant representations produced by the MIR generator. This discriminator illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(f) is composed of two linear layers with a ReLU activation function in between, followed by a Sigmoid activation function. To enhance the MIR&#8217;s capability to remain modality-agnostic, we employ adversarial learning techniques.\nSpecifically, the discriminator <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> outputs a scalar value between 0 and 1 for each frame, where values close to 0 indicate the text modality, values close to 1 indicate the speech modality, and values near 0.5 represent an ambiguous modality.\nHere, <math alttext=\"D(H)\\in\\mathbb{R}^{m\\times 1},H\\in\\{H^{(spe)}_{T},H^{(spe)}_{S},H_{ST}^{\\rm(inv)}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>D</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow></msup></mrow><mo>,</mo><mrow><mi>H</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>inv</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">D(H)\\in\\mathbb{R}^{m\\times 1},H\\in\\{H^{(spe)}_{T},H^{(spe)}_{S},H_{ST}^{\\rm(inv)}\\}</annotation></semantics></math>.\nOn one hand, we aim for the discriminator that correctly classifies frames in the modality-specific representations <math alttext=\"H^{(spe)}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H^{(spe)}_{T}</annotation></semantics></math> and <math alttext=\"H^{(spe)}_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m5\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H^{(spe)}_{S}</annotation></semantics></math> as 0 and 1, respectively. On the other hand, to enhance the modality agnosticism of the modality-invariant representation <math alttext=\"H_{ST}^{\\rm(inv)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m6\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>inv</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{ST}^{\\rm(inv)}</annotation></semantics></math> produced by the MIR generator, we aim for a discriminator that outputs values close to 0.5, indicating an ambiguous modality. With this design of the MIR generator and discriminator, we define a generative adversarial training objective, mathematically represented as</p>\n\n",
                "matched_terms": [
                    "training",
                    "respectively",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance the model&#8217;s capability to learn emotion features from multimodal data, we employ a label-based contrastive learning task to complement the cross-entropy loss, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(g). This task aids the model in extracting emotion-related features when the MF module integrates speech and text data. As depicted in the LCL task in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.F5\" title=\"Figure 5 &#8227; III-H Label-based Contrastive Learning (LCL) &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we categorize data in each batch into positive and negative samples on the basis of emotion labels. For instance, in a batch containing eight samples, we compute the set of positive samples for each sample, where those with the same label are considered positive samples (yellow squares), and those with different labels are considered negative samples (white squares). We then calculate the label-based contrastive loss (LCL) using Eq. <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:eq:lcl</span>, which promotes instances of a specific label to be more similar to each other than instances of other labels.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the training stage, the learning process is optimized using three loss functions that correspond to ER, AED, and AEC (i.e., <math alttext=\"\\mathcal{L}_{\\rm ER}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>ER</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm ER}</annotation></semantics></math>, <math alttext=\"\\mathcal{L}_{\\rm AED}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>AED</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm AED}</annotation></semantics></math>, and <math alttext=\"\\mathcal{L}_{\\rm AEC}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>AEC</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm AEC}</annotation></semantics></math>), and two training strategies (i.e., <math alttext=\"\\mathcal{L}_{\\rm GAN}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>GAN</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm GAN}</annotation></semantics></math> and <math alttext=\"\\mathcal{L}_{\\rm LCL}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p1.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>LCL</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm LCL}</annotation></semantics></math>). The specific optimization process of M<sup class=\"ltx_sup\">4</sup>SER is detailed in Alg. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#alg1\" title=\"Algorithm 1 &#8227; III-I Joint Training &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nThese five loss functions are linearly combined as the overall training objective of M<sup class=\"ltx_sup\">4</sup>SER:</p>\n\n",
                "matched_terms": [
                    "m4ser",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the GAN training strategy <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib34\" title=\"\">34</a>]</cite>, we divide the backpropagation process into two steps. Firstly, we maximize <math alttext=\"\\mathcal{L}_{\\rm GAN}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>GAN</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm GAN}</annotation></semantics></math> to update the discriminator, during which the generator is detached from the optimization. According to Eq. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.Ex1\" title=\"III-G Modality Discriminator &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III-G</span></a>), on one hand, maximizing the first term <math alttext=\"L_{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>D</mi></msub><annotation encoding=\"application/x-tex\">L_{D}</annotation></semantics></math> of <math alttext=\"\\mathcal{L}_{\\rm GAN}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>GAN</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm GAN}</annotation></semantics></math> essentially trains the discriminator to correctly distinguish between text and audio features by making <math alttext=\"H_{S}^{\\rm(spe)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mi>spe</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{S}^{\\rm(spe)}</annotation></semantics></math> close to 1 and <math alttext=\"H_{T}^{\\rm(spe)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mi>spe</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{T}^{\\rm(spe)}</annotation></semantics></math> close to 0 <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Function <math alttext=\"\\log(x)+\\log(1-y)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>&#8722;</mo><mi>y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(x)+\\log(1-y)</annotation></semantics></math>, where <math alttext=\"x,y\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x,y\\in(0,1)</annotation></semantics></math> reaches its maximum when <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m3\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> approaches 1 and <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m4\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math> approaches 0.</span></span></span>. On the other hand, maximizing the second term <math alttext=\"L_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m6\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>G</mi></msub><annotation encoding=\"application/x-tex\">L_{G}</annotation></semantics></math> (i.e., minimizing <math alttext=\"-L_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m7\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><msub><mi>L</mi><mi>G</mi></msub></mrow><annotation encoding=\"application/x-tex\">-L_{G}</annotation></semantics></math>) will make <math alttext=\"H^{\\text{(inv)}}_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mtext>(inv)</mtext></msubsup><annotation encoding=\"application/x-tex\">H^{\\text{(inv)}}_{ST}</annotation></semantics></math> approach 0 or 1 <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Function <math alttext=\"\\log(x)+\\log(1-x)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>&#8722;</mo><mi>x</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\log(x)+\\log(1-x)</annotation></semantics></math>, where <math alttext=\"x\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m2\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x\\in(0,1)</annotation></semantics></math> reaches its maximum at <math alttext=\"x=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m3\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">x=0.5</annotation></semantics></math> and its minimum near <math alttext=\"x=0\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m4\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">x=0</annotation></semantics></math> and <math alttext=\"x=1\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m5\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">x=1</annotation></semantics></math>.</span></span></span>, indicating that the discriminator recognizes <math alttext=\"H^{\\text{(inv)}}_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m9\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mtext>(inv)</mtext></msubsup><annotation encoding=\"application/x-tex\">H^{\\text{(inv)}}_{ST}</annotation></semantics></math> as modality-specific, which can be either text or audio, contrary to our desired modality invariance. Secondly, we freeze the discriminator parameters and update the remaining parameters by minimizing <math alttext=\"L_{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m10\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>G</mi></msub><annotation encoding=\"application/x-tex\">L_{G}</annotation></semantics></math>, which makes the output of the discriminator <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m11\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> for the modality-invariant features <math alttext=\"H^{\\text{(inv)}}_{ST}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m12\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow><mtext>(inv)</mtext></msubsup><annotation encoding=\"application/x-tex\">H^{\\text{(inv)}}_{ST}</annotation></semantics></math> approach 0.5, thereby blurring these features between audio and text modalities to achieve modality agnosticism. Additionally, <math alttext=\"\\mathcal{L}_{\\text{ER}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m13\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ER</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ER}}</annotation></semantics></math> optimizes the downstream emotion recognition model, <math alttext=\"\\mathcal{L}_{\\text{AED}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m14\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>AED</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{AED}}</annotation></semantics></math> and <math alttext=\"\\mathcal{L}_{\\text{AEC}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m15\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>AEC</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{AEC}}</annotation></semantics></math> optimize the auxiliary tasks of ASR error detection and ASR error correction models, respectively, and <math alttext=\"\\mathcal{L}_{\\text{LCL}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS9.p2.m16\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>LCL</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{LCL}}</annotation></semantics></math> implements the LCL strategy. The entire system is trained in an end-to-end manner and optimized by adjusting weight parameters.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "respectively",
                    "emotion",
                    "training",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the effectiveness\nof our proposed model, we carried out experiments on two public datasets:\nthe Interactive Emotional Dyadic Motion Capture (IEMOCAP) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib45\" title=\"\">45</a>]</cite> and the Multimodal Emotion Lines Dataset (MELD) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib46\" title=\"\">46</a>]</cite>. The statistics of the two datasets are shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S4.F6\" title=\"Figure 6 &#8227; IV-A Dataset and Evaluation Metrics &#8227; IV Experimental Setup &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "emotion",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">IEMOCAP</span> comprised roughly 12 hours of speech from ten speakers participating in five scripted sessions. Following prior work <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib47\" title=\"\">47</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib9\" title=\"\">9</a>]</cite> on IEMOCAP, we employed 5531 utterances that were categorized into four emotion categories: &#8220;Neutral&#8221; (1,708), &#8220;Angry&#8221; (1,103), &#8220;Happy&#8221; (including &#8220;Excited&#8221;) (595 + 1,041 = 1,636), and &#8220;Sad&#8221; (1,084).\nWe performed five-fold leave-one-session-out (LOSO) cross-validation to evaluate the emotion classification performance using weighted accuracy (WA) and unweighted accuracy (UA).</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MELD</span> included a total of 13708 samples extracted from the TV series Friends, divided into 9,989 for training, 1,109 for validation, and 2,610 for testing. The dataset contained seven labels: &#8220;Neutral&#8221; (6167), &#8220;Happy&#8221; (2198), &#8220;Fear&#8221; (350), &#8220;Sad&#8221; (985), &#8220;Disgust&#8221; (355), &#8220;Angry&#8221; (1541), and &#8220;Surprise&#8221; (1499).\nWe tuned hyperparameters on the validation set and reported final results on the test set using the best checkpoint, evaluated by accuracy (ACC) and weighted F1-score (W-F1).</p>\n\n",
                "matched_terms": [
                    "acc",
                    "training",
                    "testing",
                    "wf1",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The acoustic encoder was initialized using the <span class=\"ltx_text ltx_font_typewriter\">hubert-base-ls960<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">3</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/facebook/hubert-base-ls960\" title=\"\">https://huggingface.co/facebook/hubert-base-ls960</a></span></span></span></span> model, producing acoustic representations <math alttext=\"d_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">d_{S}</annotation></semantics></math> with a dimensionality of 768.\nThe text encoder employed the <span class=\"ltx_text ltx_font_typewriter\">bert-base-uncased<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">4</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/google-bert/bert-base-uncased\" title=\"\">https://huggingface.co/google-bert/bert-base-uncased</a></span></span></span></span> model for initialization, yielding token representations <math alttext=\"d_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">d_{T}</annotation></semantics></math> with a dimensionality of 768. The vocabulary size for word tokenization <math alttext=\"d_{vocab}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>d</mi><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">d_{vocab}</annotation></semantics></math> was set to 30,522. We set the hidden size <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> to 768, the number of attention layers to 12, and the number of attention heads to 12. Both the HuBERT and BERT models were fine-tuned during the training stage.\nThe transformer decoder in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S2.F2\" title=\"Figure 2 &#8227; II-D2 Supervised Contrastive Learning &#8227; II-D Multistrategy Learning &#8227; II Related Work &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c) adopted a single-layer transformer with a hidden size of 768. Additionally, we obtained corresponding ASR hypotheses using the Whisper ASR model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib48\" title=\"\">48</a>]</cite>. We used the <span class=\"ltx_text ltx_font_typewriter\">openai/whisper-medium.en<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">5</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/openai/whisper-medium.en\" title=\"\">https://huggingface.co/openai/whisper-medium.en</a></span></span></span></span> and <span class=\"ltx_text ltx_font_typewriter\">openai/whisper-large-v2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">6</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/openai/whisper-large-v2\" title=\"\">https://huggingface.co/openai/whisper-large-v2</a></span></span></span></span> models, achieving word error rates (WERs) of 20.48% and 37.87% on the IEMOCAP and MELD datasets, respectively.</p>\n\n",
                "matched_terms": [
                    "respectively",
                    "iemocap",
                    "corresponding",
                    "training",
                    "used",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used Adam <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib49\" title=\"\">49</a>]</cite> as the optimizer with a batch size of 16. On the basis of empirical observations and prior work <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib50\" title=\"\">50</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib51\" title=\"\">51</a>]</cite>, we kept the learning rate constant at <math alttext=\"1e^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-5}</annotation></semantics></math> for IEMOCAP and <math alttext=\"1e^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-4}</annotation></semantics></math> for MELD during training. For our multitask learning setup, we set <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m3\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> to 3 and <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m4\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> to 0.1. For our multistrategy learning setup, we set <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m5\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> to 0.01 and <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m6\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> to 0.1. The temperature parameter <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m7\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> was set to 0.07. To validate these choices, we additionally performed a one-at-a-time sensitivity analysis on the hyperparameters <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m8\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m9\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>, <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m10\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>, and <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m11\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math>, as presented in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.SS3\" title=\"V-C Sensitivity Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">V-C</span></a> and illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F7\" title=\"Figure 7 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "analysis",
                    "training",
                    "used",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the experiments, we compared various multimodal speech emotion recognition SOTA methods with our proposed method, M<sup class=\"ltx_sup\">4</sup>SER.\nThe comparison was conducted by the following methods on the IEMOCAP dataset:</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "method",
                    "iemocap",
                    "methods",
                    "emotion",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SAMS and MF-AED-AEC methods are also utilized as baselines for the MELD dataset. Additionally, the following methods were included for comparison:</p>\n\n",
                "matched_terms": [
                    "methods",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">HCAM</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib56\" title=\"\">56</a>]</cite> combines wav2vec audio and BERT text inputs with co-attention and recurrent neural networks for multimodal emotion recognition.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T2\" title=\"TABLE II &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T3\" title=\"TABLE III &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> compare the performance of the M<sup class=\"ltx_sup\">4</sup>SER method with those of recent multimodal SER methods on the IEMOCAP and MELD datasets. Our proposed M<sup class=\"ltx_sup\">4</sup>SER achieves a WA of 79.2% and a UA of 80.1% on IEMOCAP, and an ACC of 66.5% and a W-F1 of 66.0% on MELD, outperforming other models and demonstrating the superiority of M<sup class=\"ltx_sup\">4</sup>SER. Specifically, on the IEMOCAP dataset, compared with the recent MAF-DCT, the M<sup class=\"ltx_sup\">4</sup>SER method shows a 0.7% improvement in WA and a 0.8% improvement in UA, reaching a new SOTA performance. These improvements stem from our M<sup class=\"ltx_sup\">4</sup>SER&#8217;s capability to mitigate the impact of ASR errors, capture modality-specific and modality-invariant representations across different modalities, enhance commonality between modalities through adversarial learning, and excel in emotion feature learning with the LCL loss.</p>\n\n",
                "matched_terms": [
                    "method",
                    "iemocap",
                    "methods",
                    "acc",
                    "emotion",
                    "m4ser",
                    "other",
                    "wf1",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed M<sup class=\"ltx_sup\">4</sup>SER model is composed of four types of learning: multimodal learning, multirepresentation learning, multitask learning, and multistrategy learning. To determine the impact of different types of learning on the performance of the emotion recognition task, ablation experiments were conducted on the IEMOCAP and MELD datasets, as presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "iemocap",
                    "emotion",
                    "m4ser",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multimodalities.</span>\nTo assess the necessity of multimodality, we conduct single-modal comparison experiments involving text and speech modalities. These experiments operate under a baseline framework excluding the MF module, LCL loss, and GAN loss, precluding intermodal interaction.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(A), in the single-modal experiments, the performance of speech modality in the IEMOCAP dataset is better than that of text modality, whereas the reverse is observed in the MELD dataset. This divergence suggests that emotional information is more distinctly conveyed through speech in IEMOCAP, whereas MELD leans towards text for emotional expression.\nMoreover, the multimodal baseline model, which simply concatenates speech and text features for recognition (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> A(3)), significantly enhances performance compared with single-modal baselines, highlighting the essential role of multimodal information.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "iemocap",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multirepresentations.</span> We study the importance of modality-invariant and modality-specific representations by discarding each type. Removing modality-specific representations from multimodal fusion significantly reduces downstream emotion recognition performance on both datasets, confirming their effectiveness in capturing and utilizing unique information from each modality. Similarly, omitting refined modality-invariant representations from multimodal fusion also significantly decreases emotion recognition performance on both datasets, demonstrating their importance in bridging modality gaps. Clearly, removing both representations further decreases performance.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multitasks.</span> First, we discuss the impact of the AED module. Specifically, we remove the AED module and introduce only the AEC module as an auxiliary task. This requires the model to correct all errors in each utterance from scratch, rather than only the detected errors. Evidently, the absence of the AED module leads to a significant drop in emotion recognition performance, demonstrating the necessity of the AED module. Similarly, we observe that the AEC module also plays an important role. Moreover, we note that the WA results using only the AEC module (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(C)(1)) are even worse than the results with neither the AED nor AEC module (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(C)(3)) in both datasets. This phenomenon occurs because directly using the neural machine translation (NMT) model for AEC can even increase the WER <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib60\" title=\"\">60</a>]</cite>. Unlike NMT tasks, which typically require modifying almost all input tokens, AEC involves fewer but more challenging corrections. For instance, if the ASR model&#8217;s WER is 10%, then only about 10% of the input tokens require correction in the AEC model. However, these tokens are often difficult to recognize and correct because they have already been misrecognized by the ASR model. Therefore, we should consider the characteristics of ASR outputs and carefully design the AEC model, which is why we introduce both AED and AEC as auxiliary tasks.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Multistrategies.</span> To validate the effectiveness of the adversarial training strategy outlined in Alg. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#alg1\" title=\"Algorithm 1 &#8227; III-I Joint Training &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we remove the adversarial training strategy from M<sup class=\"ltx_sup\">4</sup>SER. The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(D)(1) show a decrease in performance on both datasets, demonstrating the critical role of this strategy in learning modality-invariant representations. The proposed modality discriminator effectively enhances the modality agnosticism of the refined representations from the generator.\nAdditionally, omitting the LCL strategy described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S3.SS8\" title=\"III-H Label-based Contrastive Learning (LCL) &#8227; III Methodology &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">III-H</span></a> also results in similar performance degradation (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T4\" title=\"TABLE IV &#8227; V-A Comparisons of State-of-the-art (SOTA) Methods &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>(D)(2)). We visualize the results before and after introducing this strategy to demonstrate its effectiveness, with specific analysis also detailed in the following t-SNE analysis section. Moreover, removing both of these strategies further diminishes emotion recognition performance, confirming that both learning strategies contribute significantly to performance improvement.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "analysis",
                    "emotion",
                    "m4ser",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the model exhibits relative robustness to variations in <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m1\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>, indicating stable synergy between the AED and AEC objectives. Conversely, performance is highly sensitive to <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m2\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>; large values result in unstable training due to adversarial objectives, while a small weight (e.g., <math alttext=\"\\gamma=0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=0.01</annotation></semantics></math>) ensures stable convergence and optimal results.</p>\n\n",
                "matched_terms": [
                    "result",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the trade-off between model performance and computational cost, we report the number of parameters, training time, and inference time for each model on the IEMOCAP dataset in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T5\" title=\"TABLE V &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>, with all experiments conducted on a single NVIDIA Tesla V100 GPU.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Among the single-modal models, HuBERT exhibits higher computational cost than BERT owing to the nature of speech encoders, but also delivers better recognition performance. In the multi-modal setting, each additional module introduces a moderate increase in parameter size and training cost, while consistently improving recognition accuracy.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our full model M<sup class=\"ltx_sup\">4</sup>SER achieves the best performance with WA of 79.2% and UA of 80.1%. Although its training time increases to 21.5 hours owing to the inclusion of GAN and LCL strategies and AED and AEC subtasks, these components are used only during training. Consequently, the inference latency of M<sup class=\"ltx_sup\">4</sup>SER remains nearly identical to the baseline with only MSR and MIR. Considering the significant gains in performance, M<sup class=\"ltx_sup\">4</sup>SER demonstrates a favorable trade-off between computational cost and recognition accuracy, making it suitable for practical deployment.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "used",
                    "m4ser",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">t-SNE Analysis.</span> To intuitively demonstrate the advantages of our proposed M<sup class=\"ltx_sup\">4</sup>SER model on IEMOCAP and MELD datasets, we utilize the t-distributed stochastic neighbor embedding (t-SNE) tool <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib61\" title=\"\">61</a>]</cite> to visualize the learned emotion features using all samples from session 3 of the IEMOCAP and test set of the MELD. We compare these features across the following models: the speech model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(a)), the text model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(b)), the proposed M<sup class=\"ltx_sup\">4</sup>SER model without label-based contrastive learning (LCL) (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(c)), and the proposed M<sup class=\"ltx_sup\">4</sup>SER model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(d)).</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "analysis",
                    "emotion",
                    "m4ser",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the visualization results in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, we observe that the emotion label distributions for the two single-modal baselines on the IEMOCAP and MELD datasets show a significant overlap among the emotion categories, indicating that they are often confused with each other. In contrast, the emotion label distributions for the two multimodal models are more distinguishable, demonstrating the effectiveness of multimodal models in capturing richer emotional features.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "emotion",
                    "meld",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, compared with the M<sup class=\"ltx_sup\">4</sup>SER model without the LCL strategy, our M<sup class=\"ltx_sup\">4</sup>SER model achieves better clustering for each emotion category, making them as distinct as possible. For instance, on the IEMOCAP dataset, the intra-class clustering of samples learned by the M<sup class=\"ltx_sup\">4</sup>SER model (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(d)) is more pronounced than that learned by the M<sup class=\"ltx_sup\">4</sup>SER model without the LCL strategy (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F9\" title=\"Figure 9 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(c)), especially with clearer boundaries for sad and angry samples. Additionally, the distances between the four types of emotion in the emotion vector space increase. Although distinguishing MELD samples is more challenging than distinguishing IEMOCAP samples, the overall trend remains similar.\nThis indicates that the proposed M<sup class=\"ltx_sup\">4</sup>SER model effectively leverages both modality-specific and modality-invariant representations to capture high-level shared feature representations across speech and text modalities for emotion recognition.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "iemocap",
                    "emotion",
                    "m4ser",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further explore the effectiveness of adversarial learning in M<sup class=\"ltx_sup\">4</sup>SER, we visualize the distribution of modality-invariant and modality-specific representations before and after adversarial learning using the t-SNE tool, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F10\" title=\"Figure 10 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. It can be observed that after adversarial learning, different modality-specific representations become more separated with increasing intraclass clustering. This indicates that with the introduction of adversarial learning, M<sup class=\"ltx_sup\">4</sup>SER enhances the diversity of modality-specific representations, generating superior features for the downstream emotion recognition task.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "emotion",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Representation Analysis.</span>\nWe find that our M<sup class=\"ltx_sup\">4</sup>SER method performs better when using ASR text than when using GT text, as presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T6\" title=\"TABLE VI &#8227; V-F Visualization Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a>. To investigate this finding, we visualize representation weights for two examples from IEMOCAP, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F11\" title=\"Figure 11 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. We display representations of each modality across the temporal level in two learning spaces, accumulating hidden features of <math alttext=\"H_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m2\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">H_{S}</annotation></semantics></math>, <math alttext=\"H_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m3\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">H_{T}</annotation></semantics></math>, <math alttext=\"\\hat{H}_{S}^{spe}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m4\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>H</mi><mo>^</mo></mover><mi>S</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\hat{H}_{S}^{spe}</annotation></semantics></math>, and <math alttext=\"H_{T}^{spe}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p5.m5\" intent=\":literal\"><semantics><msubsup><mi>H</mi><mi>T</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">H_{T}^{spe}</annotation></semantics></math> into one dimension.\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F11\" title=\"Figure 11 &#8227; V-B Ablation Study &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(a) illustrates that during single-modal learning with ASR text, representation weights in the text modality mainly focus on the word &#8220;oh&#8221;, whereas in the speech modality, representation weights concentrate towards the sentence&#8217;s end. After cross-modal learning, we observed a significant decrease in &#8220;oh&#8221; weight in the text modality. This change occurs as the model detects errors or inaccuracies in ASR transcription using AED and AEC modules, shifting representation weights to other words such as &#8220;that&#8217;s&#8221;. Owing to limited emotional cues in the text, the model relies more on speech features, especially anger-related cues such as intonation and volume, critical for accurate anger recognition. Conversely, with GT text, after cross-modal learning, attention in the text modality focuses on words conveying positive emotions such as &#8220;amusing&#8221;, whereas speech modality distribution remains largely unchanged. Without ASR error signals, the model leans towards these textual features, leading to a bias towards happiness in emotion recognition.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "method",
                    "iemocap",
                    "analysis",
                    "emotion",
                    "m4ser",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Lack of ASR error signals:</span> GT text lacks ASR errors, missing additional cues beneficial for emotion recognition. This limitation possibly hinders capturing intense emotional signals when relying solely on GT text.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Confusion Matrix Analysis.</span> To investigate the impact of different modules on class-wise prediction, we visualize the averaged confusion matrices over 5-folds on IEMOCAP in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.F12\" title=\"Figure 12 &#8227; V-F Visualization Analysis &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>. Compared with the baseline, introducing MSR and MIR significantly enhances the prediction of &#8220;Happy&#8221; class, which often suffer from ambiguous acoustic-textual alignment.\nBy further incorporating GAN and LCL strategies, we observe overall improvements across most emotion classes, particularly in &#8220;Neutral&#8221;. However, the performance on the &#8220;Happy&#8221; class slightly drops, potentially owing to the acoustic-textual heterogeneity introduced by merging &#8220;Excited&#8221; into &#8220;Happy&#8221;, which possibly reduces the effectiveness of local contrastive learning in capturing class-specific discriminative features.\nFinally, integrating AED and AEC modules yields the most accurate and balanced predictions overall. We observe further improvements in &#8220;Neutral&#8221; and &#8220;Sad&#8221; classes, while the performance of &#8220;Happy&#8221; slightly recovers compared with the previous setting. These results highlight the effectiveness of multi-task learning in mitigating ASR-induced errors and enhancing emotional robustness across modalities.</p>\n\n",
                "matched_terms": [
                    "iemocap",
                    "analysis",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the performance of M<sup class=\"ltx_sup\">4</sup>SER in real-world environments, we conduct cross-corpus emotion recognition experiments, which simulate the practical scenario of domain shift between different datasets. Following the experimental settings of previous works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib62\" title=\"\">62</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib63\" title=\"\">63</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#bib.bib64\" title=\"\">64</a>]</cite>, we adopt a transfer evaluation strategy where one corpus is used entirely for training, and 30% of the target corpus is reserved for validation for parameter tuning, while the remaining 70% is used for final testing.\nIn addition, as the IEMOCAP dataset contains only four emotion classes (&#8220;Neutral&#8221;, &#8220;Happy&#8221;, &#8220;Angry&#8221;, &#8220;Sad&#8221;), we restrict MELD to the same set of overlapping emotions for a fair comparison and discard the rest (&#8220;Suprise&#8221;, &#8220;Fear&#8221;, &#8220;Disgust&#8221;). This ensures consistent label space across corpora during both training and evaluation.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "iemocap",
                    "domain",
                    "crosscorpus",
                    "emotion",
                    "training",
                    "testing",
                    "m4ser",
                    "used",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18706v1#S5.T8\" title=\"TABLE VIII &#8227; V-G Cross-corpus Generalization Ability &#8227; V Experimental Results &#8227; M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition\"><span class=\"ltx_text ltx_ref_tag\">VIII</span></a> presents the cross-corpus generalization results. Compared with the reimplemented SAMS and MF-AED-AEC baselines, our full model M<sup class=\"ltx_sup\">4</sup>SER achieves the best performance in both IE<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ME and ME<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>IE directions. Specifically, M<sup class=\"ltx_sup\">4</sup>SER outperforms the strong multimodal baseline by 4.3% in ACC and 3.1% in W-F1 under the IE<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ME setting, and by 5.1% WA and 4.8% UA in the ME<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS7.p2.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>IE setting.</p>\n\n",
                "matched_terms": [
                    "wf1",
                    "crosscorpus",
                    "acc",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results demonstrate that M<sup class=\"ltx_sup\">4</sup>SER not only generalizes robustly across corpora with minimal tuning, but also effectively adapts to new domains under few-shot conditions.</p>\n\n",
                "matched_terms": [
                    "fewshot",
                    "m4ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose M<sup class=\"ltx_sup\">4</sup>SER, a novel emotion recognition method that combines multimodal, multirepresentation, multitask, and multistrategy learning. M<sup class=\"ltx_sup\">4</sup>SER leverages an innovative multimodal fusion module to learn modality-specific and modality-invariant representations, capturing unique features of each modality and common features across modalities. We then introduce a modality discriminator to enhance modality diversity through adversarial learning. Additionally, we design two auxiliary tasks, AED and AEC, aimed at enhancing the semantic consistency within the text modality. Finally, we propose a label-based contrastive learning strategy to distinguish different emotional features. Results of experiments on the IEMOCAP and MELD datasets demonstrate that M<sup class=\"ltx_sup\">4</sup>SER surpasses previous baselines, proving its effectiveness. In the future, we plan to extend our approach to the visual modality and introduce disentangled representation learning to further enhance emotion recognition performance. We also plan to investigate whether AED and AEC modules remain necessary when using powerful LLM-based ASR systems.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "method",
                    "iemocap",
                    "emotion",
                    "m4ser",
                    "meld"
                ]
            }
        ]
    }
}