{
    "S4.T1": {
        "source_file": "Protocode: Prototype-Driven Interpretability for Code Generation in LLMs",
        "caption": "Table 1: Performance comparison across different models and methods on the MBPP dataset",
        "body": "Model →\\rightarrow\nMethod ↓\\downarrow\n\n\nQwen3-0.6B\nLlama3.2-1B\nFalcon3-1B\nStarcoder-1B-base\nQwen2.5coder-0.5B\nCodellama-7B\n\n\np​a​s​s​@​1pass@1\np​a​s​s​@​10pass@10\np​a​s​s​@​1pass@1\np​a​s​s​@​10pass@10\npass@1\np​a​s​s​@​10pass@10\np​a​s​s​@​1pass@1\np​a​s​s​@​10pass@10\np​a​s​s​@​1pass@1\np​a​s​s​@​10pass@10\np​a​s​s​@​1pass@1\np​a​s​s​@​10pass@10\n\n\n\n\nbase\n0.011\n0.048\n0.007\n0.042\n0.010\n0.063\n0.008\n0.040\n0.041\n0.116\n0.021\n0.116\n\n\ndiversity\n0.0076\n0.037\n0.012\n0.061\n0.010\n0.042\n0.002\n0.011\n0.021\n0.063\n0.007\n0.035\n\n\nsimilarity\n0.009\n0.050\n0.013\n0.050\n0.011\n0.050\n0.007\n0.032\n0.023\n0.069\n0.018\n0.079\n\n\nmbpp\n0.006\n0.024\n0.007\n0.042\n0.002\n0.018\n0.005\n0.004\n0.021\n0.095\n0.009\n0.039\n\n\nprototypes(ours)\n0.019\n0.059\n0.010\n0.058\n0.020\n0.068\n0.012\n0.050\n0.048\n0.122\n0.030\n0.122",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_parbox ltx_align_middle\" style=\"width:71.1pt;\">\n<span class=\"ltx_p ltx_align_center\">Model <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math></span>\n<span class=\"ltx_p ltx_align_center\">Method <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">Qwen3-0.6B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">Llama3.2-1B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">Falcon3-1B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">Starcoder-1B-base</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">Qwen2.5coder-0.5B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">Codellama-7B</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><math alttext=\"pass@1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">pass@1</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><math alttext=\"pass@10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">pass@10</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><math alttext=\"pass@1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">pass@1</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><math alttext=\"pass@10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m6\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">pass@10</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">pass@1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><math alttext=\"pass@10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m7\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">pass@10</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><math alttext=\"pass@1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m8\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">pass@1</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><math alttext=\"pass@10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m9\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">pass@10</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><math alttext=\"pass@1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m10\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">pass@1</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><math alttext=\"pass@10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m11\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">pass@10</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><math alttext=\"pass@1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m12\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">pass@1</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><math alttext=\"pass@10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m13\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">pass@10</annotation></semantics></math></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">base</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.011</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.048</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.007</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.042</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.010</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.063</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.008</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.040</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.041</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.116</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.021</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.116</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">diversity</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.0076</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.037</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.012</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.061</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.010</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.042</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.002</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.011</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.021</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.063</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.007</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.035</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">similarity</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.009</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.050</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.013</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.050</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.011</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.050</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.007</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.032</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.023</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.069</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.018</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.079</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">mbpp</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.006</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.024</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.007</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.042</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.002</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.018</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.005</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.004</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.021</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.095</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.009</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.039</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">prototypes(ours)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.019</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.059</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.010</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.058</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.020</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.068</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.012</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.050</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.048</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.122</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.030</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.122</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "↓downarrow",
            "qwen25coder05b",
            "→rightarrow",
            "diversity",
            "across",
            "falcon31b",
            "p​a​s​s​​1pass1",
            "base",
            "qwen306b",
            "methods",
            "model",
            "pass1",
            "dataset",
            "similarity",
            "codellama7b",
            "performance",
            "starcoder1bbase",
            "p​a​s​s​​10pass10",
            "models",
            "different",
            "prototypesours",
            "method",
            "llama321b",
            "mbpp",
            "comparison"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">From Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the Llama3.2 model exhibits high sensitivity to parameter variations, displaying substantial fluctuations in performance. This trend aligns with the results reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S4.T1\" title=\"Table 1 &#8227; 4 Results &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S4.T2\" title=\"Table 2 &#8227; 4 Results &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where the similarity-based sampling method achieves the highest score for Llama3.2, further highlighting its instability under different configurations. In contrast, the Qwen2.5-coder model demonstrates relatively stable behavior, showing consistently increasing trends across most parameters, with the notable exception of the scaling factor <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS8.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Since the introduction of Large Language Models (LLMs), they have been widely adopted for various tasks such as text summarization, question answering, speech-to-text translation, and more. In recent times, the use of LLMs for code generation has gained significant attention, with tools such as Cursor and Windsurf demonstrating the ability to analyze massive code repositories and recommend relevant changes. Big tech companies have also acknowledged the growing reliance on LLMs for code generation within their codebases. Although these advances significantly improve developer productivity, increasing reliance on automated code generation can proportionally increase the risk of suboptimal solutions and insecure code. Our work focuses on automatically sampling In-Context Learning (ICL) demonstrations which can improve model performance and enhance the interpretability of the generated code. Using AST-based analysis on outputs from the MBPP test set, we identify regions of code most influenced by the chosen demonstrations. In our experiments, we show that high-quality ICL demonstrations not only make outputs easier to interpret but also yield a positive performance improvement on the pass@10 metric. Conversely, poorly chosen ICL demonstrations affected the LLM performance on the pass@10 metric negatively compared to the base model. Overall, our approach highlights the importance of efficient sampling strategies for ICL, which can affect the performance of the model on any given task.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "base",
                    "mbpp",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent years, Large Language Models (LLMs) have gained significant traction in the fields of code completion and code filling. This growth has been fueled by the availability of large-scale open-source datasets such as The vault <cite class=\"ltx_cite ltx_citemacro_citet\">Manh et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib29\" title=\"\">2023</a>)</cite>, CodeSearchNet <cite class=\"ltx_cite ltx_citemacro_citet\">Husain et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib16\" title=\"\">2020</a>)</cite>, CodeXGlue <cite class=\"ltx_cite ltx_citemacro_citet\">Lu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib26\" title=\"\">2021</a>)</cite> and many others. Alongside these datasets, we have also witnessed the emergence of open-source models designed specifically for code-related tasks, including the CodeLlama series <cite class=\"ltx_cite ltx_citemacro_citet\">Rozi&#232;re et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib40\" title=\"\">2024</a>)</cite>, Qwen Coder <cite class=\"ltx_cite ltx_citemacro_citet\">Hui et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib15\" title=\"\">2024</a>)</cite> series, and StarCoder series <cite class=\"ltx_cite ltx_citemacro_citet\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib21\" title=\"\">2023a</a>)</cite>. In parallel, closed-source models such as GPT-4o <cite class=\"ltx_cite ltx_citemacro_citet\">OpenAI et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib32\" title=\"\">2024</a>)</cite> and Claude Code <cite class=\"ltx_cite ltx_citemacro_citet\">Anthropic (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib3\" title=\"\">2025</a>)</cite> have been widely adopted by various big tech companies for generating production-ready code. Despite these advancements, most of these models remain difficult to interpret in the context of code generation. While a variety of interpretability methods have been developed to interpret the outputs generated by LLMs and foster trust in their usage across domains, many of these approaches are generic and not specifically tailored for code generation tasks. Some methods, however, are focused on interpretability in code generation. For instance, Code-Q <cite class=\"ltx_cite ltx_citemacro_citet\">Palacio et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib34\" title=\"\">2025</a>)</cite> identifies influential tokens that guide the model&#8217;s output, but it requires repeated sampling and generation, which introduces significant computational overhead during inference.</p>\n\n",
                "matched_terms": [
                    "models",
                    "methods",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another method, ASTrust <cite class=\"ltx_cite ltx_citemacro_citet\">Palacio et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib33\" title=\"\">2024</a>)</cite>, leverages Abstract Syntax Trees (ASTs) by using model-generated token probabilities. Tokens are mapped to code level subsets, which are then grouped into terminal and non-terminal nodes within the AST. Each non-terminal node is represented by the aggregated confidence of its associated terminal nodes. However, this approach requires storing the probability distribution over the entire vocabulary at every step of generation, which scales poorly as the output length increases. To address these challenges, we propose a manifold-based sampling strategy that automatically samples a set of ICL demonstrations from a given dataset. These demonstrations enable interpretability by combining attribution and AST-based analysis. Our method segments the generated code into interpretable regions, such as Iterations, Data structures, etc., allowing users to understand which regions of the generated code are most affected by the sampled demonstrations. To the best of our knowledge, we are the first to unify prototype-driven ICL sampling with AST-grounded attribution for code interpretability.</p>\n\n",
                "matched_terms": [
                    "method",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">According to <cite class=\"ltx_cite ltx_citemacro_citet\">Bilal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib6\" title=\"\">2025</a>)</cite>, explainability techniques in AI systems can be broadly divided into three categories: (1) post hoc explanations, (2) intrinsic interpretability, and (3) human-centered explanations. Post hoc explanation methods aim to interpret a model&#8217;s decisions after predictions have been made. Common approaches include Local Interpretable Model-Agnostic Explanations (LIME) <cite class=\"ltx_cite ltx_citemacro_citet\">Ribeiro et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib37\" title=\"\">2016</a>)</cite>, Shapley Additive Explanations (SHAP) <cite class=\"ltx_cite ltx_citemacro_citet\">Lundberg &amp; Lee (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib27\" title=\"\">2017</a>)</cite>. LIME provides local explanations by identifying the most important features for a single prediction. Similarly, SHAP evaluates the contribution of each feature by measuring changes in the prediction when features are systematically removed. In addition, gradient-based methods such as SmoothGrad <cite class=\"ltx_cite ltx_citemacro_citet\">Smilkov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib44\" title=\"\">2017</a>)</cite> and Integrated Gradients <cite class=\"ltx_cite ltx_citemacro_citet\">Sundararajan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib48\" title=\"\">2017</a>)</cite> calculate model gradients with respect to input features to determine the sensitivity of the model&#8217;s output to each feature.</p>\n\n",
                "matched_terms": [
                    "model",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Intrinsic interpretability, in contrast, focuses on designing model architectures so that their behavior is inherently explainable. One example is concept bottleneck models <cite class=\"ltx_cite ltx_citemacro_citet\">Koh et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib20\" title=\"\">2020</a>)</cite>, which were extended to large language models (LLMs) by <cite class=\"ltx_cite ltx_citemacro_citet\">Sun et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib47\" title=\"\">2025</a>)</cite> for sentence classification task. Their approach generates concepts for each class, making the classification process directly interpretable. However, this approach faces limitations in generating suitable concepts for diverse tasks and does not scale well to text generation. Another related method, Proto-lm <cite class=\"ltx_cite ltx_citemacro_citet\">Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib59\" title=\"\">2023</a>)</cite> , extends prototype networks to text classification. Instead of generating concepts like concept bottlenecks, it learns trainable prototypes and maps them to the nearest training samples for interpretability.</p>\n\n",
                "matched_terms": [
                    "method",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A particularly influential method within intrinsic interpretability is Chain-of-Thought (CoT) <cite class=\"ltx_cite ltx_citemacro_citet\">Wei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib55\" title=\"\">2023</a>)</cite>, which generates intermediate reasoning steps. CoT has been shown to improve both plausibility and task performance compared to demonstrations that provide only the final answers <cite class=\"ltx_cite ltx_citemacro_citet\">Wei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib55\" title=\"\">2023</a>)</cite> <cite class=\"ltx_cite ltx_citemacro_citet\">Cobbe et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib10\" title=\"\">2021</a>)</cite>. Building upon this, Self-Consistency <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib54\" title=\"\">2023</a>)</cite> was proposed as an extension of CoT. This method prompts the model to produce multiple reasoning chains and answers, and then selects the final output using a majority vote across the answers. Although effective, Self-Consistency only ensures correctness of the final prediction, without verifying whether the reasoning chains themselves are valid or faithful. To address this, SEA-CoT <cite class=\"ltx_cite ltx_citemacro_citet\">Wei&#160;Jie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib57\" title=\"\">2024</a>)</cite> was introduced. SEA-CoT evaluates generated reasoning chains based on the implication with the task context and the overlap of the token level, ensuring that both the reasoning process and the final answer align more closely with the task requirements. However, as stated by <cite class=\"ltx_cite ltx_citemacro_citet\">Jacovi &amp; Goldberg (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib17\" title=\"\">2020</a>)</cite>, the reasoning chains from LLM often appear plausible to humans but are not necessarily faithful to the true decision-making process of the LLM. Plausibility refers to how convincing the interpretation is to humans, while faithfulness measures the degree to which it truly represents the internal reasoning of the LLM.</p>\n\n",
                "matched_terms": [
                    "method",
                    "performance",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most of the above methods are designed for generic tasks, with a limited focus on code-specific applications. The method ASTrust was developed specifically for interpretability in code generation. It builds Abstract Syntax Trees (ASTs) to align with program structure and assigns confidence scores to non-terminal nodes by aggregating probabilities from their terminal nodes. These scores are derived from token-level probabilities output by the model. <cite class=\"ltx_cite ltx_citemacro_citet\">Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib28\" title=\"\">2024</a>)</cite> demonstrates that LLMs already possess strong syntactic awareness, rivaling AST-based static code analysis. However, the method ASTrust has key limitations: its token sampling method is not well justified. Greedy sampling ignores the advantages of stochastic approaches, while stochastic sampling requires storing probabilities for all vocabulary tokens at every step an impractical, memory-intensive process. In contrast, our method avoids this heavy storage by relying on attribution-based prototype influence, which captures the effect of sampled demonstrations without requiring full vocabulary distributions. As a result, our approach preserves the benefits of stochastic sampling <cite class=\"ltx_cite ltx_citemacro_citet\">Shi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib43\" title=\"\">2024</a>)</cite> while remaining significantly more scalable and practical for code generation interpretability.</p>\n\n",
                "matched_terms": [
                    "method",
                    "model",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As highlighted in <cite class=\"ltx_cite ltx_citemacro_citet\">Rodriguez-Cardenas et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib38\" title=\"\">2023</a>)</cite>, in the context of ICL, the selection of demonstrations plays a crucial role in model performance. In our approach, we dedicate considerable effort to identifying the most suitable prototypes (ICL examples) for each LLM. Our method can be divided into two main components. In the first stage, we initialize a simple neural network <math alttext=\"h_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m1\" intent=\":literal\"><semantics><msub><mi>h</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">h_{\\theta}</annotation></semantics></math> and train it on Dataset <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> to jointly optimize manifold learning and metric learning objectives. Once the training is complete, the learned proxy vectors are employed to sample prototypes.</p>\n\n",
                "matched_terms": [
                    "method",
                    "performance",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prompt Structure: <span class=\"ltx_text ltx_font_italic\">\"This is the query being assigned:\"+\" \"+ [/Q]+\" \"+\"The following is the code solution to the query\"+\" \"+[/S]\"</span>.\nWhere the placeholders <span class=\"ltx_text ltx_font_italic\">[/Q]</span> and <span class=\"ltx_text ltx_font_italic\">[/S]</span> are for query and code solution respectively. After formatting the prompts, we use the respective Large Language model(<math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math>) to encode the final prompts into the latent representations (<math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>). We simultaneously label encode the programming language ID for using them as class labels; this method gives us 9 different classes, and for each sample in the dataset, we will be storing the encoded label (<math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math>) and the latent representation (<math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>) as pairs in dataset <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m5\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "method",
                    "different",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As mentioned in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S3\" title=\"3 Methodology &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our method consists of two stages. In the first stage of our method, we initialize a simple neural network <math alttext=\"h_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>h</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">h_{\\theta}</annotation></semantics></math> Tab &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A2.T3\" title=\"Table 3 &#8227; B.1 Evaluation Dataset and Metric &#8227; Appendix B Methodology &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and train it on Dataset <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> to jointly optimize manifold learning &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S3.E3\" title=\"In Manifold Point-to-Point Loss: &#8227; 3.4 Training objectives &#8227; 3 Methodology &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and metric learning objectives &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S3.E1\" title=\"In Proxy Anchor Loss: &#8227; 3.4 Training objectives &#8227; 3 Methodology &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The neural network <math alttext=\"h_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>h</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">h_{\\theta}</annotation></semantics></math> learns to map the high-dimensional encoded representations into lower dimensions. Before the training process, we initialize the proxies <math alttext=\"\\theta_{q}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{q}</annotation></semantics></math> and <math alttext=\"\\theta_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{m}</annotation></semantics></math>. Here both the proxies are unique for each class and initialized randomly with <math alttext=\"\\theta_{q}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{q}</annotation></semantics></math> = <math alttext=\"\\theta_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m7\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{m}</annotation></semantics></math>.\nThe proxy vector <math alttext=\"\\theta_{q}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m8\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{q}</annotation></semantics></math> is updated via back-propagation, and the proxy vector <math alttext=\"\\theta_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m9\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{m}</annotation></semantics></math> is updated via the Momentum update <cite class=\"ltx_cite ltx_citemacro_citet\">He et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib14\" title=\"\">2020</a>)</cite> where <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m10\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> is the momentum constant,\n<math alttext=\"[\\theta_{k}\\leftarrow\\gamma\\theta_{k}+(1-\\gamma)\\theta_{q}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m11\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mi>&#952;</mi><mi>k</mi></msub><mo stretchy=\"false\">&#8592;</mo><mrow><mrow><mi>&#947;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#952;</mi><mi>k</mi></msub></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>&#8722;</mo><mi>&#947;</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#952;</mi><mi>q</mi></msub></mrow></mrow></mrow><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[\\theta_{k}\\leftarrow\\gamma\\theta_{k}+(1-\\gamma)\\theta_{q}]</annotation></semantics></math></p>\n\n",
                "matched_terms": [
                    "method",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the effectiveness of different sampling methods by applying them as in-context learning (ICL) examples on the MBPP test set <cite class=\"ltx_cite ltx_citemacro_citet\">Austin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib4\" title=\"\">2021</a>)</cite>. To demonstrate the effectiveness of our method we have used 2 sets of models for experimentation, the first set consisting of generic models of Qwen3 <cite class=\"ltx_cite ltx_citemacro_citet\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib60\" title=\"\">2025</a>)</cite>, Llama-3.2 <cite class=\"ltx_cite ltx_citemacro_citet\">AI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib2\" title=\"\">2024</a>)</cite>,Falcon-3 <cite class=\"ltx_cite ltx_citemacro_citet\">Team (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib50\" title=\"\">2024</a>)</cite> and for the second set we have used code heavy pre-trained models Starcoder-base <cite class=\"ltx_cite ltx_citemacro_citet\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib22\" title=\"\">2023b</a>)</cite>, Qwen2.5-Coder <cite class=\"ltx_cite ltx_citemacro_citet\">Hui et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib15\" title=\"\">2024</a>)</cite>, Codellama <cite class=\"ltx_cite ltx_citemacro_citet\">Rozi&#232;re et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib40\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "different",
                    "method",
                    "methods",
                    "mbpp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are reported on a scale from <math alttext=\"[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0,1]</annotation></semantics></math>, where <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m2\" intent=\":literal\"><mn>0</mn></math> is the lowest and <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m3\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> is the highest (For instance, 0.1 can be interpreted as 10%). While the numerical margins may appear small at first glance, even modest gains in code completion represent substantial improvements. For context, GPT-4-1106 <cite class=\"ltx_cite ltx_citemacro_citet\">ope (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib1\" title=\"\">2023</a>)</cite>, which is estimated to be at least <math alttext=\"1000\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.p2.m4\" intent=\":literal\"><semantics><mrow><mn>1000</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">1000\\times</annotation></semantics></math> larger than the models used for our experiments, achieves a score of <math alttext=\"0.786\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m5\" intent=\":literal\"><semantics><mn>0.786</mn><annotation encoding=\"application/x-tex\">0.786</annotation></semantics></math> on the MBPP test set. This comparison highlights an important distinction: in many benchmarks, partial overlap between a generated solution and the reference solution may yield a nonzero score even if the final answer is incorrect. In contrast, code benchmarks are more stringent, as each generated program is independently evaluated against unit test cases. Therefore, even incremental improvements in <math alttext=\"Pass@k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m6\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">Pass@k</annotation></semantics></math> metrics are highly significant for code generation tasks.</p>\n\n",
                "matched_terms": [
                    "models",
                    "comparison",
                    "mbpp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Qwen2.5-coder model, despite having fewer parameters than Codellama, achieves comparable performance on both the <math alttext=\"Pass@1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">Pass@1</annotation></semantics></math> and <math alttext=\"Pass@10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m2\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">Pass@10</annotation></semantics></math> metrics across the MBPP and MBPP+ test sets. Among all comparisons, the similarity-based sampling method surpasses our approach only for the Llama3.2 model; in every other case, our method consistently outperforms alternative strategies across all models. As noted by <cite class=\"ltx_cite ltx_citemacro_citet\">Rodriguez-Cardenas et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib38\" title=\"\">2023</a>)</cite>, within the ICL setting, the quality of selected demonstrations can also negatively affect model performance.</p>\n\n",
                "matched_terms": [
                    "p​a​s​s​​10pass10",
                    "model",
                    "across",
                    "models",
                    "p​a​s​s​​1pass1",
                    "method",
                    "mbpp",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the Qwen3 and Qwen2.5-coder models, using demonstrations sampled from methods other than the prototype-based approach leads to a decline in performance on both MBPP and MBPP+. A similar trend is observed for the Starcoder and Codellama models. These results suggest that the Qwen family of models, as well as code-pretrained models in general, are particularly sensitive to the choice of ICL demonstrations. An unsuitable set of demonstrations can reduce performance compared to the base model, underscoring the importance of effective sampling strategies for ICL.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "base",
                    "methods",
                    "mbpp",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AST analysis involves using the prototype-based attribution scores as token confidence scores explained in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5.E5\" title=\"In 5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. We then compute the average confidence over tokens corresponding to each AST node, and report these averages as performance values grouped by manually defined syntax categories. The process follows three steps, illustrated in Fig.1 &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5.F1\" title=\"Figure 1 &#8227; 5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. In Step1, for every generated code snippet, the tokenizer splits the code into tokens <math alttext=\"w_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">w_{i}</annotation></semantics></math> (forming the token set <math alttext=\"\\uptau\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#964;</mi><annotation encoding=\"application/x-tex\">\\uptau</annotation></semantics></math> &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A4.SS1\" title=\"D.1 Interpretable Syntax sets and interactions &#8227; Appendix D AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">D.1</span></a>), and the model assigns a confidence score to each token as described in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5\" title=\"5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. In Step2, the token-level predictions are aligned with the respective Abstract Syntax Tree (AST) terminal nodes. Terminal nodes retain the raw confidences, whereas non-terminal nodes hierarchically store aggregated values. Together, terminal and non-terminal nodes form the subcategory set <math alttext=\"\\upsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\"><semantics><mi>&#965;</mi><annotation encoding=\"application/x-tex\">\\upsilon</annotation></semantics></math> &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A4.SS1\" title=\"D.1 Interpretable Syntax sets and interactions &#8227; Appendix D AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">D.1</span></a>. For instance, the token &#8217;if_&#8217; from the token set aligns with a terminal AST node but is grouped under the non-terminal node &#8217;if_statement&#8217;. Finally, in Step 3, the analysis introduces eight syntax categories to summarize model predictions. These categories aggregate subcategories into broader, human-interpretable groups. The Syntax Categories form a fixed Category Set <math alttext=\"\\Lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#923;</mi><annotation encoding=\"application/x-tex\">\\Lambda</annotation></semantics></math> &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A4.SS1\" title=\"D.1 Interpretable Syntax sets and interactions &#8227; Appendix D AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">D.1</span></a>, providing more intuitive elements for interpretation.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the attribution-based confidence score of each Syntax Category (SC) for the 6 LLMs, we present an AST analysis.\n&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5.F3\" title=\"Figure 3 &#8227; 5.2 Code Syntactic analysis &#8227; 5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates the AST interpretability performance segregated by Syntax Categories (SCs) for\neach model type. The Qwen2.5 Coder and Qwen3 &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5.F3\" title=\"Figure 3 &#8227; 5.2 Code Syntactic analysis &#8227; 5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (a) models exhibit highly consistent confidence across all syntax categories, with nearly identical values. Both models demonstrate their strongest performance in Scope, Data Structures, and Functions, indicating reliability in handling structured data, variable and function scoping, and modular code organization. Moderate confidence is observed for Iteration, Decisions, Operators, and Data Types, while the lowest confidence is consistently assigned to Exception handling, suggesting potential limitations in generating or reasoning about robust error-handling constructs. Overall, these results suggest that both Qwen2.5 Coder and Qwen3 are best suited for structured programming tasks, while being less dependable for control-flow&#8211;intensive or exception-heavy code generation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Llama models &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5.F3\" title=\"Figure 3 &#8227; 5.2 Code Syntactic analysis &#8227; 5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (b) exhibit broadly similar confidence trends across syntax categories, with CodeLlama consistently showing a slight advantage over Llama-3.2. Both models demonstrate their highest reliability in Data Structures, Functions, and Iteration, suggesting strong capabilities in tasks that require structured data handling, modular code organization, and loop-based constructs. Moderate confidence is observed in Scope, Decisions, Operators, and Data Types, indicating stable but less pronounced strengths. In contrast, Exception handling remains the weakest category for both models, highlighting a shared limitation in generating or reasoning about robust error-handling logic. Collectively, these results suggest that while the Llama models are well-suited for structured programming tasks, they are less dependable for exception-heavy scenarios.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Falcon and StarCoder models &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5.F3\" title=\"Figure 3 &#8227; 5.2 Code Syntactic analysis &#8227; 5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (c) display distinct differences in their syntax-grounded confidence. StarCoder consistently achieves higher confidence across nearly all categories compared to Falcon, indicating stronger overall reliability. Both models perform best in Scope, Data Structures, and Functions, suggesting robustness in structured programming tasks and modular code organization. StarCoder further extends this strength to Iteration and Decisions, where it shows clear improvements over Falcon, highlighting its ability to handle control flow more effectively. In contrast, Exception handling remains the weakest category for both models, underscoring a shared limitation in generating robust error-handling constructs. Taken together, these results indicate that while Falcon is moderately capable across most categories, StarCoder offers broader syntactic reliability and is better suited for tasks requiring control flow and structured data handling.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, prototypes were sampled exclusively from the Magicoder dataset. While this choice provided a consistent basis for evaluation, extending the analysis to additional datasets could offer a broader understanding of prototype quality. In fact, our method can naturally be applied as a global metric for ranking datasets with respect to their ability to yield effective prototypes. Another limitation arises from differences in model stability. For example, Llama3.2 &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> exhibited high sensitivity to changes in nearly all hyperparameters, which led to inconsistent results on the <math alttext=\"Pass@k\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">Pass@k</annotation></semantics></math> metric. In contrast, the Qwen2.5 Coder model &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> displayed only marginal sensitivity, with the exception of the <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m2\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> parameter, resulting in more stable and reliable performance. Finally, while our current approach uses sampled prototypes as in-context learning demonstrations, the framework can be extended toward pre-hoc interpretability by design. In particular, prototype steering could be explored as a mechanism for influencing model behavior, offering new avenues for both interpretability and controllability in LLMs.</p>\n\n",
                "matched_terms": [
                    "method",
                    "performance",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The method Piecewise-Linear Manifolds for Deep Metric Learning <cite class=\"ltx_cite ltx_citemacro_citet\">Bhatnagar &amp; Ahuja (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib5\" title=\"\">2024</a>)</cite> aims to train a neural network to learn a semantic feature space where similar items are close together and dissimilar items are far apart, in an unsupervised manner. This method is based on using linearized neighborhoods of points to construct a piecewise linear manifold, which helps estimate a continuous-valued similarity between data points.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Triplet loss</span> <cite class=\"ltx_cite ltx_citemacro_citet\">Schroff et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib42\" title=\"\">2015</a>)</cite> is another metric learning objective that enforces relative similarity by ensuring that an anchor <math alttext=\"x_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">x_{a}</annotation></semantics></math> is closer to a positive sample <math alttext=\"x_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">x_{p}</annotation></semantics></math> (same class) than to a negative sample <math alttext=\"x_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>n</mi></msub><annotation encoding=\"application/x-tex\">x_{n}</annotation></semantics></math> (different class) by at least a margin. Unlike contrastive loss, which only considers pairwise distances, triplet loss leverages relative comparisons, making it more effective in learning discriminative embeddings for tasks such as face recognition and image retrieval, here <math alttext=\"f(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p3.m4\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f(\\cdot)</annotation></semantics></math> is the embedding function, <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p3.m5\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> is the margin, <math alttext=\"x_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p3.m6\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">x_{a}</annotation></semantics></math> is the anchor, <math alttext=\"x_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p3.m7\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">x_{p}</annotation></semantics></math> is a positive sample, and <math alttext=\"x_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p3.m8\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>n</mi></msub><annotation encoding=\"application/x-tex\">x_{n}</annotation></semantics></math> is a negative sample.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on <cite class=\"ltx_cite ltx_citemacro_citet\">Dong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib11\" title=\"\">2024</a>)</cite>, several unsupervised strategies have been proposed to sample effective demonstrations for ICL. A simple yet effective method is to select the nearest neighbors of the input instance based on similarity measures (<cite class=\"ltx_cite ltx_citemacro_citet\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib24\" title=\"\">2022</a>)</cite>, <cite class=\"ltx_cite ltx_citemacro_citet\">Tanwar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib49\" title=\"\">2023</a>)</cite>, <cite class=\"ltx_cite ltx_citemacro_citet\">Qin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib35\" title=\"\">2024</a>)</cite>). Common distance metrics include L2 distance and cosine similarity derived from sentence embeddings. Beyond distance-based approaches, mutual information <cite class=\"ltx_cite ltx_citemacro_citet\">Sorensen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib46\" title=\"\">2022</a>)</cite> and perplexity <cite class=\"ltx_cite ltx_citemacro_citet\">Gonen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib12\" title=\"\">2023</a>)</cite> have also been shown to be useful for selecting prompts without labeled data or model-specific assumptions.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although off-the-shelf retrievers provide convenient solutions for a wide range of NLP tasks, they are often heuristic and sub-optimal due to the absence of task-specific supervision. To overcome this limitation, supervised retriever-based methods have been introduced (<cite class=\"ltx_cite ltx_citemacro_citet\">Rubin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib41\" title=\"\">2022</a>)</cite> <cite class=\"ltx_cite ltx_citemacro_citet\">Ye et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib61\" title=\"\">2023</a>)</cite> <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib53\" title=\"\">2024</a>)</cite> <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib62\" title=\"\">2022</a>)</cite>). For instance, <cite class=\"ltx_cite ltx_citemacro_citet\">Rubin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib41\" title=\"\">2022</a>)</cite> proposed EPR, a two-stage framework for training dense retrievers to identify suitable demonstrations. Building on this, <cite class=\"ltx_cite ltx_citemacro_citet\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib23\" title=\"\">2023c</a>)</cite> developed a unified retriever capable of selecting demonstrations across diverse tasks, while <cite class=\"ltx_cite ltx_citemacro_citet\">Mavromatis et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib30\" title=\"\">2023</a>)</cite> introduced AdaICL, a model-adaptive method that leverages LLMs to predict outcomes for unlabeled data and assign uncertainty scores to guide demonstration selection.</p>\n\n",
                "matched_terms": [
                    "method",
                    "methods",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><cite class=\"ltx_cite ltx_citemacro_citet\">Rodriguez-Cardenas et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib38\" title=\"\">2023</a>)</cite> emphasized the sensitivity of demonstration selection by comparing two different prompt groups in a controlled experiment. One group exhibited a positive causal effect, improving the Average Treatment Effect (ATE) by 5.1% on Chatgpt, while the other group showed a negative causal effect, decreasing ATE by 3.3% relative to the control group. Here, ATE quantifies the average causal influence of a treatment (i.e., the chosen prompt group) on model performance. These findings highlight the critical role of demonstration quality: poorly chosen examples may reduce performance, sometimes performing worse than LLMS that do not use ICL at all. Throughout the paper, we use the terms demonstrations and examples interchangeably in the context of ICL.</p>\n\n",
                "matched_terms": [
                    "different",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MBPP dataset consists of 973 python programming questions. Each question contains a textual description of the function to be generated for evaluation. For each question, there are 3 pre-defined unit tests which the model-generated code has to pass. The samples also contain a reference code. The MBPP testset is a sampled set of 378 questions for evaluation. The MBPP+ dataset is also similar in terms to MBPP dataset except it was created by <cite class=\"ltx_cite ltx_citemacro_citet\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib25\" title=\"\">2023</a>)</cite> and here each question has more than 3 unit tests per question for evaluation.</p>\n\n",
                "matched_terms": [
                    "mbpp",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed the sampled prototypes as ICL demonstrations to generate code completions on the MBPP test set <cite class=\"ltx_cite ltx_citemacro_citet\">Austin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib4\" title=\"\">2021</a>)</cite>, and evaluated the code completions using <math alttext=\"pass@1\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">pass@1</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib9\" title=\"\">2021</a>)</cite> and <math alttext=\"pass@10\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">pass@10</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib9\" title=\"\">2021</a>)</cite> metrics. We used the evalplus <cite class=\"ltx_cite ltx_citemacro_citet\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib25\" title=\"\">2023</a>)</cite> library for code post-processing and calculating the <math alttext=\"pass@1\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">pass@1</annotation></semantics></math> and <math alttext=\"pass@10\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">pass@10</annotation></semantics></math> metrics. The <math alttext=\"pass@k\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">pass@k</annotation></semantics></math> metric assesses the functional correctness of generated code by checking performance against predefined unit tests. Unlike CodeBLEU <cite class=\"ltx_cite ltx_citemacro_citet\">Ren et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib36\" title=\"\">2020</a>)</cite>, which only reflects surface-level similarity, pass@k is more reliable for evaluating functional correctness since it directly verifies whether at least one generated program passes the test cases.</p>\n\n",
                "matched_terms": [
                    "p​a​s​s​​10pass10",
                    "p​a​s​s​​1pass1",
                    "similarity",
                    "mbpp",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage of our framework, dedicated to prototype sampling, the network <math alttext=\"h_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>h</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">h_{\\theta}</annotation></semantics></math> is trained for 200 epochs on the training dataset <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math>. Training utilizes two independent Adam optimizers: one for the network parameters and another for the proxy parameters. Both optimizers are initialized with a learning rate of <span class=\"ltx_text ltx_font_typewriter\">1e-3</span>, combined with a scheduler that decays the learning rate by a factor of <math alttext=\"\\eta_{t}=0.97\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mi>t</mi></msub><mo>=</mo><mn>0.97</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{t}=0.97</annotation></semantics></math>. The dimensionality of the encoded vector <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m4\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math> is determined by the underlying Large Language Model (<math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m5\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math>). A mini-batch size of 128 samples is maintained throughout training.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the initial set of experiments, the hyperparameters for manifold construction and manifold point-to-point loss estimation are configured as follows: <math alttext=\"T=90\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><mn>90</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">T=90\\%</annotation></semantics></math>, <math alttext=\"\\delta=2\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#948;</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\delta=2</annotation></semantics></math>, <math alttext=\"m=3\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">m=3</annotation></semantics></math>, <math alttext=\"N_{\\alpha}=4\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>N</mi><mi>&#945;</mi></msub><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">N_{\\alpha}=4</annotation></semantics></math>, and <math alttext=\"N_{\\beta}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>N</mi><mi>&#946;</mi></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">N_{\\beta}=0.5</annotation></semantics></math>. The momentum constant for updating <math alttext=\"\\theta_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p2.m6\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{m}</annotation></semantics></math> is set to <math alttext=\"\\gamma=0.99\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p2.m7\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>0.99</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=0.99</annotation></semantics></math>. For Proxy Anchor loss, we employ <math alttext=\"\\alpha=32\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p2.m8\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>32</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=32</annotation></semantics></math> and <math alttext=\"\\epsilon=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p2.m9\" intent=\":literal\"><semantics><mrow><mi>&#1013;</mi><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">\\epsilon=0.1</annotation></semantics></math>. These settings serve as the baseline configuration; subsequently, an ablation study is conducted on the above parameters for LLMs that exhibited comparatively lower performance than competing methods.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All experiments were conducted on an NVIDIA RTX A6000 GPU. In the first stage of our method, we train a lightweight neural network <math alttext=\"h_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>h</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">h_{\\theta}</annotation></semantics></math> to sample prototypes, which requires approximately 640 MB of GPU memory and about 7 hours of training time without parallelization. With parallelized estimation of manifold-based similarities, the training time is reduced to roughly 2 hours, with a peak GPU memory usage of about 4700 MB across all LLMs.</p>\n\n",
                "matched_terms": [
                    "method",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed method demonstrates resource efficiency by requiring fewer demonstrations while achieving performance on par with fine-tuning approaches. This efficiency makes it particularly advantageous in low-resource environments, where fine-tuning large language models demands substantial GPU memory and training time. Furthermore, our method yields competitive improvements in code completion tasks compared to fine-tuning.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Similarity-based sampling:</span> The test query was encoded following the same procedure as in the Magicoder dataset. Demonstrations were then selected from each programming language class based on the closest Euclidean distance to the test query. This method would be sampling 9 distinct prototypes from each class.</p>\n\n",
                "matched_terms": [
                    "method",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Base model:</span> For the LLMs being tested no ICL demonstrations were provided, only the test query was provided.</p>\n\n",
                "matched_terms": [
                    "base",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MBPP Few shots:</span> The authors of the MBPP test set used and experimented with the samples at indexes 2, 3, 4 as ICL examples. In our experiments, we also use the same set of samples for comparison.</p>\n\n",
                "matched_terms": [
                    "mbpp",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The table presents the token lengths of sampled prototypes along with the 99th percentile, 95th percentile, and average token lengths across the MBPP dataset for combined query and solution inputs. Since each input consists of the sampled prototypes used as demonstrations together with the MBPP test queries, we estimate the overall input token lengths to assess whether all prototypes can be accommodated. These token length statistics are reported separately for each LLM.</p>\n\n",
                "matched_terms": [
                    "mbpp",
                    "dataset",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the table, it can be observed that the sampled prototype token lengths exceed the context window of the Falcon3-1B model. Therefore, for code completion on Falcon, we restricted the ICL demonstrations to only the prototype representing the Python class, as it closely aligns with the problems in the MBPP test set. The same procedure was applied across all sampling strategies for the Falcon3 model.</p>\n\n",
                "matched_terms": [
                    "mbpp",
                    "model",
                    "across",
                    "falcon31b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The table also shows that the Codellama model, being code-specific, produces a higher number of tokens compared to the Llama3.2 model. This highlights the optimized tokenization techniques of the Llama3.2 series, as Codellama is derived from the Llama2 family of models. In contrast, the Qwen series follows an opposite trend, where the code-specific model generates fewer tokens relative to its general-purpose counterpart.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All reported scores in this paper have been independently recomputed across every model and sampling method. The results for the base model (without ICL) may differ from those documented in the official technical reports, which can be attributed to several factors. Based on our experimental findings, we outline the potential reasons that may have influenced performance aside from the ICL demonstrations.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across",
                    "base",
                    "method",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For generating code completions we employed the Hugging Face text generation pipeline with decoding parameters set to <span class=\"ltx_text ltx_font_typewriter\">temperature = 0.6</span> and <span class=\"ltx_text ltx_font_typewriter\">top-p = 0.9</span>. Our experiments revealed that even minor adjustments to these parameters, with only two variations, led to improved performance across all models and sampling methods. Notably, most technical reports for benchmark evaluations do not specify the decoding strategies employed, which contributes to variability in reported results. This observation underscores the importance of performing hyperparameter optimization during the decoding stage of generation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "methods",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the MBPP and MBPP+ test sets, each query is paired with pre-defined unit tests, requiring the model to produce code completions that precisely match the expected function names. While one way to ensure success would be to include the reference solution as a demonstration for each query, such an approach risks data leakage, as the model would be exposed to the ground-truth answers rather than generating them independently. To mitigate this issue, we deliberately excluded reference code solutions from the input queries.</p>\n\n",
                "matched_terms": [
                    "mbpp",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As outlined in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A2.SS2\" title=\"B.2 Training Parameters &#8227; Appendix B Methodology &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>, the baseline configurations were employed for the initial experiments. To further investigate performance limitations, we conducted an ablation study focusing on LLMs that demonstrated comparatively weaker results. Specifically, under the baseline settings, the Llama3.2 and Qwen3 models underperformed relative to other methods. Consequently, we performed an extensive hyperparameter ablation on these models to better understand their sensitivities and performance dynamics.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The parameter <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> denotes the dimension of the linear submanifold <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math>, which locally approximates the data manifold around a point <math alttext=\"h_{\\theta}(z)\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>h</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">h_{\\theta}(z)</annotation></semantics></math>.\nTo examine its effect, we vary <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> in the range <math alttext=\"[2,8]\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>2</mn><mo>,</mo><mn>8</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[2,8]</annotation></semantics></math> with a step size of <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m6\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(a), performance consistently decreases in both models as <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> increases.\nThis trend arises because <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math> is intended to approximate the immediate neighborhood of a point, which is inherently low-dimensional.\nLarger values of <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m9\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> may lead to overfitting, since only a limited number of nearby samples are available within a batch to reliably estimate <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m10\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math>, thereby degrading performance.\nFurthermore, we observe that the computational overhead for prototype sampling increases with larger <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m11\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>, underscoring the trade-off between accuracy and efficiency.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The parameter <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> denotes the momentum constant used to update the proxy vector <math alttext=\"\\theta_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{m}</annotation></semantics></math> during prototype sampling.\nFollowing <cite class=\"ltx_cite ltx_citemacro_citet\">He et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib14\" title=\"\">2020</a>)</cite>, higher values of <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m3\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> are expected to yield improved performance, as the proxy updates become smoother and more stable.\nConsistent with this observation, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(b) shows that in both models, performance improves as <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m4\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> increases, highlighting the importance of stable momentum updates for effective representation learning.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The parameters <math alttext=\"N_{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#945;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\alpha}</annotation></semantics></math> and <math alttext=\"N_{\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#946;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\beta}</annotation></semantics></math> control the decay of similarity based on the orthogonal and projected distances, respectively, of a point from the linear submanifold in the neighborhood of another point.\nWe vary <math alttext=\"N_{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m3\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#945;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\alpha}</annotation></semantics></math> in the range <math alttext=\"[1,6]\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mn>6</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[1,6]</annotation></semantics></math> with a step size of <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m5\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, and <math alttext=\"N_{\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m6\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#946;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\beta}</annotation></semantics></math> in the range <math alttext=\"[0.5,3]\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0.5</mn><mo>,</mo><mn>3</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0.5,3]</annotation></semantics></math> with a step size of <math alttext=\"0.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m8\" intent=\":literal\"><semantics><mn>0.5</mn><annotation encoding=\"application/x-tex\">0.5</annotation></semantics></math>.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(c), increasing <math alttext=\"N_{\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m9\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#946;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\beta}</annotation></semantics></math> leads to a slight performance gain in the Qwen2.5-Coder model, while the Llama3.2 model exhibits larger fluctuations but follows an overall upward trend.\nSimilarly, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(d) shows that performance improves marginally with larger <math alttext=\"N_{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m10\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#945;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\alpha}</annotation></semantics></math> in the Qwen2.5-Coder model, whereas the Llama3.2 model demonstrates a clearer and more consistent increase.\nThis effect can be explained by the relationship between <math alttext=\"N_{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m11\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#945;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\alpha}</annotation></semantics></math> and <math alttext=\"N_{\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m12\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#946;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\beta}</annotation></semantics></math>: as <math alttext=\"N_{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m13\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#945;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\alpha}</annotation></semantics></math> approaches <math alttext=\"N_{\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m14\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#946;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\beta}</annotation></semantics></math>, a point <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m15\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> at distance <math alttext=\"\\varepsilon\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m16\" intent=\":literal\"><semantics><mi>&#949;</mi><annotation encoding=\"application/x-tex\">\\varepsilon</annotation></semantics></math> within the linear neighborhood of a point <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m17\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> (and thus sharing many features with <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m18\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> and its neighbors) may be treated as equally dissimilar to <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m19\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> as another point <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m20\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> located at an orthogonal distance <math alttext=\"\\varepsilon\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m21\" intent=\":literal\"><semantics><mi>&#949;</mi><annotation encoding=\"application/x-tex\">\\varepsilon</annotation></semantics></math> from the neighborhood of <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m22\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "similarity",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The reconstruction threshold <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS4.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> determines the quality of points admitted into the linear submanifold <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math>.\nWe vary <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS4.p1.m3\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> in the range <math alttext=\"[0.7,0.95]\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS4.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0.7</mn><mo>,</mo><mn>0.95</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0.7,0.95]</annotation></semantics></math> with a step size of <math alttext=\"0.05\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS4.p1.m5\" intent=\":literal\"><semantics><mn>0.05</mn><annotation encoding=\"application/x-tex\">0.05</annotation></semantics></math>.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(e), both models exhibit a clear upward trend in performance as <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS4.p1.m6\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> increases, underscoring the importance of ensuring that only high-quality points are incorporated into <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS4.p1.m7\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math>.\nWhile the Llama3.2 model follows this overall increasing trend, it displays noticeable fluctuations compared to the more stable improvement observed in the Qwen2.5-Coder model.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The scaling factor <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS5.p1.m1\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> regulates the maximum separation between dissimilar points. We vary <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS5.p1.m2\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> in the range <math alttext=\"[0.8,3.2]\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS5.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0.8</mn><mo>,</mo><mn>3.2</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0.8,3.2]</annotation></semantics></math> with a step size of <math alttext=\"0.4\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS5.p1.m4\" intent=\":literal\"><semantics><mn>0.4</mn><annotation encoding=\"application/x-tex\">0.4</annotation></semantics></math>. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(f), the performance remains relatively stable across this range for both models, highlighting the robustness of our method.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "method",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The scaling factor <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS6.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> controls the sharpness of the exponential term in the Proxy Anchor loss. We vary its value over <math alttext=\"{5,10,15,20,25,30,32}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS6.p1.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo>,</mo><mn>10</mn><mo>,</mo><mn>15</mn><mo>,</mo><mn>20</mn><mo>,</mo><mn>25</mn><mo>,</mo><mn>30</mn><mo>,</mo><mn>32</mn></mrow><annotation encoding=\"application/x-tex\">{5,10,15,20,25,30,32}</annotation></semantics></math>. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(g), both models exhibit an overall increasing trend in performance with larger <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS6.p1.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>. However, the Qwen2.5-coder model displays higher fluctuations compared to the more stable Llama3.2 model.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The margin parameter <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS7.p1.m1\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math> enforces that positive embeddings are pulled within this distance from their corresponding class proxies. We vary its value across <math alttext=\"{0.001,0.005,0.05,0.1,0.2}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS7.p1.m2\" intent=\":literal\"><semantics><mrow><mn>0.001</mn><mo>,</mo><mn>0.005</mn><mo>,</mo><mn>0.05</mn><mo>,</mo><mn>0.1</mn><mo>,</mo><mn>0.2</mn></mrow><annotation encoding=\"application/x-tex\">{0.001,0.005,0.05,0.1,0.2}</annotation></semantics></math>. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(h), the Qwen2.5-coder model demonstrates stable performance across the range of <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS7.p1.m3\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math>, whereas the Llama3.2 model exhibits a decreasing trend with noticeable fluctuations. This indicates that larger values of <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS7.p1.m4\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math> impose overly strict constraints on the separation between positive and negative proxies, thereby hindering the embeddings from effectively satisfying the margin requirement.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A <span class=\"ltx_text ltx_font_bold\">clustering function <math alttext=\"\\zeta\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m1\" intent=\":literal\"><semantics><mi>&#950;</mi><annotation encoding=\"application/x-tex\">\\zeta</annotation></semantics></math></span> computes the confidence performance of <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m2\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> and <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> nodes (subcategories) within an AST by hierarchically aggregating Token-Level Confidences into a category <math alttext=\"c\\in\\Lambda\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m4\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>&#8712;</mo><mi mathvariant=\"normal\">&#923;</mi></mrow><annotation encoding=\"application/x-tex\">c\\in\\Lambda</annotation></semantics></math>. After tokens are aligned to their respective nodes using <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m5\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math>, AST analysis groups them into either their corresponding category or non-terminal <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m6\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> node, following the AST structure. In some cases, terminal <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m7\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> nodes may be directly aggregated into a category without involving intermediate non-terminal <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m8\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> nodes. The function <math alttext=\"\\zeta\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m9\" intent=\":literal\"><semantics><mi>&#950;</mi><annotation encoding=\"application/x-tex\">\\zeta</annotation></semantics></math> can be configured to use different aggregation strategies, such as average, median, or maximum. In our experiments, we define the clustering function as <math alttext=\"\\zeta:\\upsilon\\rightarrow\\text{avg}(w_{1:i})\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m10\" intent=\":literal\"><semantics><mrow><mi>&#950;</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi>&#965;</mi><mo stretchy=\"false\">&#8594;</mo><mrow><mtext>avg</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\zeta:\\upsilon\\rightarrow\\text{avg}(w_{1:i})</annotation></semantics></math> for a subset of tokens <math alttext=\"w_{\\leq i}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m11\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi/><mo>&#8804;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{\\leq i}</annotation></semantics></math>. The 8 defined syntax categories are:</p>\n\n",
                "matched_terms": [
                    "performance",
                    "different"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "Protocode: Prototype-Driven Interpretability for Code Generation in LLMs",
        "caption": "Table 2: Performance comparison across different models and methods on the MBPP+ Dataset",
        "body": "Model →\\rightarrow\nMethod ↓\\downarrow\n\n\nQwen3-0.6B\nLlama3.2-1B\nFalcon3-1B\nStarcoder-1B-base\nQwen2.5coder-0.5B\nCodellama-7B\n\n\np​a​s​s​@​1pass@1\np​a​s​s​@​10pass@10\np​a​s​s​@​1pass@1\np​a​s​s​@​10pass@10\np​a​s​s​@​1pass@1\np​a​s​s​@​10pass@10\np​a​s​s​@​1pass@1\np​a​s​s​@​10pass@10\np​a​s​s​@​1pass@1\np​a​s​s​@​10pass@10\np​a​s​s​@​1pass@1\np​a​s​s​@​10pass@10\n\n\n\n\nbase\n0.0078\n0.0396\n0.005\n0.037\n0.009\n0.058\n0.008\n0.037\n0.031\n0.100\n0.015\n0.090\n\n\ndiversity\n0.0067\n0.031\n0.007\n0.045\n0.007\n0.034\n0.002\n0.011\n0.017\n0.048\n0.006\n0.026\n\n\nsimilarity\n0.0061\n0.037\n0.008\n0.054\n0.007\n0.042\n0.006\n0.029\n0.017\n0.055\n0.013\n0.067\n\n\nmbpp\n0.002\n0.016\n0.005\n0.042\n0.001\n0.013\n0.004\n0.032\n0.016\n0.077\n0.006\n0.040\n\n\nprototypes(ours)\n0.016\n0.050\n0.007\n0.050\n0.015\n0.050\n0.010\n0.046\n0.039\n0.108\n0.024\n0.103",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_parbox ltx_align_middle\" style=\"width:71.1pt;\">\n<span class=\"ltx_p ltx_align_center\">Model <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math></span>\n<span class=\"ltx_p ltx_align_center\">Method <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">Qwen3-0.6B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">Llama3.2-1B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">Falcon3-1B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">Starcoder-1B-base</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">Qwen2.5coder-0.5B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">Codellama-7B</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><math alttext=\"pass@1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">pass@1</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><math alttext=\"pass@10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">pass@10</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><math alttext=\"pass@1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">pass@1</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><math alttext=\"pass@10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">pass@10</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><math alttext=\"pass@1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">pass@1</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><math alttext=\"pass@10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">pass@10</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><math alttext=\"pass@1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m9\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">pass@1</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><math alttext=\"pass@10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m10\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">pass@10</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><math alttext=\"pass@1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m11\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">pass@1</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><math alttext=\"pass@10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m12\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">pass@10</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><math alttext=\"pass@1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m13\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">pass@1</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><math alttext=\"pass@10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m14\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">pass@10</annotation></semantics></math></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">base</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.0078</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.0396</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.005</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.037</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.009</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.058</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.008</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.037</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.031</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.015</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.090</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">diversity</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.0067</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.031</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.007</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.045</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.007</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.034</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.002</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.011</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.017</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.048</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.006</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.026</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">similarity</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.0061</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.037</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.008</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.054</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.007</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.042</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.006</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.029</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.017</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.055</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.013</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.067</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">mbpp</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.002</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.016</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.005</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.042</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.001</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.013</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.004</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.032</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.016</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.077</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.006</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.040</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">prototypes(ours)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.016</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.050</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.007</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.050</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.015</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.050</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.010</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.046</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.039</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.108</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.024</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.103</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "↓downarrow",
            "qwen25coder05b",
            "→rightarrow",
            "diversity",
            "across",
            "falcon31b",
            "p​a​s​s​​1pass1",
            "base",
            "qwen306b",
            "methods",
            "model",
            "dataset",
            "similarity",
            "codellama7b",
            "performance",
            "starcoder1bbase",
            "p​a​s​s​​10pass10",
            "models",
            "different",
            "prototypesours",
            "method",
            "llama321b",
            "mbpp",
            "comparison"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">From Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the Llama3.2 model exhibits high sensitivity to parameter variations, displaying substantial fluctuations in performance. This trend aligns with the results reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S4.T1\" title=\"Table 1 &#8227; 4 Results &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S4.T2\" title=\"Table 2 &#8227; 4 Results &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where the similarity-based sampling method achieves the highest score for Llama3.2, further highlighting its instability under different configurations. In contrast, the Qwen2.5-coder model demonstrates relatively stable behavior, showing consistently increasing trends across most parameters, with the notable exception of the scaling factor <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS8.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Since the introduction of Large Language Models (LLMs), they have been widely adopted for various tasks such as text summarization, question answering, speech-to-text translation, and more. In recent times, the use of LLMs for code generation has gained significant attention, with tools such as Cursor and Windsurf demonstrating the ability to analyze massive code repositories and recommend relevant changes. Big tech companies have also acknowledged the growing reliance on LLMs for code generation within their codebases. Although these advances significantly improve developer productivity, increasing reliance on automated code generation can proportionally increase the risk of suboptimal solutions and insecure code. Our work focuses on automatically sampling In-Context Learning (ICL) demonstrations which can improve model performance and enhance the interpretability of the generated code. Using AST-based analysis on outputs from the MBPP test set, we identify regions of code most influenced by the chosen demonstrations. In our experiments, we show that high-quality ICL demonstrations not only make outputs easier to interpret but also yield a positive performance improvement on the pass@10 metric. Conversely, poorly chosen ICL demonstrations affected the LLM performance on the pass@10 metric negatively compared to the base model. Overall, our approach highlights the importance of efficient sampling strategies for ICL, which can affect the performance of the model on any given task.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "base",
                    "mbpp",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent years, Large Language Models (LLMs) have gained significant traction in the fields of code completion and code filling. This growth has been fueled by the availability of large-scale open-source datasets such as The vault <cite class=\"ltx_cite ltx_citemacro_citet\">Manh et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib29\" title=\"\">2023</a>)</cite>, CodeSearchNet <cite class=\"ltx_cite ltx_citemacro_citet\">Husain et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib16\" title=\"\">2020</a>)</cite>, CodeXGlue <cite class=\"ltx_cite ltx_citemacro_citet\">Lu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib26\" title=\"\">2021</a>)</cite> and many others. Alongside these datasets, we have also witnessed the emergence of open-source models designed specifically for code-related tasks, including the CodeLlama series <cite class=\"ltx_cite ltx_citemacro_citet\">Rozi&#232;re et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib40\" title=\"\">2024</a>)</cite>, Qwen Coder <cite class=\"ltx_cite ltx_citemacro_citet\">Hui et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib15\" title=\"\">2024</a>)</cite> series, and StarCoder series <cite class=\"ltx_cite ltx_citemacro_citet\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib21\" title=\"\">2023a</a>)</cite>. In parallel, closed-source models such as GPT-4o <cite class=\"ltx_cite ltx_citemacro_citet\">OpenAI et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib32\" title=\"\">2024</a>)</cite> and Claude Code <cite class=\"ltx_cite ltx_citemacro_citet\">Anthropic (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib3\" title=\"\">2025</a>)</cite> have been widely adopted by various big tech companies for generating production-ready code. Despite these advancements, most of these models remain difficult to interpret in the context of code generation. While a variety of interpretability methods have been developed to interpret the outputs generated by LLMs and foster trust in their usage across domains, many of these approaches are generic and not specifically tailored for code generation tasks. Some methods, however, are focused on interpretability in code generation. For instance, Code-Q <cite class=\"ltx_cite ltx_citemacro_citet\">Palacio et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib34\" title=\"\">2025</a>)</cite> identifies influential tokens that guide the model&#8217;s output, but it requires repeated sampling and generation, which introduces significant computational overhead during inference.</p>\n\n",
                "matched_terms": [
                    "models",
                    "methods",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another method, ASTrust <cite class=\"ltx_cite ltx_citemacro_citet\">Palacio et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib33\" title=\"\">2024</a>)</cite>, leverages Abstract Syntax Trees (ASTs) by using model-generated token probabilities. Tokens are mapped to code level subsets, which are then grouped into terminal and non-terminal nodes within the AST. Each non-terminal node is represented by the aggregated confidence of its associated terminal nodes. However, this approach requires storing the probability distribution over the entire vocabulary at every step of generation, which scales poorly as the output length increases. To address these challenges, we propose a manifold-based sampling strategy that automatically samples a set of ICL demonstrations from a given dataset. These demonstrations enable interpretability by combining attribution and AST-based analysis. Our method segments the generated code into interpretable regions, such as Iterations, Data structures, etc., allowing users to understand which regions of the generated code are most affected by the sampled demonstrations. To the best of our knowledge, we are the first to unify prototype-driven ICL sampling with AST-grounded attribution for code interpretability.</p>\n\n",
                "matched_terms": [
                    "method",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">According to <cite class=\"ltx_cite ltx_citemacro_citet\">Bilal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib6\" title=\"\">2025</a>)</cite>, explainability techniques in AI systems can be broadly divided into three categories: (1) post hoc explanations, (2) intrinsic interpretability, and (3) human-centered explanations. Post hoc explanation methods aim to interpret a model&#8217;s decisions after predictions have been made. Common approaches include Local Interpretable Model-Agnostic Explanations (LIME) <cite class=\"ltx_cite ltx_citemacro_citet\">Ribeiro et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib37\" title=\"\">2016</a>)</cite>, Shapley Additive Explanations (SHAP) <cite class=\"ltx_cite ltx_citemacro_citet\">Lundberg &amp; Lee (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib27\" title=\"\">2017</a>)</cite>. LIME provides local explanations by identifying the most important features for a single prediction. Similarly, SHAP evaluates the contribution of each feature by measuring changes in the prediction when features are systematically removed. In addition, gradient-based methods such as SmoothGrad <cite class=\"ltx_cite ltx_citemacro_citet\">Smilkov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib44\" title=\"\">2017</a>)</cite> and Integrated Gradients <cite class=\"ltx_cite ltx_citemacro_citet\">Sundararajan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib48\" title=\"\">2017</a>)</cite> calculate model gradients with respect to input features to determine the sensitivity of the model&#8217;s output to each feature.</p>\n\n",
                "matched_terms": [
                    "model",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Intrinsic interpretability, in contrast, focuses on designing model architectures so that their behavior is inherently explainable. One example is concept bottleneck models <cite class=\"ltx_cite ltx_citemacro_citet\">Koh et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib20\" title=\"\">2020</a>)</cite>, which were extended to large language models (LLMs) by <cite class=\"ltx_cite ltx_citemacro_citet\">Sun et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib47\" title=\"\">2025</a>)</cite> for sentence classification task. Their approach generates concepts for each class, making the classification process directly interpretable. However, this approach faces limitations in generating suitable concepts for diverse tasks and does not scale well to text generation. Another related method, Proto-lm <cite class=\"ltx_cite ltx_citemacro_citet\">Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib59\" title=\"\">2023</a>)</cite> , extends prototype networks to text classification. Instead of generating concepts like concept bottlenecks, it learns trainable prototypes and maps them to the nearest training samples for interpretability.</p>\n\n",
                "matched_terms": [
                    "method",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A particularly influential method within intrinsic interpretability is Chain-of-Thought (CoT) <cite class=\"ltx_cite ltx_citemacro_citet\">Wei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib55\" title=\"\">2023</a>)</cite>, which generates intermediate reasoning steps. CoT has been shown to improve both plausibility and task performance compared to demonstrations that provide only the final answers <cite class=\"ltx_cite ltx_citemacro_citet\">Wei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib55\" title=\"\">2023</a>)</cite> <cite class=\"ltx_cite ltx_citemacro_citet\">Cobbe et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib10\" title=\"\">2021</a>)</cite>. Building upon this, Self-Consistency <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib54\" title=\"\">2023</a>)</cite> was proposed as an extension of CoT. This method prompts the model to produce multiple reasoning chains and answers, and then selects the final output using a majority vote across the answers. Although effective, Self-Consistency only ensures correctness of the final prediction, without verifying whether the reasoning chains themselves are valid or faithful. To address this, SEA-CoT <cite class=\"ltx_cite ltx_citemacro_citet\">Wei&#160;Jie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib57\" title=\"\">2024</a>)</cite> was introduced. SEA-CoT evaluates generated reasoning chains based on the implication with the task context and the overlap of the token level, ensuring that both the reasoning process and the final answer align more closely with the task requirements. However, as stated by <cite class=\"ltx_cite ltx_citemacro_citet\">Jacovi &amp; Goldberg (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib17\" title=\"\">2020</a>)</cite>, the reasoning chains from LLM often appear plausible to humans but are not necessarily faithful to the true decision-making process of the LLM. Plausibility refers to how convincing the interpretation is to humans, while faithfulness measures the degree to which it truly represents the internal reasoning of the LLM.</p>\n\n",
                "matched_terms": [
                    "method",
                    "performance",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most of the above methods are designed for generic tasks, with a limited focus on code-specific applications. The method ASTrust was developed specifically for interpretability in code generation. It builds Abstract Syntax Trees (ASTs) to align with program structure and assigns confidence scores to non-terminal nodes by aggregating probabilities from their terminal nodes. These scores are derived from token-level probabilities output by the model. <cite class=\"ltx_cite ltx_citemacro_citet\">Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib28\" title=\"\">2024</a>)</cite> demonstrates that LLMs already possess strong syntactic awareness, rivaling AST-based static code analysis. However, the method ASTrust has key limitations: its token sampling method is not well justified. Greedy sampling ignores the advantages of stochastic approaches, while stochastic sampling requires storing probabilities for all vocabulary tokens at every step an impractical, memory-intensive process. In contrast, our method avoids this heavy storage by relying on attribution-based prototype influence, which captures the effect of sampled demonstrations without requiring full vocabulary distributions. As a result, our approach preserves the benefits of stochastic sampling <cite class=\"ltx_cite ltx_citemacro_citet\">Shi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib43\" title=\"\">2024</a>)</cite> while remaining significantly more scalable and practical for code generation interpretability.</p>\n\n",
                "matched_terms": [
                    "method",
                    "model",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As highlighted in <cite class=\"ltx_cite ltx_citemacro_citet\">Rodriguez-Cardenas et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib38\" title=\"\">2023</a>)</cite>, in the context of ICL, the selection of demonstrations plays a crucial role in model performance. In our approach, we dedicate considerable effort to identifying the most suitable prototypes (ICL examples) for each LLM. Our method can be divided into two main components. In the first stage, we initialize a simple neural network <math alttext=\"h_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m1\" intent=\":literal\"><semantics><msub><mi>h</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">h_{\\theta}</annotation></semantics></math> and train it on Dataset <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> to jointly optimize manifold learning and metric learning objectives. Once the training is complete, the learned proxy vectors are employed to sample prototypes.</p>\n\n",
                "matched_terms": [
                    "method",
                    "performance",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prompt Structure: <span class=\"ltx_text ltx_font_italic\">\"This is the query being assigned:\"+\" \"+ [/Q]+\" \"+\"The following is the code solution to the query\"+\" \"+[/S]\"</span>.\nWhere the placeholders <span class=\"ltx_text ltx_font_italic\">[/Q]</span> and <span class=\"ltx_text ltx_font_italic\">[/S]</span> are for query and code solution respectively. After formatting the prompts, we use the respective Large Language model(<math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math>) to encode the final prompts into the latent representations (<math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>). We simultaneously label encode the programming language ID for using them as class labels; this method gives us 9 different classes, and for each sample in the dataset, we will be storing the encoded label (<math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math>) and the latent representation (<math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>) as pairs in dataset <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m5\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "method",
                    "different",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As mentioned in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S3\" title=\"3 Methodology &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our method consists of two stages. In the first stage of our method, we initialize a simple neural network <math alttext=\"h_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>h</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">h_{\\theta}</annotation></semantics></math> Tab &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A2.T3\" title=\"Table 3 &#8227; B.1 Evaluation Dataset and Metric &#8227; Appendix B Methodology &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and train it on Dataset <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> to jointly optimize manifold learning &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S3.E3\" title=\"In Manifold Point-to-Point Loss: &#8227; 3.4 Training objectives &#8227; 3 Methodology &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and metric learning objectives &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S3.E1\" title=\"In Proxy Anchor Loss: &#8227; 3.4 Training objectives &#8227; 3 Methodology &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The neural network <math alttext=\"h_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>h</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">h_{\\theta}</annotation></semantics></math> learns to map the high-dimensional encoded representations into lower dimensions. Before the training process, we initialize the proxies <math alttext=\"\\theta_{q}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{q}</annotation></semantics></math> and <math alttext=\"\\theta_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{m}</annotation></semantics></math>. Here both the proxies are unique for each class and initialized randomly with <math alttext=\"\\theta_{q}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{q}</annotation></semantics></math> = <math alttext=\"\\theta_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m7\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{m}</annotation></semantics></math>.\nThe proxy vector <math alttext=\"\\theta_{q}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m8\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{q}</annotation></semantics></math> is updated via back-propagation, and the proxy vector <math alttext=\"\\theta_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m9\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{m}</annotation></semantics></math> is updated via the Momentum update <cite class=\"ltx_cite ltx_citemacro_citet\">He et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib14\" title=\"\">2020</a>)</cite> where <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m10\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> is the momentum constant,\n<math alttext=\"[\\theta_{k}\\leftarrow\\gamma\\theta_{k}+(1-\\gamma)\\theta_{q}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m11\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mi>&#952;</mi><mi>k</mi></msub><mo stretchy=\"false\">&#8592;</mo><mrow><mrow><mi>&#947;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#952;</mi><mi>k</mi></msub></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>&#8722;</mo><mi>&#947;</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#952;</mi><mi>q</mi></msub></mrow></mrow></mrow><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[\\theta_{k}\\leftarrow\\gamma\\theta_{k}+(1-\\gamma)\\theta_{q}]</annotation></semantics></math></p>\n\n",
                "matched_terms": [
                    "method",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the effectiveness of different sampling methods by applying them as in-context learning (ICL) examples on the MBPP test set <cite class=\"ltx_cite ltx_citemacro_citet\">Austin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib4\" title=\"\">2021</a>)</cite>. To demonstrate the effectiveness of our method we have used 2 sets of models for experimentation, the first set consisting of generic models of Qwen3 <cite class=\"ltx_cite ltx_citemacro_citet\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib60\" title=\"\">2025</a>)</cite>, Llama-3.2 <cite class=\"ltx_cite ltx_citemacro_citet\">AI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib2\" title=\"\">2024</a>)</cite>,Falcon-3 <cite class=\"ltx_cite ltx_citemacro_citet\">Team (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib50\" title=\"\">2024</a>)</cite> and for the second set we have used code heavy pre-trained models Starcoder-base <cite class=\"ltx_cite ltx_citemacro_citet\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib22\" title=\"\">2023b</a>)</cite>, Qwen2.5-Coder <cite class=\"ltx_cite ltx_citemacro_citet\">Hui et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib15\" title=\"\">2024</a>)</cite>, Codellama <cite class=\"ltx_cite ltx_citemacro_citet\">Rozi&#232;re et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib40\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "different",
                    "method",
                    "methods",
                    "mbpp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are reported on a scale from <math alttext=\"[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0,1]</annotation></semantics></math>, where <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m2\" intent=\":literal\"><mn>0</mn></math> is the lowest and <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m3\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> is the highest (For instance, 0.1 can be interpreted as 10%). While the numerical margins may appear small at first glance, even modest gains in code completion represent substantial improvements. For context, GPT-4-1106 <cite class=\"ltx_cite ltx_citemacro_citet\">ope (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib1\" title=\"\">2023</a>)</cite>, which is estimated to be at least <math alttext=\"1000\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.p2.m4\" intent=\":literal\"><semantics><mrow><mn>1000</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">1000\\times</annotation></semantics></math> larger than the models used for our experiments, achieves a score of <math alttext=\"0.786\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m5\" intent=\":literal\"><semantics><mn>0.786</mn><annotation encoding=\"application/x-tex\">0.786</annotation></semantics></math> on the MBPP test set. This comparison highlights an important distinction: in many benchmarks, partial overlap between a generated solution and the reference solution may yield a nonzero score even if the final answer is incorrect. In contrast, code benchmarks are more stringent, as each generated program is independently evaluated against unit test cases. Therefore, even incremental improvements in <math alttext=\"Pass@k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m6\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">Pass@k</annotation></semantics></math> metrics are highly significant for code generation tasks.</p>\n\n",
                "matched_terms": [
                    "models",
                    "comparison",
                    "mbpp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Qwen2.5-coder model, despite having fewer parameters than Codellama, achieves comparable performance on both the <math alttext=\"Pass@1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">Pass@1</annotation></semantics></math> and <math alttext=\"Pass@10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m2\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">Pass@10</annotation></semantics></math> metrics across the MBPP and MBPP+ test sets. Among all comparisons, the similarity-based sampling method surpasses our approach only for the Llama3.2 model; in every other case, our method consistently outperforms alternative strategies across all models. As noted by <cite class=\"ltx_cite ltx_citemacro_citet\">Rodriguez-Cardenas et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib38\" title=\"\">2023</a>)</cite>, within the ICL setting, the quality of selected demonstrations can also negatively affect model performance.</p>\n\n",
                "matched_terms": [
                    "p​a​s​s​​10pass10",
                    "model",
                    "across",
                    "models",
                    "p​a​s​s​​1pass1",
                    "method",
                    "mbpp",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the Qwen3 and Qwen2.5-coder models, using demonstrations sampled from methods other than the prototype-based approach leads to a decline in performance on both MBPP and MBPP+. A similar trend is observed for the Starcoder and Codellama models. These results suggest that the Qwen family of models, as well as code-pretrained models in general, are particularly sensitive to the choice of ICL demonstrations. An unsuitable set of demonstrations can reduce performance compared to the base model, underscoring the importance of effective sampling strategies for ICL.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "base",
                    "methods",
                    "mbpp",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AST analysis involves using the prototype-based attribution scores as token confidence scores explained in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5.E5\" title=\"In 5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. We then compute the average confidence over tokens corresponding to each AST node, and report these averages as performance values grouped by manually defined syntax categories. The process follows three steps, illustrated in Fig.1 &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5.F1\" title=\"Figure 1 &#8227; 5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. In Step1, for every generated code snippet, the tokenizer splits the code into tokens <math alttext=\"w_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">w_{i}</annotation></semantics></math> (forming the token set <math alttext=\"\\uptau\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#964;</mi><annotation encoding=\"application/x-tex\">\\uptau</annotation></semantics></math> &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A4.SS1\" title=\"D.1 Interpretable Syntax sets and interactions &#8227; Appendix D AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">D.1</span></a>), and the model assigns a confidence score to each token as described in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5\" title=\"5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. In Step2, the token-level predictions are aligned with the respective Abstract Syntax Tree (AST) terminal nodes. Terminal nodes retain the raw confidences, whereas non-terminal nodes hierarchically store aggregated values. Together, terminal and non-terminal nodes form the subcategory set <math alttext=\"\\upsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\"><semantics><mi>&#965;</mi><annotation encoding=\"application/x-tex\">\\upsilon</annotation></semantics></math> &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A4.SS1\" title=\"D.1 Interpretable Syntax sets and interactions &#8227; Appendix D AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">D.1</span></a>. For instance, the token &#8217;if_&#8217; from the token set aligns with a terminal AST node but is grouped under the non-terminal node &#8217;if_statement&#8217;. Finally, in Step 3, the analysis introduces eight syntax categories to summarize model predictions. These categories aggregate subcategories into broader, human-interpretable groups. The Syntax Categories form a fixed Category Set <math alttext=\"\\Lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#923;</mi><annotation encoding=\"application/x-tex\">\\Lambda</annotation></semantics></math> &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A4.SS1\" title=\"D.1 Interpretable Syntax sets and interactions &#8227; Appendix D AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">D.1</span></a>, providing more intuitive elements for interpretation.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the attribution-based confidence score of each Syntax Category (SC) for the 6 LLMs, we present an AST analysis.\n&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5.F3\" title=\"Figure 3 &#8227; 5.2 Code Syntactic analysis &#8227; 5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates the AST interpretability performance segregated by Syntax Categories (SCs) for\neach model type. The Qwen2.5 Coder and Qwen3 &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5.F3\" title=\"Figure 3 &#8227; 5.2 Code Syntactic analysis &#8227; 5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (a) models exhibit highly consistent confidence across all syntax categories, with nearly identical values. Both models demonstrate their strongest performance in Scope, Data Structures, and Functions, indicating reliability in handling structured data, variable and function scoping, and modular code organization. Moderate confidence is observed for Iteration, Decisions, Operators, and Data Types, while the lowest confidence is consistently assigned to Exception handling, suggesting potential limitations in generating or reasoning about robust error-handling constructs. Overall, these results suggest that both Qwen2.5 Coder and Qwen3 are best suited for structured programming tasks, while being less dependable for control-flow&#8211;intensive or exception-heavy code generation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Llama models &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5.F3\" title=\"Figure 3 &#8227; 5.2 Code Syntactic analysis &#8227; 5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (b) exhibit broadly similar confidence trends across syntax categories, with CodeLlama consistently showing a slight advantage over Llama-3.2. Both models demonstrate their highest reliability in Data Structures, Functions, and Iteration, suggesting strong capabilities in tasks that require structured data handling, modular code organization, and loop-based constructs. Moderate confidence is observed in Scope, Decisions, Operators, and Data Types, indicating stable but less pronounced strengths. In contrast, Exception handling remains the weakest category for both models, highlighting a shared limitation in generating or reasoning about robust error-handling logic. Collectively, these results suggest that while the Llama models are well-suited for structured programming tasks, they are less dependable for exception-heavy scenarios.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Falcon and StarCoder models &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5.F3\" title=\"Figure 3 &#8227; 5.2 Code Syntactic analysis &#8227; 5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (c) display distinct differences in their syntax-grounded confidence. StarCoder consistently achieves higher confidence across nearly all categories compared to Falcon, indicating stronger overall reliability. Both models perform best in Scope, Data Structures, and Functions, suggesting robustness in structured programming tasks and modular code organization. StarCoder further extends this strength to Iteration and Decisions, where it shows clear improvements over Falcon, highlighting its ability to handle control flow more effectively. In contrast, Exception handling remains the weakest category for both models, underscoring a shared limitation in generating robust error-handling constructs. Taken together, these results indicate that while Falcon is moderately capable across most categories, StarCoder offers broader syntactic reliability and is better suited for tasks requiring control flow and structured data handling.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, prototypes were sampled exclusively from the Magicoder dataset. While this choice provided a consistent basis for evaluation, extending the analysis to additional datasets could offer a broader understanding of prototype quality. In fact, our method can naturally be applied as a global metric for ranking datasets with respect to their ability to yield effective prototypes. Another limitation arises from differences in model stability. For example, Llama3.2 &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> exhibited high sensitivity to changes in nearly all hyperparameters, which led to inconsistent results on the <math alttext=\"Pass@k\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">Pass@k</annotation></semantics></math> metric. In contrast, the Qwen2.5 Coder model &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> displayed only marginal sensitivity, with the exception of the <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m2\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> parameter, resulting in more stable and reliable performance. Finally, while our current approach uses sampled prototypes as in-context learning demonstrations, the framework can be extended toward pre-hoc interpretability by design. In particular, prototype steering could be explored as a mechanism for influencing model behavior, offering new avenues for both interpretability and controllability in LLMs.</p>\n\n",
                "matched_terms": [
                    "method",
                    "performance",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The method Piecewise-Linear Manifolds for Deep Metric Learning <cite class=\"ltx_cite ltx_citemacro_citet\">Bhatnagar &amp; Ahuja (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib5\" title=\"\">2024</a>)</cite> aims to train a neural network to learn a semantic feature space where similar items are close together and dissimilar items are far apart, in an unsupervised manner. This method is based on using linearized neighborhoods of points to construct a piecewise linear manifold, which helps estimate a continuous-valued similarity between data points.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Triplet loss</span> <cite class=\"ltx_cite ltx_citemacro_citet\">Schroff et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib42\" title=\"\">2015</a>)</cite> is another metric learning objective that enforces relative similarity by ensuring that an anchor <math alttext=\"x_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">x_{a}</annotation></semantics></math> is closer to a positive sample <math alttext=\"x_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">x_{p}</annotation></semantics></math> (same class) than to a negative sample <math alttext=\"x_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>n</mi></msub><annotation encoding=\"application/x-tex\">x_{n}</annotation></semantics></math> (different class) by at least a margin. Unlike contrastive loss, which only considers pairwise distances, triplet loss leverages relative comparisons, making it more effective in learning discriminative embeddings for tasks such as face recognition and image retrieval, here <math alttext=\"f(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p3.m4\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f(\\cdot)</annotation></semantics></math> is the embedding function, <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p3.m5\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> is the margin, <math alttext=\"x_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p3.m6\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">x_{a}</annotation></semantics></math> is the anchor, <math alttext=\"x_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p3.m7\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">x_{p}</annotation></semantics></math> is a positive sample, and <math alttext=\"x_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p3.m8\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>n</mi></msub><annotation encoding=\"application/x-tex\">x_{n}</annotation></semantics></math> is a negative sample.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on <cite class=\"ltx_cite ltx_citemacro_citet\">Dong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib11\" title=\"\">2024</a>)</cite>, several unsupervised strategies have been proposed to sample effective demonstrations for ICL. A simple yet effective method is to select the nearest neighbors of the input instance based on similarity measures (<cite class=\"ltx_cite ltx_citemacro_citet\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib24\" title=\"\">2022</a>)</cite>, <cite class=\"ltx_cite ltx_citemacro_citet\">Tanwar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib49\" title=\"\">2023</a>)</cite>, <cite class=\"ltx_cite ltx_citemacro_citet\">Qin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib35\" title=\"\">2024</a>)</cite>). Common distance metrics include L2 distance and cosine similarity derived from sentence embeddings. Beyond distance-based approaches, mutual information <cite class=\"ltx_cite ltx_citemacro_citet\">Sorensen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib46\" title=\"\">2022</a>)</cite> and perplexity <cite class=\"ltx_cite ltx_citemacro_citet\">Gonen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib12\" title=\"\">2023</a>)</cite> have also been shown to be useful for selecting prompts without labeled data or model-specific assumptions.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although off-the-shelf retrievers provide convenient solutions for a wide range of NLP tasks, they are often heuristic and sub-optimal due to the absence of task-specific supervision. To overcome this limitation, supervised retriever-based methods have been introduced (<cite class=\"ltx_cite ltx_citemacro_citet\">Rubin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib41\" title=\"\">2022</a>)</cite> <cite class=\"ltx_cite ltx_citemacro_citet\">Ye et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib61\" title=\"\">2023</a>)</cite> <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib53\" title=\"\">2024</a>)</cite> <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib62\" title=\"\">2022</a>)</cite>). For instance, <cite class=\"ltx_cite ltx_citemacro_citet\">Rubin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib41\" title=\"\">2022</a>)</cite> proposed EPR, a two-stage framework for training dense retrievers to identify suitable demonstrations. Building on this, <cite class=\"ltx_cite ltx_citemacro_citet\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib23\" title=\"\">2023c</a>)</cite> developed a unified retriever capable of selecting demonstrations across diverse tasks, while <cite class=\"ltx_cite ltx_citemacro_citet\">Mavromatis et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib30\" title=\"\">2023</a>)</cite> introduced AdaICL, a model-adaptive method that leverages LLMs to predict outcomes for unlabeled data and assign uncertainty scores to guide demonstration selection.</p>\n\n",
                "matched_terms": [
                    "method",
                    "methods",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><cite class=\"ltx_cite ltx_citemacro_citet\">Rodriguez-Cardenas et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib38\" title=\"\">2023</a>)</cite> emphasized the sensitivity of demonstration selection by comparing two different prompt groups in a controlled experiment. One group exhibited a positive causal effect, improving the Average Treatment Effect (ATE) by 5.1% on Chatgpt, while the other group showed a negative causal effect, decreasing ATE by 3.3% relative to the control group. Here, ATE quantifies the average causal influence of a treatment (i.e., the chosen prompt group) on model performance. These findings highlight the critical role of demonstration quality: poorly chosen examples may reduce performance, sometimes performing worse than LLMS that do not use ICL at all. Throughout the paper, we use the terms demonstrations and examples interchangeably in the context of ICL.</p>\n\n",
                "matched_terms": [
                    "different",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MBPP dataset consists of 973 python programming questions. Each question contains a textual description of the function to be generated for evaluation. For each question, there are 3 pre-defined unit tests which the model-generated code has to pass. The samples also contain a reference code. The MBPP testset is a sampled set of 378 questions for evaluation. The MBPP+ dataset is also similar in terms to MBPP dataset except it was created by <cite class=\"ltx_cite ltx_citemacro_citet\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib25\" title=\"\">2023</a>)</cite> and here each question has more than 3 unit tests per question for evaluation.</p>\n\n",
                "matched_terms": [
                    "mbpp",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed the sampled prototypes as ICL demonstrations to generate code completions on the MBPP test set <cite class=\"ltx_cite ltx_citemacro_citet\">Austin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib4\" title=\"\">2021</a>)</cite>, and evaluated the code completions using <math alttext=\"pass@1\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">pass@1</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib9\" title=\"\">2021</a>)</cite> and <math alttext=\"pass@10\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">pass@10</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib9\" title=\"\">2021</a>)</cite> metrics. We used the evalplus <cite class=\"ltx_cite ltx_citemacro_citet\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib25\" title=\"\">2023</a>)</cite> library for code post-processing and calculating the <math alttext=\"pass@1\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">pass@1</annotation></semantics></math> and <math alttext=\"pass@10\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">pass@10</annotation></semantics></math> metrics. The <math alttext=\"pass@k\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">pass@k</annotation></semantics></math> metric assesses the functional correctness of generated code by checking performance against predefined unit tests. Unlike CodeBLEU <cite class=\"ltx_cite ltx_citemacro_citet\">Ren et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib36\" title=\"\">2020</a>)</cite>, which only reflects surface-level similarity, pass@k is more reliable for evaluating functional correctness since it directly verifies whether at least one generated program passes the test cases.</p>\n\n",
                "matched_terms": [
                    "p​a​s​s​​10pass10",
                    "p​a​s​s​​1pass1",
                    "similarity",
                    "mbpp",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage of our framework, dedicated to prototype sampling, the network <math alttext=\"h_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>h</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">h_{\\theta}</annotation></semantics></math> is trained for 200 epochs on the training dataset <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math>. Training utilizes two independent Adam optimizers: one for the network parameters and another for the proxy parameters. Both optimizers are initialized with a learning rate of <span class=\"ltx_text ltx_font_typewriter\">1e-3</span>, combined with a scheduler that decays the learning rate by a factor of <math alttext=\"\\eta_{t}=0.97\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mi>t</mi></msub><mo>=</mo><mn>0.97</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{t}=0.97</annotation></semantics></math>. The dimensionality of the encoded vector <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m4\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math> is determined by the underlying Large Language Model (<math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m5\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math>). A mini-batch size of 128 samples is maintained throughout training.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the initial set of experiments, the hyperparameters for manifold construction and manifold point-to-point loss estimation are configured as follows: <math alttext=\"T=90\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><mn>90</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">T=90\\%</annotation></semantics></math>, <math alttext=\"\\delta=2\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#948;</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\delta=2</annotation></semantics></math>, <math alttext=\"m=3\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">m=3</annotation></semantics></math>, <math alttext=\"N_{\\alpha}=4\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>N</mi><mi>&#945;</mi></msub><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">N_{\\alpha}=4</annotation></semantics></math>, and <math alttext=\"N_{\\beta}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>N</mi><mi>&#946;</mi></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">N_{\\beta}=0.5</annotation></semantics></math>. The momentum constant for updating <math alttext=\"\\theta_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p2.m6\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{m}</annotation></semantics></math> is set to <math alttext=\"\\gamma=0.99\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p2.m7\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mn>0.99</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=0.99</annotation></semantics></math>. For Proxy Anchor loss, we employ <math alttext=\"\\alpha=32\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p2.m8\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>32</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=32</annotation></semantics></math> and <math alttext=\"\\epsilon=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p2.m9\" intent=\":literal\"><semantics><mrow><mi>&#1013;</mi><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">\\epsilon=0.1</annotation></semantics></math>. These settings serve as the baseline configuration; subsequently, an ablation study is conducted on the above parameters for LLMs that exhibited comparatively lower performance than competing methods.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All experiments were conducted on an NVIDIA RTX A6000 GPU. In the first stage of our method, we train a lightweight neural network <math alttext=\"h_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>h</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">h_{\\theta}</annotation></semantics></math> to sample prototypes, which requires approximately 640 MB of GPU memory and about 7 hours of training time without parallelization. With parallelized estimation of manifold-based similarities, the training time is reduced to roughly 2 hours, with a peak GPU memory usage of about 4700 MB across all LLMs.</p>\n\n",
                "matched_terms": [
                    "method",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed method demonstrates resource efficiency by requiring fewer demonstrations while achieving performance on par with fine-tuning approaches. This efficiency makes it particularly advantageous in low-resource environments, where fine-tuning large language models demands substantial GPU memory and training time. Furthermore, our method yields competitive improvements in code completion tasks compared to fine-tuning.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Similarity-based sampling:</span> The test query was encoded following the same procedure as in the Magicoder dataset. Demonstrations were then selected from each programming language class based on the closest Euclidean distance to the test query. This method would be sampling 9 distinct prototypes from each class.</p>\n\n",
                "matched_terms": [
                    "method",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Base model:</span> For the LLMs being tested no ICL demonstrations were provided, only the test query was provided.</p>\n\n",
                "matched_terms": [
                    "base",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MBPP Few shots:</span> The authors of the MBPP test set used and experimented with the samples at indexes 2, 3, 4 as ICL examples. In our experiments, we also use the same set of samples for comparison.</p>\n\n",
                "matched_terms": [
                    "mbpp",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The table presents the token lengths of sampled prototypes along with the 99th percentile, 95th percentile, and average token lengths across the MBPP dataset for combined query and solution inputs. Since each input consists of the sampled prototypes used as demonstrations together with the MBPP test queries, we estimate the overall input token lengths to assess whether all prototypes can be accommodated. These token length statistics are reported separately for each LLM.</p>\n\n",
                "matched_terms": [
                    "mbpp",
                    "dataset",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the table, it can be observed that the sampled prototype token lengths exceed the context window of the Falcon3-1B model. Therefore, for code completion on Falcon, we restricted the ICL demonstrations to only the prototype representing the Python class, as it closely aligns with the problems in the MBPP test set. The same procedure was applied across all sampling strategies for the Falcon3 model.</p>\n\n",
                "matched_terms": [
                    "mbpp",
                    "model",
                    "across",
                    "falcon31b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The table also shows that the Codellama model, being code-specific, produces a higher number of tokens compared to the Llama3.2 model. This highlights the optimized tokenization techniques of the Llama3.2 series, as Codellama is derived from the Llama2 family of models. In contrast, the Qwen series follows an opposite trend, where the code-specific model generates fewer tokens relative to its general-purpose counterpart.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All reported scores in this paper have been independently recomputed across every model and sampling method. The results for the base model (without ICL) may differ from those documented in the official technical reports, which can be attributed to several factors. Based on our experimental findings, we outline the potential reasons that may have influenced performance aside from the ICL demonstrations.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across",
                    "base",
                    "method",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For generating code completions we employed the Hugging Face text generation pipeline with decoding parameters set to <span class=\"ltx_text ltx_font_typewriter\">temperature = 0.6</span> and <span class=\"ltx_text ltx_font_typewriter\">top-p = 0.9</span>. Our experiments revealed that even minor adjustments to these parameters, with only two variations, led to improved performance across all models and sampling methods. Notably, most technical reports for benchmark evaluations do not specify the decoding strategies employed, which contributes to variability in reported results. This observation underscores the importance of performing hyperparameter optimization during the decoding stage of generation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "methods",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the MBPP and MBPP+ test sets, each query is paired with pre-defined unit tests, requiring the model to produce code completions that precisely match the expected function names. While one way to ensure success would be to include the reference solution as a demonstration for each query, such an approach risks data leakage, as the model would be exposed to the ground-truth answers rather than generating them independently. To mitigate this issue, we deliberately excluded reference code solutions from the input queries.</p>\n\n",
                "matched_terms": [
                    "mbpp",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As outlined in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A2.SS2\" title=\"B.2 Training Parameters &#8227; Appendix B Methodology &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>, the baseline configurations were employed for the initial experiments. To further investigate performance limitations, we conducted an ablation study focusing on LLMs that demonstrated comparatively weaker results. Specifically, under the baseline settings, the Llama3.2 and Qwen3 models underperformed relative to other methods. Consequently, we performed an extensive hyperparameter ablation on these models to better understand their sensitivities and performance dynamics.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The parameter <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> denotes the dimension of the linear submanifold <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math>, which locally approximates the data manifold around a point <math alttext=\"h_{\\theta}(z)\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>h</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">h_{\\theta}(z)</annotation></semantics></math>.\nTo examine its effect, we vary <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> in the range <math alttext=\"[2,8]\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>2</mn><mo>,</mo><mn>8</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[2,8]</annotation></semantics></math> with a step size of <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m6\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(a), performance consistently decreases in both models as <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> increases.\nThis trend arises because <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math> is intended to approximate the immediate neighborhood of a point, which is inherently low-dimensional.\nLarger values of <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m9\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> may lead to overfitting, since only a limited number of nearby samples are available within a batch to reliably estimate <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m10\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math>, thereby degrading performance.\nFurthermore, we observe that the computational overhead for prototype sampling increases with larger <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m11\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>, underscoring the trade-off between accuracy and efficiency.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The parameter <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> denotes the momentum constant used to update the proxy vector <math alttext=\"\\theta_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{m}</annotation></semantics></math> during prototype sampling.\nFollowing <cite class=\"ltx_cite ltx_citemacro_citet\">He et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib14\" title=\"\">2020</a>)</cite>, higher values of <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m3\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> are expected to yield improved performance, as the proxy updates become smoother and more stable.\nConsistent with this observation, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(b) shows that in both models, performance improves as <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m4\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> increases, highlighting the importance of stable momentum updates for effective representation learning.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The parameters <math alttext=\"N_{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#945;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\alpha}</annotation></semantics></math> and <math alttext=\"N_{\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#946;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\beta}</annotation></semantics></math> control the decay of similarity based on the orthogonal and projected distances, respectively, of a point from the linear submanifold in the neighborhood of another point.\nWe vary <math alttext=\"N_{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m3\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#945;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\alpha}</annotation></semantics></math> in the range <math alttext=\"[1,6]\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mn>6</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[1,6]</annotation></semantics></math> with a step size of <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m5\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, and <math alttext=\"N_{\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m6\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#946;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\beta}</annotation></semantics></math> in the range <math alttext=\"[0.5,3]\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0.5</mn><mo>,</mo><mn>3</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0.5,3]</annotation></semantics></math> with a step size of <math alttext=\"0.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m8\" intent=\":literal\"><semantics><mn>0.5</mn><annotation encoding=\"application/x-tex\">0.5</annotation></semantics></math>.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(c), increasing <math alttext=\"N_{\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m9\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#946;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\beta}</annotation></semantics></math> leads to a slight performance gain in the Qwen2.5-Coder model, while the Llama3.2 model exhibits larger fluctuations but follows an overall upward trend.\nSimilarly, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(d) shows that performance improves marginally with larger <math alttext=\"N_{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m10\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#945;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\alpha}</annotation></semantics></math> in the Qwen2.5-Coder model, whereas the Llama3.2 model demonstrates a clearer and more consistent increase.\nThis effect can be explained by the relationship between <math alttext=\"N_{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m11\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#945;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\alpha}</annotation></semantics></math> and <math alttext=\"N_{\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m12\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#946;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\beta}</annotation></semantics></math>: as <math alttext=\"N_{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m13\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#945;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\alpha}</annotation></semantics></math> approaches <math alttext=\"N_{\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m14\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#946;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\beta}</annotation></semantics></math>, a point <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m15\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> at distance <math alttext=\"\\varepsilon\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m16\" intent=\":literal\"><semantics><mi>&#949;</mi><annotation encoding=\"application/x-tex\">\\varepsilon</annotation></semantics></math> within the linear neighborhood of a point <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m17\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> (and thus sharing many features with <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m18\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> and its neighbors) may be treated as equally dissimilar to <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m19\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> as another point <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m20\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> located at an orthogonal distance <math alttext=\"\\varepsilon\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m21\" intent=\":literal\"><semantics><mi>&#949;</mi><annotation encoding=\"application/x-tex\">\\varepsilon</annotation></semantics></math> from the neighborhood of <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m22\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "similarity",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The reconstruction threshold <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS4.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> determines the quality of points admitted into the linear submanifold <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math>.\nWe vary <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS4.p1.m3\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> in the range <math alttext=\"[0.7,0.95]\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS4.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0.7</mn><mo>,</mo><mn>0.95</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0.7,0.95]</annotation></semantics></math> with a step size of <math alttext=\"0.05\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS4.p1.m5\" intent=\":literal\"><semantics><mn>0.05</mn><annotation encoding=\"application/x-tex\">0.05</annotation></semantics></math>.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(e), both models exhibit a clear upward trend in performance as <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS4.p1.m6\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> increases, underscoring the importance of ensuring that only high-quality points are incorporated into <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS4.p1.m7\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math>.\nWhile the Llama3.2 model follows this overall increasing trend, it displays noticeable fluctuations compared to the more stable improvement observed in the Qwen2.5-Coder model.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The scaling factor <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS5.p1.m1\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> regulates the maximum separation between dissimilar points. We vary <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS5.p1.m2\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> in the range <math alttext=\"[0.8,3.2]\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS5.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0.8</mn><mo>,</mo><mn>3.2</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0.8,3.2]</annotation></semantics></math> with a step size of <math alttext=\"0.4\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS5.p1.m4\" intent=\":literal\"><semantics><mn>0.4</mn><annotation encoding=\"application/x-tex\">0.4</annotation></semantics></math>. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(f), the performance remains relatively stable across this range for both models, highlighting the robustness of our method.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "method",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The scaling factor <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS6.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> controls the sharpness of the exponential term in the Proxy Anchor loss. We vary its value over <math alttext=\"{5,10,15,20,25,30,32}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS6.p1.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo>,</mo><mn>10</mn><mo>,</mo><mn>15</mn><mo>,</mo><mn>20</mn><mo>,</mo><mn>25</mn><mo>,</mo><mn>30</mn><mo>,</mo><mn>32</mn></mrow><annotation encoding=\"application/x-tex\">{5,10,15,20,25,30,32}</annotation></semantics></math>. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(g), both models exhibit an overall increasing trend in performance with larger <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS6.p1.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>. However, the Qwen2.5-coder model displays higher fluctuations compared to the more stable Llama3.2 model.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The margin parameter <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS7.p1.m1\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math> enforces that positive embeddings are pulled within this distance from their corresponding class proxies. We vary its value across <math alttext=\"{0.001,0.005,0.05,0.1,0.2}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS7.p1.m2\" intent=\":literal\"><semantics><mrow><mn>0.001</mn><mo>,</mo><mn>0.005</mn><mo>,</mo><mn>0.05</mn><mo>,</mo><mn>0.1</mn><mo>,</mo><mn>0.2</mn></mrow><annotation encoding=\"application/x-tex\">{0.001,0.005,0.05,0.1,0.2}</annotation></semantics></math>. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(h), the Qwen2.5-coder model demonstrates stable performance across the range of <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS7.p1.m3\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math>, whereas the Llama3.2 model exhibits a decreasing trend with noticeable fluctuations. This indicates that larger values of <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS7.p1.m4\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math> impose overly strict constraints on the separation between positive and negative proxies, thereby hindering the embeddings from effectively satisfying the margin requirement.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A <span class=\"ltx_text ltx_font_bold\">clustering function <math alttext=\"\\zeta\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m1\" intent=\":literal\"><semantics><mi>&#950;</mi><annotation encoding=\"application/x-tex\">\\zeta</annotation></semantics></math></span> computes the confidence performance of <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m2\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> and <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> nodes (subcategories) within an AST by hierarchically aggregating Token-Level Confidences into a category <math alttext=\"c\\in\\Lambda\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m4\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>&#8712;</mo><mi mathvariant=\"normal\">&#923;</mi></mrow><annotation encoding=\"application/x-tex\">c\\in\\Lambda</annotation></semantics></math>. After tokens are aligned to their respective nodes using <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m5\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math>, AST analysis groups them into either their corresponding category or non-terminal <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m6\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> node, following the AST structure. In some cases, terminal <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m7\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> nodes may be directly aggregated into a category without involving intermediate non-terminal <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m8\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> nodes. The function <math alttext=\"\\zeta\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m9\" intent=\":literal\"><semantics><mi>&#950;</mi><annotation encoding=\"application/x-tex\">\\zeta</annotation></semantics></math> can be configured to use different aggregation strategies, such as average, median, or maximum. In our experiments, we define the clustering function as <math alttext=\"\\zeta:\\upsilon\\rightarrow\\text{avg}(w_{1:i})\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m10\" intent=\":literal\"><semantics><mrow><mi>&#950;</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi>&#965;</mi><mo stretchy=\"false\">&#8594;</mo><mrow><mtext>avg</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\zeta:\\upsilon\\rightarrow\\text{avg}(w_{1:i})</annotation></semantics></math> for a subset of tokens <math alttext=\"w_{\\leq i}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m11\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi/><mo>&#8804;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{\\leq i}</annotation></semantics></math>. The 8 defined syntax categories are:</p>\n\n",
                "matched_terms": [
                    "performance",
                    "different"
                ]
            }
        ]
    },
    "A2.T3": {
        "source_file": "Protocode: Prototype-Driven Interpretability for Code Generation in LLMs",
        "caption": "Table 3: Model Architecture",
        "body": "Layer\nLayer Parameters\n\n\n\n\nLinear\n(latent size zz, Prototype size )\n\n\nInstanceNorm1d\nPrototype size zz\n\n\n\nReLU\n-",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Layer</span></th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Layer Parameters</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Linear</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\">(latent size <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T3.m1\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>, Prototype size )</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">InstanceNorm1d</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">Prototype size <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T3.m2\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">ReLU</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_bb\">-</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "model",
            "size",
            "architecture",
            "latent",
            "linear",
            "instancenorm1d",
            "layer",
            "relu",
            "parameters",
            "prototype"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As mentioned in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S3\" title=\"3 Methodology &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our method consists of two stages. In the first stage of our method, we initialize a simple neural network <math alttext=\"h_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>h</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">h_{\\theta}</annotation></semantics></math> Tab &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A2.T3\" title=\"Table 3 &#8227; B.1 Evaluation Dataset and Metric &#8227; Appendix B Methodology &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and train it on Dataset <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> to jointly optimize manifold learning &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S3.E3\" title=\"In Manifold Point-to-Point Loss: &#8227; 3.4 Training objectives &#8227; 3 Methodology &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and metric learning objectives &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S3.E1\" title=\"In Proxy Anchor Loss: &#8227; 3.4 Training objectives &#8227; 3 Methodology &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The neural network <math alttext=\"h_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>h</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">h_{\\theta}</annotation></semantics></math> learns to map the high-dimensional encoded representations into lower dimensions. Before the training process, we initialize the proxies <math alttext=\"\\theta_{q}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{q}</annotation></semantics></math> and <math alttext=\"\\theta_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{m}</annotation></semantics></math>. Here both the proxies are unique for each class and initialized randomly with <math alttext=\"\\theta_{q}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{q}</annotation></semantics></math> = <math alttext=\"\\theta_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m7\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{m}</annotation></semantics></math>.\nThe proxy vector <math alttext=\"\\theta_{q}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m8\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{q}</annotation></semantics></math> is updated via back-propagation, and the proxy vector <math alttext=\"\\theta_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m9\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{m}</annotation></semantics></math> is updated via the Momentum update <cite class=\"ltx_cite ltx_citemacro_citet\">He et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib14\" title=\"\">2020</a>)</cite> where <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m10\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> is the momentum constant,\n<math alttext=\"[\\theta_{k}\\leftarrow\\gamma\\theta_{k}+(1-\\gamma)\\theta_{q}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m11\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mi>&#952;</mi><mi>k</mi></msub><mo stretchy=\"false\">&#8592;</mo><mrow><mrow><mi>&#947;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#952;</mi><mi>k</mi></msub></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>&#8722;</mo><mi>&#947;</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#952;</mi><mi>q</mi></msub></mrow></mrow></mrow><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[\\theta_{k}\\leftarrow\\gamma\\theta_{k}+(1-\\gamma)\\theta_{q}]</annotation></semantics></math></p>\n\n",
            "<p class=\"ltx_p\">In equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S3.E3\" title=\"In Manifold Point-to-Point Loss: &#8227; 3.4 Training objectives &#8227; 3 Methodology &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <math alttext=\"h_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p5.m1\" intent=\":literal\"><semantics><msub><mi>h</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">h_{\\theta}</annotation></semantics></math> is a simple neural network with a structure specified in Table &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A2.T3\" title=\"Table 3 &#8227; B.1 Evaluation Dataset and Metric &#8227; Appendix B Methodology &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p5.m2\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> is a scaling factor that determines the maximum separation between dissimilar points. The loss encourages Euclidean distances in the embedding space to match manifold-based dissimilarities <math alttext=\"1-s(z_{i},z_{j})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p5.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>&#8722;</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><msub><mi>z</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">1-s(z_{i},z_{j})</annotation></semantics></math>, ensuring that the learned metric space respects the underlying manifold structure.\n<math alttext=\"o(z_{i},z_{j})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p5.m4\" intent=\":literal\"><semantics><mrow><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><msub><mi>z</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">o(z_{i},z_{j})</annotation></semantics></math> is the orthogonal distance from point <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p5.m5\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math> to the manifold of point <math alttext=\"z_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p5.m6\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">z_{j}</annotation></semantics></math>, and <math alttext=\"p(z_{i},z_{j})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p5.m7\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><msub><mi>z</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p(z_{i},z_{j})</annotation></semantics></math> is the projected distance between point <math alttext=\"z_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p5.m8\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">z_{j}</annotation></semantics></math> and the projection of <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p5.m9\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math> on the manifold. The parameters <math alttext=\"N_{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p5.m10\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#945;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\alpha}</annotation></semantics></math> and <math alttext=\"N_{\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p5.m11\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#946;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\beta}</annotation></semantics></math> control how rapidly similarity decays with distance, with <math alttext=\"N_{\\alpha}&gt;N_{\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p5.m12\" intent=\":literal\"><semantics><mrow><msub><mi>N</mi><mi>&#945;</mi></msub><mo>&gt;</mo><msub><mi>N</mi><mi>&#946;</mi></msub></mrow><annotation encoding=\"application/x-tex\">N_{\\alpha}&gt;N_{\\beta}</annotation></semantics></math> ensuring that similarity decreases more rapidly for points lying off the manifold than for points on the same manifold.</p>\n\n",
            "<p class=\"ltx_p\">The below is the architecture of <math alttext=\"h_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p4.m1\" intent=\":literal\"><semantics><msub><mi>h</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">h_{\\theta}</annotation></semantics></math> neural network we used. It is a Single-layer network &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A2.T3\" title=\"Table 3 &#8227; B.1 Evaluation Dataset and Metric &#8227; Appendix B Methodology &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> with intermediate normalizations. For most of the LLMs the prototype size is set to 50. All of the layers of <math alttext=\"h_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p4.m2\" intent=\":literal\"><semantics><msub><mi>h</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">h_{\\theta}</annotation></semantics></math> are used during training and updated via backpropagation.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Intrinsic interpretability, in contrast, focuses on designing model architectures so that their behavior is inherently explainable. One example is concept bottleneck models <cite class=\"ltx_cite ltx_citemacro_citet\">Koh et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib20\" title=\"\">2020</a>)</cite>, which were extended to large language models (LLMs) by <cite class=\"ltx_cite ltx_citemacro_citet\">Sun et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib47\" title=\"\">2025</a>)</cite> for sentence classification task. Their approach generates concepts for each class, making the classification process directly interpretable. However, this approach faces limitations in generating suitable concepts for diverse tasks and does not scale well to text generation. Another related method, Proto-lm <cite class=\"ltx_cite ltx_citemacro_citet\">Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib59\" title=\"\">2023</a>)</cite> , extends prototype networks to text classification. Instead of generating concepts like concept bottlenecks, it learns trainable prototypes and maps them to the nearest training samples for interpretability.</p>\n\n",
                "matched_terms": [
                    "model",
                    "prototype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most of the above methods are designed for generic tasks, with a limited focus on code-specific applications. The method ASTrust was developed specifically for interpretability in code generation. It builds Abstract Syntax Trees (ASTs) to align with program structure and assigns confidence scores to non-terminal nodes by aggregating probabilities from their terminal nodes. These scores are derived from token-level probabilities output by the model. <cite class=\"ltx_cite ltx_citemacro_citet\">Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib28\" title=\"\">2024</a>)</cite> demonstrates that LLMs already possess strong syntactic awareness, rivaling AST-based static code analysis. However, the method ASTrust has key limitations: its token sampling method is not well justified. Greedy sampling ignores the advantages of stochastic approaches, while stochastic sampling requires storing probabilities for all vocabulary tokens at every step an impractical, memory-intensive process. In contrast, our method avoids this heavy storage by relying on attribution-based prototype influence, which captures the effect of sampled demonstrations without requiring full vocabulary distributions. As a result, our approach preserves the benefits of stochastic sampling <cite class=\"ltx_cite ltx_citemacro_citet\">Shi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib43\" title=\"\">2024</a>)</cite> while remaining significantly more scalable and practical for code generation interpretability.</p>\n\n",
                "matched_terms": [
                    "model",
                    "prototype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training, for every mini-batch <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> we build linear piecewise manifolds as outlined in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S3.SS3\" title=\"3.3 Manifold Construction &#8227; 3 Methodology &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>. For every point in <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math>, we then compute the manifold-based similarity following the procedure in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S3.SS4.SSS0.Px2\" title=\"Manifold Point-to-Point Loss: &#8227; 3.4 Training objectives &#8227; 3 Methodology &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a>. This similarity measure is used to compute the manifold point-to-point loss <math alttext=\"\\mathcal{L}_{\\text{manifold}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>manifold</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{manifold}}</annotation></semantics></math>. At the same time, we compute the Proxy Anchor loss <math alttext=\"\\mathcal{L}_{\\text{PA}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>PA</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{PA}}</annotation></semantics></math> using randomly initialized class proxies <math alttext=\"\\theta_{q}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m5\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{q}</annotation></semantics></math> and latent representations <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m6\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math> in batch <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m7\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math>. The final loss is computed as, <math alttext=\"\\mathcal{L}_{\\text{total}}=\\mathcal{L}_{\\text{PA}}+\\mathcal{L}_{\\text{manifold}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m8\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>total</mtext></msub><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>PA</mtext></msub><mo>+</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>manifold</mtext></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{total}}=\\mathcal{L}_{\\text{PA}}+\\mathcal{L}_{\\text{manifold}}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "linear"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the manifold loss preserves local geometric structure, the Proxy-Anchor loss promotes intra-class compactness and inter-class separation, thereby facilitating the discriminative learning of prototypes.\nAcross epochs, the network parameters are updated via backpropagation.\nAfter training, the momentum-updated proxies <math alttext=\"\\theta_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{m}</annotation></semantics></math> are used to select the nearest training instance as the prototype for each class, yielding a single prototype per class for the subsequent stage.</p>\n\n",
                "matched_terms": [
                    "parameters",
                    "prototype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After the completion of the training process, we proceed to the second stage, where we generate the code completions using the prototypes. After that, we utilize the encoded latent representations of the prototypes to calculate the confidence score per token for AST analysis. For each code completion, we compute a prototype attribution-based score to quantify the influence of the prototypes on the generated code. Specifically, influence is measured by the attribution between the sampled demonstrations and the code completions. Finally, we perform an AST analysis to analyze how the prototypes impact the syntactic structure of the generated code.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "prototype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the Manifold hypothesis, we can assume that the encoded latent representations <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>, which are inherently complex and non-linear, can be locally approximated into smaller chunks of linear regions. Our approach leverages this structural assumption to automatically identify representative prototypes that capture the essential characteristics of each action class.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "linear"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Qwen2.5-coder model, despite having fewer parameters than Codellama, achieves comparable performance on both the <math alttext=\"Pass@1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">Pass@1</annotation></semantics></math> and <math alttext=\"Pass@10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m2\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">Pass@10</annotation></semantics></math> metrics across the MBPP and MBPP+ test sets. Among all comparisons, the similarity-based sampling method surpasses our approach only for the Llama3.2 model; in every other case, our method consistently outperforms alternative strategies across all models. As noted by <cite class=\"ltx_cite ltx_citemacro_citet\">Rodriguez-Cardenas et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib38\" title=\"\">2023</a>)</cite>, within the ICL setting, the quality of selected demonstrations can also negatively affect model performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "parameters"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section 2 &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S2\" title=\"2 Related work &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, this method suffers from high memory overhead when combined with stochastic sampling strategies. To mitigate this limitation, we instead leverage attribution-based scores between the sampled prototypes and the generated code samples, and use these scores as token-level confidence in AST analysis. Concretely, for a model-generated code snippet <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>, we extract the tokens <math alttext=\"w_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">w_{i}</annotation></semantics></math> along with their latent representations <math alttext=\"z_{w_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m3\" intent=\":literal\"><semantics><msub><mi>z</mi><msub><mi>w</mi><mi>i</mi></msub></msub><annotation encoding=\"application/x-tex\">z_{w_{i}}</annotation></semantics></math>. Let the latent representation of a sampled prototype <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> be <math alttext=\"z_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m5\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">z_{p}</annotation></semantics></math>. We compute the mean prototype vector <math alttext=\"z_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m6\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">z_{a}</annotation></semantics></math> as <math alttext=\"z_{a}=\\sum_{i\\in P}z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m7\" intent=\":literal\"><semantics><mrow><msub><mi>z</mi><mi>a</mi></msub><mo rspace=\"0.111em\">=</mo><mrow><msub><mo>&#8721;</mo><mrow><mi>i</mi><mo>&#8712;</mo><mi>P</mi></mrow></msub><msub><mi>z</mi><mi>i</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">z_{a}=\\sum_{i\\in P}z_{i}</annotation></semantics></math>. Next, we compute the dot product between <math alttext=\"z_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m8\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">z_{a}</annotation></semantics></math> and each <math alttext=\"z_{w_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m9\" intent=\":literal\"><semantics><msub><mi>z</mi><msub><mi>w</mi><mi>i</mi></msub></msub><annotation encoding=\"application/x-tex\">z_{w_{i}}</annotation></semantics></math>, and compute its gradient with respect to <math alttext=\"z_{w_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m10\" intent=\":literal\"><semantics><msub><mi>z</mi><msub><mi>w</mi><mi>i</mi></msub></msub><annotation encoding=\"application/x-tex\">z_{w_{i}}</annotation></semantics></math>. The normalized gradients <math alttext=\"\\nabla_{z_{w_{i}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m11\" intent=\":literal\"><semantics><msub><mo>&#8711;</mo><msub><mi>z</mi><msub><mi>w</mi><mi>i</mi></msub></msub></msub><annotation encoding=\"application/x-tex\">\\nabla_{z_{w_{i}}}</annotation></semantics></math>(&#160;equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5.E5\" title=\"In 5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) are then used as confidence scores per token in the AST analysis.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "prototype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, prototypes were sampled exclusively from the Magicoder dataset. While this choice provided a consistent basis for evaluation, extending the analysis to additional datasets could offer a broader understanding of prototype quality. In fact, our method can naturally be applied as a global metric for ranking datasets with respect to their ability to yield effective prototypes. Another limitation arises from differences in model stability. For example, Llama3.2 &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> exhibited high sensitivity to changes in nearly all hyperparameters, which led to inconsistent results on the <math alttext=\"Pass@k\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">Pass@k</annotation></semantics></math> metric. In contrast, the Qwen2.5 Coder model &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> displayed only marginal sensitivity, with the exception of the <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m2\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> parameter, resulting in more stable and reliable performance. Finally, while our current approach uses sampled prototypes as in-context learning demonstrations, the framework can be extended toward pre-hoc interpretability by design. In particular, prototype steering could be explored as a mechanism for influencing model behavior, offering new avenues for both interpretability and controllability in LLMs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "prototype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage of our framework, dedicated to prototype sampling, the network <math alttext=\"h_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>h</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">h_{\\theta}</annotation></semantics></math> is trained for 200 epochs on the training dataset <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math>. Training utilizes two independent Adam optimizers: one for the network parameters and another for the proxy parameters. Both optimizers are initialized with a learning rate of <span class=\"ltx_text ltx_font_typewriter\">1e-3</span>, combined with a scheduler that decays the learning rate by a factor of <math alttext=\"\\eta_{t}=0.97\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mi>t</mi></msub><mo>=</mo><mn>0.97</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{t}=0.97</annotation></semantics></math>. The dimensionality of the encoded vector <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m4\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math> is determined by the underlying Large Language Model (<math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m5\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math>). A mini-batch size of 128 samples is maintained throughout training.</p>\n\n",
                "matched_terms": [
                    "prototype",
                    "model",
                    "parameters",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the table, it can be observed that the sampled prototype token lengths exceed the context window of the Falcon3-1B model. Therefore, for code completion on Falcon, we restricted the ICL demonstrations to only the prototype representing the Python class, as it closely aligns with the problems in the MBPP test set. The same procedure was applied across all sampling strategies for the Falcon3 model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "prototype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The parameter <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> denotes the dimension of the linear submanifold <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math>, which locally approximates the data manifold around a point <math alttext=\"h_{\\theta}(z)\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>h</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">h_{\\theta}(z)</annotation></semantics></math>.\nTo examine its effect, we vary <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> in the range <math alttext=\"[2,8]\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>2</mn><mo>,</mo><mn>8</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[2,8]</annotation></semantics></math> with a step size of <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m6\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(a), performance consistently decreases in both models as <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> increases.\nThis trend arises because <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math> is intended to approximate the immediate neighborhood of a point, which is inherently low-dimensional.\nLarger values of <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m9\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> may lead to overfitting, since only a limited number of nearby samples are available within a batch to reliably estimate <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m10\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math>, thereby degrading performance.\nFurthermore, we observe that the computational overhead for prototype sampling increases with larger <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m11\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>, underscoring the trade-off between accuracy and efficiency.</p>\n\n",
                "matched_terms": [
                    "prototype",
                    "linear",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The parameters <math alttext=\"N_{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#945;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\alpha}</annotation></semantics></math> and <math alttext=\"N_{\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#946;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\beta}</annotation></semantics></math> control the decay of similarity based on the orthogonal and projected distances, respectively, of a point from the linear submanifold in the neighborhood of another point.\nWe vary <math alttext=\"N_{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m3\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#945;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\alpha}</annotation></semantics></math> in the range <math alttext=\"[1,6]\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mn>6</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[1,6]</annotation></semantics></math> with a step size of <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m5\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, and <math alttext=\"N_{\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m6\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#946;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\beta}</annotation></semantics></math> in the range <math alttext=\"[0.5,3]\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0.5</mn><mo>,</mo><mn>3</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0.5,3]</annotation></semantics></math> with a step size of <math alttext=\"0.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m8\" intent=\":literal\"><semantics><mn>0.5</mn><annotation encoding=\"application/x-tex\">0.5</annotation></semantics></math>.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(c), increasing <math alttext=\"N_{\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m9\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#946;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\beta}</annotation></semantics></math> leads to a slight performance gain in the Qwen2.5-Coder model, while the Llama3.2 model exhibits larger fluctuations but follows an overall upward trend.\nSimilarly, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(d) shows that performance improves marginally with larger <math alttext=\"N_{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m10\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#945;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\alpha}</annotation></semantics></math> in the Qwen2.5-Coder model, whereas the Llama3.2 model demonstrates a clearer and more consistent increase.\nThis effect can be explained by the relationship between <math alttext=\"N_{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m11\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#945;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\alpha}</annotation></semantics></math> and <math alttext=\"N_{\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m12\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#946;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\beta}</annotation></semantics></math>: as <math alttext=\"N_{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m13\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#945;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\alpha}</annotation></semantics></math> approaches <math alttext=\"N_{\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m14\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>&#946;</mi></msub><annotation encoding=\"application/x-tex\">N_{\\beta}</annotation></semantics></math>, a point <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m15\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> at distance <math alttext=\"\\varepsilon\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m16\" intent=\":literal\"><semantics><mi>&#949;</mi><annotation encoding=\"application/x-tex\">\\varepsilon</annotation></semantics></math> within the linear neighborhood of a point <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m17\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> (and thus sharing many features with <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m18\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> and its neighbors) may be treated as equally dissimilar to <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m19\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> as another point <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m20\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> located at an orthogonal distance <math alttext=\"\\varepsilon\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m21\" intent=\":literal\"><semantics><mi>&#949;</mi><annotation encoding=\"application/x-tex\">\\varepsilon</annotation></semantics></math> from the neighborhood of <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m22\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "linear",
                    "size",
                    "parameters"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The reconstruction threshold <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS4.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> determines the quality of points admitted into the linear submanifold <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math>.\nWe vary <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS4.p1.m3\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> in the range <math alttext=\"[0.7,0.95]\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS4.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0.7</mn><mo>,</mo><mn>0.95</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0.7,0.95]</annotation></semantics></math> with a step size of <math alttext=\"0.05\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS4.p1.m5\" intent=\":literal\"><semantics><mn>0.05</mn><annotation encoding=\"application/x-tex\">0.05</annotation></semantics></math>.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(e), both models exhibit a clear upward trend in performance as <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS4.p1.m6\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> increases, underscoring the importance of ensuring that only high-quality points are incorporated into <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS4.p1.m7\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math>.\nWhile the Llama3.2 model follows this overall increasing trend, it displays noticeable fluctuations compared to the more stable improvement observed in the Qwen2.5-Coder model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "linear",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the Llama3.2 model exhibits high sensitivity to parameter variations, displaying substantial fluctuations in performance. This trend aligns with the results reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S4.T1\" title=\"Table 1 &#8227; 4 Results &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S4.T2\" title=\"Table 2 &#8227; 4 Results &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where the similarity-based sampling method achieves the highest score for Llama3.2, further highlighting its instability under different configurations. In contrast, the Qwen2.5-coder model demonstrates relatively stable behavior, showing consistently increasing trends across most parameters, with the notable exception of the scaling factor <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS8.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "parameters"
                ]
            }
        ]
    },
    "A2.T4": {
        "source_file": "Protocode: Prototype-Driven Interpretability for Code Generation in LLMs",
        "caption": "Table 4: Comparison of token lengths vs context length for respective LLM (all lengths are reported in terms of no.of tokens)",
        "body": "Model\nPrototype Length\n99%\n95%\nAvg\nContext Length\n\n\n\n\nStarcoder-1B-base\n6000\n253.8\n186\n80.74\n8192\n\n\nCodellama-7B\n5734\n296\n217\n94\n16000\n\n\nFalcon3-1B-base\n5877\n320\n225\n94\n4000\n\n\nLlama3.2-1B\n4288\n228\n163\n74\n128000\n\n\nQwen2.5coder-0.5B\n3054\n228\n166\n73\n32000\n\n\nQwen3-0.6B\n5069\n229\n166\n73\n32000",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Prototype Length</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">99%</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">95%</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Avg</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Context Length</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Starcoder-1B-base</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">6000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">253.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">186</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">80.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">8192</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Codellama-7B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">5734</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">296</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">217</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">16000</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Falcon3-1B-base</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">5877</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">320</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">225</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4000</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Llama3.2-1B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4288</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">228</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">163</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">128000</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Qwen2.5coder-0.5B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3054</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">228</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">166</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">32000</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">Qwen3-0.6B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">5069</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">229</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">166</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">32000</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "llm",
            "falcon31bbase",
            "avg",
            "qwen25coder05b",
            "length",
            "all",
            "qwen306b",
            "noof",
            "prototype",
            "model",
            "lengths",
            "context",
            "token",
            "codellama7b",
            "starcoder1bbase",
            "terms",
            "reported",
            "respective",
            "llama321b",
            "tokens",
            "comparison"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Since the introduction of Large Language Models (LLMs), they have been widely adopted for various tasks such as text summarization, question answering, speech-to-text translation, and more. In recent times, the use of LLMs for code generation has gained significant attention, with tools such as Cursor and Windsurf demonstrating the ability to analyze massive code repositories and recommend relevant changes. Big tech companies have also acknowledged the growing reliance on LLMs for code generation within their codebases. Although these advances significantly improve developer productivity, increasing reliance on automated code generation can proportionally increase the risk of suboptimal solutions and insecure code. Our work focuses on automatically sampling In-Context Learning (ICL) demonstrations which can improve model performance and enhance the interpretability of the generated code. Using AST-based analysis on outputs from the MBPP test set, we identify regions of code most influenced by the chosen demonstrations. In our experiments, we show that high-quality ICL demonstrations not only make outputs easier to interpret but also yield a positive performance improvement on the pass@10 metric. Conversely, poorly chosen ICL demonstrations affected the LLM performance on the pass@10 metric negatively compared to the base model. Overall, our approach highlights the importance of efficient sampling strategies for ICL, which can affect the performance of the model on any given task.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent years, Large Language Models (LLMs) have gained significant traction in the fields of code completion and code filling. This growth has been fueled by the availability of large-scale open-source datasets such as The vault <cite class=\"ltx_cite ltx_citemacro_citet\">Manh et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib29\" title=\"\">2023</a>)</cite>, CodeSearchNet <cite class=\"ltx_cite ltx_citemacro_citet\">Husain et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib16\" title=\"\">2020</a>)</cite>, CodeXGlue <cite class=\"ltx_cite ltx_citemacro_citet\">Lu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib26\" title=\"\">2021</a>)</cite> and many others. Alongside these datasets, we have also witnessed the emergence of open-source models designed specifically for code-related tasks, including the CodeLlama series <cite class=\"ltx_cite ltx_citemacro_citet\">Rozi&#232;re et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib40\" title=\"\">2024</a>)</cite>, Qwen Coder <cite class=\"ltx_cite ltx_citemacro_citet\">Hui et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib15\" title=\"\">2024</a>)</cite> series, and StarCoder series <cite class=\"ltx_cite ltx_citemacro_citet\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib21\" title=\"\">2023a</a>)</cite>. In parallel, closed-source models such as GPT-4o <cite class=\"ltx_cite ltx_citemacro_citet\">OpenAI et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib32\" title=\"\">2024</a>)</cite> and Claude Code <cite class=\"ltx_cite ltx_citemacro_citet\">Anthropic (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib3\" title=\"\">2025</a>)</cite> have been widely adopted by various big tech companies for generating production-ready code. Despite these advancements, most of these models remain difficult to interpret in the context of code generation. While a variety of interpretability methods have been developed to interpret the outputs generated by LLMs and foster trust in their usage across domains, many of these approaches are generic and not specifically tailored for code generation tasks. Some methods, however, are focused on interpretability in code generation. For instance, Code-Q <cite class=\"ltx_cite ltx_citemacro_citet\">Palacio et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib34\" title=\"\">2025</a>)</cite> identifies influential tokens that guide the model&#8217;s output, but it requires repeated sampling and generation, which introduces significant computational overhead during inference.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another method, ASTrust <cite class=\"ltx_cite ltx_citemacro_citet\">Palacio et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib33\" title=\"\">2024</a>)</cite>, leverages Abstract Syntax Trees (ASTs) by using model-generated token probabilities. Tokens are mapped to code level subsets, which are then grouped into terminal and non-terminal nodes within the AST. Each non-terminal node is represented by the aggregated confidence of its associated terminal nodes. However, this approach requires storing the probability distribution over the entire vocabulary at every step of generation, which scales poorly as the output length increases. To address these challenges, we propose a manifold-based sampling strategy that automatically samples a set of ICL demonstrations from a given dataset. These demonstrations enable interpretability by combining attribution and AST-based analysis. Our method segments the generated code into interpretable regions, such as Iterations, Data structures, etc., allowing users to understand which regions of the generated code are most affected by the sampled demonstrations. To the best of our knowledge, we are the first to unify prototype-driven ICL sampling with AST-grounded attribution for code interpretability.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "token",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prototype-Gradient Attribution for AST-Grounded Interpretability:</span> We propose a novel attribution mechanism using the gradient of similarity between prototype and token embeddings to estimate token-level influence. These scores are then propagated through the <span class=\"ltx_text ltx_font_italic\">Abstract Syntax Tree (AST)</span> to produce <span class=\"ltx_text ltx_font_italic\">faithful, syntax-aware confidence maps</span>, enabling both <span class=\"ltx_text ltx_font_italic\">local (node-level)</span> and <span class=\"ltx_text ltx_font_italic\">global (category-level)</span> interpretability of generated code&#8212;while avoiding the memory overhead of storing token probabilities.</p>\n\n",
                "matched_terms": [
                    "token",
                    "prototype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Intrinsic interpretability, in contrast, focuses on designing model architectures so that their behavior is inherently explainable. One example is concept bottleneck models <cite class=\"ltx_cite ltx_citemacro_citet\">Koh et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib20\" title=\"\">2020</a>)</cite>, which were extended to large language models (LLMs) by <cite class=\"ltx_cite ltx_citemacro_citet\">Sun et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib47\" title=\"\">2025</a>)</cite> for sentence classification task. Their approach generates concepts for each class, making the classification process directly interpretable. However, this approach faces limitations in generating suitable concepts for diverse tasks and does not scale well to text generation. Another related method, Proto-lm <cite class=\"ltx_cite ltx_citemacro_citet\">Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib59\" title=\"\">2023</a>)</cite> , extends prototype networks to text classification. Instead of generating concepts like concept bottlenecks, it learns trainable prototypes and maps them to the nearest training samples for interpretability.</p>\n\n",
                "matched_terms": [
                    "model",
                    "prototype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A particularly influential method within intrinsic interpretability is Chain-of-Thought (CoT) <cite class=\"ltx_cite ltx_citemacro_citet\">Wei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib55\" title=\"\">2023</a>)</cite>, which generates intermediate reasoning steps. CoT has been shown to improve both plausibility and task performance compared to demonstrations that provide only the final answers <cite class=\"ltx_cite ltx_citemacro_citet\">Wei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib55\" title=\"\">2023</a>)</cite> <cite class=\"ltx_cite ltx_citemacro_citet\">Cobbe et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib10\" title=\"\">2021</a>)</cite>. Building upon this, Self-Consistency <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib54\" title=\"\">2023</a>)</cite> was proposed as an extension of CoT. This method prompts the model to produce multiple reasoning chains and answers, and then selects the final output using a majority vote across the answers. Although effective, Self-Consistency only ensures correctness of the final prediction, without verifying whether the reasoning chains themselves are valid or faithful. To address this, SEA-CoT <cite class=\"ltx_cite ltx_citemacro_citet\">Wei&#160;Jie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib57\" title=\"\">2024</a>)</cite> was introduced. SEA-CoT evaluates generated reasoning chains based on the implication with the task context and the overlap of the token level, ensuring that both the reasoning process and the final answer align more closely with the task requirements. However, as stated by <cite class=\"ltx_cite ltx_citemacro_citet\">Jacovi &amp; Goldberg (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib17\" title=\"\">2020</a>)</cite>, the reasoning chains from LLM often appear plausible to humans but are not necessarily faithful to the true decision-making process of the LLM. Plausibility refers to how convincing the interpretation is to humans, while faithfulness measures the degree to which it truly represents the internal reasoning of the LLM.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context",
                    "model",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most of the above methods are designed for generic tasks, with a limited focus on code-specific applications. The method ASTrust was developed specifically for interpretability in code generation. It builds Abstract Syntax Trees (ASTs) to align with program structure and assigns confidence scores to non-terminal nodes by aggregating probabilities from their terminal nodes. These scores are derived from token-level probabilities output by the model. <cite class=\"ltx_cite ltx_citemacro_citet\">Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib28\" title=\"\">2024</a>)</cite> demonstrates that LLMs already possess strong syntactic awareness, rivaling AST-based static code analysis. However, the method ASTrust has key limitations: its token sampling method is not well justified. Greedy sampling ignores the advantages of stochastic approaches, while stochastic sampling requires storing probabilities for all vocabulary tokens at every step an impractical, memory-intensive process. In contrast, our method avoids this heavy storage by relying on attribution-based prototype influence, which captures the effect of sampled demonstrations without requiring full vocabulary distributions. As a result, our approach preserves the benefits of stochastic sampling <cite class=\"ltx_cite ltx_citemacro_citet\">Shi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib43\" title=\"\">2024</a>)</cite> while remaining significantly more scalable and practical for code generation interpretability.</p>\n\n",
                "matched_terms": [
                    "model",
                    "all",
                    "token",
                    "tokens",
                    "prototype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As highlighted in <cite class=\"ltx_cite ltx_citemacro_citet\">Rodriguez-Cardenas et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib38\" title=\"\">2023</a>)</cite>, in the context of ICL, the selection of demonstrations plays a crucial role in model performance. In our approach, we dedicate considerable effort to identifying the most suitable prototypes (ICL examples) for each LLM. Our method can be divided into two main components. In the first stage, we initialize a simple neural network <math alttext=\"h_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m1\" intent=\":literal\"><semantics><msub><mi>h</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">h_{\\theta}</annotation></semantics></math> and train it on Dataset <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> to jointly optimize manifold learning and metric learning objectives. Once the training is complete, the learned proxy vectors are employed to sample prototypes.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After the completion of the training process, we proceed to the second stage, where we generate the code completions using the prototypes. After that, we utilize the encoded latent representations of the prototypes to calculate the confidence score per token for AST analysis. For each code completion, we compute a prototype attribution-based score to quantify the influence of the prototypes on the generated code. Specifically, influence is measured by the attribution between the sampled demonstrations and the code completions. Finally, we perform an AST analysis to analyze how the prototypes impact the syntactic structure of the generated code.</p>\n\n",
                "matched_terms": [
                    "token",
                    "prototype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are reported on a scale from <math alttext=\"[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0,1]</annotation></semantics></math>, where <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m2\" intent=\":literal\"><mn>0</mn></math> is the lowest and <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m3\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> is the highest (For instance, 0.1 can be interpreted as 10%). While the numerical margins may appear small at first glance, even modest gains in code completion represent substantial improvements. For context, GPT-4-1106 <cite class=\"ltx_cite ltx_citemacro_citet\">ope (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib1\" title=\"\">2023</a>)</cite>, which is estimated to be at least <math alttext=\"1000\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.p2.m4\" intent=\":literal\"><semantics><mrow><mn>1000</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">1000\\times</annotation></semantics></math> larger than the models used for our experiments, achieves a score of <math alttext=\"0.786\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m5\" intent=\":literal\"><semantics><mn>0.786</mn><annotation encoding=\"application/x-tex\">0.786</annotation></semantics></math> on the MBPP test set. This comparison highlights an important distinction: in many benchmarks, partial overlap between a generated solution and the reference solution may yield a nonzero score even if the final answer is incorrect. In contrast, code benchmarks are more stringent, as each generated program is independently evaluated against unit test cases. Therefore, even incremental improvements in <math alttext=\"Pass@k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m6\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">Pass@k</annotation></semantics></math> metrics are highly significant for code generation tasks.</p>\n\n",
                "matched_terms": [
                    "reported",
                    "context",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Qwen2.5-coder model, despite having fewer parameters than Codellama, achieves comparable performance on both the <math alttext=\"Pass@1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">Pass@1</annotation></semantics></math> and <math alttext=\"Pass@10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m2\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">Pass@10</annotation></semantics></math> metrics across the MBPP and MBPP+ test sets. Among all comparisons, the similarity-based sampling method surpasses our approach only for the Llama3.2 model; in every other case, our method consistently outperforms alternative strategies across all models. As noted by <cite class=\"ltx_cite ltx_citemacro_citet\">Rodriguez-Cardenas et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib38\" title=\"\">2023</a>)</cite>, within the ICL setting, the quality of selected demonstrations can also negatively affect model performance.</p>\n\n",
                "matched_terms": [
                    "all",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform an Abstract Syntax Tree (AST) analysis to identify which syntactic regions of the generated code are most influenced by the sampled prototypes. In the ASTrust framework, the authors employ token-level probabilities produced by the model <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> as the confidence scores in the token set. For a sequence of tokens <math alttext=\"w_{1},w_{2},\\dots,w_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><msub><mi>w</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">w_{1},w_{2},\\dots,w_{i}</annotation></semantics></math>, the probability of generating the next token <math alttext=\"w_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m3\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">w_{i+1}</annotation></semantics></math> is defined as &#160;equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5.E4\" title=\"In 5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> where <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m4\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> denotes the Large Language Model and <math alttext=\"M(w_{1:i})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m5\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">M(w_{1:i})</annotation></semantics></math> represents the non-normalized log probabilities output by the model for the given context.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "context",
                    "model",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section 2 &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S2\" title=\"2 Related work &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, this method suffers from high memory overhead when combined with stochastic sampling strategies. To mitigate this limitation, we instead leverage attribution-based scores between the sampled prototypes and the generated code samples, and use these scores as token-level confidence in AST analysis. Concretely, for a model-generated code snippet <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>, we extract the tokens <math alttext=\"w_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">w_{i}</annotation></semantics></math> along with their latent representations <math alttext=\"z_{w_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m3\" intent=\":literal\"><semantics><msub><mi>z</mi><msub><mi>w</mi><mi>i</mi></msub></msub><annotation encoding=\"application/x-tex\">z_{w_{i}}</annotation></semantics></math>. Let the latent representation of a sampled prototype <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> be <math alttext=\"z_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m5\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">z_{p}</annotation></semantics></math>. We compute the mean prototype vector <math alttext=\"z_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m6\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">z_{a}</annotation></semantics></math> as <math alttext=\"z_{a}=\\sum_{i\\in P}z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m7\" intent=\":literal\"><semantics><mrow><msub><mi>z</mi><mi>a</mi></msub><mo rspace=\"0.111em\">=</mo><mrow><msub><mo>&#8721;</mo><mrow><mi>i</mi><mo>&#8712;</mo><mi>P</mi></mrow></msub><msub><mi>z</mi><mi>i</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">z_{a}=\\sum_{i\\in P}z_{i}</annotation></semantics></math>. Next, we compute the dot product between <math alttext=\"z_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m8\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">z_{a}</annotation></semantics></math> and each <math alttext=\"z_{w_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m9\" intent=\":literal\"><semantics><msub><mi>z</mi><msub><mi>w</mi><mi>i</mi></msub></msub><annotation encoding=\"application/x-tex\">z_{w_{i}}</annotation></semantics></math>, and compute its gradient with respect to <math alttext=\"z_{w_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m10\" intent=\":literal\"><semantics><msub><mi>z</mi><msub><mi>w</mi><mi>i</mi></msub></msub><annotation encoding=\"application/x-tex\">z_{w_{i}}</annotation></semantics></math>. The normalized gradients <math alttext=\"\\nabla_{z_{w_{i}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m11\" intent=\":literal\"><semantics><msub><mo>&#8711;</mo><msub><mi>z</mi><msub><mi>w</mi><mi>i</mi></msub></msub></msub><annotation encoding=\"application/x-tex\">\\nabla_{z_{w_{i}}}</annotation></semantics></math>(&#160;equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5.E5\" title=\"In 5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) are then used as confidence scores per token in the AST analysis.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "token",
                    "prototype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AST analysis involves using the prototype-based attribution scores as token confidence scores explained in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5.E5\" title=\"In 5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. We then compute the average confidence over tokens corresponding to each AST node, and report these averages as performance values grouped by manually defined syntax categories. The process follows three steps, illustrated in Fig.1 &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5.F1\" title=\"Figure 1 &#8227; 5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. In Step1, for every generated code snippet, the tokenizer splits the code into tokens <math alttext=\"w_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">w_{i}</annotation></semantics></math> (forming the token set <math alttext=\"\\uptau\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#964;</mi><annotation encoding=\"application/x-tex\">\\uptau</annotation></semantics></math> &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A4.SS1\" title=\"D.1 Interpretable Syntax sets and interactions &#8227; Appendix D AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">D.1</span></a>), and the model assigns a confidence score to each token as described in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5\" title=\"5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. In Step2, the token-level predictions are aligned with the respective Abstract Syntax Tree (AST) terminal nodes. Terminal nodes retain the raw confidences, whereas non-terminal nodes hierarchically store aggregated values. Together, terminal and non-terminal nodes form the subcategory set <math alttext=\"\\upsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\"><semantics><mi>&#965;</mi><annotation encoding=\"application/x-tex\">\\upsilon</annotation></semantics></math> &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A4.SS1\" title=\"D.1 Interpretable Syntax sets and interactions &#8227; Appendix D AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">D.1</span></a>. For instance, the token &#8217;if_&#8217; from the token set aligns with a terminal AST node but is grouped under the non-terminal node &#8217;if_statement&#8217;. Finally, in Step 3, the analysis introduces eight syntax categories to summarize model predictions. These categories aggregate subcategories into broader, human-interpretable groups. The Syntax Categories form a fixed Category Set <math alttext=\"\\Lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#923;</mi><annotation encoding=\"application/x-tex\">\\Lambda</annotation></semantics></math> &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A4.SS1\" title=\"D.1 Interpretable Syntax sets and interactions &#8227; Appendix D AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">D.1</span></a>, providing more intuitive elements for interpretation.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "respective",
                    "model",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For example, the sub-categories &#8217;if_statement&#8217; and &#8217;if&#8217; are grouped under the syntax category &#8217;Decisions&#8217; &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5.F2\" title=\"Figure 2 &#8227; 5.1 Syntax Grounded explanations &#8227; 5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Ultimately, ASTrust outputs an averaged score for each category to provide global explanations, along with an AST tree visualization that embeds confidence scores at every node for local explanations. In essence, we argue that syntax elements encode semantic information that contextualizes token-level confidence scores, though this semantic value differs depending on the granularity of the elements. For instance, tokens alone convey less interpretable meaning compared to higher-level categories. AST analysis thus serves as a post-hoc explanation framework at both local and global levels. Local explanations focus on breaking down a single code snippet into AST elements to interpret its generation, while global explanations rely on multiple generated snippets to provide a holistic view of the model through Syntax Categories (SCs) &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A4.SS1\" title=\"D.1 Interpretable Syntax sets and interactions &#8227; Appendix D AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">D.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the attribution-based confidence score of each Syntax Category (SC) for the 6 LLMs, we present an AST analysis.\n&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5.F3\" title=\"Figure 3 &#8227; 5.2 Code Syntactic analysis &#8227; 5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates the AST interpretability performance segregated by Syntax Categories (SCs) for\neach model type. The Qwen2.5 Coder and Qwen3 &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5.F3\" title=\"Figure 3 &#8227; 5.2 Code Syntactic analysis &#8227; 5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (a) models exhibit highly consistent confidence across all syntax categories, with nearly identical values. Both models demonstrate their strongest performance in Scope, Data Structures, and Functions, indicating reliability in handling structured data, variable and function scoping, and modular code organization. Moderate confidence is observed for Iteration, Decisions, Operators, and Data Types, while the lowest confidence is consistently assigned to Exception handling, suggesting potential limitations in generating or reasoning about robust error-handling constructs. Overall, these results suggest that both Qwen2.5 Coder and Qwen3 are best suited for structured programming tasks, while being less dependable for control-flow&#8211;intensive or exception-heavy code generation.</p>\n\n",
                "matched_terms": [
                    "all",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, prototypes were sampled exclusively from the Magicoder dataset. While this choice provided a consistent basis for evaluation, extending the analysis to additional datasets could offer a broader understanding of prototype quality. In fact, our method can naturally be applied as a global metric for ranking datasets with respect to their ability to yield effective prototypes. Another limitation arises from differences in model stability. For example, Llama3.2 &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> exhibited high sensitivity to changes in nearly all hyperparameters, which led to inconsistent results on the <math alttext=\"Pass@k\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">Pass@k</annotation></semantics></math> metric. In contrast, the Qwen2.5 Coder model &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> displayed only marginal sensitivity, with the exception of the <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m2\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> parameter, resulting in more stable and reliable performance. Finally, while our current approach uses sampled prototypes as in-context learning demonstrations, the framework can be extended toward pre-hoc interpretability by design. In particular, prototype steering could be explored as a mechanism for influencing model behavior, offering new avenues for both interpretability and controllability in LLMs.</p>\n\n",
                "matched_terms": [
                    "all",
                    "model",
                    "prototype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><cite class=\"ltx_cite ltx_citemacro_citet\">Rodriguez-Cardenas et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#bib.bib38\" title=\"\">2023</a>)</cite> emphasized the sensitivity of demonstration selection by comparing two different prompt groups in a controlled experiment. One group exhibited a positive causal effect, improving the Average Treatment Effect (ATE) by 5.1% on Chatgpt, while the other group showed a negative causal effect, decreasing ATE by 3.3% relative to the control group. Here, ATE quantifies the average causal influence of a treatment (i.e., the chosen prompt group) on model performance. These findings highlight the critical role of demonstration quality: poorly chosen examples may reduce performance, sometimes performing worse than LLMS that do not use ICL at all. Throughout the paper, we use the terms demonstrations and examples interchangeably in the context of ICL.</p>\n\n",
                "matched_terms": [
                    "terms",
                    "all",
                    "model",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <math alttext=\"pass@k\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">pass@k</annotation></semantics></math> metric, <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p3.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> is the total no.of problems, <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p3.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> (<math alttext=\"n\\geq k\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p3.m4\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&#8805;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">n\\geq k</annotation></semantics></math>) is the no.of code samples generated per problem, <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p3.m5\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> (<math alttext=\"c\\leq n\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p3.m6\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>&#8804;</mo><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">c\\leq n</annotation></semantics></math>)\nrepresents the count of correct samples which pass unit tests. A problem is considered solved if any sample passes the unit tests, and the total fraction of problems solved is reported.</p>\n\n",
                "matched_terms": [
                    "reported",
                    "noof"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The below is the architecture of <math alttext=\"h_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p4.m1\" intent=\":literal\"><semantics><msub><mi>h</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">h_{\\theta}</annotation></semantics></math> neural network we used. It is a Single-layer network &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A2.T3\" title=\"Table 3 &#8227; B.1 Evaluation Dataset and Metric &#8227; Appendix B Methodology &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> with intermediate normalizations. For most of the LLMs the prototype size is set to 50. All of the layers of <math alttext=\"h_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p4.m2\" intent=\":literal\"><semantics><msub><mi>h</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">h_{\\theta}</annotation></semantics></math> are used during training and updated via backpropagation.</p>\n\n",
                "matched_terms": [
                    "all",
                    "prototype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage of our framework, dedicated to prototype sampling, the network <math alttext=\"h_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>h</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">h_{\\theta}</annotation></semantics></math> is trained for 200 epochs on the training dataset <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math>. Training utilizes two independent Adam optimizers: one for the network parameters and another for the proxy parameters. Both optimizers are initialized with a learning rate of <span class=\"ltx_text ltx_font_typewriter\">1e-3</span>, combined with a scheduler that decays the learning rate by a factor of <math alttext=\"\\eta_{t}=0.97\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mi>t</mi></msub><mo>=</mo><mn>0.97</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{t}=0.97</annotation></semantics></math>. The dimensionality of the encoded vector <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m4\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math> is determined by the underlying Large Language Model (<math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m5\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math>). A mini-batch size of 128 samples is maintained throughout training.</p>\n\n",
                "matched_terms": [
                    "model",
                    "prototype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The table presents the token lengths of sampled prototypes along with the 99th percentile, 95th percentile, and average token lengths across the MBPP dataset for combined query and solution inputs. Since each input consists of the sampled prototypes used as demonstrations together with the MBPP test queries, we estimate the overall input token lengths to assess whether all prototypes can be accommodated. These token length statistics are reported separately for each LLM.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "reported",
                    "lengths",
                    "all",
                    "token",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the table, it can be observed that the sampled prototype token lengths exceed the context window of the Falcon3-1B model. Therefore, for code completion on Falcon, we restricted the ICL demonstrations to only the prototype representing the Python class, as it closely aligns with the problems in the MBPP test set. The same procedure was applied across all sampling strategies for the Falcon3 model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "lengths",
                    "all",
                    "context",
                    "token",
                    "prototype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The table also shows that the Codellama model, being code-specific, produces a higher number of tokens compared to the Llama3.2 model. This highlights the optimized tokenization techniques of the Llama3.2 series, as Codellama is derived from the Llama2 family of models. In contrast, the Qwen series follows an opposite trend, where the code-specific model generates fewer tokens relative to its general-purpose counterpart.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All reported scores in this paper have been independently recomputed across every model and sampling method. The results for the base model (without ICL) may differ from those documented in the official technical reports, which can be attributed to several factors. Based on our experimental findings, we outline the potential reasons that may have influenced performance aside from the ICL demonstrations.</p>\n\n",
                "matched_terms": [
                    "reported",
                    "all",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For generating code completions we employed the Hugging Face text generation pipeline with decoding parameters set to <span class=\"ltx_text ltx_font_typewriter\">temperature = 0.6</span> and <span class=\"ltx_text ltx_font_typewriter\">top-p = 0.9</span>. Our experiments revealed that even minor adjustments to these parameters, with only two variations, led to improved performance across all models and sampling methods. Notably, most technical reports for benchmark evaluations do not specify the decoding strategies employed, which contributes to variability in reported results. This observation underscores the importance of performing hyperparameter optimization during the decoding stage of generation.</p>\n\n",
                "matched_terms": [
                    "reported",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It can be inferred that code sanitization procedures also play a crucial role in determining benchmark performance. In our experiments, we employed the evalplus library to sanitize the generated code completions. However, despite this sanitization, certain residual tokens were not removed, which in turn impacted the execution outcomes and consequently affected the reported performance. In &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A2.F4\" title=\"Figure 4 &#8227; B.5 Model Analysis &#8227; Appendix B Methodology &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> even though the evalplus managed to remove the below text, the extra tokens are still in the code which will result in an error when running on pre-defined unit tests in spite of generating the correct code.</p>\n\n",
                "matched_terms": [
                    "reported",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#A3.F5\" title=\"Figure 5 &#8227; C.8 Overall effect &#8227; Appendix C Ablation Study &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the Llama3.2 model exhibits high sensitivity to parameter variations, displaying substantial fluctuations in performance. This trend aligns with the results reported in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S4.T1\" title=\"Table 1 &#8227; 4 Results &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S4.T2\" title=\"Table 2 &#8227; 4 Results &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where the similarity-based sampling method achieves the highest score for Llama3.2, further highlighting its instability under different configurations. In contrast, the Qwen2.5-coder model demonstrates relatively stable behavior, showing consistently increasing trends across most parameters, with the notable exception of the scaling factor <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS8.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "reported",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Token Set <math alttext=\"\\uptau\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#964;</mi><annotation encoding=\"application/x-tex\">\\uptau</annotation></semantics></math></span>, this set contains the code tokens <math alttext=\"w_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">w_{i}</annotation></semantics></math> derived from the generated code snippets <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m3\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>, where each token&#8217;s confidence is computed as outlined in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5\" title=\"5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. <span class=\"ltx_text ltx_font_bold\">Subcategory Set <math alttext=\"\\upsilon\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m4\" intent=\":literal\"><semantics><mi>&#965;</mi><annotation encoding=\"application/x-tex\">\\upsilon</annotation></semantics></math></span>, this set consists of elements from Context-Free Grammars (CFGs), which are rules that capture the syntactic and structural aspects of a programming language. Formally, a CFG is defined as <math alttext=\"G=(\\alpha,\\lambda,\\omega,\\beta)\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>G</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#945;</mi><mo>,</mo><mi>&#955;</mi><mo>,</mo><mi>&#969;</mi><mo>,</mo><mi>&#946;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">G=(\\alpha,\\lambda,\\omega,\\beta)</annotation></semantics></math>, where <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m6\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> is the finite set of non-terminal nodes, <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m7\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> the finite set of terminal nodes, <math alttext=\"\\omega\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m8\" intent=\":literal\"><semantics><mi>&#969;</mi><annotation encoding=\"application/x-tex\">\\omega</annotation></semantics></math> the finite set of production rules, and <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m9\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> the start symbol. CFGs utilize terminal and non-terminal nodes (i.e., subcategories) to specify production rules <math alttext=\"\\omega\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m10\" intent=\":literal\"><semantics><mi>&#969;</mi><annotation encoding=\"application/x-tex\">\\omega</annotation></semantics></math> for statements such as conditionals, assignments, or operators. Importantly, terminal and non-terminal nodes serve distinct purposes. These nodes correspond to the elements of the subcategory set <math alttext=\"\\upsilon\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m11\" intent=\":literal\"><semantics><mi>&#965;</mi><annotation encoding=\"application/x-tex\">\\upsilon</annotation></semantics></math>, with <math alttext=\"\\lambda,\\alpha\\in\\upsilon\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m12\" intent=\":literal\"><semantics><mrow><mrow><mi>&#955;</mi><mo>,</mo><mi>&#945;</mi></mrow><mo>&#8712;</mo><mi>&#965;</mi></mrow><annotation encoding=\"application/x-tex\">\\lambda,\\alpha\\in\\upsilon</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The interaction between the token set <math alttext=\"\\uptau\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p2.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#964;</mi><annotation encoding=\"application/x-tex\">\\uptau</annotation></semantics></math> and the subcategory set <math alttext=\"\\upsilon\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p2.m2\" intent=\":literal\"><semantics><mi>&#965;</mi><annotation encoding=\"application/x-tex\">\\upsilon</annotation></semantics></math> is governed by the <span class=\"ltx_text ltx_font_bold\">Alignment Function <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p2.m3\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math></span>. This function establishes a many-to-one or one-to-one mapping from each token <math alttext=\"w_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p2.m4\" intent=\":literal\"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">w_{i}</annotation></semantics></math> in the token set <math alttext=\"\\uptau\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p2.m5\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#964;</mi><annotation encoding=\"application/x-tex\">\\uptau</annotation></semantics></math> to a terminal node <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p2.m6\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> in the subcategory set <math alttext=\"\\upsilon\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p2.m7\" intent=\":literal\"><semantics><mi>&#965;</mi><annotation encoding=\"application/x-tex\">\\upsilon</annotation></semantics></math>. For example, Fig.2 &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5.F2\" title=\"Figure 2 &#8227; 5.1 Syntax Grounded explanations &#8227; 5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the alignment of the token &#8217;try_&#8217; with the terminal node &#8217;try&#8217;, where the character \"_\" is disregarded. It is important to note that tokenization may produce sequences in which tokens do not align one-to-one with terminal nodes. For instance, Fig.2 &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5.F2\" title=\"Figure 2 &#8227; 5.1 Syntax Grounded explanations &#8227; 5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates how the tokens &#8217;flo_&#8217; and &#8217;at&#8217; are both aligned with the terminal node &#8217;float&#8217;. Formally, this can be expressed as <math alttext=\"\\delta(^{\\prime}flo\\_^{\\prime},^{\\prime}at^{\\prime})\\rightarrow[^{\\prime}float^{\\prime}]\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"A4.SS1.p2.m8\" intent=\":literal\"><semantics><mrow><mi>&#948;</mi><mrow><msup><mo stretchy=\"false\">(</mo><mo>&#8242;</mo></msup><mi>f</mi><mi>l</mi><mi>o</mi><msup><mi mathvariant=\"normal\">_</mi><mo>&#8242;</mo></msup><msup><mo>,</mo><mo>&#8242;</mo></msup><mi>a</mi><msup><mi>t</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">&#8594;</mo><mrow><msup><mo stretchy=\"false\">[</mo><mo>&#8242;</mo></msup><mi>f</mi><mi>l</mi><mi>o</mi><mi>a</mi><msup><mi>t</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\delta(^{\\prime}flo\\_^{\\prime},^{\\prime}at^{\\prime})\\rightarrow[^{\\prime}float^{\\prime}]</annotation></semantics></math>, representing a many-to-one mapping. Thus, the alignment between code tokens and terminal nodes is strictly many-to-one (which includes the special case of one-to-one), but never one-to-many or many-to-many.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Category Set <math alttext=\"\\Lambda\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p3.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#923;</mi><annotation encoding=\"application/x-tex\">\\Lambda</annotation></semantics></math></span>. Step 3 in Fig.1 &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25247v1#S5.F1\" title=\"Figure 1 &#8227; 5 AST analysis &#8227; Protocode: Prototype-Driven Interpretability for Code Generation in LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates how <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p3.m2\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> and <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p3.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> are combined into a category <math alttext=\"c\\in\\Lambda\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p3.m4\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>&#8712;</mo><mi mathvariant=\"normal\">&#923;</mi></mrow><annotation encoding=\"application/x-tex\">c\\in\\Lambda</annotation></semantics></math>. The elements of the Category Set <math alttext=\"\\Lambda\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p3.m5\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#923;</mi><annotation encoding=\"application/x-tex\">\\Lambda</annotation></semantics></math> are referred to as Syntax Categories (SCs). Based on tree-sitter bindings for Python, we define eight distinct SCs. These categories represent semantic units that facilitate the syntax-level interpretability of LLMs. Consequently, AST analysis provides a developer-oriented explanation of Token-Level confidence. In summary, each token in a sequence <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p3.m6\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> can be mapped to a category <math alttext=\"c\\in\\Lambda\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p3.m7\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>&#8712;</mo><mi mathvariant=\"normal\">&#923;</mi></mrow><annotation encoding=\"application/x-tex\">c\\in\\Lambda</annotation></semantics></math>. Through AST analysis, developers can directly relate LLM code predictions to meaningful structural attributes.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A <span class=\"ltx_text ltx_font_bold\">clustering function <math alttext=\"\\zeta\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m1\" intent=\":literal\"><semantics><mi>&#950;</mi><annotation encoding=\"application/x-tex\">\\zeta</annotation></semantics></math></span> computes the confidence performance of <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m2\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> and <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> nodes (subcategories) within an AST by hierarchically aggregating Token-Level Confidences into a category <math alttext=\"c\\in\\Lambda\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m4\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>&#8712;</mo><mi mathvariant=\"normal\">&#923;</mi></mrow><annotation encoding=\"application/x-tex\">c\\in\\Lambda</annotation></semantics></math>. After tokens are aligned to their respective nodes using <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m5\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math>, AST analysis groups them into either their corresponding category or non-terminal <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m6\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> node, following the AST structure. In some cases, terminal <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m7\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> nodes may be directly aggregated into a category without involving intermediate non-terminal <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m8\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> nodes. The function <math alttext=\"\\zeta\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m9\" intent=\":literal\"><semantics><mi>&#950;</mi><annotation encoding=\"application/x-tex\">\\zeta</annotation></semantics></math> can be configured to use different aggregation strategies, such as average, median, or maximum. In our experiments, we define the clustering function as <math alttext=\"\\zeta:\\upsilon\\rightarrow\\text{avg}(w_{1:i})\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m10\" intent=\":literal\"><semantics><mrow><mi>&#950;</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi>&#965;</mi><mo stretchy=\"false\">&#8594;</mo><mrow><mtext>avg</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\zeta:\\upsilon\\rightarrow\\text{avg}(w_{1:i})</annotation></semantics></math> for a subset of tokens <math alttext=\"w_{\\leq i}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p4.m11\" intent=\":literal\"><semantics><msub><mi>w</mi><mrow><mi/><mo>&#8804;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">w_{\\leq i}</annotation></semantics></math>. The 8 defined syntax categories are:</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "respective"
                ]
            }
        ]
    }
}