{
    "S5.T1": {
        "source_file": "Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages",
        "caption": "TABLE I:   Results after Fine-tuning the Whisper ASR Model on Swahili and Kinyarwanda Datasets: Training and validation losses are measured using cross-entropy loss. Word Error Rate (WER) is given as a percentage, indicating errors (insertions, substitutions, deletions) compared to a human-generated reference transcript.",
        "body": "Swahili Fine-tuned Whisper Model Results\nKinyarwanda Fine-tuned Whisper Model Results\n\n\nStep\nTraining Loss\nValidation Loss\nWER\nStep\nTraining Loss\nValidation Loss\nWER\n\n\n20\n1.7887\n2.1523\n26.3736\n20\n2.1422\n2.3111\n34.5953\n\n\n40\n1.7887\n2.0751\n26.2515\n40\n2.1422\n2.2022\n33.9426\n\n\n60\n1.6873\n2.0161\n26.3736\n60\n1.8742\n2.1406\n33.6815\n\n\n80\n1.5626\n1.9788\n26.3736\n80\n1.7608\n2.1077\n33.2898\n\n\n100\n1.4991\n1.9641\n26.3736\n100\n1.6573\n2.0954\n33.4204",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t\" colspan=\"4\">Swahili Fine-tuned Whisper Model Results</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"4\">Kinyarwanda Fine-tuned Whisper Model Results</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Step</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Training Loss</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Validation Loss</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">WER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Step</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Training Loss</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Validation Loss</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">WER</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.7887</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.1523</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">26.3736</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.1422</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.3111</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">34.5953</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.7887</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.0751</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">26.2515</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.1422</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.2022</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">33.9426</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.6873</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.0161</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">26.3736</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.8742</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.1406</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">33.6815</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.5626</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.9788</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">26.3736</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.7608</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.1077</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">33.2898</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1.4991</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1.9641</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t\">26.3736</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1.6573</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">2.0954</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">33.4204</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "training",
            "step",
            "percentage",
            "rate",
            "datasets",
            "loss",
            "indicating",
            "losses",
            "finetuning",
            "errors",
            "swahili",
            "humangenerated",
            "given",
            "reference",
            "insertions",
            "wer",
            "whisper",
            "results",
            "validation",
            "model",
            "measured",
            "word",
            "substitutions",
            "crossentropy",
            "asr",
            "transcript",
            "finetuned",
            "compared",
            "deletions",
            "after",
            "error",
            "kinyarwanda"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The Whisper model was fine-tuned for each language independently, where the checkpoints in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx33\" title=\"\">33</a>]</cite> and <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx34\" title=\"\">34</a>]</cite> were used for Swahili and Kinyarwanda, repsectively. The process achieved a WER of 26.37% for Swahili and 33.42% for Kinyarwanda. The training process did not significantly change the initial WER. However, the improving trends of the cross-entropy loss (shown in in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S5.T1\" title=\"TABLE I &#8227; V EXPERIMENTS AND RESULTS &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>) promise improved performances, given additional model finetuning.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This paper presents a novel framework for speech transcription and synthesis, leveraging edge-cloud parallelism to enhance processing speed and accessibility for Kinyarwanda and Swahili speakers. It addresses the scarcity of powerful language processing tools for these widely spoken languages in East African countries with limited technological infrastructure. The framework utilizes the Whisper and SpeechT5 pre-trained models to enable speech-to-text (STT) and text-to-speech (TTS) translation. The architecture uses a cascading mechanism that distributes the model inference workload between the edge device and the cloud, thereby reducing latency and resource usage, benefiting both ends. On the edge device, our approach achieves a memory usage compression of 9.5% for the SpeechT5 model and 14% for the Whisper model, with a maximum memory usage of 149 MB. Experimental results indicate that on a 1.7 GHz CPU edge device with a 1 MB/s network bandwidth, the system can process a 270-character text in less than a minute for both speech-to-text and text-to-speech transcription. Using real-world survey data from Kenya, it is shown that the cascaded edge-cloud architecture proposed could easily serve as an excellent platform for STT and TTS transcription with good accuracy and response time.</p>\n\n",
                "matched_terms": [
                    "model",
                    "swahili",
                    "whisper",
                    "results",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In today&#8217;s digital age, the need for accurate and efficient speech transcription and synthesis models has been increasing rapidly. These models play an important role in a variety of applications, such as learning new language(s), accessibility tools for people with difficulties in reading and hearing, as well as automated voice assistants <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx1\" title=\"\">1</a>]</cite>. Kinyarwanda and Swahili are two of the local languages spoken in East Africa. While Swahili is the most widely spoken language in Eastern Africa, the speakers range from 60 million to over 150 million <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx2\" title=\"\">2</a>]</cite>. Swahili serves as the national language of four African nations: Tanzania, Kenya, Uganda, Rwanda, and the Democratic Republic of the Congo. On the other hand, Kinyarwanda is the national language of Rwanda, spoken by approximately 24 million people in Rwanda and beyond <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx3\" title=\"\">3</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "swahili",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this shortcoming, in this paper, we present a novel, computationally efficient cascading approach for edge-cloud speech transcription and synthesis, specifically designed for Kinyarwanda and Swahili languages. This model and the underlying mechanism was inspired by the approach proposed in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx11\" title=\"\">11</a>]</cite>, a cascading neural network to efficiently couple the computation between models deployed at the edge as well as the cloud. In a nutshell, a cascading neural network is capable of distributing a deep neural network between a local device and the cloud while keeping the required communication network traffic to a minimum. The network begins processing on the constrained device and only relies on the remote part (i.e., the Cloud) when the local part does not provide an accurate or fast enough result. For instance, in Text-to-Speech (TTS), when the processed output voice after the edge processing is very noisy, it might be necessary to send the internal representation to the Cloud. Similarly, in Speech-to-Text (STT), when the input voice at the edge is noisy, its internal representations are sent to the Cloud for better processing. We present the results of encoder-decoder models for both speech-to-text as well as text-to-speech for both Kinyarwanda and Swahili.</p>\n\n",
                "matched_terms": [
                    "model",
                    "swahili",
                    "after",
                    "results",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remainder of the paper is organized as follows. Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S2\" title=\"II RELATED WORK &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> examines the related work. Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S3\" title=\"III APPROACH &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> gives an overview of the approach used, focusing on the architecture of the models considered. Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S4\" title=\"IV Model Training &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> shows the details of the model training conducted and the results obtained. Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S5\" title=\"V EXPERIMENTS AND RESULTS &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> presents the numerical results of the experiments and a qualitative discussion of the findings while Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S6\" title=\"VI Deployment On The Edge &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a> describes deployment on the edge. Finally, Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S7\" title=\"VII CONCLUSION AND FUTURE WORK &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">VII</span></a> concludes the paper and discusses possible future work.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in speech-to-text (STT) and text-to-speech (TTS) technologies depended on the significant progress made in machine learning and deep learning architectures. These advances emphasize the emerging challenges and opportunities in deploying these advanced models on edge devices, particularly for under-resourced languages like Kinyarwanda and Swahili. This section presents different approaches that cover STT and TTS technologies while targeting edge device computation.</p>\n\n",
                "matched_terms": [
                    "swahili",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent years, STT models have made significant strides, thanks to advances in deep neural networks. These models aim to transcribe spoken language into written text, with applications ranging from transcription services to voice assistants.\nThe survey conducted in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx12\" title=\"\">12</a>]</cite> provided a synopsis of speech-to-text models that incorporated deep neural networks. Among the various architectures present today, covering various aspects of this task including modeling, training, encoding, and decoding, transformer-based models, such as those presented in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx13\" title=\"\">13</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx14\" title=\"\">14</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx15\" title=\"\">15</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx16\" title=\"\">16</a>]</cite> have emerged as state-of-the-art, achieving impressive word error rates (WER) on challenging datasets like Librispeech. They reduced the WER trend to 3.7% - 1.8% in transcribing the Librispeech dataset.\nDeployment of the models mentioned above critically requires compute capability considerations for the platform. Taking the Whisper model, introduced in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx17\" title=\"\">17</a>]</cite>) as an example, through varying the number of attention layers, five types of the model were proposed: tiny (39 million parameters), base (74 million parameters), small (244 million parameters), medium (769 million parameters) and large (1550 million parameters). With such deep neural network models, tradeoffs between model size and accuracy need to be examined, per the specifications of the targeted edge device(s), as the results shown in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx17\" title=\"\">17</a>]</cite> showed that smaller models had poorer performance. However, network topologies that allow distributing a large model across more than a single device would enable workload sharing and maintain accuracy while meeting edge devices&#8217; constraints.</p>\n\n",
                "matched_terms": [
                    "whisper",
                    "model",
                    "training",
                    "word",
                    "datasets",
                    "wer",
                    "results",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development of EfficientSpeech aligns with the trend towards on-device processing highlighted in studies like \"Streaming End-to-End Speech Recognition for Mobile Devices\" <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx23\" title=\"\">23</a>]</cite> and \"Speech Recognition and Speech Synthesis Models for Micro Devices\" <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx24\" title=\"\">24</a>]</cite>. These works emphasize the importance of deploying speech recognition and synthesis models on resource-constrained devices like mobile phones and microcontrollers, enabling applications that require low latency and privacy preservation. LightGrad <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx25\" title=\"\">25</a>]</cite> takes this concept a step further by adopting a non-autoregressive approach that utilizes a lightweight U-Net architecture and streaming inference to achieve even lower latency. TTS technology is not limited to well-resourced languages. LRSpeech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx26\" title=\"\">26</a>]</cite> addresses the challenge of building TTS systems for languages with limited data availability. This model leverages pre-training on rich-resource languages, multi-task learning, and knowledge distillation to achieve high speech quality and recognition accuracy even with minimal training data. This paves the way to broader language coverage and caters to the needs of diverse communities, especially those with low-resource languages.</p>\n\n",
                "matched_terms": [
                    "step",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the paper <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx27\" title=\"\">27</a>]</cite>, the authors developed a multilingual Automatic Speech Recognition (ASR) model for Kinyarwanda, Swahili, and Luganda. They utilized the Common Voice project's African language datasets and fine-tuned a pre-trained Conformer model with Connectionist Temporal Classification (CTC) decoding and Byte Pair Encoding (BPE) tokenization. The results demonstrated that the Kinyarwanda model achieved a WER of 17.57, while the average WER across all three languages was 21.91.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuned",
                    "swahili",
                    "kinyarwanda",
                    "datasets",
                    "wer",
                    "results",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The authors <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx28\" title=\"\">28</a>]</cite> developed a text-to-speech (TTS) model for Kinyarwanda, a Bantu language spoken in Rwanda, by leveraging an existing Kinyarwanda speech-to-text (STT) model and aligning audio recordings of the Kinyarwanda Bible with their corresponding text using CTC-Segmentation23. This resulted in a dataset containing 67.84 hours of studio-quality audio from multiple speakers. The TTS model was trained using the YourTTS framework, which supports multilingual and multi-speaker capabilities. The synthesized audio achieved a WER of 30.09% compared to natural speech. Native Kinyarwanda speakers rated the naturalness of the synthesized speech with an average Mean Opinion Score (MOS) of 2.3 (on a scale of 1 to 5, where 5 represents natural human speech), and approximately 87% of the synthesized samples were rated as intelligible or partially intelligible by listeners.</p>\n\n",
                "matched_terms": [
                    "kinyarwanda",
                    "model",
                    "compared",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The paper <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx29\" title=\"\">29</a>]</cite> describes the development of a Kiswahili TTS system using Tacotron 2 architecture and WaveNet vocoder. Tacotron 2 is a sequence-to-sequence model that consists of an encoder, a decoder, and a vocoder. The encoder converts input text into a sequence of characters, the decoder predicts Mel-spectrograms for each character sequence, and the vocoder transforms Mel-spectrograms into speech waveforms. The Kiswahili TTS system achieved a Mean Opinion Score (MOS) of 4.05, indicating that the generated speech is comparable to human speech.</p>\n\n",
                "matched_terms": [
                    "model",
                    "indicating"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In these experiments, we explored the development of a speech-to-text synthesis system for Kinyarwanda and Swahili languages. We leveraged the power of a pre-trained model called Whisper (shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S3.F3\" title=\"Figure 3 &#8227; III-A Speech-To-Text based on Whisper model &#8227; III APPROACH &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) and fine-tuned it on Mozilla Common Voice datasets specific to the Kinyarwanda and Swahili languages <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx31\" title=\"\">31</a>]</cite>.\nWhisper, as it was introduced in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx17\" title=\"\">17</a>]</cite>, is a transformer-based neural network architecture designed for a variety of speech processing tasks, including automatic speech recognition (ASR) and speech translation. The main purpose of this model was to create an ASR model that &#8216;works reliably without the need for dataset-specific fine-tuning to achieve high-quality results on specific distributions&#8217;. It was trained on 680,000 hours of multilingual and multitasking-supervised data collected from the web. The Whisper architecture is based on an encoder-decoder transformer. This model enabled transcription in multiple languages as well as translation from those languages into English.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuned",
                    "swahili",
                    "kinyarwanda",
                    "datasets",
                    "finetuning",
                    "whisper",
                    "results",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whisper's end-to-end design converts audio input into a log-Mel spectrogram, which is processed by the model to produce text transcripts and additional information, simplifying traditional speech processing systems. The model's training on a diverse, multilingual dataset enables it to handle various speech characteristics and perform multiple tasks, including speech recognition and language identification. These characteristics made Whisper an ideal pre-trained model for our text-to-speech synthesis project in Kinyarwanda and Swahili. By fine-tuning Whisper on language-specific datasets, we specialized its capabilities for high-quality speech generation in the two targeted languages.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "swahili",
                    "datasets",
                    "finetuning",
                    "whisper",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the TTS case, we explored the usage of the SpeechT5 alternative for Kinyarwanda and Swahili language speech synthesis. Here, we leveraged the capabilities of a pre-trained model called SpeechT5, fine-tuned on the Mozilla Common Voice datasets for these languages.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuned",
                    "swahili",
                    "datasets",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SpeechT5's unified encoder-decoder architecture processes both speech and text, enabling tasks like text-to-speech synthesis through a shared representation of language. Pre-trained on extensive speech and text data, SpeechT5 has capabilities for understanding linguistic structures and generating natural speech in multiple languages. Its speech synthesis capabilities also allow effective tuning on specific datasets which improve performance in languages such as Kinyarwanda and Swahili.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "swahili",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both TTS and STT, we used the Mozilla Common Voice dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx31\" title=\"\">31</a>]</cite>, a diverse collection of community-supported Kinyarwanda and Kiswahili voice recordings and their transcription publicly available and widely recognized in speech technology research. We selected a subset dataset that comprised a total sample of 256 voice recordings, along with their corresponding textual transcriptions, for both Kiswahili and Kinyarwanda languages. To ensure the robustness and generalizability of our models, we divided the dataset into two distinct subsets: training and testing. The training set, which constitutes the majority of the data, is used to train our models, allowing them to learn the patterns and nuances of Kinyarwanda and Kiswahili speech. This process involves exposing the models to a wide array of vocal tones, dialects, and speech contexts, thereby enhancing their ability to accurately transcribe and synthesize speech in these languages. The testing set, on the other hand, serves a critical role in evaluating the performance of our models.</p>\n\n",
                "matched_terms": [
                    "training",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance the accuracy of our system, we implemented a cascading mechanism in both the STT and the TTS processes. For STT, we utilized the Whisper model at the edge. The model's final output logits were averaged and compared to a predefined threshold. If the average confidence level fell below this threshold, the cascading mechanism was triggered. This mechanism involved sending the internal representation of the audio data to the cloud for further processing, thereby leveraging the cloud's more robust computational resources to achieve higher accuracy.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whisper",
                    "compared"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Whisper ASR model is based on a standard Transformer-based encoder-decoder architecture. It follows the architecture of a log-Mel spectrogram input to the encoder, with cross-attention mechanisms connecting the encoder and decoder. The decoder autoregressively predicts text tokens, conditioned on the encoder&#8217;s hidden states and previously predicted tokens. The model comes in various configurations, including tiny, base, small, medium, and large. Each configuration has different numbers of layers, widths, and heads. In this paper, we fine-tuned the multilingual version of the &#8220;small&#8221; checkpoint with approximately 244 million parameters.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whisper",
                    "finetuned",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During fine-tuning, we employed several hyperparameters and a structured training process. The learning rate was set to 1e-5 to allow for gradual weight updates during training, and warmup_steps were used to gradually increase the learning rate during the initial stages. The training process was defined by a set number of max_steps and included gradient_accumulation_steps to accumulate gradients, simulating larger batch sizes for efficiency. We enabled mixed-precision training (fp16) for faster convergence. The evaluation strategy involved assessing the model every 1000 steps, with the generation_max_length set to 225 to define the maximum output sequence length during inference. Model checkpoints were saved every 1000 steps, and the model was evaluated on the test set at the same interval. Training progress was logged every 25 steps to keep track of the model's performance. The metric for selecting the best model was based on the word error rate (WER), and the best model was loaded at the end of the training process. Finally, we enabled push_to_hub, allowing the trained model to be shared and version-controlled on the Hugging Face Hub.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "rate",
                    "word",
                    "wer",
                    "finetuning",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the performance of our fine-tuned model, we used the WER metric. WER calculates the number of errors (insertions, substitutions, and deletions) an automatic speech recognition system makes when compared to a human-generated reference transcript. In the context of speech-to-text transcription, a lower WER indicates a higher fidelity between the synthesized speech and the original written text.</p>\n\n",
                "matched_terms": [
                    "errors",
                    "model",
                    "transcript",
                    "finetuned",
                    "humangenerated",
                    "compared",
                    "deletions",
                    "reference",
                    "substitutions",
                    "insertions",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tuned the Speech-T5 model using the <span class=\"ltx_text ltx_font_italic\">Seq2SeqTrainer</span> from the Transformers library. The process began with tokenizing the input text, and converting it into a sequence of token IDs. These token IDs, along with speaker embeddings, were then fed into the encoder. The encoder processed these inputs to generate hidden state representations. The decoder was subsequently trained to predict the log-Mel spectrogram from these hidden states, effectively learning to map the text and speaker information to the corresponding speech features. The log-Mel spectrogram would then be converted to audio using Microsoft's SpeechT5HifiGan. This end-to-end training approach allowed the model to generate speech outputs from textual inputs.</p>\n\n",
                "matched_terms": [
                    "finetuned",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This configuration batch size for training is set to 16 examples, and gradients are accumulated over 2 steps before updating the model to improve efficiency. The learning rate is established at 1e-5, with 500 warmup steps to gradually increase it. Training will conclude after 4000 steps. Gradient checkpointing is enabled to save memory, and mixed precision (FP16) training is utilized for faster computations.</p>\n\n",
                "matched_terms": [
                    "after",
                    "model",
                    "rate",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The model was evaluated every 1000 steps, with an evaluation batch size of 8 examples. Model checkpoints are also saved every 1000 steps. Training progress is logged every 25 steps, with logs reported to TensorBoard <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_href\" href=\"https://www.tensorflow.org/tensorboard\" title=\"\">https://www.tensorflow.org/tensorboard</a></span></span></span> for better tracking. At the end of training, the best model based on validation performance will be loaded. The target label name is specified as &#8220;labels&#8221; for the dataset.</p>\n\n",
                "matched_terms": [
                    "validation",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this text-to-speech exploration, we employed mean-square error loss (MSE loss) as a performance indicator. During model training, the model is exposed to both training and validation data. The training data is used to adjust the model's internal parameters, while the validation data helps assess how well the model generalizes to unseen examples. Validation loss measures the model's performance on the validation data. Lower validation loss signifies better model performance during fine-tuning, indicating the model is learning to generate speech that aligns well with the ground truth (written text).</p>\n\n",
                "matched_terms": [
                    "validation",
                    "model",
                    "training",
                    "loss",
                    "indicating",
                    "finetuning",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S5.T2\" title=\"TABLE II &#8227; V EXPERIMENTS AND RESULTS &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> presents the results of fine-tuning the SpeechT5 text-to-speech model on Swahili and Kinyarwanda datasets and reveal distinct trends in training and validation MSE losses, reflecting the model's adaptability and learning efficiency across different languages. For Swahili, the training loss shows a consistent decrease from 0.59 at step 1000 to 0.56 at step 4000, accompanied by a parallel reduction in validation loss from 0.55 to 0.52. This steady improvement across both metrics indicates a robust enhancement in the model's performance, suggesting effective learning and generalization on the Swahili dataset.</p>\n\n",
                "matched_terms": [
                    "validation",
                    "model",
                    "training",
                    "swahili",
                    "step",
                    "loss",
                    "datasets",
                    "losses",
                    "finetuning",
                    "results",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Kinyarwanda results display more fluctuating patterns in the MSE losses. The training loss significantly drops from 0.69 at step 1000 to 0.31 at step 3000, only to rise again to 0.53 at step 4000. The validation loss mirrors this volatility, starting at 0.991951, decreasing to 0.93, rising slightly, and then dropping sharply to 0.49 by step 4000. This erratic behavior suggests initial challenges in model generalization on the Kinyarwanda dataset, but ultimately, the substantial decrease in validation loss by the final step indicates significant progress, highlighting the model's capacity to adjust and improve through the training process.</p>\n\n",
                "matched_terms": [
                    "validation",
                    "model",
                    "training",
                    "step",
                    "loss",
                    "losses",
                    "results",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On a computer equipped with 16 GB of memory and a 1.7 GHz CPU, NetLimiter <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx35\" title=\"\">35</a>]</cite> was employed to simulate various local network bandwidths. This setup was used to compare the inference times required to generate speech for both short and long texts using the finetuned SpeechT5 model. The simulation involved a simple Swahili greeting, &#8220;Habari gani?&#8221; (literally translating to &#8220;How is it going?&#8221;), consisting of 12 characters, and the first paragraph from <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx36\" title=\"\">36</a>]</cite> translated into Swahili, resulting in a text of 270 characters.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuned",
                    "swahili"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For text transcription, the same method was applied to simulate various bandwidths, using the generated speech audios as input and the finetuned Whisper model within the cascaded framework. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S6.F6\" title=\"Figure 6 &#8227; VI Deployment On The Edge &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> illustrates the inference times for a 1-second &#8220;Habari Gani?&#8221; audio and a 19-second audio derived from a 270-character text, in relation to bandwidth limits. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S6.F6\" title=\"Figure 6 &#8227; VI Deployment On The Edge &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, the inference time generally remains constant regardless of sequence length, due to the fixed length of the model&#8217;s encoder output, which requires the same duration to transmit back to the edge model. However, both graphs indicate a significant increase in inference time when the bandwidth drops below 1024 KB/s. These results suggest that both short and long audio files will take nearly the same time to process. Consequently, a minimum bandwidth of 1024 KB/s (1 MB/s) is recommended for deploying this framework, as detailed in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S6.F7\" title=\"Figure 7 &#8227; VI Deployment On The Edge &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whisper",
                    "finetuned",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to bandwidth considerations, the computing capabilities of the edge devices are also very important. Monitoring the models&#8217; sizes, in terms of parameters and data types, reveals that the cascaded framework utilizes 38% of the SpeechT5 model and 56% of the Whisper model on the edge&#8217;s memory. The edge device must manage a load of 226 MB for the SpeechT5 model and 567 MB for the Whisper model, which can be reduced by up to 25% through quantizing the model parameters from FP32 to INT8 data types <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx38\" title=\"\">38</a>]</cite>. Consequently, the cascaded framework theoretically requires edge devices to provide at least 57 MB (25% of 226 MB) of memory for the SpeechT5 model and at least 149 MB (25% of 567 MB) for the Whisper model. This translates to an overall edge memory usage of 9.5% for the SpeechT5 model and 14% for the Whisper model which seems quite reasonable.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Considering the computation speed of edge devices is important as it highlights the performance of devices used in the East African region. The experiments in this study utilized an edge device with a 1.7 GHz CPU clock rate. Given that the model inference on the edge is not parallelized, the CPU time for any other edge device can be estimated, as it is inversely proportional to the clock rate. Using the market data on mobile phones in Kenya <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx39\" title=\"\">39</a>]</cite> and the phone models&#8217; specifications dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx40\" title=\"\">40</a>]</cite>, CPU time approximations were calculated based on the average CPU times observed on the 1.7 GHz CPU device.</p>\n\n",
                "matched_terms": [
                    "given",
                    "model",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The paper presents a novel approach for edge-based speech transcription and synthesis targeting the Kinyarwanda and Swahili languages, leveraging a hybrid edge-cloud network architecture. This system processes speech data locally on edge devices and escalates to cloud-based resources when necessary, facilitating efficient use of computational resources and enhanced accessibility. Key advantages of this approach include the efficient handling of local language processing on low-resource devices which are typical in Eastern Africa, enabling real-time speech processing, and reducing reliance on cloud compute services. By integrating local processing capabilities with cloud-based support, the system ensures that speech data can be managed efficiently, even in regions with limited access to high-end computational infrastructure. This hybrid approach not only optimizes resource usage but also enhances the accessibility and reliability of speech processing technologies in underrepresented languages, such as Kinyarwanda and Swahili.</p>\n\n",
                "matched_terms": [
                    "swahili",
                    "kinyarwanda"
                ]
            }
        ]
    },
    "S5.T2": {
        "source_file": "Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages",
        "caption": "TABLE II:  Results after Fine-tuning the SpeechT5 Text-to-Speech Model on Swahili and Kinyarwanda Datasets: Training and validation losses are measured using mean squared error (MSE) loss.",
        "body": "Swahili Fine-tuned SpeechT5 Model Results\nKinyarwanda Fine-tuned SpeechT5 Model Results\n\n\nStep\nTraining Loss\nValidation Loss\nStep\nTraining Loss(\nValidation Loss\n\n\n1000\n0.598900\n0.553199\n1000\n0.695200\n0.991951\n\n\n2000\n0.564900\n0.534779\n2000\n0.477100\n0.926026\n\n\n3000\n0.562600\n0.526816\n3000\n0.313200\n0.950552\n\n\n4000\n0.556600\n0.523976\n4000\n0.533400\n0.497876",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t\" colspan=\"3\">Swahili Fine-tuned SpeechT5 Model Results</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\">Kinyarwanda Fine-tuned SpeechT5 Model Results</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Step</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Training Loss</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Validation Loss</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Step</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Training Loss(</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Validation Loss</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">1000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.598900</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">0.553199</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.695200</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.991951</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">2000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.564900</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">0.534779</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.477100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.926026</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">3000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.562600</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">0.526816</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.313200</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.950552</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">4000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.556600</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t\">0.523976</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">4000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.533400</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.497876</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "training",
            "step",
            "datasets",
            "loss",
            "speecht5",
            "losses",
            "finetuning",
            "swahili",
            "mean",
            "results",
            "validation",
            "model",
            "measured",
            "texttospeech",
            "finetuned",
            "mse",
            "squared",
            "after",
            "error",
            "kinyarwanda"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S5.T2\" title=\"TABLE II &#8227; V EXPERIMENTS AND RESULTS &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> presents the results of fine-tuning the SpeechT5 text-to-speech model on Swahili and Kinyarwanda datasets and reveal distinct trends in training and validation MSE losses, reflecting the model's adaptability and learning efficiency across different languages. For Swahili, the training loss shows a consistent decrease from 0.59 at step 1000 to 0.56 at step 4000, accompanied by a parallel reduction in validation loss from 0.55 to 0.52. This steady improvement across both metrics indicates a robust enhancement in the model's performance, suggesting effective learning and generalization on the Swahili dataset.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This paper presents a novel framework for speech transcription and synthesis, leveraging edge-cloud parallelism to enhance processing speed and accessibility for Kinyarwanda and Swahili speakers. It addresses the scarcity of powerful language processing tools for these widely spoken languages in East African countries with limited technological infrastructure. The framework utilizes the Whisper and SpeechT5 pre-trained models to enable speech-to-text (STT) and text-to-speech (TTS) translation. The architecture uses a cascading mechanism that distributes the model inference workload between the edge device and the cloud, thereby reducing latency and resource usage, benefiting both ends. On the edge device, our approach achieves a memory usage compression of 9.5% for the SpeechT5 model and 14% for the Whisper model, with a maximum memory usage of 149 MB. Experimental results indicate that on a 1.7 GHz CPU edge device with a 1 MB/s network bandwidth, the system can process a 270-character text in less than a minute for both speech-to-text and text-to-speech transcription. Using real-world survey data from Kenya, it is shown that the cascaded edge-cloud architecture proposed could easily serve as an excellent platform for STT and TTS transcription with good accuracy and response time.</p>\n\n",
                "matched_terms": [
                    "model",
                    "texttospeech",
                    "swahili",
                    "speecht5",
                    "results",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In today&#8217;s digital age, the need for accurate and efficient speech transcription and synthesis models has been increasing rapidly. These models play an important role in a variety of applications, such as learning new language(s), accessibility tools for people with difficulties in reading and hearing, as well as automated voice assistants <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx1\" title=\"\">1</a>]</cite>. Kinyarwanda and Swahili are two of the local languages spoken in East Africa. While Swahili is the most widely spoken language in Eastern Africa, the speakers range from 60 million to over 150 million <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx2\" title=\"\">2</a>]</cite>. Swahili serves as the national language of four African nations: Tanzania, Kenya, Uganda, Rwanda, and the Democratic Republic of the Congo. On the other hand, Kinyarwanda is the national language of Rwanda, spoken by approximately 24 million people in Rwanda and beyond <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx3\" title=\"\">3</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "swahili",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the past, different research efforts have been reported to develop Edge-Based speech transcription and synthesis models for languages, including under-resourced ones. For instance, EfficientSpeech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx5\" title=\"\">5</a>]</cite> provides an On-Device text-to-speech model designed specifically for edge devices. This model aims to provide natural-sounding speech synthesis while being efficient enough to run on a local device such as a smartphone. To name a few, other approaches such as Tacotron <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx6\" title=\"\">6</a>]</cite>, Tacotron 2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx7\" title=\"\">7</a>]</cite>, and WaveGlow <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx8\" title=\"\">8</a>]</cite>, have pushed the boundaries of end-to-end generative text-to-speech models.</p>\n\n",
                "matched_terms": [
                    "model",
                    "texttospeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this shortcoming, in this paper, we present a novel, computationally efficient cascading approach for edge-cloud speech transcription and synthesis, specifically designed for Kinyarwanda and Swahili languages. This model and the underlying mechanism was inspired by the approach proposed in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx11\" title=\"\">11</a>]</cite>, a cascading neural network to efficiently couple the computation between models deployed at the edge as well as the cloud. In a nutshell, a cascading neural network is capable of distributing a deep neural network between a local device and the cloud while keeping the required communication network traffic to a minimum. The network begins processing on the constrained device and only relies on the remote part (i.e., the Cloud) when the local part does not provide an accurate or fast enough result. For instance, in Text-to-Speech (TTS), when the processed output voice after the edge processing is very noisy, it might be necessary to send the internal representation to the Cloud. Similarly, in Speech-to-Text (STT), when the input voice at the edge is noisy, its internal representations are sent to the Cloud for better processing. We present the results of encoder-decoder models for both speech-to-text as well as text-to-speech for both Kinyarwanda and Swahili.</p>\n\n",
                "matched_terms": [
                    "model",
                    "texttospeech",
                    "swahili",
                    "after",
                    "results",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remainder of the paper is organized as follows. Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S2\" title=\"II RELATED WORK &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> examines the related work. Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S3\" title=\"III APPROACH &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> gives an overview of the approach used, focusing on the architecture of the models considered. Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S4\" title=\"IV Model Training &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> shows the details of the model training conducted and the results obtained. Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S5\" title=\"V EXPERIMENTS AND RESULTS &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> presents the numerical results of the experiments and a qualitative discussion of the findings while Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S6\" title=\"VI Deployment On The Edge &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a> describes deployment on the edge. Finally, Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S7\" title=\"VII CONCLUSION AND FUTURE WORK &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">VII</span></a> concludes the paper and discusses possible future work.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in speech-to-text (STT) and text-to-speech (TTS) technologies depended on the significant progress made in machine learning and deep learning architectures. These advances emphasize the emerging challenges and opportunities in deploying these advanced models on edge devices, particularly for under-resourced languages like Kinyarwanda and Swahili. This section presents different approaches that cover STT and TTS technologies while targeting edge device computation.</p>\n\n",
                "matched_terms": [
                    "texttospeech",
                    "swahili",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent years, STT models have made significant strides, thanks to advances in deep neural networks. These models aim to transcribe spoken language into written text, with applications ranging from transcription services to voice assistants.\nThe survey conducted in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx12\" title=\"\">12</a>]</cite> provided a synopsis of speech-to-text models that incorporated deep neural networks. Among the various architectures present today, covering various aspects of this task including modeling, training, encoding, and decoding, transformer-based models, such as those presented in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx13\" title=\"\">13</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx14\" title=\"\">14</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx15\" title=\"\">15</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx16\" title=\"\">16</a>]</cite> have emerged as state-of-the-art, achieving impressive word error rates (WER) on challenging datasets like Librispeech. They reduced the WER trend to 3.7% - 1.8% in transcribing the Librispeech dataset.\nDeployment of the models mentioned above critically requires compute capability considerations for the platform. Taking the Whisper model, introduced in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx17\" title=\"\">17</a>]</cite>) as an example, through varying the number of attention layers, five types of the model were proposed: tiny (39 million parameters), base (74 million parameters), small (244 million parameters), medium (769 million parameters) and large (1550 million parameters). With such deep neural network models, tradeoffs between model size and accuracy need to be examined, per the specifications of the targeted edge device(s), as the results shown in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx17\" title=\"\">17</a>]</cite> showed that smaller models had poorer performance. However, network topologies that allow distributing a large model across more than a single device would enable workload sharing and maintain accuracy while meeting edge devices&#8217; constraints.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "datasets",
                    "results",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One approach to achieving speech synthesis involves leveraging neural network architectures with a focus on maximizing audio quality. One of the current state-of-the-art approaches is FastSpeech 2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx18\" title=\"\">18</a>]</cite>. FastSpeech 2 is an end-to-end text-to-speech synthesis model that focuses on generating non-autoregressive mel-spectrograms directly from text. FastSpeech 2 has been recognized for its ability to produce high-quality speech output efficiently. Despite its remarkable capabilities in generating natural-sounding speech, its computational cost and memory footprint limit its deployment on devices with limited resources.\nRecognizing the limitations of high-resource models, researchers are actively exploring alternative strategies for building efficient and resource-constrained TTS systems. EfficientSpeech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx19\" title=\"\">19</a>]</cite> introduces a novel TTS model designed specifically for edge devices. This model aims to provide natural-sounding speech synthesis while being efficient enough to run on-device. EfficientSpeech joins a sequence of advancements in TTS technology, such as Tacotron <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx20\" title=\"\">20</a>]</cite>, Tacotron 2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx21\" title=\"\">21</a>]</cite>, and WaveGlow <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx22\" title=\"\">22</a>]</cite>, which have pushed the boundaries of text-to-speech models.</p>\n\n",
                "matched_terms": [
                    "model",
                    "texttospeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development of EfficientSpeech aligns with the trend towards on-device processing highlighted in studies like \"Streaming End-to-End Speech Recognition for Mobile Devices\" <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx23\" title=\"\">23</a>]</cite> and \"Speech Recognition and Speech Synthesis Models for Micro Devices\" <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx24\" title=\"\">24</a>]</cite>. These works emphasize the importance of deploying speech recognition and synthesis models on resource-constrained devices like mobile phones and microcontrollers, enabling applications that require low latency and privacy preservation. LightGrad <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx25\" title=\"\">25</a>]</cite> takes this concept a step further by adopting a non-autoregressive approach that utilizes a lightweight U-Net architecture and streaming inference to achieve even lower latency. TTS technology is not limited to well-resourced languages. LRSpeech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx26\" title=\"\">26</a>]</cite> addresses the challenge of building TTS systems for languages with limited data availability. This model leverages pre-training on rich-resource languages, multi-task learning, and knowledge distillation to achieve high speech quality and recognition accuracy even with minimal training data. This paves the way to broader language coverage and caters to the needs of diverse communities, especially those with low-resource languages.</p>\n\n",
                "matched_terms": [
                    "step",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the paper <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx27\" title=\"\">27</a>]</cite>, the authors developed a multilingual Automatic Speech Recognition (ASR) model for Kinyarwanda, Swahili, and Luganda. They utilized the Common Voice project's African language datasets and fine-tuned a pre-trained Conformer model with Connectionist Temporal Classification (CTC) decoding and Byte Pair Encoding (BPE) tokenization. The results demonstrated that the Kinyarwanda model achieved a WER of 17.57, while the average WER across all three languages was 21.91.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuned",
                    "swahili",
                    "datasets",
                    "results",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The authors <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx28\" title=\"\">28</a>]</cite> developed a text-to-speech (TTS) model for Kinyarwanda, a Bantu language spoken in Rwanda, by leveraging an existing Kinyarwanda speech-to-text (STT) model and aligning audio recordings of the Kinyarwanda Bible with their corresponding text using CTC-Segmentation23. This resulted in a dataset containing 67.84 hours of studio-quality audio from multiple speakers. The TTS model was trained using the YourTTS framework, which supports multilingual and multi-speaker capabilities. The synthesized audio achieved a WER of 30.09% compared to natural speech. Native Kinyarwanda speakers rated the naturalness of the synthesized speech with an average Mean Opinion Score (MOS) of 2.3 (on a scale of 1 to 5, where 5 represents natural human speech), and approximately 87% of the synthesized samples were rated as intelligible or partially intelligible by listeners.</p>\n\n",
                "matched_terms": [
                    "mean",
                    "model",
                    "texttospeech",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The paper <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx29\" title=\"\">29</a>]</cite> describes the development of a Kiswahili TTS system using Tacotron 2 architecture and WaveNet vocoder. Tacotron 2 is a sequence-to-sequence model that consists of an encoder, a decoder, and a vocoder. The encoder converts input text into a sequence of characters, the decoder predicts Mel-spectrograms for each character sequence, and the vocoder transforms Mel-spectrograms into speech waveforms. The Kiswahili TTS system achieved a Mean Opinion Score (MOS) of 4.05, indicating that the generated speech is comparable to human speech.</p>\n\n",
                "matched_terms": [
                    "mean",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In these experiments, we explored the development of a speech-to-text synthesis system for Kinyarwanda and Swahili languages. We leveraged the power of a pre-trained model called Whisper (shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S3.F3\" title=\"Figure 3 &#8227; III-A Speech-To-Text based on Whisper model &#8227; III APPROACH &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) and fine-tuned it on Mozilla Common Voice datasets specific to the Kinyarwanda and Swahili languages <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx31\" title=\"\">31</a>]</cite>.\nWhisper, as it was introduced in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx17\" title=\"\">17</a>]</cite>, is a transformer-based neural network architecture designed for a variety of speech processing tasks, including automatic speech recognition (ASR) and speech translation. The main purpose of this model was to create an ASR model that &#8216;works reliably without the need for dataset-specific fine-tuning to achieve high-quality results on specific distributions&#8217;. It was trained on 680,000 hours of multilingual and multitasking-supervised data collected from the web. The Whisper architecture is based on an encoder-decoder transformer. This model enabled transcription in multiple languages as well as translation from those languages into English.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuned",
                    "swahili",
                    "datasets",
                    "finetuning",
                    "results",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whisper's end-to-end design converts audio input into a log-Mel spectrogram, which is processed by the model to produce text transcripts and additional information, simplifying traditional speech processing systems. The model's training on a diverse, multilingual dataset enables it to handle various speech characteristics and perform multiple tasks, including speech recognition and language identification. These characteristics made Whisper an ideal pre-trained model for our text-to-speech synthesis project in Kinyarwanda and Swahili. By fine-tuning Whisper on language-specific datasets, we specialized its capabilities for high-quality speech generation in the two targeted languages.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "swahili",
                    "texttospeech",
                    "datasets",
                    "finetuning",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the TTS case, we explored the usage of the SpeechT5 alternative for Kinyarwanda and Swahili language speech synthesis. Here, we leveraged the capabilities of a pre-trained model called SpeechT5, fine-tuned on the Mozilla Common Voice datasets for these languages.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuned",
                    "swahili",
                    "datasets",
                    "speecht5",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S3.F4\" title=\"Figure 4 &#8227; III-B Text-To-Speech, using SpeechT5 model &#8227; III APPROACH &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows, SpeechT5 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx32\" title=\"\">32</a>]</cite>, is a unified pre-trained model designed for various spoken language processing tasks. This model builds upon the success of T5 (Text-to-Text Transfer Transformer) but extends its capabilities to the speech domain. Below is a detailed description of SpeechT5's key features:</p>\n\n",
                "matched_terms": [
                    "model",
                    "speecht5"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SpeechT5's unified encoder-decoder architecture processes both speech and text, enabling tasks like text-to-speech synthesis through a shared representation of language. Pre-trained on extensive speech and text data, SpeechT5 has capabilities for understanding linguistic structures and generating natural speech in multiple languages. Its speech synthesis capabilities also allow effective tuning on specific datasets which improve performance in languages such as Kinyarwanda and Swahili.</p>\n\n",
                "matched_terms": [
                    "texttospeech",
                    "swahili",
                    "datasets",
                    "speecht5",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both TTS and STT, we used the Mozilla Common Voice dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx31\" title=\"\">31</a>]</cite>, a diverse collection of community-supported Kinyarwanda and Kiswahili voice recordings and their transcription publicly available and widely recognized in speech technology research. We selected a subset dataset that comprised a total sample of 256 voice recordings, along with their corresponding textual transcriptions, for both Kiswahili and Kinyarwanda languages. To ensure the robustness and generalizability of our models, we divided the dataset into two distinct subsets: training and testing. The training set, which constitutes the majority of the data, is used to train our models, allowing them to learn the patterns and nuances of Kinyarwanda and Kiswahili speech. This process involves exposing the models to a wide array of vocal tones, dialects, and speech contexts, thereby enhancing their ability to accurately transcribe and synthesize speech in these languages. The testing set, on the other hand, serves a critical role in evaluating the performance of our models.</p>\n\n",
                "matched_terms": [
                    "training",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Whisper ASR model is based on a standard Transformer-based encoder-decoder architecture. It follows the architecture of a log-Mel spectrogram input to the encoder, with cross-attention mechanisms connecting the encoder and decoder. The decoder autoregressively predicts text tokens, conditioned on the encoder&#8217;s hidden states and previously predicted tokens. The model comes in various configurations, including tiny, base, small, medium, and large. Each configuration has different numbers of layers, widths, and heads. In this paper, we fine-tuned the multilingual version of the &#8220;small&#8221; checkpoint with approximately 244 million parameters.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During fine-tuning, we employed several hyperparameters and a structured training process. The learning rate was set to 1e-5 to allow for gradual weight updates during training, and warmup_steps were used to gradually increase the learning rate during the initial stages. The training process was defined by a set number of max_steps and included gradient_accumulation_steps to accumulate gradients, simulating larger batch sizes for efficiency. We enabled mixed-precision training (fp16) for faster convergence. The evaluation strategy involved assessing the model every 1000 steps, with the generation_max_length set to 225 to define the maximum output sequence length during inference. Model checkpoints were saved every 1000 steps, and the model was evaluated on the test set at the same interval. Training progress was logged every 25 steps to keep track of the model's performance. The metric for selecting the best model was based on the word error rate (WER), and the best model was loaded at the end of the training process. Finally, we enabled push_to_hub, allowing the trained model to be shared and version-controlled on the Hugging Face Hub.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "model",
                    "training",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the performance of our fine-tuned model, we used the WER metric. WER calculates the number of errors (insertions, substitutions, and deletions) an automatic speech recognition system makes when compared to a human-generated reference transcript. In the context of speech-to-text transcription, a lower WER indicates a higher fidelity between the synthesized speech and the original written text.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tuned the Speech-T5 model using the <span class=\"ltx_text ltx_font_italic\">Seq2SeqTrainer</span> from the Transformers library. The process began with tokenizing the input text, and converting it into a sequence of token IDs. These token IDs, along with speaker embeddings, were then fed into the encoder. The encoder processed these inputs to generate hidden state representations. The decoder was subsequently trained to predict the log-Mel spectrogram from these hidden states, effectively learning to map the text and speaker information to the corresponding speech features. The log-Mel spectrogram would then be converted to audio using Microsoft's SpeechT5HifiGan. This end-to-end training approach allowed the model to generate speech outputs from textual inputs.</p>\n\n",
                "matched_terms": [
                    "finetuned",
                    "model",
                    "training",
                    "speecht5"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This configuration batch size for training is set to 16 examples, and gradients are accumulated over 2 steps before updating the model to improve efficiency. The learning rate is established at 1e-5, with 500 warmup steps to gradually increase it. Training will conclude after 4000 steps. Gradient checkpointing is enabled to save memory, and mixed precision (FP16) training is utilized for faster computations.</p>\n\n",
                "matched_terms": [
                    "after",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The model was evaluated every 1000 steps, with an evaluation batch size of 8 examples. Model checkpoints are also saved every 1000 steps. Training progress is logged every 25 steps, with logs reported to TensorBoard <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_href\" href=\"https://www.tensorflow.org/tensorboard\" title=\"\">https://www.tensorflow.org/tensorboard</a></span></span></span> for better tracking. At the end of training, the best model based on validation performance will be loaded. The target label name is specified as &#8220;labels&#8221; for the dataset.</p>\n\n",
                "matched_terms": [
                    "validation",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this text-to-speech exploration, we employed mean-square error loss (MSE loss) as a performance indicator. During model training, the model is exposed to both training and validation data. The training data is used to adjust the model's internal parameters, while the validation data helps assess how well the model generalizes to unseen examples. Validation loss measures the model's performance on the validation data. Lower validation loss signifies better model performance during fine-tuning, indicating the model is learning to generate speech that aligns well with the ground truth (written text).</p>\n\n",
                "matched_terms": [
                    "validation",
                    "model",
                    "training",
                    "texttospeech",
                    "mse",
                    "loss",
                    "finetuning",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Whisper model was fine-tuned for each language independently, where the checkpoints in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx33\" title=\"\">33</a>]</cite> and <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx34\" title=\"\">34</a>]</cite> were used for Swahili and Kinyarwanda, repsectively. The process achieved a WER of 26.37% for Swahili and 33.42% for Kinyarwanda. The training process did not significantly change the initial WER. However, the improving trends of the cross-entropy loss (shown in in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S5.T1\" title=\"TABLE I &#8227; V EXPERIMENTS AND RESULTS &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>) promise improved performances, given additional model finetuning.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "swahili",
                    "finetuned",
                    "loss",
                    "finetuning",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Kinyarwanda results display more fluctuating patterns in the MSE losses. The training loss significantly drops from 0.69 at step 1000 to 0.31 at step 3000, only to rise again to 0.53 at step 4000. The validation loss mirrors this volatility, starting at 0.991951, decreasing to 0.93, rising slightly, and then dropping sharply to 0.49 by step 4000. This erratic behavior suggests initial challenges in model generalization on the Kinyarwanda dataset, but ultimately, the substantial decrease in validation loss by the final step indicates significant progress, highlighting the model's capacity to adjust and improve through the training process.</p>\n\n",
                "matched_terms": [
                    "validation",
                    "model",
                    "training",
                    "step",
                    "mse",
                    "loss",
                    "losses",
                    "results",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study presents a cascading architecture that utilizes both edge and Cloud resources for the deployment of Text-to-Speech (TTS) and Speech-to-Text (STT) models. The architecture incorporates two distinct models for each task: a compressed encoder model deployed at the edge and a non-compressed encoder model hosted in the Cloud. As depicted in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S3.F2\" title=\"Figure 2 &#8227; III APPROACH &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the transformer model is deployed in a cascaded manner, allowing engineered features from the edge model&#8217;s encoder pre-net layer to be transmitted to the Cloud model, and enhanced hidden states to be retrieved from the Cloud back to the edge.</p>\n\n",
                "matched_terms": [
                    "model",
                    "texttospeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On a computer equipped with 16 GB of memory and a 1.7 GHz CPU, NetLimiter <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx35\" title=\"\">35</a>]</cite> was employed to simulate various local network bandwidths. This setup was used to compare the inference times required to generate speech for both short and long texts using the finetuned SpeechT5 model. The simulation involved a simple Swahili greeting, &#8220;Habari gani?&#8221; (literally translating to &#8220;How is it going?&#8221;), consisting of 12 characters, and the first paragraph from <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx36\" title=\"\">36</a>]</cite> translated into Swahili, resulting in a text of 270 characters.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuned",
                    "swahili",
                    "speecht5"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For text transcription, the same method was applied to simulate various bandwidths, using the generated speech audios as input and the finetuned Whisper model within the cascaded framework. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S6.F6\" title=\"Figure 6 &#8227; VI Deployment On The Edge &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> illustrates the inference times for a 1-second &#8220;Habari Gani?&#8221; audio and a 19-second audio derived from a 270-character text, in relation to bandwidth limits. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S6.F6\" title=\"Figure 6 &#8227; VI Deployment On The Edge &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, the inference time generally remains constant regardless of sequence length, due to the fixed length of the model&#8217;s encoder output, which requires the same duration to transmit back to the edge model. However, both graphs indicate a significant increase in inference time when the bandwidth drops below 1024 KB/s. These results suggest that both short and long audio files will take nearly the same time to process. Consequently, a minimum bandwidth of 1024 KB/s (1 MB/s) is recommended for deploying this framework, as detailed in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#S6.F7\" title=\"Figure 7 &#8227; VI Deployment On The Edge &#8227; Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to bandwidth considerations, the computing capabilities of the edge devices are also very important. Monitoring the models&#8217; sizes, in terms of parameters and data types, reveals that the cascaded framework utilizes 38% of the SpeechT5 model and 56% of the Whisper model on the edge&#8217;s memory. The edge device must manage a load of 226 MB for the SpeechT5 model and 567 MB for the Whisper model, which can be reduced by up to 25% through quantizing the model parameters from FP32 to INT8 data types <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16497v1#bib.bibx38\" title=\"\">38</a>]</cite>. Consequently, the cascaded framework theoretically requires edge devices to provide at least 57 MB (25% of 226 MB) of memory for the SpeechT5 model and at least 149 MB (25% of 567 MB) for the Whisper model. This translates to an overall edge memory usage of 9.5% for the SpeechT5 model and 14% for the Whisper model which seems quite reasonable.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speecht5"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The paper presents a novel approach for edge-based speech transcription and synthesis targeting the Kinyarwanda and Swahili languages, leveraging a hybrid edge-cloud network architecture. This system processes speech data locally on edge devices and escalates to cloud-based resources when necessary, facilitating efficient use of computational resources and enhanced accessibility. Key advantages of this approach include the efficient handling of local language processing on low-resource devices which are typical in Eastern Africa, enabling real-time speech processing, and reducing reliance on cloud compute services. By integrating local processing capabilities with cloud-based support, the system ensures that speech data can be managed efficiently, even in regions with limited access to high-end computational infrastructure. This hybrid approach not only optimizes resource usage but also enhances the accessibility and reliability of speech processing technologies in underrepresented languages, such as Kinyarwanda and Swahili.</p>\n\n",
                "matched_terms": [
                    "swahili",
                    "kinyarwanda"
                ]
            }
        ]
    }
}