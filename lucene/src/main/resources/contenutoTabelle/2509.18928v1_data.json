{
    "S4.T1": {
        "caption": "Table 1: Selected objective evaluation results for Task A.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">F0V </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SIM </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">WER </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">KL </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Base Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">14.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.770</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8212;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Best-of-16</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">22.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.770</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.74</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8212;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Best-of-64</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.770</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8212;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><math alttext=\"\\text{RAFT }^{\\text{300 steps}}_{\\text{iter }1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m5\" intent=\":literal\"><semantics><msubsup><mtext mathsize=\"0.900em\">RAFT&#160;</mtext><mrow><mtext mathsize=\"0.900em\">iter&#160;</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn mathsize=\"0.900em\">1</mn></mrow><mtext mathsize=\"0.900em\">300 steps</mtext></msubsup><annotation encoding=\"application/x-tex\">\\text{RAFT }^{\\text{300 steps}}_{\\text{iter }1}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.763</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.057</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><math alttext=\"\\text{RAFT }^{\\text{300 steps}}_{\\text{iter }2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m6\" intent=\":literal\"><semantics><msubsup><mtext mathsize=\"0.900em\">RAFT&#160;</mtext><mrow><mtext mathsize=\"0.900em\">iter&#160;</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn mathsize=\"0.900em\">2</mn></mrow><mtext mathsize=\"0.900em\">300 steps</mtext></msubsup><annotation encoding=\"application/x-tex\">\\text{RAFT }^{\\text{300 steps}}_{\\text{iter }2}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.758</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.91</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.230</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><math alttext=\"\\text{RAFT }^{\\text{300 steps}}_{\\text{iter }3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m7\" intent=\":literal\"><semantics><msubsup><mtext mathsize=\"0.900em\">RAFT&#160;</mtext><mrow><mtext mathsize=\"0.900em\">iter&#160;</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn mathsize=\"0.900em\">3</mn></mrow><mtext mathsize=\"0.900em\">300 steps</mtext></msubsup><annotation encoding=\"application/x-tex\">\\text{RAFT }^{\\text{300 steps}}_{\\text{iter }3}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.756</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.99</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.237</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\"><math alttext=\"\\text{DPO}^{\\,\\text{200 steps}}_{\\,\\beta\\text{ = 200}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m8\" intent=\":literal\"><semantics><msubsup><mtext mathsize=\"0.900em\">DPO</mtext><mrow><mi mathsize=\"0.900em\">&#946;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext mathsize=\"0.900em\">&#160;= 200</mtext></mrow><mtext mathsize=\"0.900em\">200 steps</mtext></msubsup><annotation encoding=\"application/x-tex\">\\text{DPO}^{\\,\\text{200 steps}}_{\\,\\beta\\text{ = 200}}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.765</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.73</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.010</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "f0v",
            "wer",
            "​3300",
            "evaluation",
            "bestof64",
            "dpoβ​",
            "↓downarrow",
            "​2300",
            "selected",
            "base",
            "objective",
            "iter",
            "stepstextraft",
            "​1300",
            "results",
            "↑uparrow",
            "bestof16",
            "task",
            "text300",
            "stepstextiter",
            "model",
            "sim",
            "raft",
            "method",
            "stepstextdpotext200",
            "stepsbetatext"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Grid Search for Optimal <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m1\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> In DPO training, </span>\n  <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#946;</mi>\n      <annotation encoding=\"application/x-tex\">\\beta</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> controls the strength of KL regularization. We report results for </span>\n  <math alttext=\"\\beta\\in\\{200,\\allowbreak 400,\\allowbreak 800\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#946;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mn mathsize=\"0.900em\">200</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">400</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">800</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta\\in\\{200,\\allowbreak 400,\\allowbreak 800\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The evaluation results can be found in Tab.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Task A: Improving F0 Variance &#8227; 4 Experiments &#8227; Direct Preference Optimization for Speech Autoregressive Diffusion Models\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4.2 Task A: Improving F0 Variance &#8227; 4 Experiments &#8227; Direct Preference Optimization for Speech Autoregressive Diffusion Models\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We observe that SIM gradually decreases throughout the training process for all values of </span>\n  <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#946;</mi>\n      <annotation encoding=\"application/x-tex\">\\beta</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This decrease is not caused by changes in prosody, as the best-of-K&#160;(BoK) sampling results in Tab.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Task A: Improving F0 Variance &#8227; 4 Experiments &#8227; Direct Preference Optimization for Speech Autoregressive Diffusion Models\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> indicate that an increase in F0V does not significantly reduce SIM. For larger values of </span>\n  <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#946;</mi>\n      <annotation encoding=\"application/x-tex\">\\beta</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the KL constraint is stronger, resulting in less degradation in SIM; however, the improvement in F0V is also smaller. We recommend applying early stopping to prevent significant quality degradation.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Direct Preference Optimization&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (DPO) was originally proposed for aligning language models and was later adapted for fine-tuning diffusion models&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In this work, we extend DPO to ARDMs&#160;(ARDM-DPO) and apply it to fine-tune the recently proposed DiTAR model&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which achieves state-of-the-art performance in zero-shot TTS. DiTAR features a highly efficient ARDM architecture that separates computation for encoding the generation history from denoising the next token. To our knowledge, this is the first preference-alignment method tailored to ARDMs for TTS.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The DiTAR model used in our experiments is based on </span>\n  <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m30\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">v</mi>\n      <annotation encoding=\"application/x-tex\">v</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-prediction, with continuous time </span>\n  <math alttext=\"t\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m31\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <mn mathsize=\"0.900em\">0</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">t\\in[0,1]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> where </span>\n  <math alttext=\"t=1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m32\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">t=1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> corresponds to pure noise. We can derive the ARDM-DPO training objective for DiTAR as follows:</span>\n</p>\n\n",
                "matched_terms": [
                    "objective",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Base Model.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We fine-tuned a base DiTAR model with 0.4B parameters, pretrained on an internal corpus of around 280,000 hours of Chinese and English audio. The LM in the base model has 24 Transformer blocks, and the diffusion head contains 4 blocks. Each Transformer block has 1024 hidden dimensions and 16 attention heads.</span>\n</p>\n\n",
                "matched_terms": [
                    "base",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Inference.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We used the same diffusion sampler for training sample generation and model evaluation. We use a 16-step DDPM sampler with a linear time schedule. We enabled LM Guidance&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is similar to classifier-free guidance (CFG), with weight </span>\n  <math alttext=\"w=2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">w</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">2</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">w=2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Objective Evaluations.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> In all experiments, we report the word error rate (WER) using Whisper-large-v3 for English and the character error rate (CER) using Paraformer-zh for Chinese. Additionally, we calculate the cosine similarity of speaker embeddings (SIM) between the prompt and the generated audio using the WavLM-TDCNN model. All metrics were computed using Seed-TTS-Eval</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">2</sup>\n        <span class=\"ltx_tag ltx_tag_note\">2</span>\n        <a class=\"ltx_ref ltx_href\" href=\"https://github.com/BytedanceSpeech/seed-tts-eval\" title=\"\">https://github.com/BytedanceSpeech/seed-tts-eval</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We also report the token average KL divergence on the test set, which is defined as</span>\n</p>\n\n",
                "matched_terms": [
                    "objective",
                    "sim",
                    "model",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">as a measure of the divergence between the fine-tuned model </span>\n  <math alttext=\"\\pi_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#960;</mi>\n        <mi mathsize=\"0.900em\">&#952;</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\pi_{\\theta}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and the reference model </span>\n  <math alttext=\"\\mu\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#956;</mi>\n      <annotation encoding=\"application/x-tex\">\\mu</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For each objective metric, we report the average value across 8 random runs.</span>\n</p>\n\n",
                "matched_terms": [
                    "objective",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Task.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> The fundamental frequency variance&#160;(F0V) is strongly correlated with the perceived expressiveness of the generated speech. Optimizing F0V can effectively prevent the model from producing monotone responses.</span>\n</p>\n\n",
                "matched_terms": [
                    "task",
                    "model",
                    "f0v"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Preference Dataset.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We randomly sampled speech prompts and texts from the LibriTTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#bib.bib35\" title=\"\">35</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset. For each prompt-text pair, we generated 32 candidate responses using the base model. We then measured the F0V of these responses and selected the best and worst in terms of F0V to form a preference pair. In total, we collected 256k preference pairs, amounting to approximately 1,000 hours of speech.</span>\n</p>\n\n",
                "matched_terms": [
                    "selected",
                    "base",
                    "model",
                    "f0v"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Evaluation Dataset.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> For evaluation, we randomly selected 38 prompt audios and target texts from different speakers in the LibriTTS </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> subset.</span>\n</p>\n\n",
                "matched_terms": [
                    "selected",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Rejection Sampling Fine-Tuning.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We compare ARDM-DPO with rejection sampling fine-tuning (RAFT)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#bib.bib36\" title=\"\">36</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. RAFT performs supervised fine-tuning (SFT) iteratively. In each RAFT iteration, we collect approximately 1,000 hours of speech continuations from the best policy found in the previous iteration. For each prompt, we sample 32 candidate responses and retain the one with the highest F0V. RAFT experiments use a batch size of 512 and a learning rate of </span>\n  <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">5</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "raft",
                    "f0v"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Diffusion Loss During DPO.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We visualize the changes in diffusion loss for the winning and losing samples (</span>\n  <math alttext=\"\\Delta_{+},\\Delta_{-}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p6.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n          <mo mathsize=\"0.900em\">+</mo>\n        </msub>\n        <mo mathsize=\"0.900em\">,</mo>\n        <msub>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n          <mo mathsize=\"0.900em\">&#8722;</mo>\n        </msub>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\Delta_{+},\\Delta_{-}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) during training with </span>\n  <math alttext=\"\\beta=200\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p6.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#946;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">200</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta=200</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#S4.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 4.2 Task A: Improving F0 Variance &#8227; 4 Experiments &#8227; Direct Preference Optimization for Speech Autoregressive Diffusion Models\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Although the training objective in Eq.&#160;(</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#S3.E16\" style=\"font-size:90%;\" title=\"In 3 ARDM-DPO &#8227; Direct Preference Optimization for Speech Autoregressive Diffusion Models\">\n    <span class=\"ltx_text ltx_ref_tag\">16</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) is supposed to decrease the diffusion loss of the winning samples while increasing the diffusion loss of the losing samples, we observe that the model tends to increase both losses during training. A similar phenomenon has been observed in LLM DPO training&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#bib.bib37\" title=\"\">37</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Investigating this behavior is left as future work.</span>\n</p>\n\n",
                "matched_terms": [
                    "objective",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Subjective Evaluations.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> For each of the 38 test cases, we generated three random responses from both the base model and the DPO model (200 steps, </span>\n  <math alttext=\"\\beta=200\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p7.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#946;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">200</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta=200</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) for comparison. Evaluators assessed the response pairs based on three criteria: naturalness, speaker similarity to the prompt, and expressiveness. As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#S4.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 4.2 Task A: Improving F0 Variance &#8227; 4 Experiments &#8227; Direct Preference Optimization for Speech Autoregressive Diffusion Models\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, DPO training slightly reduces naturalness and speaker similarity but significantly enhances perceived expressiveness.</span>\n</p>\n\n",
                "matched_terms": [
                    "base",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Task.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> When evaluated on out-of-domain complex texts containing repetitions, autoregressive TTS models often make mistakes in audio-text alignment, such as missing or inserting words. Following prior work, we trained a phoneme-based CTC model&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#bib.bib38\" title=\"\">38</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and used its negative log likelihood per phoneme (NLL) as a proxy for speech intelligibility. The CTC model consists of 6 transformer blocks, each with a hidden dimension of 1024 and 16 attention heads.</span>\n</p>\n\n",
                "matched_terms": [
                    "task",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Preference Dataset.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> The prompts were randomly sampled from DidiSpeech-2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#bib.bib39\" title=\"\">39</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a Chinese speech corpus consisting of 227 hours of recordings from 1,500 speakers. The texts were selected from a dataset of 100,000 long Chinese sentences, with randomly introduced repetitive phrases and clauses. For each prompt-text pair, we generated 16 candidate responses using the base model. We then calculated the CTC loss with our CTC model and selected the best and worst responses to form a preference pair. In total, we collected 430,000 preference pairs, totaling approximately 3,500 hours.</span>\n</p>\n\n",
                "matched_terms": [
                    "selected",
                    "base",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Grid Search for Optimal <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m1\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We initially experimented with </span>\n  <math alttext=\"\\beta\\in\\{200,\\allowbreak 400,\\allowbreak\\dots,\\allowbreak 3200,\\allowbreak 6400\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#946;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mn mathsize=\"0.900em\">200</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">400</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">3200</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">6400</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta\\in\\{200,\\allowbreak 400,\\allowbreak\\dots,\\allowbreak 3200,\\allowbreak 6400\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and observed that </span>\n  <math alttext=\"\\beta\\leq 400\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#946;</mi>\n        <mo mathsize=\"0.900em\">&#8804;</mo>\n        <mn mathsize=\"0.900em\">400</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta\\leq 400</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> led to increases in NLL and CER within 300 steps, while </span>\n  <math alttext=\"\\beta\\geq 6400\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#946;</mi>\n        <mo mathsize=\"0.900em\">&#8805;</mo>\n        <mn mathsize=\"0.900em\">6400</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta\\geq 6400</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> resulted in very slow optimization. Consequently, we focused on </span>\n  <math alttext=\"\\beta\\in\\{800,\\allowbreak 1600,\\allowbreak 3200\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#946;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mn mathsize=\"0.900em\">800</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">1600</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">3200</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta\\in\\{800,\\allowbreak 1600,\\allowbreak 3200\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for further analysis. The trajectories of NLL, SIM, and CER during training are shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#S4.F5\" style=\"font-size:90%;\" title=\"Figure 5 &#8227; 4.3 Task B: Improving Text Likelihood &#8227; 4 Experiments &#8227; Direct Preference Optimization for Speech Autoregressive Diffusion Models\">\n    <span class=\"ltx_text ltx_ref_tag\">5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We found that the DPO model trained for 9000 steps with </span>\n  <math alttext=\"\\beta=1600\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#946;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">1600</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta=1600</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> achieved the best performance, with a 25% reduction in CER. Detailed results are provided in Tab.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.3 Task B: Improving Text Likelihood &#8227; 4 Experiments &#8227; Direct Preference Optimization for Speech Autoregressive Diffusion Models\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "sim",
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Subjective evaluations.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We randomly sampled 40 test cases from the test set and generated three random response pairs for each test case from the base model and the DPO model (9,000 steps, </span>\n  <math alttext=\"\\beta\\text{ = 1600}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#946;</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mtext mathsize=\"0.900em\">&#160;= 1600</mtext>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta\\text{ = 1600}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). Evaluators assessed all pairs for naturalness and speaker similarity. We find that the DPO model performs similarly to the base model. For naturalness, the lose/tie/win probabilities are 4.4%, 88.7%, and 6.9%, respectively; for speaker similarity, they are 2.1%, 94.3%, and 3.6%, respectively. This indicates good prior preservation of ARDM-DPO on Task B.</span>\n</p>\n\n",
                "matched_terms": [
                    "base",
                    "model",
                    "task"
                ]
            }
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Selected objective evaluation results for Task B.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">NLL </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SIM </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CER </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">KL </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Base Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.55</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.711</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.37</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8212;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Best-of-8 (CER)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.39</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.713</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#EBFFEB;\">4.99</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8212;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Best-of-8 (NLL)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#EBFFEB;\">0.27</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.712</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.79</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8212;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\"><math alttext=\"\\text{DPO}^{\\,\\text{9000 steps}}_{\\,\\beta\\text{ = 1600}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><msubsup><mtext mathsize=\"0.900em\">DPO</mtext><mrow><mi mathsize=\"0.900em\">&#946;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext mathsize=\"0.900em\">&#160;= 1600</mtext></mrow><mtext mathsize=\"0.900em\">9000 steps</mtext></msubsup><annotation encoding=\"application/x-tex\">\\text{DPO}^{\\,\\text{9000 steps}}_{\\,\\beta\\text{ = 1600}}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.712</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.009</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "selected",
            "↑uparrow",
            "nll",
            "task",
            "base",
            "model",
            "objective",
            "evaluation",
            "sim",
            "stepstextdpotext9000",
            "results",
            "method",
            "cer",
            "stepsbetatext",
            "↓downarrow",
            "bestof8",
            "dpoβ​"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Grid Search for Optimal <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m1\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We initially experimented with </span>\n  <math alttext=\"\\beta\\in\\{200,\\allowbreak 400,\\allowbreak\\dots,\\allowbreak 3200,\\allowbreak 6400\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#946;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mn mathsize=\"0.900em\">200</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">400</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">3200</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">6400</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta\\in\\{200,\\allowbreak 400,\\allowbreak\\dots,\\allowbreak 3200,\\allowbreak 6400\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and observed that </span>\n  <math alttext=\"\\beta\\leq 400\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#946;</mi>\n        <mo mathsize=\"0.900em\">&#8804;</mo>\n        <mn mathsize=\"0.900em\">400</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta\\leq 400</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> led to increases in NLL and CER within 300 steps, while </span>\n  <math alttext=\"\\beta\\geq 6400\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#946;</mi>\n        <mo mathsize=\"0.900em\">&#8805;</mo>\n        <mn mathsize=\"0.900em\">6400</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta\\geq 6400</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> resulted in very slow optimization. Consequently, we focused on </span>\n  <math alttext=\"\\beta\\in\\{800,\\allowbreak 1600,\\allowbreak 3200\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#946;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mn mathsize=\"0.900em\">800</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">1600</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">3200</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta\\in\\{800,\\allowbreak 1600,\\allowbreak 3200\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for further analysis. The trajectories of NLL, SIM, and CER during training are shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#S4.F5\" style=\"font-size:90%;\" title=\"Figure 5 &#8227; 4.3 Task B: Improving Text Likelihood &#8227; 4 Experiments &#8227; Direct Preference Optimization for Speech Autoregressive Diffusion Models\">\n    <span class=\"ltx_text ltx_ref_tag\">5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We found that the DPO model trained for 9000 steps with </span>\n  <math alttext=\"\\beta=1600\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#946;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">1600</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta=1600</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> achieved the best performance, with a 25% reduction in CER. Detailed results are provided in Tab.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.3 Task B: Improving Text Likelihood &#8227; 4 Experiments &#8227; Direct Preference Optimization for Speech Autoregressive Diffusion Models\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Direct Preference Optimization&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (DPO) was originally proposed for aligning language models and was later adapted for fine-tuning diffusion models&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In this work, we extend DPO to ARDMs&#160;(ARDM-DPO) and apply it to fine-tune the recently proposed DiTAR model&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which achieves state-of-the-art performance in zero-shot TTS. DiTAR features a highly efficient ARDM architecture that separates computation for encoding the generation history from denoising the next token. To our knowledge, this is the first preference-alignment method tailored to ARDMs for TTS.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The DiTAR model used in our experiments is based on </span>\n  <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m30\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">v</mi>\n      <annotation encoding=\"application/x-tex\">v</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-prediction, with continuous time </span>\n  <math alttext=\"t\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m31\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <mn mathsize=\"0.900em\">0</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">t\\in[0,1]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> where </span>\n  <math alttext=\"t=1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m32\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">t=1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> corresponds to pure noise. We can derive the ARDM-DPO training objective for DiTAR as follows:</span>\n</p>\n\n",
                "matched_terms": [
                    "objective",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Base Model.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We fine-tuned a base DiTAR model with 0.4B parameters, pretrained on an internal corpus of around 280,000 hours of Chinese and English audio. The LM in the base model has 24 Transformer blocks, and the diffusion head contains 4 blocks. Each Transformer block has 1024 hidden dimensions and 16 attention heads.</span>\n</p>\n\n",
                "matched_terms": [
                    "base",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Inference.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We used the same diffusion sampler for training sample generation and model evaluation. We use a 16-step DDPM sampler with a linear time schedule. We enabled LM Guidance&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is similar to classifier-free guidance (CFG), with weight </span>\n  <math alttext=\"w=2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">w</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">2</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">w=2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Objective Evaluations.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> In all experiments, we report the word error rate (WER) using Whisper-large-v3 for English and the character error rate (CER) using Paraformer-zh for Chinese. Additionally, we calculate the cosine similarity of speaker embeddings (SIM) between the prompt and the generated audio using the WavLM-TDCNN model. All metrics were computed using Seed-TTS-Eval</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">2</sup>\n        <span class=\"ltx_tag ltx_tag_note\">2</span>\n        <a class=\"ltx_ref ltx_href\" href=\"https://github.com/BytedanceSpeech/seed-tts-eval\" title=\"\">https://github.com/BytedanceSpeech/seed-tts-eval</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We also report the token average KL divergence on the test set, which is defined as</span>\n</p>\n\n",
                "matched_terms": [
                    "objective",
                    "sim",
                    "model",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">as a measure of the divergence between the fine-tuned model </span>\n  <math alttext=\"\\pi_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#960;</mi>\n        <mi mathsize=\"0.900em\">&#952;</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\pi_{\\theta}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and the reference model </span>\n  <math alttext=\"\\mu\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#956;</mi>\n      <annotation encoding=\"application/x-tex\">\\mu</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For each objective metric, we report the average value across 8 random runs.</span>\n</p>\n\n",
                "matched_terms": [
                    "objective",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Task.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> The fundamental frequency variance&#160;(F0V) is strongly correlated with the perceived expressiveness of the generated speech. Optimizing F0V can effectively prevent the model from producing monotone responses.</span>\n</p>\n\n",
                "matched_terms": [
                    "task",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Preference Dataset.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We randomly sampled speech prompts and texts from the LibriTTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#bib.bib35\" title=\"\">35</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset. For each prompt-text pair, we generated 32 candidate responses using the base model. We then measured the F0V of these responses and selected the best and worst in terms of F0V to form a preference pair. In total, we collected 256k preference pairs, amounting to approximately 1,000 hours of speech.</span>\n</p>\n\n",
                "matched_terms": [
                    "selected",
                    "base",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Evaluation Dataset.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> For evaluation, we randomly selected 38 prompt audios and target texts from different speakers in the LibriTTS </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> subset.</span>\n</p>\n\n",
                "matched_terms": [
                    "selected",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Grid Search for Optimal <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m1\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> In DPO training, </span>\n  <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#946;</mi>\n      <annotation encoding=\"application/x-tex\">\\beta</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> controls the strength of KL regularization. We report results for </span>\n  <math alttext=\"\\beta\\in\\{200,\\allowbreak 400,\\allowbreak 800\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#946;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mn mathsize=\"0.900em\">200</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">400</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">800</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta\\in\\{200,\\allowbreak 400,\\allowbreak 800\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The evaluation results can be found in Tab.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Task A: Improving F0 Variance &#8227; 4 Experiments &#8227; Direct Preference Optimization for Speech Autoregressive Diffusion Models\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4.2 Task A: Improving F0 Variance &#8227; 4 Experiments &#8227; Direct Preference Optimization for Speech Autoregressive Diffusion Models\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We observe that SIM gradually decreases throughout the training process for all values of </span>\n  <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#946;</mi>\n      <annotation encoding=\"application/x-tex\">\\beta</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This decrease is not caused by changes in prosody, as the best-of-K&#160;(BoK) sampling results in Tab.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Task A: Improving F0 Variance &#8227; 4 Experiments &#8227; Direct Preference Optimization for Speech Autoregressive Diffusion Models\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> indicate that an increase in F0V does not significantly reduce SIM. For larger values of </span>\n  <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#946;</mi>\n      <annotation encoding=\"application/x-tex\">\\beta</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the KL constraint is stronger, resulting in less degradation in SIM; however, the improvement in F0V is also smaller. We recommend applying early stopping to prevent significant quality degradation.</span>\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "evaluation",
                    "sim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Diffusion Loss During DPO.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We visualize the changes in diffusion loss for the winning and losing samples (</span>\n  <math alttext=\"\\Delta_{+},\\Delta_{-}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p6.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n          <mo mathsize=\"0.900em\">+</mo>\n        </msub>\n        <mo mathsize=\"0.900em\">,</mo>\n        <msub>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n          <mo mathsize=\"0.900em\">&#8722;</mo>\n        </msub>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\Delta_{+},\\Delta_{-}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) during training with </span>\n  <math alttext=\"\\beta=200\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p6.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#946;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">200</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta=200</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#S4.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 4.2 Task A: Improving F0 Variance &#8227; 4 Experiments &#8227; Direct Preference Optimization for Speech Autoregressive Diffusion Models\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Although the training objective in Eq.&#160;(</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#S3.E16\" style=\"font-size:90%;\" title=\"In 3 ARDM-DPO &#8227; Direct Preference Optimization for Speech Autoregressive Diffusion Models\">\n    <span class=\"ltx_text ltx_ref_tag\">16</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) is supposed to decrease the diffusion loss of the winning samples while increasing the diffusion loss of the losing samples, we observe that the model tends to increase both losses during training. A similar phenomenon has been observed in LLM DPO training&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#bib.bib37\" title=\"\">37</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Investigating this behavior is left as future work.</span>\n</p>\n\n",
                "matched_terms": [
                    "objective",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Subjective Evaluations.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> For each of the 38 test cases, we generated three random responses from both the base model and the DPO model (200 steps, </span>\n  <math alttext=\"\\beta=200\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p7.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#946;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">200</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta=200</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) for comparison. Evaluators assessed the response pairs based on three criteria: naturalness, speaker similarity to the prompt, and expressiveness. As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#S4.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 4.2 Task A: Improving F0 Variance &#8227; 4 Experiments &#8227; Direct Preference Optimization for Speech Autoregressive Diffusion Models\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, DPO training slightly reduces naturalness and speaker similarity but significantly enhances perceived expressiveness.</span>\n</p>\n\n",
                "matched_terms": [
                    "base",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Task.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> When evaluated on out-of-domain complex texts containing repetitions, autoregressive TTS models often make mistakes in audio-text alignment, such as missing or inserting words. Following prior work, we trained a phoneme-based CTC model&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#bib.bib38\" title=\"\">38</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and used its negative log likelihood per phoneme (NLL) as a proxy for speech intelligibility. The CTC model consists of 6 transformer blocks, each with a hidden dimension of 1024 and 16 attention heads.</span>\n</p>\n\n",
                "matched_terms": [
                    "task",
                    "model",
                    "nll"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Preference Dataset.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> The prompts were randomly sampled from DidiSpeech-2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18928v1#bib.bib39\" title=\"\">39</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a Chinese speech corpus consisting of 227 hours of recordings from 1,500 speakers. The texts were selected from a dataset of 100,000 long Chinese sentences, with randomly introduced repetitive phrases and clauses. For each prompt-text pair, we generated 16 candidate responses using the base model. We then calculated the CTC loss with our CTC model and selected the best and worst responses to form a preference pair. In total, we collected 430,000 preference pairs, totaling approximately 3,500 hours.</span>\n</p>\n\n",
                "matched_terms": [
                    "selected",
                    "base",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Subjective evaluations.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We randomly sampled 40 test cases from the test set and generated three random response pairs for each test case from the base model and the DPO model (9,000 steps, </span>\n  <math alttext=\"\\beta\\text{ = 1600}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#946;</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mtext mathsize=\"0.900em\">&#160;= 1600</mtext>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta\\text{ = 1600}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). Evaluators assessed all pairs for naturalness and speaker similarity. We find that the DPO model performs similarly to the base model. For naturalness, the lose/tie/win probabilities are 4.4%, 88.7%, and 6.9%, respectively; for speaker similarity, they are 2.1%, 94.3%, and 3.6%, respectively. This indicates good prior preservation of ARDM-DPO on Task B.</span>\n</p>\n\n",
                "matched_terms": [
                    "task",
                    "base",
                    "model"
                ]
            }
        ]
    }
}