{
    "S3.T1": {
        "source_file": "VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion",
        "caption": "Table 1: Training data analysis",
        "body": "Dataset\n#Hours\n#Samples\n\n\n\n\nGPC-2400\n2447\n430K\n\n\nGPC-50\n800\n488K\n\n\nFleurs\n13\n3,2K\n\n\nCV\n12\n10,8K\n\n\nLG\n72\n23,5K\n\n\nHParl\n120\n76K\n\n\nTotal\n\n∼\\sim3300\n\n∼\\sim1M",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">#Hours</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">#Samples</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\">GPC-2400</th>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\">2447</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\">430K</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">GPC-50</th>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">800</td>\n<td class=\"ltx_td ltx_align_right\">488K</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Fleurs</th>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">13</td>\n<td class=\"ltx_td ltx_align_right\">3,2K</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">CV</th>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">12</td>\n<td class=\"ltx_td ltx_align_right\">10,8K</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">LG</th>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">72</td>\n<td class=\"ltx_td ltx_align_right\">23,5K</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">HParl</th>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">120</td>\n<td class=\"ltx_td ltx_align_right\">76K</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Total</th>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3300</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>1M</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "data",
            "gpc50",
            "gpc2400",
            "samples",
            "108k",
            "hparl",
            "total",
            "235k",
            "∼sim3300",
            "fleurs",
            "32k",
            "analysis",
            "76k",
            "training",
            "hours",
            "dataset",
            "∼sim1m",
            "488k",
            "430k"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Greek Podcast Corpus (GPC)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#bib.bib28\" title=\"\">28</a>]</cite></span> is a weakly supervised dataset of Greek podcasts, covering 16 diverse categories (e.g., <span class=\"ltx_text ltx_font_italic\">True Crime</span>, <span class=\"ltx_text ltx_font_italic\">Comedy</span>). We use the GPC-50 subset for training, containing 50 hours per category (800 hours total), and a test set of 1 hour per category (16 hours total). In addition, we collected 2,447 hours of transcribed Greek podcast audio (annotated as <span class=\"ltx_text ltx_font_italic\">GPC-2400</span>), totaling 434,530 samples. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#S3.T2\" title=\"Table 2 &#8227; 3.3 Datasets &#8227; 3 Experimental Setup &#8227; VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> details its category distribution, while Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#S3.T1\" title=\"Table 1 &#8227; 3.2 LLM and LoRA Adaptation &#8227; 3 Experimental Setup &#8227; VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> overviews the full training data.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Logotypografia (LG)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#bib.bib24\" title=\"\">24</a>]</cite></span> is one of the earliest Greek corpora, comprising approximately 72 hours of speech for training and 9 hours for testing.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Common Voice (CV)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#bib.bib25\" title=\"\">25</a>]</cite></span> is a multilingual, crowd-sourced dataset developed by Mozilla. In our experiments, we use version 9.0 of the Greek subset, which contains 12 hours of training data, and 2 hours of test data.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "dataset",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">HParl (HP)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#bib.bib26\" title=\"\">26</a>]</cite></span> consists of parliamentary recordings from the Hellenic Parliament, which consists of 120 hours of training and 11 hours of testing data.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "training",
                    "data",
                    "hparl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fleurs&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#bib.bib27\" title=\"\">27</a>]</cite></span> is a multilingual speech corpus. The Greek portion comprises 13 hours of speech for training and 2 hours for testing.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "training",
                    "fleurs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the <span class=\"ltx_text ltx_font_italic\">VoxKrikri-1,9,15,21,30</span> models on Automatic Speech Recognition (ASR) using Word Error Rate (WER) and compare them against the state-of-the-art Whisper-large-v3 baseline across five test sets (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#S3.T3\" title=\"Table 3 &#8227; 3.3 Datasets &#8227; 3 Experimental Setup &#8227; VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Both full-sequence and causal fusion strategies are considered. Across all benchmarks, every <span class=\"ltx_text ltx_font_italic\">VoxKrikri</span> variant consistently outperforms Whisper, with improvements ranging from &#160;0.6 to over 5 WER points depending on the dataset. Full-sequence fusion generally yields slightly lower WER than causal fusion, as seen for example on GPC-test (12.08 vs. 12.27) and Fleurs-test (9.33 vs. 9.52). Moreover, intermediate-to-late fusion layers (15, 21, and 30) tend to outperform early fusion (1, 9), with <span class=\"ltx_text ltx_font_italic\">VoxKrikri-21</span> achieving the best overall results on three datasets (GPC, Fleurs, and LG) and <span class=\"ltx_text ltx_font_italic\">VoxKrikri-15</span> excelling on CV-test. Notably, <span class=\"ltx_text ltx_font_italic\">VoxKrikri-21 (full)</span> reduces WER by over 5 points on GPC-test compared to Whisper (12.08 vs. 17.27). These results highlight the robustness of our continuous latent fusion approach and demonstrate that carefully chosen fusion depths enable substantial performance gains.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "fleurs"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion",
        "caption": "Table 2: GPC-2400 categories breakdown",
        "body": "Category\n#Hours\n#Samples\n\n\n\n\nArts\n361\n62K\n\n\nBusiness\n192\n33K\n\n\nComedy\n263\n44K\n\n\nEducation\n417\n70K\n\n\nFiction\n290\n49K\n\n\nHealth\n312\n52K\n\n\nHistory\n170\n48K\n\n\nKids\n144\n28K\n\n\nLeisure\n130\n20K\n\n\nMusic\n168\n28K\n\n\nTotal\n\n∼\\sim2400\n\n∼\\sim430K",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">Category</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">#Hours</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">#Samples</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\">Arts</th>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\">361</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\">62K</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Business</th>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">192</td>\n<td class=\"ltx_td ltx_align_right\">33K</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Comedy</th>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">263</td>\n<td class=\"ltx_td ltx_align_right\">44K</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Education</th>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">417</td>\n<td class=\"ltx_td ltx_align_right\">70K</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Fiction</th>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">290</td>\n<td class=\"ltx_td ltx_align_right\">49K</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Health</th>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">312</td>\n<td class=\"ltx_td ltx_align_right\">52K</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">History</th>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">170</td>\n<td class=\"ltx_td ltx_align_right\">48K</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Kids</th>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">144</td>\n<td class=\"ltx_td ltx_align_right\">28K</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Leisure</th>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">130</td>\n<td class=\"ltx_td ltx_align_right\">20K</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Music</th>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">168</td>\n<td class=\"ltx_td ltx_align_right\">28K</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Total</th>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>2400</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>430K</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "arts",
            "comedy",
            "70k",
            "business",
            "gpc2400",
            "kids",
            "categories",
            "samples",
            "62k",
            "48k",
            "fiction",
            "28k",
            "52k",
            "total",
            "∼sim2400",
            "∼sim430k",
            "music",
            "category",
            "hours",
            "health",
            "education",
            "33k",
            "breakdown",
            "44k",
            "leisure",
            "20k",
            "49k",
            "history"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Greek Podcast Corpus (GPC)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#bib.bib28\" title=\"\">28</a>]</cite></span> is a weakly supervised dataset of Greek podcasts, covering 16 diverse categories (e.g., <span class=\"ltx_text ltx_font_italic\">True Crime</span>, <span class=\"ltx_text ltx_font_italic\">Comedy</span>). We use the GPC-50 subset for training, containing 50 hours per category (800 hours total), and a test set of 1 hour per category (16 hours total). In addition, we collected 2,447 hours of transcribed Greek podcast audio (annotated as <span class=\"ltx_text ltx_font_italic\">GPC-2400</span>), totaling 434,530 samples. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#S3.T2\" title=\"Table 2 &#8227; 3.3 Datasets &#8227; 3 Experimental Setup &#8227; VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> details its category distribution, while Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#S3.T1\" title=\"Table 1 &#8227; 3.2 LLM and LoRA Adaptation &#8227; 3 Experimental Setup &#8227; VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> overviews the full training data.</p>\n\n"
        ],
        "contextual_paragraphs": []
    },
    "S3.T3": {
        "source_file": "VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion",
        "caption": "Table 3: ASR results on benchmark test sets (WER) using both causal and full-sequence fusion",
        "body": "Dataset\nVoxKrikri-1\nVoxKrikri-9\nVoxKrikri-15\nVoxKrikri-21\nVoxKrikri-30\nWhisper-large-v3\n\n\n\ncausal\nfull\ncausal\nfull\ncausal\nfull\ncausal\nfull\ncausal\nfull\n\n\n\n\n\nGPC\n14.88\n14.81\n13.06\n14.26\n12.65\n12.75\n12.27\n12.08\n12.64\n12.6\n17.27\n\n\nFleurs\n10.85\n10.5\n11.13\n10.21\n10.2\n10.3\n9.52\n9.33\n9.48\n10.2\n11.02\n\n\nCV\n13.8\n13.6\n12.42\n12.2\n12.3\n11.97\n12.01\n12.04\n12.43\n12.1\n13.98\n\n\nLG\n10.55\n10.41\n10.03\n9.89\n9.78\n9.54\n9.47\n9.4\n9.66\n9.52\n10.86\n\n\nHParl\n14.7\n15.01\n13.9\n13.8\n13.55\n13.32\n12.9\n13.01\n13.41\n13.3\n16.99",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_rr\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">VoxKrikri-1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">VoxKrikri-9</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">VoxKrikri-15</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">VoxKrikri-21</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">VoxKrikri-30</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Whisper-large-v3</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_rr\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">causal</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">full</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">causal</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">full</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">causal</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">full</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">causal</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">full</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">causal</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr\">full</th>\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_rr ltx_border_tt\">GPC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">14.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">14.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">13.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">14.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">12.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">12.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">12.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">12.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">12.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_tt\">12.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">17.27</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_rr ltx_border_t\">Fleurs</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">10.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">10.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">11.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">10.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">10.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">10.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">9.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">9.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">9.48</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">10.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">11.02</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_rr ltx_border_t\">CV</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">13.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">13.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">12.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">12.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">12.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">11.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">12.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">12.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">12.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">12.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">13.98</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_rr ltx_border_t\">LG</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">10.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">10.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">10.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">9.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">9.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">9.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">9.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">9.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">9.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">9.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">10.86</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_rr ltx_border_t\">HParl</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">14.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">15.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">13.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">13.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">13.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">13.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">12.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">13.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">13.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t\">13.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">16.99</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "both",
            "wer",
            "benchmark",
            "hparl",
            "asr",
            "gpc",
            "results",
            "causal",
            "sets",
            "fleurs",
            "fullsequence",
            "voxkrikri15",
            "dataset",
            "test",
            "voxkrikri30",
            "voxkrikri9",
            "voxkrikri1",
            "voxkrikri21",
            "whisperlargev3",
            "full",
            "fusion"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluate the <span class=\"ltx_text ltx_font_italic\">VoxKrikri-1,9,15,21,30</span> models on Automatic Speech Recognition (ASR) using Word Error Rate (WER) and compare them against the state-of-the-art Whisper-large-v3 baseline across five test sets (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#S3.T3\" title=\"Table 3 &#8227; 3.3 Datasets &#8227; 3 Experimental Setup &#8227; VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Both full-sequence and causal fusion strategies are considered. Across all benchmarks, every <span class=\"ltx_text ltx_font_italic\">VoxKrikri</span> variant consistently outperforms Whisper, with improvements ranging from &#160;0.6 to over 5 WER points depending on the dataset. Full-sequence fusion generally yields slightly lower WER than causal fusion, as seen for example on GPC-test (12.08 vs. 12.27) and Fleurs-test (9.33 vs. 9.52). Moreover, intermediate-to-late fusion layers (15, 21, and 30) tend to outperform early fusion (1, 9), with <span class=\"ltx_text ltx_font_italic\">VoxKrikri-21</span> achieving the best overall results on three datasets (GPC, Fleurs, and LG) and <span class=\"ltx_text ltx_font_italic\">VoxKrikri-15</span> excelling on CV-test. Notably, <span class=\"ltx_text ltx_font_italic\">VoxKrikri-21 (full)</span> reduces WER by over 5 points on GPC-test compared to Whisper (12.08 vs. 17.27). These results highlight the robustness of our continuous latent fusion approach and demonstrate that carefully chosen fusion depths enable substantial performance gains.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present a multimodal fusion framework that bridges pre-trained decoder-based large language models (LLM) and acoustic encoder-decoder architectures such as Whisper, with the aim of building speech-enabled LLMs. Instead of directly using audio embeddings, we explore an intermediate audio-conditioned text space as a more effective mechanism for alignment. Our method operates fully in continuous text representation spaces, fusing Whisper&#8217;s hidden decoder states with those of an LLM through cross-modal attention, and supports both offline and streaming modes. We introduce <span class=\"ltx_text ltx_font_italic\">VoxKrikri</span>, the first Greek speech LLM, and show through analysis that our approach effectively aligns representations across modalities. These results highlight continuous space fusion as a promising path for multilingual and low-resource speech LLMs, while achieving state-of-the-art results for Automatic Speech Recognition in Greek, providing an average <math alttext=\"\\sim 20\\%\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mrow><mn>20</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\sim 20\\%</annotation></semantics></math> relative improvement across benchmarks.</p>\n\n",
                "matched_terms": [
                    "results",
                    "both",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;</span></span>\nSpeech LLMs, modality fusion, continuous latent space, causal masking, ASR</p>\n\n",
                "matched_terms": [
                    "causal",
                    "asr",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by these observations, we investigate whether leveraging an intermediate audio-conditioned text space can serve as a more effective mechanism for multimodal fusion compared to directly utilizing audio embeddings. To this end, we propose a novel multimodal fusion framework that operates entirely within the continuous representation space of both audio and language models. This approach is the first, to the best of our knowledge, to explore such continuous-space alignment for this purpose. Our contributions are: (1) we introduce a cross-modal adaptation framework that fuses Whisper&#8217;s decoder hidden states with a designated decoder layer of a large language model (LLM), enabling both offline and streaming fusion; (2) we develop VoxKrikri, the first Greek Speech LLM that achieves state-of-the-art ASR results across benchmarks; and (3) we conduct an analysis of the latent feature space, demonstrating that our framework successfully aligns the continuous representations of Whisper and the LLM. VoxKrikri will be made available under the Llama 3.1 Community License Agreement upon acceptance.</p>\n\n",
                "matched_terms": [
                    "results",
                    "both",
                    "asr",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We assume that the last hidden layer of Whisper&#8217;s decoder encodes audio-conditioned continuous textual representations, denoted as <math alttext=\"A_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>A</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">A_{s}</annotation></semantics></math>. These are integrated into the LLM through a cross-attention module inserted at a decoder layer <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math>, where text states <math alttext=\"Y_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>Y</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">Y_{t}</annotation></semantics></math> attend to audio-derived keys and values. Varying the insertion depth (early, intermediate, or late) allows us to study fusion at different semantic levels. Unlike approaches that project audio embeddings into the LLM input space&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#bib.bib11\" title=\"\">11</a>]</cite>&#8212;which require the full audio sequence and thus only support offline use&#8212;our method enables both offline and streaming. Specifically, full-sequence fusion lets the LLM attend to the entire audio sequence, while causal fusion restricts attention to past frames for real-time applications.</p>\n\n",
                "matched_terms": [
                    "both",
                    "fullsequence",
                    "causal",
                    "full",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In full-sequence fusion, the entire audio sequence <math alttext=\"A_{1:S}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>A</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>S</mi></mrow></msub><annotation encoding=\"application/x-tex\">A_{1:S}</annotation></semantics></math> is fused with the text sequence <math alttext=\"Y_{1:T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">Y_{1:T}</annotation></semantics></math> via the cross-attention module. The autoregressive capabilities of the LLM are preserved, since each prediction</p>\n\n",
                "matched_terms": [
                    "fullsequence",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"s_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">s_{t}</annotation></semantics></math> denotes the last audio frame aligned with token <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. This design ensures that predictions are based only on past and current context, maintaining temporal consistency and the autoregressive nature of both Whisper and the LLM. During training and inference, a causal attention mask enforces this restriction.</p>\n\n",
                "matched_terms": [
                    "causal",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">and is applied directly to the attention logits before softmax. This formulation allows flexible fusion strategies: setting <math alttext=\"s_{t}=S-1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>S</mi><mo>&#8722;</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">s_{t}=S-1</annotation></semantics></math> for all <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> recovers full-sequence fusion, while Eq.&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#S2.E3\" title=\"In 2.4 Causal Alignment Mask &#8227; 2 Proposed Method &#8227; VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) enforces causal fusion. Finally, the fused hidden state of the LLM passed to layer <math alttext=\"I+1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m3\" intent=\":literal\"><semantics><mrow><mi>I</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">I+1</annotation></semantics></math> is computed as:</p>\n\n",
                "matched_terms": [
                    "fullsequence",
                    "causal",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt Whisper-large-v3 as the acoustic backbone. This encoder&#8211;decoder model achieves state-of-the-art results on Greek ASR benchmarks and serves as a robust foundation. During training, Whisper parameters remain frozen.</p>\n\n",
                "matched_terms": [
                    "results",
                    "whisperlargev3",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the language backbone, we employ Llama-KriKri-8B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#bib.bib22\" title=\"\">22</a>]</cite>, a Greek-adapted version of Llama 3.1-8B, which substantially outperforms the base Llama 3.1-8B on Greek benchmarks.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/collections/ilsp/ilsp-greek-evaluation-suite-6827304d5bf8b70d0346b02c\" title=\"\">https://huggingface.co/collections/ilsp/ilsp-greek-evaluation-suite-6827304d5bf8b70d0346b02c</a></span></span></span> We apply LoRA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#bib.bib23\" title=\"\">23</a>]</cite> adapters for parameter-efficient adaptation during training. Regarding the injection layer <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math> of the LLM, whose hidden states are fused with audio features, we experiment with the 1st, 9th, 15th, 21st and 30th layers out of the 32 decoder layers of Llama 3.1-8B. Each configuration is denoted by its corresponding model name: <span class=\"ltx_text ltx_font_italic\">VoxKrikri-1</span>, <span class=\"ltx_text ltx_font_italic\">VoxKrikri-9</span>, <span class=\"ltx_text ltx_font_italic\">VoxKrikri-15</span>, <span class=\"ltx_text ltx_font_italic\">VoxKrikri-21</span>, and <span class=\"ltx_text ltx_font_italic\">VoxKrikri-30</span>.\n<br class=\"ltx_break\"/></p>\n\n",
                "matched_terms": [
                    "voxkrikri30",
                    "voxkrikri9",
                    "voxkrikri1",
                    "voxkrikri15",
                    "voxkrikri21"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Common Voice (CV)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#bib.bib25\" title=\"\">25</a>]</cite></span> is a multilingual, crowd-sourced dataset developed by Mozilla. In our experiments, we use version 9.0 of the Greek subset, which contains 12 hours of training data, and 2 hours of test data.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Greek Podcast Corpus (GPC)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#bib.bib28\" title=\"\">28</a>]</cite></span> is a weakly supervised dataset of Greek podcasts, covering 16 diverse categories (e.g., <span class=\"ltx_text ltx_font_italic\">True Crime</span>, <span class=\"ltx_text ltx_font_italic\">Comedy</span>). We use the GPC-50 subset for training, containing 50 hours per category (800 hours total), and a test set of 1 hour per category (16 hours total). In addition, we collected 2,447 hours of transcribed Greek podcast audio (annotated as <span class=\"ltx_text ltx_font_italic\">GPC-2400</span>), totaling 434,530 samples. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#S3.T2\" title=\"Table 2 &#8227; 3.3 Datasets &#8227; 3 Experimental Setup &#8227; VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> details its category distribution, while Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#S3.T1\" title=\"Table 1 &#8227; 3.2 LLM and LoRA Adaptation &#8227; 3 Experimental Setup &#8227; VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> overviews the full training data.</p>\n\n",
                "matched_terms": [
                    "test",
                    "full",
                    "dataset",
                    "gpc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To study cross-modal alignment, we measure correlations between the LLM&#8217;s layer-wise representations (encoding transcriptions) and Whisper&#8217;s decoder features (processing speech) using rCCA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#bib.bib30\" title=\"\">30</a>]</cite> on 1,000 samples from the GPC test set, shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15667v1#S3.F2\" title=\"Figure 2 &#8227; 3.3 Datasets &#8227; 3 Experimental Setup &#8227; VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. To address dimensionality mismatch, we subsample 20,000 features and run rCCA with 1,000 components per view and regularization <math alttext=\"\\lambda=10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\lambda=10^{-4}</annotation></semantics></math>. Results show that intermediate-to-late fusion consistently increases correlation, strengthening representational alignment between modalities. Additionally, the sudden increases and decreases indicate that early-to-intermediate fusion benefits from access to the entire sequence, while late fusion is less sensitive to it. Overall, intermediate-to-late fusion achieves the strongest alignment, in line with the WER improvements observed.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "gpc",
                    "results",
                    "fusion",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we proposed a novel framework for narrowing the modality gap between pre-trained language and acoustic models by operating directly in their textual, continuous embedding spaces. Rather than relying on raw audio embeddings, our approach leverages an intermediate audio-conditioned text space, providing a linguistically grounded representation for more effective multimodal fusion. Building on this framework, we introduced <span class=\"ltx_text ltx_font_italic\">VoxKrikri</span>, the first Greek Speech-LLM, which achieves state-of-the-art performance on ASR tasks. Our analysis further showed that the proposed methodology effectively improves the alignment between speech and language representations. We believe this framework offers a strong foundation for future research on multimodal alignment in continuous latent spaces. Moreover, our introduction of causal fusion via causal cross-modal masking paves the way for streaming and real-time applications of SpeechLLMs.</p>\n\n",
                "matched_terms": [
                    "causal",
                    "asr",
                    "fusion"
                ]
            }
        ]
    }
}