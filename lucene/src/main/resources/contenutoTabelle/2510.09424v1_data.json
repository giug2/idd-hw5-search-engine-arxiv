{
    "S3.T1": {
        "source_file": "The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach",
        "caption": "Table 1: Comparison of our two best models with prior work.",
        "body": "WavLM + conn. +\n\n\n\nGemma-2-9B-Instruct [13]",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">WavLM + conn. +</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Gemma-2-9B-Instruct </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib13\" title=\"\">13</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "wavlm",
            "best",
            "models",
            "comparison",
            "conn",
            "prior",
            "our",
            "two",
            "work",
            "gemma29binstruct"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For fair comparison with prior work, the reported JGA for our model in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.2 Implementation details &#8227; 3 Results &#8227; The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> uses post-processing, which includes (i) canonicalizing time expressions to 24-hour format and (ii) case-insensitive fuzzy matching for open/proper-noun slots with a Levenshtein&#160;ratio&#160;</span>\n  <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#8805;</mo>\n      <annotation encoding=\"application/x-tex\">\\geq</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;0.90, applied symmetrically to predictions and references.\nTable&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.2 Implementation details &#8227; 3 Results &#8227; The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> presents a comparison between published results on the SpokenWOZ test set and our two best systems: the compressed context method using 10 queries and the full spoken context method. For our systems the post-processing yields a 3 points JGA increase, which is comparable to the post-processing reported in </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Our approach substantially outperforms other systems of comparable size. To the best of our knowledge, the only system that surpasses our results is the Gemma-2-9B variant reported in&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We did not opt to train a Gemma-based variant of our model due to its high computational requirements, as our primary objective is to demonstrate the effectiveness of our method on small and compact models.\nFurthermore, as shown in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#S3.SS4\" style=\"font-size:90%;\" title=\"3.4 Context Management Methods Comparison &#8227; 3 Results &#8227; The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach\">\n    <span class=\"ltx_text ltx_ref_tag\">3.4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, when using the same model components, our context management strategy significantly outperforms that of previous work.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper presents a comparative study of context management strategies for end-to-end Spoken Dialog State Tracking using Speech-LLMs. We systematically evaluate traditional multimodal context (combining text history and spoken current turn), full spoken history, and compressed spoken history approaches. Our experiments on the SpokenWOZ corpus demonstrate that providing the full spoken conversation as input yields the highest performance among models of similar size, significantly surpassing prior methods. Furthermore, we show that attention-pooling-based compression of the spoken history offers a strong trade-off, maintaining competitive accuracy with reduced context size. Detailed analysis confirms that improvements stem from more effective context utilization.</span>\n</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "prior"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">End-to-end (E2E) systems have emerged as a promising alternative, as they may potentially mitigate the error propagation inherent in cascade systems.\nIn particular, </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> demonstrated the effectiveness of E2E approaches, particularly in fully spoken contexts without access to ground-truth transcriptions, such as the SpokenWOZ&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset. In these settings, E2E models have been shown to outperform traditional cascade systems.\nConcurrently, speech-aware large language models (LLMs), which are also considered end-to-end (E2E) systems, have gained increasing popularity in a variety of spoken language tasks, including automatic speech recognition (ASR) and response generation &#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Recent work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> applied speech-aware LLMs to the spoken DST task, achieving state-of-the-art performance in the SpokenWOZ dataset.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "work"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we explore these possibilities for context management when using a Speech-LLM model.\nOur contributions are three-fold: </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">(a)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> we validate the use of Speech-LLMs as an accurate approach for spoken DST </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">(b)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> we propose two context management approaches reaching the SOTA and </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">(c)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> our best performing approach demonstrates a simple yet effective method: feeding the entire spoken conversation to the model without additional compression or modality mixing.</span>\n</p>\n\n",
                "matched_terms": [
                    "our",
                    "best",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We train our models in two stages, as described in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; Compressed Spoken Context &#8227; 2.1 Context Management &#8227; 2 Methodology &#8227; The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The first stage is ASR pre-training, where we freeze the LLM and train the speech encoder and connector to produce speech representations that align with the LLM&#8217;s input space. Specifically, we task the LLM with generating the transcription from the speech embeddings, propagating the LLM gradients back to the encoder and connector. This approach allows us to leverage the large-scale ASR datasets that are publicly available, resulting in robust alignment between the speech and text modalities.</span>\n</p>\n\n",
                "matched_terms": [
                    "our",
                    "two",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further analyze our best model, we selected the six slots with the highest error counts. In Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#S3.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 3.3 Best Model Analysis &#8227; 3 Results &#8227; The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, blue bars represent the Levenshtein (fuzzy) ratio for slot values present in both prediction and reference, while orange and red bars indicate the counts of insertions and deletions, respectively. Most predictions achieve high fuzzy ratios (above 0.8), suggesting that when the model predicts a slot present in the reference, it usually gets the value nearly correct.\nInterestingly, for </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">restaurant-name</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">attraction-name</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">hotel-name</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the number of substitutions (fuzzy ratio </span>\n  <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&lt;</mo>\n      <annotation encoding=\"application/x-tex\">&lt;</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 1) is very low, with most errors arising from insertions and deletions. This indicates that the model is generally able to correctly predict these proper nouns when it attempts them. In contrast, profile-related slots (e.g., </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">profile-name</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">profile-idnumber</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) remain highly challenging due to their variable content and frequent spelling across multiple turns.\nFinally, although the error rate for </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">train-leaveat</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is relatively low compared to its total occurrences, its high frequency means it still contributes substantially to the overall error count.</span>\n</p>\n\n",
                "matched_terms": [
                    "our",
                    "best"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dialogue Turn Analysis</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nFigure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#S3.F3.sf2\" style=\"font-size:90%;\" title=\"In Figure 3 &#8227; 3.4 Context Management Methods Comparison &#8227; 3 Results &#8227; The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach\">\n    <span class=\"ltx_text ltx_ref_tag\">3(b)</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> displays the evolution of Joint Goal Accuracy (JGA) across dialogue turns. All models perform well in the early turns (1&#8211;5), but accuracy declines quickly in the mid turns (5&#8211;30) and approaches zero by turn 40. This drop can be attributed to the increasing length and complexity of dialogue states, combined with the strictness of the JGA metric, as well as the limited capacity of the relatively small LLM used in our experiments.\nThe full spoken context method consistently outperforms the others, particularly during the mid turns. In the very late turns, it shows occasional performance peaks, though these are difficult to interpret given the small sample size. The 10-query attention pooling method remains competitive, but still underperforms compared to full spoken context in the late turns, even though it benefits from a much smaller context size.</span>\n</p>\n\n",
                "matched_terms": [
                    "our",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Additional Experiences</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> To further understand the contributions of individual components and design choices in our system, we conducted a series of ablation studies and supplementary experiments. Specifically, we investigated the impact of ASR pretraining data, the connector, the compression module, and DST data preprocessing. For ASR pretraining, we compared using the LibriSpeech dataset&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> alone versus the mixed dataset described in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#S3.SS1\" style=\"font-size:90%;\" title=\"3.1 Datasets &#8227; 3 Results &#8227; The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach\">\n    <span class=\"ltx_text ltx_ref_tag\">3.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In baseline experiments with the multimodal method, we observed that when the encoder is unfrozen during DST finetuning, the choice of ASR pretraining data has little impact. However, when freezing the encoder (which is a more practical setup for the Full/Compressed Spoken Context methods), we found that relying solely on LibriSpeech resulted in up to a 3-point drop in JGA compared to using the mixed dataset. During ASR pretraining, we also experimented with different numbers of layers (1, 2, and 4) in the encoder. We found that a single layer provided the fastest convergence and the best performance. For the compression module, we varied the number of layers and found that increasing to three layers led to a 2% absolute drop in JGA. We attribute this to the limited amount of DST finetuning data, as the compression module is only initialized at this stage. Finally, for the multimodal context method, we normalized Whisper transcripts using NeMo Inverse Text Normalization (ITN)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, along with additional processing for time expressions. This preprocessing yielded a 1% absolute gain in JGA.</span>\n</p>\n\n",
                "matched_terms": [
                    "our",
                    "best"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">While our full spoken context approach achieves the highest performance, it could become computationally demanding for very long dialogues. The compressed context method offers a good compromise, with strong results and reduced input size. Additionally, we did not scale our experiments to larger LLMs such as Gemma-2-9B. Both directions will be explored in future work.</span>\n</p>\n\n",
                "matched_terms": [
                    "our",
                    "work"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach",
        "caption": "Table 2: JGA Evaluation of different context management approaches on SpokenWOZ.",
        "body": "SWOZ Dev\nSWOZ Test\n\n\n\n\nMultimodal Context (baseline)\n31.85%\n32.06%\n\n\nFull Spoken Context\n36.89%\n36.29%\n\n\nCompressed Spoken Context\n\n\n\n\n1 query\n31.03%\n30.99%\n\n\n10 queries\n34.26%\n33.51%",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">SWOZ Dev</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">SWOZ Test</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Multimodal Context (baseline)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.85%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">32.06%</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Full Spoken Context</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">36.89%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">36.29%</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Compressed Spoken Context</span></th>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">1 query</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.03%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.99%</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">10 queries</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">34.26%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">33.51%</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "context",
            "management",
            "compressed",
            "jga",
            "evaluation",
            "approaches",
            "swoz",
            "dev",
            "baseline",
            "query",
            "queries",
            "different",
            "spoken",
            "multimodal",
            "spokenwoz",
            "full",
            "test"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">All subsequent analyses use JGA with no post processing.\nTable </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.3 Best Model Analysis &#8227; 3 Results &#8227; The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows the JGA score on SpokenWOZ dev and test splits for each method. Overall, both the full spoken context and the 10-queries-per-turn methods outperformed the baseline. In particular, the full spoken context approach achieved a significantly higher JGA, demonstrating the effectiveness of leveraging the entire spoken conversation as input. The competitive performance of the 10-queries method further suggests that a substantial portion of the speech representations is redundant, and that it is possible to reduce the input size without a significant loss in performance, provided that a sufficient number of queries is used. We next provide a fine-grained comparison based on slot group and dialogue turn analyses.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper presents a comparative study of context management strategies for end-to-end Spoken Dialog State Tracking using Speech-LLMs. We systematically evaluate traditional multimodal context (combining text history and spoken current turn), full spoken history, and compressed spoken history approaches. Our experiments on the SpokenWOZ corpus demonstrate that providing the full spoken conversation as input yields the highest performance among models of similar size, significantly surpassing prior methods. Furthermore, we show that attention-pooling-based compression of the spoken history offers a strong trade-off, maintaining competitive accuracy with reduced context size. Detailed analysis confirms that improvements stem from more effective context utilization.</span>\n</p>\n\n",
                "matched_terms": [
                    "context",
                    "management",
                    "compressed",
                    "approaches",
                    "spoken",
                    "multimodal",
                    "spokenwoz",
                    "full"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nSpeech-LLM, SpokenDST, Multimodal, Context Propagation</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "context",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Dialog State Tracking (DST) is a vital component in task-oriented dialog (TOD) systems&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib2\" title=\"\">2</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, enabling them to understand and maintain the context of a conversation over multiple turns. By accurately tracking user intents and relevant information, DST allows systems to reason over dialog states and effectively fulfill user requests.\nHowever, in the context of spoken dialog, Spoken DST remains a relatively immature research area, with current system performance significantly lagging behind those achieved in written dialog scenarios&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. One of the most common recent approaches is the cascade system. It typically involves an Automatic Speech Recognition (ASR) module followed by an eventual ASR correction module and then a written DST component&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, often based on models such as T5&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This pipeline approach leverages the strengths of existing text-based DST models and was notably popular in the DSTC11 challenge&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where it was used by the winning system, OLISIA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "approaches",
                    "context",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">End-to-end (E2E) systems have emerged as a promising alternative, as they may potentially mitigate the error propagation inherent in cascade systems.\nIn particular, </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> demonstrated the effectiveness of E2E approaches, particularly in fully spoken contexts without access to ground-truth transcriptions, such as the SpokenWOZ&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset. In these settings, E2E models have been shown to outperform traditional cascade systems.\nConcurrently, speech-aware large language models (LLMs), which are also considered end-to-end (E2E) systems, have gained increasing popularity in a variety of spoken language tasks, including automatic speech recognition (ASR) and response generation &#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Recent work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> applied speech-aware LLMs to the spoken DST task, achieving state-of-the-art performance in the SpokenWOZ dataset.</span>\n</p>\n\n",
                "matched_terms": [
                    "approaches",
                    "spoken",
                    "spokenwoz"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">A notable advantage of E2E systems is their flexibility in context management, as they can seamlessly integrate written and spoken information. For instance, </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> both utilize the spoken representation of the user&#8217;s last turn, but differ in how they handle the rest of the context: the former combines the spoken user turn with the written previous state, while the latter combines it with the written representations of all previous turns. This raises an important question. What would happen if we relied solely on spoken context, either by feeding the system the speech representations for the entire conversation or by condensing them using an intermediate module?</span>\n</p>\n\n",
                "matched_terms": [
                    "context",
                    "management",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we explore these possibilities for context management when using a Speech-LLM model.\nOur contributions are three-fold: </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">(a)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> we validate the use of Speech-LLMs as an accurate approach for spoken DST </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">(b)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> we propose two context management approaches reaching the SOTA and </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">(c)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> our best performing approach demonstrates a simple yet effective method: feeding the entire spoken conversation to the model without additional compression or modality mixing.</span>\n</p>\n\n",
                "matched_terms": [
                    "approaches",
                    "context",
                    "management",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; Compressed Spoken Context &#8227; 2.1 Context Management &#8227; 2 Methodology &#8227; The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> illustrates our proposed systems, composed of three main components: a speech encoder, a connector, and a Large Language Model (LLM). In order to reduce the context length, we optionally add a &#8221;compression module&#8221; between the connector and LLM. The speech encoder processes the entire dialog history and computes dense representations for each turn. These representations are then down-sampled, using x6 stride, and passed to the connector module, which maps the speech features into the LLM&#8217;s input space. They may be passed through the compression module for the approaches that need it. Finally, the LLM generates the dialogue state in an auto-regressive manner.</span>\n</p>\n\n",
                "matched_terms": [
                    "approaches",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In practice, the speech representation </span>\n  <math alttext=\"h_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px1.p3.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">h</mi>\n        <mi mathsize=\"0.900em\">n</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">h_{n}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is concatenated with embeddings that represent the prompt&#8217;s text, yielding a multimodal input sequence.\nDuring inference, the model autoregressively completes the prompt starting\nfrom the field </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">\"user_last_turn\"</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The generated ASR hypothesis\n</span>\n  <math alttext=\"{\\color[rgb]{0,.5,.5}U^{text}_{n}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px1.p3.m2\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathcolor=\"#008080\" mathsize=\"0.900em\" style=\"--ltx-fg-color:#008080;\">U</mi>\n        <mi mathcolor=\"#008080\" mathsize=\"0.900em\" style=\"--ltx-fg-color:#008080;\">n</mi>\n        <mrow>\n          <mi mathcolor=\"#008080\" mathsize=\"0.900em\" style=\"--ltx-fg-color:#008080;\">t</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathcolor=\"#008080\" mathsize=\"0.900em\" style=\"--ltx-fg-color:#008080;\">e</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathcolor=\"#008080\" mathsize=\"0.900em\" style=\"--ltx-fg-color:#008080;\">x</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathcolor=\"#008080\" mathsize=\"0.900em\" style=\"--ltx-fg-color:#008080;\">t</mi>\n        </mrow>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">{\\color[rgb]{0,.5,.5}U^{text}_{n}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is then fed back to construct the textual context\n</span>\n  <math alttext=\"Context_{n+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px1.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathcolor=\"#BF8040\" mathsize=\"0.900em\" style=\"--ltx-fg-color:#BF8040;\">C</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathcolor=\"#BF8040\" mathsize=\"0.900em\" style=\"--ltx-fg-color:#BF8040;\">o</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathcolor=\"#BF8040\" mathsize=\"0.900em\" style=\"--ltx-fg-color:#BF8040;\">n</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathcolor=\"#BF8040\" mathsize=\"0.900em\" style=\"--ltx-fg-color:#BF8040;\">t</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathcolor=\"#BF8040\" mathsize=\"0.900em\" style=\"--ltx-fg-color:#BF8040;\">e</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathcolor=\"#BF8040\" mathsize=\"0.900em\" style=\"--ltx-fg-color:#BF8040;\">x</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <msub>\n          <mi mathcolor=\"#BF8040\" mathsize=\"0.900em\" style=\"--ltx-fg-color:#BF8040;\">t</mi>\n          <mrow>\n            <mi mathcolor=\"#BF8040\" mathsize=\"0.900em\" style=\"--ltx-fg-color:#BF8040;\">n</mi>\n            <mo mathcolor=\"#BF8040\" mathsize=\"0.900em\" style=\"--ltx-fg-color:#BF8040;\">+</mo>\n            <mn mathcolor=\"#BF8040\" mathsize=\"0.900em\" style=\"--ltx-fg-color:#BF8040;\">1</mn>\n          </mrow>\n        </msub>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">Context_{n+1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for subsequent turns.</span>\n</p>\n\n",
                "matched_terms": [
                    "context",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">With this context-management strategy, </span><math alttext=\"\\text{Context}_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><msub><mtext mathsize=\"0.900em\">Context</mtext><mi mathsize=\"0.900em\">n</mi></msub><annotation encoding=\"application/x-tex\">\\text{Context}_{n}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">, corresponding to the full spoken conversation, is provided to the model. The model predicts the active domain </span><math alttext=\"{\\color[rgb]{0,0,0}D_{n}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.900em\">D</mi><mi mathsize=\"0.900em\">n</mi></msub><annotation encoding=\"application/x-tex\">{\\color[rgb]{0,0,0}D_{n}}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> and the dialogue state </span><math alttext=\"S_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><msub><mi mathcolor=\"#FF8000\" mathsize=\"0.900em\" style=\"--ltx-fg-color:#FF8000;\">S</mi><mi mathcolor=\"#FF8000\" mathsize=\"0.900em\" style=\"--ltx-fg-color:#FF8000;\">n</mi></msub><annotation encoding=\"application/x-tex\">S_{n}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe prompt employed for this strategy is:</span>\n<br class=\"ltx_break\"/></p>\n\n",
                "matched_terms": [
                    "full",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As in the multimodal context setting, the sequence of speech embeddings </span>\n  <math alttext=\"Speech\\_Emb\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px2.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">S</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">e</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">e</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">c</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">h</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">_</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">m</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">b</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">Speech\\_Emb</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is pre-pended to the embeddings of the textual part of the prompt before being fed to the LLM. During inference, the model receives the speech embeddings as input and auto-regressively generates the remaining fields of the prompt.</span>\n</p>\n\n",
                "matched_terms": [
                    "context",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The only difference with full spoken context is how </span>\n  <math alttext=\"Speech\\_Emb\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">S</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">e</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">e</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">c</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">h</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">_</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">m</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">b</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">Speech\\_Emb</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is obtained.\nInstead of using the entire sequences </span>\n  <math alttext=\"h_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">h</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">h_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we introduce a set of\n</span>\n  <math alttext=\"N_{\\text{queries}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px3.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">N</mi>\n        <mtext mathsize=\"0.900em\">queries</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">N_{\\text{queries}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> trainable query vectors </span>\n  <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px3.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">Q</mi>\n      <annotation encoding=\"application/x-tex\">Q</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and compute </span>\n  <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px3.p1.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">z</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">z_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> through query-based pooling using a TransformerDecoder architecture:</span>\n</p>\n\n",
                "matched_terms": [
                    "full",
                    "context",
                    "query",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this formulation, the decoder treats </span>\n  <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px3.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">Q</mi>\n      <annotation encoding=\"application/x-tex\">Q</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the target sequence\nand </span>\n  <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px3.p1.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">z</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">z_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the memory. Each decoder layer first applies\n</span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">self-attention</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> over the query tokens, allowing them to\ninteract and share information. It then applies\n</span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">cross-attention</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where the queries attend to the speech\nsequence </span>\n  <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px3.p1.m8\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">z</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">z_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, extracting the most relevant aspects from it.\nThe final output is a set of </span>\n  <math alttext=\"N_{\\text{queries}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px3.p1.m9\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">N</mi>\n        <mtext mathsize=\"0.900em\">queries</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">N_{\\text{queries}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> vectors\nthat serve as a compressed representation of the turn.\nThese vectors are concatenated and used in downstream\ndialogue modeling.</span>\n</p>\n\n",
                "matched_terms": [
                    "queries",
                    "compressed",
                    "query"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For the ASR pre-training stage, we train our model on a combination of the Loquacious Medium dataset (2,500 hours)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the Fisher corpus (1,960 hours)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the train split from SpokenWOZ dataset (200 hours)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Although SpokenWOZ does not provide ground-truth transcripts, we include it in the ASR pre-training phase because the speech encoder is frozen during DST fine-tuning, and we want the encoder to be exposed to the characteristics of SpokenWOZ data. To address the lack of transcripts on SpokenWOZ, we use Whisper-large-v3</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://huggingface.co/openai/whisper-large-v3</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to generate automatic transcriptions for SpokenWOZ audio. These generated transcripts are also used later for the multimodal context method in the DST stage.</span>\n</p>\n\n",
                "matched_terms": [
                    "spokenwoz",
                    "context",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For DST fine-tuning, we primarily use the SpokenWOZ dataset for both training and evaluation. As in </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib13\" title=\"\">13</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> we remove the nine corrupted dialogues from the SpokenWOZ test set</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/AlibabaResearch/DAMO-ConvAI/issues/87</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and report the Joint Goal Accuracy (JGA)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on both the dev and test sets.</span>\n</p>\n\n",
                "matched_terms": [
                    "jga",
                    "evaluation",
                    "dev",
                    "spokenwoz",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For our component selection, we use W2v-BERT </span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://huggingface.co/facebook/w2v-bert-2.0</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the speech encoder. The connector module is implemented as a single-layer Transformer encoder with a hidden dimension of 1024 and 16 attention heads. Similarly, we employ a one-layer Transformer Decoder with a hidden dimension of 1024, 16 heads, and a trainable number of queries (</span>\n  <math alttext=\"N_{queries}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">N</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">q</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">u</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">e</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">i</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">e</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">s</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">N_{queries}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) as the compression module. This module is also used for attention pooling by setting </span>\n  <math alttext=\"N_{queries}=1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">N</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">q</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">u</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">e</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">r</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">i</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">e</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">s</mi>\n          </mrow>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">N_{queries}=1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFor the language model, we use OLMo 2 1B</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\">\n    <sup class=\"ltx_note_mark\">4</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://huggingface.co/allenai/OLMo-2-0425-1B-Instruct</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We apply a LoRA adapter with a rank of 16 and an alpha value of 1, as determined by grid search. During inference, we employ beam search with 5 beams, which was also selected based on grid search results.\nDuring ASR pre-training, we use a virtual batch size of 256, a learning rate of </span>\n  <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and 5,000 warm-up steps. Training proceeds until the word error rate (WER) on the combined validation sets of all datasets ceases to improve.\nFor DST fine-tuning, we maintain the same virtual batch size of 256, use a learning rate of </span>\n  <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">2</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and 500 warm-up steps. The model is trained until the JGA on the validation set no longer improves.\nAll our experiments</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\">\n    <sup class=\"ltx_note_mark\">5</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>The source code will be released after acceptance</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> were performed using SpeechBrain toolkit</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\">\n    <sup class=\"ltx_note_mark\">6</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>https://github.com/speechbrain/speechbrain</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> &#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</p>\n\n",
                "matched_terms": [
                    "queries",
                    "jga"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For fair comparison with prior work, the reported JGA for our model in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.2 Implementation details &#8227; 3 Results &#8227; The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> uses post-processing, which includes (i) canonicalizing time expressions to 24-hour format and (ii) case-insensitive fuzzy matching for open/proper-noun slots with a Levenshtein&#160;ratio&#160;</span>\n  <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#8805;</mo>\n      <annotation encoding=\"application/x-tex\">\\geq</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;0.90, applied symmetrically to predictions and references.\nTable&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.2 Implementation details &#8227; 3 Results &#8227; The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> presents a comparison between published results on the SpokenWOZ test set and our two best systems: the compressed context method using 10 queries and the full spoken context method. For our systems the post-processing yields a 3 points JGA increase, which is comparable to the post-processing reported in </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Our approach substantially outperforms other systems of comparable size. To the best of our knowledge, the only system that surpasses our results is the Gemma-2-9B variant reported in&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We did not opt to train a Gemma-based variant of our model due to its high computational requirements, as our primary objective is to demonstrate the effectiveness of our method on small and compact models.\nFurthermore, as shown in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#S3.SS4\" style=\"font-size:90%;\" title=\"3.4 Context Management Methods Comparison &#8227; 3 Results &#8227; The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach\">\n    <span class=\"ltx_text ltx_ref_tag\">3.4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, when using the same model components, our context management strategy significantly outperforms that of previous work.</span>\n</p>\n\n",
                "matched_terms": [
                    "context",
                    "management",
                    "compressed",
                    "spokenwoz",
                    "jga",
                    "spoken",
                    "queries",
                    "full",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Slot Group Analysis</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe categorize slots into four groups: categorical, time, open, and profile. Categorical slots have a fixed set of values (e.g., yes/no, area, price range). Time slots correspond to temporal expressions (e.g., departure time). Open slots can take a wide range of values such as place names, while profile slots, which are treated separately for finer analysis, contain personal information (e.g., names, IDs, emails) and are often spelled out across multiple turns.\nFigure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#S3.F3.sf1\" style=\"font-size:90%;\" title=\"In Figure 3 &#8227; 3.4 Context Management Methods Comparison &#8227; 3 Results &#8227; The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach\">\n    <span class=\"ltx_text ltx_ref_tag\">3(a)</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows the average F1 score by slot type. All models perform well on categorical slots, with full spoken context slightly ahead. Performance drops for time and open slots, where full spoken context and 10-query compression clearly outperform the others. Profile slots are the hardest: full spoken context again leads, while the 1-query model performs worst, indicating that compressing each turn to a single embedding discards too much information.</span>\n</p>\n\n",
                "matched_terms": [
                    "full",
                    "context",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dialogue Turn Analysis</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nFigure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#S3.F3.sf2\" style=\"font-size:90%;\" title=\"In Figure 3 &#8227; 3.4 Context Management Methods Comparison &#8227; 3 Results &#8227; The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach\">\n    <span class=\"ltx_text ltx_ref_tag\">3(b)</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> displays the evolution of Joint Goal Accuracy (JGA) across dialogue turns. All models perform well in the early turns (1&#8211;5), but accuracy declines quickly in the mid turns (5&#8211;30) and approaches zero by turn 40. This drop can be attributed to the increasing length and complexity of dialogue states, combined with the strictness of the JGA metric, as well as the limited capacity of the relatively small LLM used in our experiments.\nThe full spoken context method consistently outperforms the others, particularly during the mid turns. In the very late turns, it shows occasional performance peaks, though these are difficult to interpret given the small sample size. The 10-query attention pooling method remains competitive, but still underperforms compared to full spoken context in the late turns, even though it benefits from a much smaller context size.</span>\n</p>\n\n",
                "matched_terms": [
                    "context",
                    "jga",
                    "approaches",
                    "spoken",
                    "full"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Additional Experiences</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> To further understand the contributions of individual components and design choices in our system, we conducted a series of ablation studies and supplementary experiments. Specifically, we investigated the impact of ASR pretraining data, the connector, the compression module, and DST data preprocessing. For ASR pretraining, we compared using the LibriSpeech dataset&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> alone versus the mixed dataset described in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#S3.SS1\" style=\"font-size:90%;\" title=\"3.1 Datasets &#8227; 3 Results &#8227; The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach\">\n    <span class=\"ltx_text ltx_ref_tag\">3.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In baseline experiments with the multimodal method, we observed that when the encoder is unfrozen during DST finetuning, the choice of ASR pretraining data has little impact. However, when freezing the encoder (which is a more practical setup for the Full/Compressed Spoken Context methods), we found that relying solely on LibriSpeech resulted in up to a 3-point drop in JGA compared to using the mixed dataset. During ASR pretraining, we also experimented with different numbers of layers (1, 2, and 4) in the encoder. We found that a single layer provided the fastest convergence and the best performance. For the compression module, we varied the number of layers and found that increasing to three layers led to a 2% absolute drop in JGA. We attribute this to the limited amount of DST finetuning data, as the compression module is only initialized at this stage. Finally, for the multimodal context method, we normalized Whisper transcripts using NeMo Inverse Text Normalization (ITN)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09424v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, along with additional processing for time expressions. This preprocessing yielded a 1% absolute gain in JGA.</span>\n</p>\n\n",
                "matched_terms": [
                    "context",
                    "baseline",
                    "jga",
                    "different",
                    "spoken",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">While our full spoken context approach achieves the highest performance, it could become computationally demanding for very long dialogues. The compressed context method offers a good compromise, with strong results and reduced input size. Additionally, we did not scale our experiments to larger LLMs such as Gemma-2-9B. Both directions will be explored in future work.</span>\n</p>\n\n",
                "matched_terms": [
                    "full",
                    "context",
                    "compressed",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we have proposed a fully E2E approach to Spoken Dialog State Tracking, drawing inspiration from Speech-LLMs.\nIn contrast to traditional multimodal context approaches, we show that it is possible to use the entire spoken conversation as input (until the current turn) and achieve state-of-the-art results.\nWe also have performed a fine-grained analysis to illustrate the causes of improvements brought by using a full spoken context: less error propagation through the dialog and better performance on the most challenging slots.\nIn future work, a more sophisticated and compact handling of the spoken context may be explored. Moreover, scaling the used model would be a promising extension.</span>\n</p>\n\n",
                "matched_terms": [
                    "context",
                    "approaches",
                    "spoken",
                    "multimodal",
                    "full"
                ]
            }
        ]
    }
}