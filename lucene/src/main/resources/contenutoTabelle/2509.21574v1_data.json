{
    "S4.T1": {
        "source_file": "X-Streamer: Unified Human World Modeling with Audiovisual Interaction",
        "caption": "Table 1: Quantitative evaluation. The best and second-best scores are highlighted.",
        "body": "Method\nCPBD↑\\uparrow\n\nFVD↓\\downarrow\n\nID-Sim↑\\uparrow\n\nSynC↑\\uparrow\n\nSynD↓\\downarrow\n\nGlo↑\\uparrow\n\nExp↑\\uparrow\n\nID↑\\uparrow\n\nLip↑\\uparrow\n\nDiv↑\\uparrow\n\nVQ↑\\uparrow\n\n\n\n\n\nJoyVasa\n0.37\n748.99\n0.73\n2.84\n11.10\n0.03\n0.021\n0.1\n0.13\n0.08\n0.08\n\n\nSadTalker\n0.20\n777.52\n0.78\n3.39\n11.15\n0.04\n0.035\n0.13\n0.08\n0.05\n0.06\n\n\nPD-FGC\n0.17\n1183.82\n0.42\n4.22\n11.20\n0.003\n0.008\n0\n0.09\n0.02\n0\n\n\nX-Streamer\n0.55\n573.36\n0.75\n3.41\n10.93\n0.081\n0.033\n0.77\n0.7\n0.85\n0.86",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding:0.25pt 4.0pt;\">Method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" style=\"padding:0.25pt 4.0pt;\">CPBD<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" style=\"padding:0.25pt 4.0pt;\">FVD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" style=\"padding:0.25pt 4.0pt;\">ID-Sim<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" style=\"padding:0.25pt 4.0pt;\">SynC<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" style=\"padding:0.25pt 4.0pt;\">SynD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" style=\"padding:0.25pt 4.0pt;\">Glo<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_border_t\" style=\"padding:0.25pt 4.0pt;\">Exp<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" style=\"padding:0.25pt 4.0pt;\">ID<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" style=\"padding:0.25pt 4.0pt;\">Lip<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" style=\"padding:0.25pt 4.0pt;\">Div<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m10\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" style=\"padding:0.25pt 4.0pt;\">VQ<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m11\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:0.25pt 4.0pt;\">JoyVasa</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.25pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.37</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.25pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">748.99</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.25pt 4.0pt;\">0.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.25pt 4.0pt;\">2.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.25pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">11.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.25pt 4.0pt;\">0.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.25pt 4.0pt;\">0.021</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.25pt 4.0pt;\">0.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.25pt 4.0pt;\">0.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.25pt 4.0pt;\">0.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.25pt 4.0pt;\">0.08</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:0.25pt 4.0pt;\">SadTalker</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 4.0pt;\">0.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 4.0pt;\">777.52</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.78</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 4.0pt;\">3.39</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 4.0pt;\">11.15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.035</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 4.0pt;\">0.13</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 4.0pt;\">0.08</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 4.0pt;\">0.05</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 4.0pt;\">0.06</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:0.25pt 4.0pt;\">PD-FGC</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 4.0pt;\">0.17</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 4.0pt;\">1183.82</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 4.0pt;\">0.42</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">4.22</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 4.0pt;\">11.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 4.0pt;\">0.003</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.25pt 4.0pt;\">0.008</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 4.0pt;\">0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 4.0pt;\">0.09</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 4.0pt;\">0.02</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 4.0pt;\">0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding:0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">X-Streamer</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.55</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">573.36</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:0.25pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:0.25pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.41</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">10.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.081</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding:0.25pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.033</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.77</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.86</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "fvd↓downarrow",
            "secondbest",
            "vq↑uparrow",
            "lip↑uparrow",
            "div↑uparrow",
            "joyvasa",
            "evaluation",
            "best",
            "scores",
            "sync↑uparrow",
            "xstreamer",
            "id↑uparrow",
            "method",
            "idsim↑uparrow",
            "pdfgc",
            "glo↑uparrow",
            "highlighted",
            "exp↑uparrow",
            "synd↓downarrow",
            "quantitative",
            "sadtalker",
            "cpbd↑uparrow"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluate X-Streamer against real-time audio-driven portrait animation baselines using metrics that assess visual fidelity, identity preservation, audiovisual synchronization, and temporal dynamics (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#S4.T1\" title=\"Table 1 &#8227; Baselines. &#8227; 4.2 Evaluation &#8227; 4 Experiments &#8227; X-Streamer: Unified Human World Modeling with Audiovisual Interaction\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nVisual quality is measured with Cumulative Probability of Blur Detection (CPBD<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Narvekar &amp; Karam (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib60\" title=\"\">2011</a>)</cite>) and Fr&#233;chet Video Distance (FVD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Unterthiner et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib81\" title=\"\">2019</a>)</cite>). Identity consistency is quantified by cosine similarity of ArcFace embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Deng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib20\" title=\"\">2019</a>)</cite>, reported as ID-Sim<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>. Audiovisual alignment is evaluated with SynC<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> and SynD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chung &amp; Zisserman (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib17\" title=\"\">2016</a>)</cite>, which measure speech&#8211;lip synchronization. Naturalistic dynamics are captured with Global Motion (Glo<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>) and Dynamic Expression (Exp<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>), quantifying head motion and upper-face expressions while excluding the mouth region.\nIn addition to objective metrics, we conduct a user study (20 participants, 100 choices per dimension) comparing our method and baselines across four aspects: identity preservation (ID<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>), lip synchronization (Lip<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>), motion diversity (Div<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m10\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>), and overall video quality (VQ<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m11\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>).</p>\n\n",
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#S4.T1\" title=\"Table 1 &#8227; Baselines. &#8227; 4.2 Evaluation &#8227; 4 Experiments &#8227; X-Streamer: Unified Human World Modeling with Audiovisual Interaction\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method outperforms all baselines in visual fidelity (CPBD<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Narvekar &amp; Karam (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib60\" title=\"\">2011</a>)</cite>, FVD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Unterthiner et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib81\" title=\"\">2019</a>)</cite>, and VQ<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>) as well as motion dynamics (Glo<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> and Div<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>). While X-Streamer ranks second in objective ID similarity (ID-Sim<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p2.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>) due to SadTalker&#8217;s restricted motion and zoomed-in facial framing, the user study highlights X-Streamer&#8217;s superior identity preservation (ID<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p2.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>). Our approach also demonstrates strong lip synchronization, achieving the lowest SynD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p2.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> alongside superior Lip<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p2.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> scores.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Real-time audiovisual interaction remains underexplored&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ao (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib4\" title=\"\">2024</a>); Low &amp; Wang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib54\" title=\"\">2025</a>); Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib105\" title=\"\">2025</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib15\" title=\"\">2025c</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib87\" title=\"\">2025b</a>)</cite>, with no open-source methods currently available. We therefore compare X-Streamer against representative <em class=\"ltx_emph ltx_font_italic\">real-time audio-driven</em> portrait animation work: JoyVasa&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Cao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib11\" title=\"\">2024</a>)</cite>, an open-source implementation of VASA-1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Xu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib91\" title=\"\">2024b</a>)</cite>; SadTalker&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib100\" title=\"\">2023</a>)</cite>, a GAN-based method decoding from implicit facial motion latents; and PD-FGC&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib84\" title=\"\">2023</a>)</cite>, which offers disentangled control over lip motion and facial expressions. For fairness, all baselines are driven by audio synthesized with X-Streamer, and evaluations are conducted on generated videos at a fixed resolution of <math alttext=\"256\\times 256\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>256</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>256</mn></mrow><annotation encoding=\"application/x-tex\">256\\times 256</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "joyvasa",
                    "method",
                    "sadtalker",
                    "pdfgc",
                    "xstreamer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qualitative comparisons between X-Streamer and baseline methods are presented in Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#S4.F4\" title=\"Figure 4 &#8227; 4 Experiments &#8227; X-Streamer: Unified Human World Modeling with Audiovisual Interaction\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. X-Streamer generalizes well to chest-level portraits and remains robust under occlusion, side views, and complex environments, producing dynamic and natural motions. In contrast, SadTalker<cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib100\" title=\"\">2023</a>)</cite> and PD-FGC&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib84\" title=\"\">2023</a>)</cite> focus narrowly on facial regions and often exhibit artifacts when the face is partially occluded (e.g., the microphone obscuring the mouth in the left example). JoyVasa&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Cao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib11\" title=\"\">2024</a>)</cite> shows stronger robustness but generates motion that is relatively rigid and constrained, whereas X-Streamer produces coordinated head movements and expressive hand gestures, yielding more lifelike interactions.\nWe further compare with CausVid&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Yin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib94\" title=\"\">2025</a>)</cite> (bottom rows in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#S4.F4\" title=\"Figure 4 &#8227; 4 Experiments &#8227; X-Streamer: Unified Human World Modeling with Audiovisual Interaction\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) to evaluate stability in long-horizon video streaming. CausVid remains stable for the first few seconds, but its spatial fidelity and identity consistency degrade noticeably after around 10 seconds. Similarly, SkyReels-V2 and MAGI-1, though running offline, suffer from identity drift and color inconsistencies within 30 seconds. In contrast, X-Streamer maintains temporally stable generation with consistent identity throughout the entire sequence.</p>\n\n",
                "matched_terms": [
                    "joyvasa",
                    "pdfgc",
                    "xstreamer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We ablate key components of our framework by replacing them with alternative designs and evaluating on the test set. Quantitative results are reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#S4.T2\" title=\"Table 2 &#8227; Baselines. &#8227; 4.2 Evaluation &#8227; 4 Experiments &#8227; X-Streamer: Unified Human World Modeling with Audiovisual Interaction\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, with visual comparisons in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#S4.F5\" title=\"Figure 5 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; X-Streamer: Unified Human World Modeling with Audiovisual Interaction\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and on our supplementary webpage.\nReplacing diffusion forcing with standard teacher forcing causes prediction errors to accumulate, leading to motion drift and degraded visual quality. This variant shows the highest Glo<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> due to undesirable motion artifacts, consistent with its lowest FVD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>.\nRemoving the global identity reference forces the model to rely sorely on visual history, which leads to facial distortions and color drift in long-horizon sequences, as reflected in lower ID-Sim<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>.\nFinally, replacing our spatiotemporal attention design (temporal-causal with spatially bidirectional attention) with fully causal token-wise attention reduces temporal coherence and weakens visual fidelity, lowering CPBD<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> and worsening FVD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>.\nTogether, these results confirm that our full model achieves stable long-duration video streaming with strong fidelity and identity consistency.</p>\n\n",
                "matched_terms": [
                    "glo↑uparrow",
                    "fvd↓downarrow",
                    "quantitative",
                    "idsim↑uparrow",
                    "cpbd↑uparrow"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "X-Streamer: Unified Human World Modeling with Audiovisual Interaction",
        "caption": "Table 2: Qualitative Ablation. The best and second-best scores are highlighted.",
        "body": "Method\nCPBD ↑\\uparrow\n\nFVD ↓\\downarrow\n\nID-Sim ↑\\uparrow\n\nGlo ↑\\uparrow\n\nExp ↑\\uparrow\n\n\n\n\nw/o diffusion forcing\n0.17\n1989.52\n0.29\n0.246\n0.012\n\n\n\nw/o global ID ref\n0.26\n794.78\n0.41\n0.053\n0.02\n\n\n\ntoken-wise causal attn\n0.37\n628.76\n0.70\n0.035\n0.023\n\n\n\nX-Streamer\n0.55\n573.36\n0.7542\n0.081\n0.033",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:12.0pt;padding-right:12.0pt;\">Method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:12.0pt;padding-right:12.0pt;\">CPBD <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:12.0pt;padding-right:12.0pt;\">FVD <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:12.0pt;padding-right:12.0pt;\">ID-Sim <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:12.0pt;padding-right:12.0pt;\">Glo <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:12.0pt;padding-right:12.0pt;\">Exp <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_border_tt\" style=\"padding-left:12.0pt;padding-right:12.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:12.0pt;padding-right:12.0pt;\">w/o diffusion forcing</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:12.0pt;padding-right:12.0pt;\">0.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:12.0pt;padding-right:12.0pt;\">1989.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:12.0pt;padding-right:12.0pt;\">0.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:12.0pt;padding-right:12.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.246</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:12.0pt;padding-right:12.0pt;\">0.012</td>\n<td class=\"ltx_td ltx_nopad_r ltx_border_t\" style=\"padding-left:12.0pt;padding-right:12.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:12.0pt;padding-right:12.0pt;\">w/o global ID ref</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:12.0pt;padding-right:12.0pt;\">0.26</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:12.0pt;padding-right:12.0pt;\">794.78</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:12.0pt;padding-right:12.0pt;\">0.41</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:12.0pt;padding-right:12.0pt;\">0.053</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:12.0pt;padding-right:12.0pt;\">0.02</td>\n<td class=\"ltx_td ltx_nopad_r\" style=\"padding-left:12.0pt;padding-right:12.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:12.0pt;padding-right:12.0pt;\">token-wise causal attn</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:12.0pt;padding-right:12.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.37</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:12.0pt;padding-right:12.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">628.76</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:12.0pt;padding-right:12.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.70</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:12.0pt;padding-right:12.0pt;\">0.035</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:12.0pt;padding-right:12.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.023</span></td>\n<td class=\"ltx_td ltx_nopad_r\" style=\"padding-left:12.0pt;padding-right:12.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding-left:12.0pt;padding-right:12.0pt;\"><span class=\"ltx_text ltx_font_bold\">X-Streamer</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:12.0pt;padding-right:12.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.55</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:12.0pt;padding-right:12.0pt;\"><span class=\"ltx_text ltx_font_bold\">573.36</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:12.0pt;padding-right:12.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.7542</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:12.0pt;padding-right:12.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.081</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:12.0pt;padding-right:12.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.033</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_border_bb ltx_border_t\" style=\"padding-left:12.0pt;padding-right:12.0pt;\"/>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "forcing",
            "secondbest",
            "diffusion",
            "↑uparrow",
            "best",
            "causal",
            "scores",
            "global",
            "xstreamer",
            "exp",
            "method",
            "ablation",
            "↓downarrow",
            "attn",
            "cpbd",
            "idsim",
            "fvd",
            "tokenwise",
            "highlighted",
            "qualitative",
            "ref",
            "glo"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We ablate key components of our framework by replacing them with alternative designs and evaluating on the test set. Quantitative results are reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#S4.T2\" title=\"Table 2 &#8227; Baselines. &#8227; 4.2 Evaluation &#8227; 4 Experiments &#8227; X-Streamer: Unified Human World Modeling with Audiovisual Interaction\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, with visual comparisons in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#S4.F5\" title=\"Figure 5 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; X-Streamer: Unified Human World Modeling with Audiovisual Interaction\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and on our supplementary webpage.\nReplacing diffusion forcing with standard teacher forcing causes prediction errors to accumulate, leading to motion drift and degraded visual quality. This variant shows the highest Glo<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> due to undesirable motion artifacts, consistent with its lowest FVD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>.\nRemoving the global identity reference forces the model to rely sorely on visual history, which leads to facial distortions and color drift in long-horizon sequences, as reflected in lower ID-Sim<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>.\nFinally, replacing our spatiotemporal attention design (temporal-causal with spatially bidirectional attention) with fully causal token-wise attention reduces temporal coherence and weakens visual fidelity, lowering CPBD<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> and worsening FVD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>.\nTogether, these results confirm that our full model achieves stable long-duration video streaming with strong fidelity and identity consistency.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce X-Streamer, an end-to-end multimodal human world modeling framework for building digital human agents capable of infinite interactions across text, speech, and video within a single unified architecture. Starting from a single portrait, X-Streamer enables real-time, open-ended video calls driven by streaming multimodal inputs. At its core is a Thinker&#8211;Actor dual-transformer architecture that unifies multimodal understanding and generation, turning a static portrait into persistent and intelligent audiovisual interactions. The Thinker module perceives and reasons over streaming user inputs, while its hidden states are translated by the Actor into synchronized multimodal streams in real time. Concretely, the Thinker leverages a pretrained large language&#8211;speech model, while the Actor employs a chunk-wise autoregressive diffusion model that cross-attends to the Thinker&#8217;s hidden states to produce time-aligned multimodal responses with interleaved discrete text and audio tokens and continuous video latents. To ensure long-horizon stability, we design inter- and intra-chunk attentions with time-aligned multimodal positional embeddings for fine-grained cross-modality alignment and context retention, further reinforced by chunk-wise diffusion forcing and global identity referencing. X-Streamer runs in real time on two A100 GPUs, sustaining hours-long consistent video chat experiences from arbitrary portraits and paving the way toward unified world modeling of interactive digital humans. Please\nrefer to <a class=\"ltx_ref ltx_href\" href=\"https://byteaigc.github.io/X-Streamer\" title=\"\">https://byteaigc.github.io/X-Streamer</a> for more results.</p>\n\n",
                "matched_terms": [
                    "forcing",
                    "xstreamer",
                    "diffusion",
                    "global"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To achieve this, we adopt a Thinker&#8211;Actor architecture, inspired by Qwen2.5-Omni&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Xu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib89\" title=\"\">2025</a>)</cite>, which mirrors human cognition and behavior through synergistic dual-track multimodal autoregressive models. The Thinker module leverages a pretrained language&#8211;speech model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib96\" title=\"\">2024</a>)</cite> to provide conversational intelligence by interpreting user intent from streaming text and audio queries. Its hidden embeddings are then autoregressively translated by the Actor, a learnable module also initialized from a pretrained language model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib96\" title=\"\">2024</a>)</cite>, into interleaved discrete text and audio tokens alongside continuous video latent tokens.\nOur design preserves the pretrained language&#8211;speech capabilities while extending them to the video modality through autoregressive diffusion in a continuous latent space. To satisfy real-time constraints while maintaining long-range temporal coherence, we adopt a highly compressed video VAE latent tokenization&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">HaCohen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib33\" title=\"\">2024</a>)</cite>. Within the Actor, temporal continuity and semantic alignment across modalities are enforced by cross-attention between the Thinker&#8217;s audio&#8211;text hidden states and the visual tokens. All outputs are temporally synchronized using a unified 3D multimodal rotary positional embedding (RoPE) and generated in an interleaved manner to minimize latency.\nFor long-horizon stability, we employ a chunk-wise diffusion-forcing scheme&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib12\" title=\"\">2024</a>)</cite> and an optimized inference-time noise scheduler, reinforced by lightweight global reference image conditioning.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "global"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib36\" title=\"\">2020</a>); Song et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib70\" title=\"\">2020</a>); Rombach et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib65\" title=\"\">2022</a>)</cite> have become the dominant paradigm for video generation, training models to iteratively denoise sequences from noisy inputs. Existing approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib37\" title=\"\">2022</a>); Blattmann et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib7\" title=\"\">2023a</a>); Mei &amp; Patel (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib58\" title=\"\">2023</a>); Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib56\" title=\"\">2024</a>); Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib93\" title=\"\">2024</a>); Wan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib83\" title=\"\">2025</a>); Kong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib46\" title=\"\">2024</a>); Gao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib25\" title=\"\">2025</a>)</cite> typically adopt uniform-step schedulers during training and inference to preserve temporal consistency, but their reliance on fixed-length sequences limits scalability to streaming settings with variable horizons. Chunk-wise diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Blattmann et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib8\" title=\"\">2023b</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib16\" title=\"\">2023</a>); Luo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib55\" title=\"\">2023</a>); Voleti et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib82\" title=\"\">2022</a>)</cite> extend sequence length via sliding windows, yet still suffer motion and semantic discontinuities due to restricted context. Autoregressive approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Yan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib92\" title=\"\">2021</a>); Hong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib38\" title=\"\">2022</a>); Ge et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib27\" title=\"\">2022</a>); Yu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib95\" title=\"\">2023</a>); Kondratyuk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib44\" title=\"\">2023</a>)</cite> instead generate frames sequentially conditioned on past outputs, but error accumulation under teacher forcing&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Rasul et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib64\" title=\"\">2021</a>)</cite> leads to drift and quality degradation over long horizons. Recent work mitigates this mismatch through self-forcing strategies&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib40\" title=\"\">2025b</a>); Lin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib50\" title=\"\">2025b</a>)</cite>, narrowing the training&#8211;inference gap. Asynchronous diffusion methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib12\" title=\"\">2024</a>); Song et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib71\" title=\"\">2025b</a>); Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib53\" title=\"\">2024b</a>); Sun et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib74\" title=\"\">2025</a>); Kodaira et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib43\" title=\"\">2025</a>); Teng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib78\" title=\"\">2025</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib13\" title=\"\">2025a</a>)</cite> further enhance robustness by applying independent noise schedules per frame, reducing drift and corruption across extended sequences. Building on these advances, we unify multimodal generation with chunk-wise autoregressive video diffusion under asynchronous noise&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib12\" title=\"\">2024</a>)</cite>, enabling infinite-horizon, real-time multimodal interaction for digital humans.</p>\n\n",
                "matched_terms": [
                    "forcing",
                    "diffusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While diffusion forcing alleviates error accumulation in video generation, maintaining long-range identity consistency remains challenging, directly impacting user immersion and interaction quality. Instead of relying on a heavyweight reference network to repeatedly inject identity features of <math alttext=\"I_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><msub><mi>I</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">I_{s}</annotation></semantics></math>, we adopt a simpler yet effective approach: treating <math alttext=\"i_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi>i</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">i_{s}</annotation></semantics></math> as a global condition and placing it at the start of the context sequence. This allows all generated video latents to consistently attend to the identity tokens. Notably, we observe that under this setup the model learns to balance identity cues dynamically, drawing from the global identity embedding while also leveraging historical context, resulting in outputs that are both coherent and identity-preserving.</p>\n\n",
                "matched_terms": [
                    "forcing",
                    "diffusion",
                    "global"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Real-time audiovisual interaction remains underexplored&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ao (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib4\" title=\"\">2024</a>); Low &amp; Wang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib54\" title=\"\">2025</a>); Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib105\" title=\"\">2025</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib15\" title=\"\">2025c</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib87\" title=\"\">2025b</a>)</cite>, with no open-source methods currently available. We therefore compare X-Streamer against representative <em class=\"ltx_emph ltx_font_italic\">real-time audio-driven</em> portrait animation work: JoyVasa&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Cao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib11\" title=\"\">2024</a>)</cite>, an open-source implementation of VASA-1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Xu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib91\" title=\"\">2024b</a>)</cite>; SadTalker&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib100\" title=\"\">2023</a>)</cite>, a GAN-based method decoding from implicit facial motion latents; and PD-FGC&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib84\" title=\"\">2023</a>)</cite>, which offers disentangled control over lip motion and facial expressions. For fairness, all baselines are driven by audio synthesized with X-Streamer, and evaluations are conducted on generated videos at a fixed resolution of <math alttext=\"256\\times 256\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>256</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>256</mn></mrow><annotation encoding=\"application/x-tex\">256\\times 256</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "xstreamer",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Real-time video streaming remains underexplored&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Yin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib94\" title=\"\">2025</a>); Lin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib50\" title=\"\">2025b</a>); Kodaira et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib43\" title=\"\">2025</a>); Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib40\" title=\"\">2025b</a>)</cite>. We compare our model against the publicly available method of&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Yin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib94\" title=\"\">2025</a>)</cite>. The Self Forcing approach&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib40\" title=\"\">2025b</a>)</cite> is excluded, as its released model supports only short generations under 10 seconds, whereas our setting requires sustained interaction lasting minutes to hours. We also include SkyReels-V2 (1.3B)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib13\" title=\"\">2025a</a>)</cite> and MAGI-1 (4.5B)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Teng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib78\" title=\"\">2025</a>)</cite> as autoregressive video diffusion baselines, though they require hours to synthesize a single one-minute video. Notably, these baselines are neither audio-conditioned nor capable of generating audio; for fairness, we condition their video outputs on a fixed text prompt.</p>\n\n",
                "matched_terms": [
                    "forcing",
                    "method",
                    "diffusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qualitative comparisons between X-Streamer and baseline methods are presented in Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#S4.F4\" title=\"Figure 4 &#8227; 4 Experiments &#8227; X-Streamer: Unified Human World Modeling with Audiovisual Interaction\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. X-Streamer generalizes well to chest-level portraits and remains robust under occlusion, side views, and complex environments, producing dynamic and natural motions. In contrast, SadTalker<cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib100\" title=\"\">2023</a>)</cite> and PD-FGC&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib84\" title=\"\">2023</a>)</cite> focus narrowly on facial regions and often exhibit artifacts when the face is partially occluded (e.g., the microphone obscuring the mouth in the left example). JoyVasa&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Cao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib11\" title=\"\">2024</a>)</cite> shows stronger robustness but generates motion that is relatively rigid and constrained, whereas X-Streamer produces coordinated head movements and expressive hand gestures, yielding more lifelike interactions.\nWe further compare with CausVid&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Yin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib94\" title=\"\">2025</a>)</cite> (bottom rows in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#S4.F4\" title=\"Figure 4 &#8227; 4 Experiments &#8227; X-Streamer: Unified Human World Modeling with Audiovisual Interaction\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) to evaluate stability in long-horizon video streaming. CausVid remains stable for the first few seconds, but its spatial fidelity and identity consistency degrade noticeably after around 10 seconds. Similarly, SkyReels-V2 and MAGI-1, though running offline, suffer from identity drift and color inconsistencies within 30 seconds. In contrast, X-Streamer maintains temporally stable generation with consistent identity throughout the entire sequence.</p>\n\n",
                "matched_terms": [
                    "qualitative",
                    "xstreamer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate X-Streamer against real-time audio-driven portrait animation baselines using metrics that assess visual fidelity, identity preservation, audiovisual synchronization, and temporal dynamics (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#S4.T1\" title=\"Table 1 &#8227; Baselines. &#8227; 4.2 Evaluation &#8227; 4 Experiments &#8227; X-Streamer: Unified Human World Modeling with Audiovisual Interaction\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nVisual quality is measured with Cumulative Probability of Blur Detection (CPBD<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Narvekar &amp; Karam (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib60\" title=\"\">2011</a>)</cite>) and Fr&#233;chet Video Distance (FVD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Unterthiner et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib81\" title=\"\">2019</a>)</cite>). Identity consistency is quantified by cosine similarity of ArcFace embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Deng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib20\" title=\"\">2019</a>)</cite>, reported as ID-Sim<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>. Audiovisual alignment is evaluated with SynC<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> and SynD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chung &amp; Zisserman (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib17\" title=\"\">2016</a>)</cite>, which measure speech&#8211;lip synchronization. Naturalistic dynamics are captured with Global Motion (Glo<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>) and Dynamic Expression (Exp<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>), quantifying head motion and upper-face expressions while excluding the mouth region.\nIn addition to objective metrics, we conduct a user study (20 participants, 100 choices per dimension) comparing our method and baselines across four aspects: identity preservation (ID<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>), lip synchronization (Lip<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>), motion diversity (Div<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m10\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>), and overall video quality (VQ<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m11\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "xstreamer",
                    "global",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#S4.T1\" title=\"Table 1 &#8227; Baselines. &#8227; 4.2 Evaluation &#8227; 4 Experiments &#8227; X-Streamer: Unified Human World Modeling with Audiovisual Interaction\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method outperforms all baselines in visual fidelity (CPBD<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Narvekar &amp; Karam (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib60\" title=\"\">2011</a>)</cite>, FVD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Unterthiner et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib81\" title=\"\">2019</a>)</cite>, and VQ<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>) as well as motion dynamics (Glo<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> and Div<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>). While X-Streamer ranks second in objective ID similarity (ID-Sim<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p2.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>) due to SadTalker&#8217;s restricted motion and zoomed-in facial framing, the user study highlights X-Streamer&#8217;s superior identity preservation (ID<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p2.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>). Our approach also demonstrates strong lip synchronization, achieving the lowest SynD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p2.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> alongside superior Lip<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p2.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> scores.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "xstreamer",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced X-Streamer, an end-to-end multimodal interactive human world modeling framework that unifies text, speech, and video understanding and generation within a single architecture. At its core, we proposed a Thinker&#8211;Actor dual-transformer design: the Thinker performs conversational reasoning, while the Actor converts its hidden states into synchronized, streaming multimodal responses. Extending language models to the video modality with chunk-wise diffusion forcing, our framework balances real-time efficiency, long-range consistency, and temporal multimodal synchronization. Extensive experiments demonstrate that X-Streamer is a significant step toward persistent, interactive, and intelligent digital humans and world modeling.</p>\n\n",
                "matched_terms": [
                    "forcing",
                    "xstreamer",
                    "diffusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Diffusion forcing&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#bib.bib12\" title=\"\">2024</a>)</cite> organizes the denoising schedule for each latent through a scheduling matrix <math alttext=\"\\mathcal{K}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119974;</mi><annotation encoding=\"application/x-tex\">\\mathcal{K}</annotation></semantics></math>. Building on this idea, we propose a chunk-wise pyramid variant, <math alttext=\"\\mathcal{K}^{\\text{chunk}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m2\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119974;</mi><mtext>chunk</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{K}^{\\text{chunk}}</annotation></semantics></math>, where denoising proceeds sequentially across chunks. This design enables chunk-level parallelism during inference and reduces the number of forward passes from the conventional <math alttext=\"|c|\\times N\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>c</mi><mo rspace=\"0.055em\" stretchy=\"false\">|</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">|c|\\times N</annotation></semantics></math> required by a chunk-by-chunk DDIM scheduler to <math alttext=\"|c|+N-1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mrow><mrow><mo stretchy=\"false\">|</mo><mi>c</mi><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mi>N</mi></mrow><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">|c|+N-1</annotation></semantics></math>, where <math alttext=\"|c|\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><mi>c</mi><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|c|</annotation></semantics></math> is the number of chunks and <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m6\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> the number of denoising steps. An illustration of <math alttext=\"\\mathcal{K}^{\\text{chunk}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m7\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119974;</mi><mtext>chunk</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{K}^{\\text{chunk}}</annotation></semantics></math> is shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21574v1#A1.F7\" title=\"Figure 7 &#8227; A.3 Extending X-Streamer with Visual Perception &#8227; Appendix A Appendix &#8227; X-Streamer: Unified Human World Modeling with Audiovisual Interaction\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. Each row in the matrix specifies the noise level assigned to tokens at a given denoising round. Starting from a fully noised sequence (top row), the algorithm progressively denoises chunks in order, refining their latent representations. The height of <math alttext=\"\\mathcal{K}^{\\text{chunk}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.p1.m8\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119974;</mi><mtext>chunk</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{K}^{\\text{chunk}}</annotation></semantics></math> thus corresponds to the total number of forward passes needed to generate the full sequence.</p>\n\n",
                "matched_terms": [
                    "forcing",
                    "diffusion"
                ]
            }
        ]
    }
}