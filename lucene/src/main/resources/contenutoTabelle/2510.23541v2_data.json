{
    "S3.T1": {
        "caption": "Table 1: \nTTS performance of different models on the Seed test sets (test-zh for Chinese, test-en for English). Arrows indicate the desired direction (↓\\downarrow = lower is better, ↑\\uparrow = higher is better). Best values per column are in bold. * represents results reproduced from the official release.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:17.1pt;padding-right:17.1pt;\">&#8195;&#8194;&#8201;SIM (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n</tr>\n</table>\n",
        "informative_terms_identified": [
            "models",
            "column",
            "direction",
            "seed",
            "↓downarrow",
            "desired",
            "tts",
            "official",
            "reproduced",
            "from",
            "release",
            "test",
            "represents",
            "performance",
            "english",
            "indicate",
            "sets",
            "bold",
            "higher",
            "lower",
            "results",
            "arrows",
            "values",
            "↑uparrow",
            "better",
            "best",
            "sim",
            "testen",
            "chinese",
            "different",
            "testzh"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To evaluate the zero-shot (voice cloning) TTS capability of SoulX-Podcast, we assess its performance on Seed-TTS-eval and compare it with existing zero-shot TTS models. The results are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Monologue Speech Generation &#8227; 3 Performance of SoulX-Podcast &#8227; SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and Paralinguistic Diversity\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, where speech intelligibility is measured using CER for Chinese and WER for English, and speaker similarity (SIM) is quantified via the cosine similarity of speaker embeddings, following the Seed-TTS-eval protocol<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>https://github.com/BytedanceSpeech/seed-tts-eval</span></span></span>. As can been seen, SoulX-Podcast demonstrates significant superiority in intelligibility for zero-shot monologue TTS scenarios. Specifically, SoulX-Podcast achieves lowest CER in the Chinese test set. In the English test set, SoulX-Podcast only seconds to F5-TTS. In terms of speaker similarity, SoulX-Podcast also achieves strong results. Specifically, on both the Chinese and English test sets, it ranks just behind Seed-TTS and MaskGCT, demonstrating its excellent performance in conventional zero-shot TTS.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in text-to-speech (TTS) synthesis have significantly improved speech expressiveness and naturalness. However, most existing systems are tailored for single-speaker synthesis and fall short in generating coherent multi-speaker conversational speech. This technical report presents SoulX-Podcast, a system designed for podcast-style multi-turn, multi-speaker dialogic speech generation, while also achieving state-of-the-art performance in conventional text-to-speech (TTS) tasks.\nTo meet the higher naturalness demands of multi-turn spoken dialogue, SoulX-Podcast integrates a range of paralinguistic controls and supports both Mandarin and English, as well as several Chinese dialects, including Sichuanese, Henanese, and Cantonese, enabling more personalized podcast-style speech generation. Experimental results demonstrate that SoulX-Podcast can continuously produce over 90 minutes of conversation with stable speaker timbre and smooth speaker transitions. Moreover, speakers exhibit contextually adaptive prosody, reflecting natural rhythm and intonation changes as dialogues progress. Across multiple evaluation metrics, SoulX-Podcast achieves state-of-the-art performance in both monologue TTS and multi-turn conversational speech synthesis.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "english",
                    "higher",
                    "results",
                    "chinese",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building upon the generative power of large language models (LLMs), modern text-to-speech (TTS) systems have reached a point where they can produce speech that is nearly indistinguishable from human voices, achieving remarkable naturalness and zero-shot voice cloning performance&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx5\" title=\"\">5</a>]</cite>. However, most previous work has primarily focused on single-speaker speech generation. These systems, while effective in isolated speech tasks, struggle to maintain fluency and naturalness in multi-speaker, multi-turn conversation scenarios. In response to this gap, this technical report introduces <span class=\"ltx_text ltx_font_bold\">SoulX-Podcast</span>, a speech synthesis model specifically designed for seamless multi-speaker, multi-turn dialogues. To further enhance conversational realism and diversity, SoulX-Podcast incorporates robust support for various paralinguistic features and dialects, ensuring a more dynamic and natural dialogue experience.</p>\n\n",
                "matched_terms": [
                    "models",
                    "from",
                    "tts",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech tokenization methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx14\" title=\"\">14</a>]</cite>, based on Vector Quantization (VQ)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx15\" title=\"\">15</a>]</cite> or Finite Scalar Quantization&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx16\" title=\"\">16</a>]</cite>, bridge the gap between continuous speech signals and discrete token-based large language models (LLMs). Vall-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx17\" title=\"\">17</a>]</cite> is a pioneering speech generation system that leverages LLMs with discrete tokens and employs a tokenizer based\non residual vector quantization (RVQ)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx18\" title=\"\">18</a>]</cite>. Specifically, tokens from the first layer are predicted by an autoregressive (AR) LLM, while the remaining tokens are generated using a non-autoregressive (NAR) model. Subsequent work, depending on the choice of tokenizer, can be categorized into several main modeling approaches: predicting a single stream of semantic tokens with LLMs and then generating acoustic features via flow matching&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx5\" title=\"\">5</a>]</cite>; directly predicting acoustic tokens spanning multiple codebooks according to specific patterns&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx25\" title=\"\">25</a>]</cite>; or directly predicting a single stream of acoustic tokens&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx3\" title=\"\">3</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most of the aforementioned research primarily focuses on monologue-style speech generation, overlooking the challenges of multi-speaker, multi-turn conversational synthesis. In contrast, conversational speech generation places higher demands on natural prosodic and rhythmic variation to ensure smooth and coherent dialogue flow. Recently, several studies have begun to explore this direction. For example, Covomix&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx26\" title=\"\">26</a>]</cite> adopts a parallel-channel modeling strategy that simultaneously predicts the speech of different speakers through separate channels, while MoonCast&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx27\" title=\"\">27</a>]</cite> and MOSS-TTSD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx28\" title=\"\">28</a>]</cite> merge dialogue text with speaker labels to generate integrated multi-speaker conversations. The latest FireRedTTS-2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx25\" title=\"\">25</a>]</cite> model, on the other hand, produces speech from multiple speakers in an alternating manner. Although these methods achieve improved dialogue continuity and prosodic variation compared with standard TTS systems, their limited control over paralinguistic features still constrains the expressiveness and realism of the generated conversations.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "direction",
                    "higher",
                    "from",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce SoulX-Podcast, a large language model&#8211;driven framework for long-form, multi-speaker, and multi-dialect podcast speech synthesis. SoulX-Podcast is designed to generate stable, coherent, and expressive podcast-style dialogic speech by effectively modeling dialectal variation, paralinguistic cues, and context-dependent prosody. The framework represents interleaved text&#8211;speech sequences, where speaker-labeled text and corresponding speech tokens are chronologically aligned, thereby facilitating the generation of long-form conversational audio with consistent quality and speaker similarity. Experimental results demonstrate that SoulX-Podcast delivers superior performance in multi-turn dialogue synthesis and exhibits strong generalization to conventional TTS tasks, highlighting its versatility across diverse speech generation scenarios.</p>\n\n",
                "matched_terms": [
                    "represents",
                    "tts",
                    "results",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to <span class=\"ltx_text ltx_font_bold\">Mandarin</span> and <span class=\"ltx_text ltx_font_bold\">English</span>, SoulX-Podcast provides robust support for several Chinese dialects, including <span class=\"ltx_text ltx_font_bold\">Sichuanese</span>, <span class=\"ltx_text ltx_font_bold\">Henanese</span>, and <span class=\"ltx_text ltx_font_bold\">Cantonese</span>, enabling more diverse and personalized voice generation. Importantly, all of these dialects support <span class=\"ltx_text ltx_font_bold\">Cross-dialectal, zero-shot voice cloning</span>, allowing a single audio prompt to generate speech in any of the supported dialects.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SoulX-Podcast demonstrates superior performance not only in multi-turn conversational speech synthesis but also in conventional TTS tasks, such as <span class=\"ltx_text ltx_font_bold\">voice cloning</span>, highlighting its effectiveness and versatility across diverse speech synthesis scenarios.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate this issue, we first apply Voice Activity Detection (VAD)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx29\" title=\"\">29</a>]</cite> to segment long recordings into short utterances. These utterances are then concatenated into dialogue segments of approximately five minutes. During this process, we enforce a silence-duration constraint to prevent segment boundaries from crossing different sessions or long transitional silences: if the inter-utterance silence exceeds a predefined threshold, the adjacent utterances are treated as the end and start of separate segments.</p>\n\n",
                "matched_terms": [
                    "different",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quality Filtering.</span>\nAlthough the audio recordings were enhanced in the initial stage, some segments still exhibited suboptimal denoising results or inherently poor recording quality. To prevent such low-quality data from negatively impacting model training, we applied a series of filtering criteria to the dialogue segments, including signal-to-noise ratio (SNR) and perceptual quality estimated by DNSMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx31\" title=\"\">31</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Recognition.</span>\nFollowing the quality filtering process, we employed a dual-ASR transcription strategy to obtain reliable transcripts. Specifically, each utterance within a dialogue segment was transcribed by two independent ASR models. For Chinese speech, we used <span class=\"ltx_text ltx_font_typewriter\">Paraformer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">2</span></span><span class=\"ltx_text ltx_font_serif\">https://huggingface.co/funasr/Paraformer-large</span></span></span></span></span> and <span class=\"ltx_text ltx_font_typewriter\">Whisper<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">3</span></span><span class=\"ltx_text ltx_font_serif\">https://huggingface.co/openai/whisper-large-v3</span></span></span></span></span>, while for English speech, we adopted <span class=\"ltx_text ltx_font_typewriter\">Parakeet<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">4</span></span><span class=\"ltx_text ltx_font_serif\">https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2</span></span></span></span></span> and <span class=\"ltx_text ltx_font_typewriter\">Whisper</span>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each utterance, two transcription results were obtained, and the Character Error Rate (CER) for Chinese or Word Error Rate (WER) for English was computed. Utterances with CER or WER below a predefined threshold were fully retained, with Paraformer outputs used as the final transcripts for Chinese and Whisper outputs used for English. For utterances whose CER or WER exceeded the threshold, only the textual transcripts were preserved, while the corresponding audio was discarded.</p>\n\n",
                "matched_terms": [
                    "results",
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage, we employ language-specific ASR models fine-tuned for paralinguistic event detection to process the raw audio corpus. For Mandarin Chinese data, we use <span class=\"ltx_text ltx_font_typewriter\">Beats</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx33\" title=\"\">33</a>]</cite> for coarse identification of nonverbal cues, while for English data, we adopt <span class=\"ltx_text ltx_font_typewriter\">Whisperd</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx34\" title=\"\">34</a>]</cite>. This stage efficiently filters out segments unlikely to contain relevant paralinguistic events.</p>\n\n",
                "matched_terms": [
                    "models",
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dialectal Annotation.</span>\nTo efficiently collect dialectal speech data, we employed two complementary strategies. First, we collected publicly available recordings in specific Chinese dialects. Second, we trained a dialect identification model to retrieve and categorize dialectal utterances from the broader in-the-wild dataset.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage, the LLM backbone is initialized from Qwen3-1.7B&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>https://huggingface.co/Qwen/Qwen3-1.7B</span></span></span> and trained on a mixture of monologue and dialogue data to acquire fundamental text-to-speech capabilities. Subsequently, the model is further trained on multi-speaker dialogue data in both Chinese and English, incorporating dialectal and paralinguistic elements. Since the amount of Chinese dialect data is significantly smaller than that of Mandarin and English, we perform additional fine-tuning on dialectal data to enhance the model&#8217;s dialectal capability, resulting in a podcast model specifically optimized for dialect generation.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "from",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-dialectal Voice Cloning.</span> For dialect generation, our goal is to enable cross-dialectal voice cloning. However, this is nontrivial. Unlike the clear orthographic differences between Chinese and English, various Chinese dialects&#8212;particularly Mandarin, Henanese, and Sichuanese&#8212;share an identical written form. Even Cantonese, though linguistically more distinct, still exhibits substantial textual overlap with Mandarin. Consequently, when the target text is highly similar to Mandarin and the speech prompt is also in Mandarin, the dialectal control signal becomes weak.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although SoulX-Podcast is designed for multi-turn, multi-speaker dialogue synthesis, it is also capable of conventional monologue speech synthesis. Accordingly, we first compare its performance against SOTA TTS models on the standard monologue synthesis task. We then evaluate SoulX-Podcast&#8217;s capabilities in dialogue generation, as well as in paralinguistic and dialectal control.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate multi-turn, multi-speaker dialogue generation, we compare SoulX-Podcast with representative dialogue TTS systems on the ZipVoice-Dia test set. This benchmark comprises natural multi-turn conversations, enabling assessment of both intelligibility and cross-speaker consistency (cpSIM) in long-form synthesis. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#S3.T2\" title=\"Table 2 &#8227; 3.2 Podcast Generation &#8227; 3 Performance of SoulX-Podcast &#8227; SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and Paralinguistic Diversity\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, SoulX-Podcast outperforms recent state-of-the-art models on both the Chinese and English subsets. Specifically, it achieves the lowest WER/CER and the highest cpSIM, while maintaining competitive UTMOS scores, demonstrating superior speaker coherence and perceived quality.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "english",
                    "test",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SoulX-Podcast currently supports three major Chinese dialects: Sichuanese, Henanese, and Cantonese. We evaluate its performance on these dialects in both monologue TTS and dialogue generation settings. The monologue test set includes 1,000 samples per dialect, drawn from internal GPT-generated data as well as SeedTTS, Wenetspeech-Yue-eval&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx40\" title=\"\">40</a>]</cite>, and Wenetspeech-Chuan-eval&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx41\" title=\"\">41</a>]</cite>. The dialogue test set contains 100 GPT-generated items per dialect.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "from",
                    "test",
                    "chinese",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Dialect-specific ASR systems are used to compute CER, including Wenetspeech-Chuan-ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx41\" title=\"\">41</a>]</cite> for Sichuanese, TeleSpeech<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/Tele-AI/TeleSpeech-ASR\" title=\"\">https://github.com/Tele-AI/TeleSpeech-ASR</a></span></span></span>\nfor Henanese, and Wenetspeech-Yue-ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx40\" title=\"\">40</a>]</cite> for Cantonese. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#S3.T4\" title=\"Table 4 &#8227; 3.4 Dialect Generation &#8227; 3 Performance of SoulX-Podcast &#8227; SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and Paralinguistic Diversity\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, SoulX-Podcast achieves consistent speaker similarity across all three dialects, comparable to its performance on Mandarin and English. The relatively high CER values may partly arise from limitations of the ASR systems.</p>\n\n",
                "matched_terms": [
                    "values",
                    "from",
                    "english",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced SoulX-Podcast, a large language model&#8211;driven framework for long-form, multi-speaker, and multi-dialect conversational speech synthesis. Through an interleaved text&#8211;speech modeling paradigm, SoulX-Podcast enables the generation of long-form, multi-turn conversational speech with consistent quality and coherence. Experimental results demonstrate that SoulX-Podcast not only excels in multi-turn dialogue synthesis but also generalizes effectively to zero-shot monologue TTS. Its capability to handle multiple Chinese dialects and paralinguistic cues further highlights its versatility and potential as a unified framework for speech generation.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "tts",
                    "results"
                ]
            }
        ]
    },
    "S3.T2": {
        "caption": "Table 2: \nObjective evaluation of multi-speaker TTS systems on ZipVoice-Dia test sets. Arrows indicate the desired direction (↓\\downarrow = lower is better, ↑\\uparrow = higher is better). Best values per column are in bold. All model test results below are reproduced from the official release.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">UTMOS (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m10\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n</tr>\n</table>\n",
        "informative_terms_identified": [
            "utmos",
            "column",
            "direction",
            "evaluation",
            "↓downarrow",
            "desired",
            "tts",
            "systems",
            "official",
            "reproduced",
            "objective",
            "from",
            "release",
            "test",
            "indicate",
            "sets",
            "bold",
            "higher",
            "lower",
            "results",
            "below",
            "arrows",
            "values",
            "↑uparrow",
            "model",
            "better",
            "zipvoicedia",
            "multispeaker",
            "best",
            "all"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To evaluate multi-turn, multi-speaker dialogue generation, we compare SoulX-Podcast with representative dialogue TTS systems on the ZipVoice-Dia test set. This benchmark comprises natural multi-turn conversations, enabling assessment of both intelligibility and cross-speaker consistency (cpSIM) in long-form synthesis. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#S3.T2\" title=\"Table 2 &#8227; 3.2 Podcast Generation &#8227; 3 Performance of SoulX-Podcast &#8227; SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and Paralinguistic Diversity\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, SoulX-Podcast outperforms recent state-of-the-art models on both the Chinese and English subsets. Specifically, it achieves the lowest WER/CER and the highest cpSIM, while maintaining competitive UTMOS scores, demonstrating superior speaker coherence and perceived quality.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in text-to-speech (TTS) synthesis have significantly improved speech expressiveness and naturalness. However, most existing systems are tailored for single-speaker synthesis and fall short in generating coherent multi-speaker conversational speech. This technical report presents SoulX-Podcast, a system designed for podcast-style multi-turn, multi-speaker dialogic speech generation, while also achieving state-of-the-art performance in conventional text-to-speech (TTS) tasks.\nTo meet the higher naturalness demands of multi-turn spoken dialogue, SoulX-Podcast integrates a range of paralinguistic controls and supports both Mandarin and English, as well as several Chinese dialects, including Sichuanese, Henanese, and Cantonese, enabling more personalized podcast-style speech generation. Experimental results demonstrate that SoulX-Podcast can continuously produce over 90 minutes of conversation with stable speaker timbre and smooth speaker transitions. Moreover, speakers exhibit contextually adaptive prosody, reflecting natural rhythm and intonation changes as dialogues progress. Across multiple evaluation metrics, SoulX-Podcast achieves state-of-the-art performance in both monologue TTS and multi-turn conversational speech synthesis.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "systems",
                    "multispeaker",
                    "evaluation",
                    "higher",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building upon the generative power of large language models (LLMs), modern text-to-speech (TTS) systems have reached a point where they can produce speech that is nearly indistinguishable from human voices, achieving remarkable naturalness and zero-shot voice cloning performance&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx5\" title=\"\">5</a>]</cite>. However, most previous work has primarily focused on single-speaker speech generation. These systems, while effective in isolated speech tasks, struggle to maintain fluency and naturalness in multi-speaker, multi-turn conversation scenarios. In response to this gap, this technical report introduces <span class=\"ltx_text ltx_font_bold\">SoulX-Podcast</span>, a speech synthesis model specifically designed for seamless multi-speaker, multi-turn dialogues. To further enhance conversational realism and diversity, SoulX-Podcast incorporates robust support for various paralinguistic features and dialects, ensuring a more dynamic and natural dialogue experience.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "systems",
                    "model",
                    "multispeaker",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech tokenization methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx14\" title=\"\">14</a>]</cite>, based on Vector Quantization (VQ)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx15\" title=\"\">15</a>]</cite> or Finite Scalar Quantization&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx16\" title=\"\">16</a>]</cite>, bridge the gap between continuous speech signals and discrete token-based large language models (LLMs). Vall-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx17\" title=\"\">17</a>]</cite> is a pioneering speech generation system that leverages LLMs with discrete tokens and employs a tokenizer based\non residual vector quantization (RVQ)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx18\" title=\"\">18</a>]</cite>. Specifically, tokens from the first layer are predicted by an autoregressive (AR) LLM, while the remaining tokens are generated using a non-autoregressive (NAR) model. Subsequent work, depending on the choice of tokenizer, can be categorized into several main modeling approaches: predicting a single stream of semantic tokens with LLMs and then generating acoustic features via flow matching&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx5\" title=\"\">5</a>]</cite>; directly predicting acoustic tokens spanning multiple codebooks according to specific patterns&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx25\" title=\"\">25</a>]</cite>; or directly predicting a single stream of acoustic tokens&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx3\" title=\"\">3</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most of the aforementioned research primarily focuses on monologue-style speech generation, overlooking the challenges of multi-speaker, multi-turn conversational synthesis. In contrast, conversational speech generation places higher demands on natural prosodic and rhythmic variation to ensure smooth and coherent dialogue flow. Recently, several studies have begun to explore this direction. For example, Covomix&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx26\" title=\"\">26</a>]</cite> adopts a parallel-channel modeling strategy that simultaneously predicts the speech of different speakers through separate channels, while MoonCast&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx27\" title=\"\">27</a>]</cite> and MOSS-TTSD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx28\" title=\"\">28</a>]</cite> merge dialogue text with speaker labels to generate integrated multi-speaker conversations. The latest FireRedTTS-2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx25\" title=\"\">25</a>]</cite> model, on the other hand, produces speech from multiple speakers in an alternating manner. Although these methods achieve improved dialogue continuity and prosodic variation compared with standard TTS systems, their limited control over paralinguistic features still constrains the expressiveness and realism of the generated conversations.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "systems",
                    "direction",
                    "model",
                    "multispeaker",
                    "higher",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce SoulX-Podcast, a large language model&#8211;driven framework for long-form, multi-speaker, and multi-dialect podcast speech synthesis. SoulX-Podcast is designed to generate stable, coherent, and expressive podcast-style dialogic speech by effectively modeling dialectal variation, paralinguistic cues, and context-dependent prosody. The framework represents interleaved text&#8211;speech sequences, where speaker-labeled text and corresponding speech tokens are chronologically aligned, thereby facilitating the generation of long-form conversational audio with consistent quality and speaker similarity. Experimental results demonstrate that SoulX-Podcast delivers superior performance in multi-turn dialogue synthesis and exhibits strong generalization to conventional TTS tasks, highlighting its versatility across diverse speech generation scenarios.</p>\n\n",
                "matched_terms": [
                    "multispeaker",
                    "tts",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quality Filtering.</span>\nAlthough the audio recordings were enhanced in the initial stage, some segments still exhibited suboptimal denoising results or inherently poor recording quality. To prevent such low-quality data from negatively impacting model training, we applied a series of filtering criteria to the dialogue segments, including signal-to-noise ratio (SNR) and perceptual quality estimated by DNSMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx31\" title=\"\">31</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "from",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each utterance, two transcription results were obtained, and the Character Error Rate (CER) for Chinese or Word Error Rate (WER) for English was computed. Utterances with CER or WER below a predefined threshold were fully retained, with Paraformer outputs used as the final transcripts for Chinese and Whisper outputs used for English. For utterances whose CER or WER exceeded the threshold, only the textual transcripts were preserved, while the corresponding audio was discarded.</p>\n\n",
                "matched_terms": [
                    "results",
                    "below"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaker Purity Refinement.</span> To ensure speaker label consistency, we conducted a speaker-purity refinement based on speaker embedding clustering. For each dialogue segment, the embeddings of all utterances belonging to the same speaker were clustered, and utterances whose embeddings deviated excessively from the cluster centroid were identified as outliers. These outlier utterances were excluded from the audio data&#8212;only their transcriptions were retained. This strategy effectively mitigates potential speaker confusion during multi-turn dialogue synthesis while maximizing overall data retention. Here, we extract speaker embeddings with WavLM-large, finetuned on the speaker verification task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx32\" title=\"\">32</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dialectal Annotation.</span>\nTo efficiently collect dialectal speech data, we employed two complementary strategies. First, we collected publicly available recordings in specific Chinese dialects. Second, we trained a dialect identification model to retrieve and categorize dialectal utterances from the broader in-the-wild dataset.</p>\n\n",
                "matched_terms": [
                    "model",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable flexible, multi-turn dialogue generation, we adopt a text&#8211;speech interleaved sequence that allows sentence-by-sentence synthesis. Specifically, each speaker&#8217;s text tokens are followed by their corresponding speech tokens, which are then concatenated with the next speaker&#8217;s text and speech tokens in temporal order. Each utterance begins with a speaker token to indicate the speaker identity. Likewise, dialect control is achieved by inserting a dialect-specific token immediately after the speaker token, while paralinguistic cues (e.g., laughter, sighs) are treated as textual tokens and placed at their corresponding positions within the sequence.\nAn example with a dialect label is shown below:</p>\n\n",
                "matched_terms": [
                    "indicate",
                    "below"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage, the LLM backbone is initialized from Qwen3-1.7B&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>https://huggingface.co/Qwen/Qwen3-1.7B</span></span></span> and trained on a mixture of monologue and dialogue data to acquire fundamental text-to-speech capabilities. Subsequently, the model is further trained on multi-speaker dialogue data in both Chinese and English, incorporating dialectal and paralinguistic elements. Since the amount of Chinese dialect data is significantly smaller than that of Mandarin and English, we perform additional fine-tuning on dialectal data to enhance the model&#8217;s dialectal capability, resulting in a podcast model specifically optimized for dialect generation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "multispeaker",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During inference, we follow the token organization established in training: initial text and speech tokens from multiple speakers are interleaved, and the model autoregressively generates subsequent speech tokens in the same interleaved manner.</p>\n\n",
                "matched_terms": [
                    "model",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this issue and allow a Mandarin prompt to generate speech in any target dialect, we propose <span class=\"ltx_text ltx_font_bold\">Dialect-Guided Prompting (DGP)</span> inference strategy. Specifically, before generating a dialectal podcast, we prepend a short dialect-typical sentence&#8212;one that strongly reflects the target dialectal style&#8212;to the input text. This initial utterance effectively guides the model toward producing speech with the desired dialectal characteristics in subsequent generations.</p>\n\n",
                "matched_terms": [
                    "desired",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although SoulX-Podcast is designed for multi-turn, multi-speaker dialogue synthesis, it is also capable of conventional monologue speech synthesis. Accordingly, we first compare its performance against SOTA TTS models on the standard monologue synthesis task. We then evaluate SoulX-Podcast&#8217;s capabilities in dialogue generation, as well as in paralinguistic and dialectal control.</p>\n\n",
                "matched_terms": [
                    "multispeaker",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the zero-shot (voice cloning) TTS capability of SoulX-Podcast, we assess its performance on Seed-TTS-eval and compare it with existing zero-shot TTS models. The results are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Monologue Speech Generation &#8227; 3 Performance of SoulX-Podcast &#8227; SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and Paralinguistic Diversity\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, where speech intelligibility is measured using CER for Chinese and WER for English, and speaker similarity (SIM) is quantified via the cosine similarity of speaker embeddings, following the Seed-TTS-eval protocol<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>https://github.com/BytedanceSpeech/seed-tts-eval</span></span></span>. As can been seen, SoulX-Podcast demonstrates significant superiority in intelligibility for zero-shot monologue TTS scenarios. Specifically, SoulX-Podcast achieves lowest CER in the Chinese test set. In the English test set, SoulX-Podcast only seconds to F5-TTS. In terms of speaker similarity, SoulX-Podcast also achieves strong results. Specifically, on both the Chinese and English test sets, it ranks just behind Seed-TTS and MaskGCT, demonstrating its excellent performance in conventional zero-shot TTS.</p>\n\n",
                "matched_terms": [
                    "sets",
                    "tts",
                    "results",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For objective evaluation, we adopted the Qwen-2.5 Omni-FT model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx39\" title=\"\">39</a>]</cite> as an automated paralinguistic recognizer. The evaluator was tasked with verifying whether each synthesized utterance contained the target paralinguistic event specified in the prompt. The resulting recognition accuracies are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#S3.T3\" title=\"Table 3 &#8227; 3.3 Evaluation of Paralinguistic Control &#8227; 3 Performance of SoulX-Podcast &#8227; SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and Paralinguistic Diversity\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "objective",
                    "evaluation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SoulX-Podcast currently supports three major Chinese dialects: Sichuanese, Henanese, and Cantonese. We evaluate its performance on these dialects in both monologue TTS and dialogue generation settings. The monologue test set includes 1,000 samples per dialect, drawn from internal GPT-generated data as well as SeedTTS, Wenetspeech-Yue-eval&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx40\" title=\"\">40</a>]</cite>, and Wenetspeech-Chuan-eval&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx41\" title=\"\">41</a>]</cite>. The dialogue test set contains 100 GPT-generated items per dialect.</p>\n\n",
                "matched_terms": [
                    "from",
                    "tts",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Dialect-specific ASR systems are used to compute CER, including Wenetspeech-Chuan-ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx41\" title=\"\">41</a>]</cite> for Sichuanese, TeleSpeech<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/Tele-AI/TeleSpeech-ASR\" title=\"\">https://github.com/Tele-AI/TeleSpeech-ASR</a></span></span></span>\nfor Henanese, and Wenetspeech-Yue-ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx40\" title=\"\">40</a>]</cite> for Cantonese. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#S3.T4\" title=\"Table 4 &#8227; 3.4 Dialect Generation &#8227; 3 Performance of SoulX-Podcast &#8227; SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and Paralinguistic Diversity\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, SoulX-Podcast achieves consistent speaker similarity across all three dialects, comparable to its performance on Mandarin and English. The relatively high CER values may partly arise from limitations of the ASR systems.</p>\n\n",
                "matched_terms": [
                    "all",
                    "from",
                    "values",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced SoulX-Podcast, a large language model&#8211;driven framework for long-form, multi-speaker, and multi-dialect conversational speech synthesis. Through an interleaved text&#8211;speech modeling paradigm, SoulX-Podcast enables the generation of long-form, multi-turn conversational speech with consistent quality and coherence. Experimental results demonstrate that SoulX-Podcast not only excels in multi-turn dialogue synthesis but also generalizes effectively to zero-shot monologue TTS. Its capability to handle multiple Chinese dialects and paralinguistic cues further highlights its versatility and potential as a unified framework for speech generation.</p>\n\n",
                "matched_terms": [
                    "multispeaker",
                    "tts",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work focuses on advancing speech synthesis technology through the development of SoulX-Podcast, a large language model&#8211;driven framework for multi-speaker and multi-dialect conversational speech generation. All datasets used in this study were either publicly available or synthetically generated, and no personally identifiable information or private recordings were included.</p>\n\n",
                "matched_terms": [
                    "multispeaker",
                    "all"
                ]
            }
        ]
    },
    "S3.T3": {
        "caption": "Table 3: Recognition accuracy for different paralinguistic labels.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Label</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Count</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Correct</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Error</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\">Accuracy</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">laughter</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">1.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_typewriter\">sigh</span></td>\n<td class=\"ltx_td ltx_align_center\">20</td>\n<td class=\"ltx_td ltx_align_center\">17</td>\n<td class=\"ltx_td ltx_align_center\">3</td>\n<td class=\"ltx_td ltx_align_right\">0.85</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_typewriter\">breathing</span></td>\n<td class=\"ltx_td ltx_align_center\">20</td>\n<td class=\"ltx_td ltx_align_center\">15</td>\n<td class=\"ltx_td ltx_align_center\">5</td>\n<td class=\"ltx_td ltx_align_right\">0.75</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_typewriter\">coughing</span></td>\n<td class=\"ltx_td ltx_align_center\">20</td>\n<td class=\"ltx_td ltx_align_center\">14</td>\n<td class=\"ltx_td ltx_align_center\">6</td>\n<td class=\"ltx_td ltx_align_right\">0.70</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_typewriter\">throat_clearing</span></td>\n<td class=\"ltx_td ltx_align_center\">20</td>\n<td class=\"ltx_td ltx_align_center\">16</td>\n<td class=\"ltx_td ltx_align_center\">4</td>\n<td class=\"ltx_td ltx_align_right\">0.80</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">Total / Average</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">18</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">0.82</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "recognition",
            "labels",
            "count",
            "laughter",
            "correct",
            "label",
            "sigh",
            "total",
            "average",
            "breathing",
            "paralinguistic",
            "throatclearing",
            "coughing",
            "accuracy",
            "different",
            "error"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">For objective evaluation, we adopted the Qwen-2.5 Omni-FT model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx39\" title=\"\">39</a>]</cite> as an automated paralinguistic recognizer. The evaluator was tasked with verifying whether each synthesized utterance contained the target paralinguistic event specified in the prompt. The resulting recognition accuracies are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#S3.T3\" title=\"Table 3 &#8227; 3.3 Evaluation of Paralinguistic Control &#8227; 3 Performance of SoulX-Podcast &#8227; SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and Paralinguistic Diversity\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#S3.T3\" title=\"Table 3 &#8227; 3.3 Evaluation of Paralinguistic Control &#8227; 3 Performance of SoulX-Podcast &#8227; SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and Paralinguistic Diversity\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our model achieves a strong overall accuracy of 0.82 in controlling these paralinguistic events. It demonstrates near-perfect control over distinct events like <span class=\"ltx_text ltx_font_typewriter\">&lt;|laughter|&gt;</span> and high fidelity for <span class=\"ltx_text ltx_font_typewriter\">&lt;|sigh|&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;|throat_clearing|&gt;</span>. The primary sources of error appear concentrated in more acoustically subtle or ambiguous events, namely <span class=\"ltx_text ltx_font_typewriter\">&lt;|breathing|&gt;</span> (0.75) and <span class=\"ltx_text ltx_font_typewriter\">&lt;|coughing|&gt;</span> (0.70), which may be more challenging for the evaluator model to distinguish.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Most of the aforementioned research primarily focuses on monologue-style speech generation, overlooking the challenges of multi-speaker, multi-turn conversational synthesis. In contrast, conversational speech generation places higher demands on natural prosodic and rhythmic variation to ensure smooth and coherent dialogue flow. Recently, several studies have begun to explore this direction. For example, Covomix&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx26\" title=\"\">26</a>]</cite> adopts a parallel-channel modeling strategy that simultaneously predicts the speech of different speakers through separate channels, while MoonCast&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx27\" title=\"\">27</a>]</cite> and MOSS-TTSD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx28\" title=\"\">28</a>]</cite> merge dialogue text with speaker labels to generate integrated multi-speaker conversations. The latest FireRedTTS-2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx25\" title=\"\">25</a>]</cite> model, on the other hand, produces speech from multiple speakers in an alternating manner. Although these methods achieve improved dialogue continuity and prosodic variation compared with standard TTS systems, their limited control over paralinguistic features still constrains the expressiveness and realism of the generated conversations.</p>\n\n",
                "matched_terms": [
                    "labels",
                    "different",
                    "paralinguistic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It supports <span class=\"ltx_text ltx_font_bold\">long-form, natural dialogue speech generation</span> with a variety of <span class=\"ltx_text ltx_font_bold\">paralinguistic labels</span>, achieving high fluency and coherence across extended multi-turn dialogues.</p>\n\n",
                "matched_terms": [
                    "labels",
                    "paralinguistic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Paralinguistic cues, such as laughter and sighs, play a crucial role in enhancing the naturalness and expressiveness of dialogue. To enable controllable generation of such cues, we performed paralinguistic mining and annotation on the collected data. Moreover, most previous speech synthesis research has primarily focused on Mandarin Chinese, while major Chinese dialects such as Cantonese and Sichuanese have received limited attention. To facilitate effective dialectal controllability, we further annotated the collected data with dialectal labels, enabling the model to capture and reproduce dialect-specific characteristics.</p>\n\n",
                "matched_terms": [
                    "labels",
                    "laughter",
                    "paralinguistic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Paralinguistic Annotation.</span>\nTo ensure both large-scale coverage and fine-grained precision of paralinguistic labels, we design a two-stage refinement framework for data annotation. This framework combines high-throughput automated detection with model-assisted verification to achieve both efficiency and accuracy.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "labels",
                    "paralinguistic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable flexible, multi-turn dialogue generation, we adopt a text&#8211;speech interleaved sequence that allows sentence-by-sentence synthesis. Specifically, each speaker&#8217;s text tokens are followed by their corresponding speech tokens, which are then concatenated with the next speaker&#8217;s text and speech tokens in temporal order. Each utterance begins with a speaker token to indicate the speaker identity. Likewise, dialect control is achieved by inserting a dialect-specific token immediately after the speaker token, while paralinguistic cues (e.g., laughter, sighs) are treated as textual tokens and placed at their corresponding positions within the sequence.\nAn example with a dialect label is shown below:</p>\n\n",
                "matched_terms": [
                    "label",
                    "paralinguistic",
                    "laughter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the proposed model&#8217;s capability for controllable paralinguistic generation, we constructed a dedicated paralinguistic test set. Concretely, we employed GPT-5 to generate 20 test utterances for each of five paralinguistic labels, i.e., <span class=\"ltx_text ltx_font_typewriter\">&lt;|laughter|&gt;</span>, <span class=\"ltx_text ltx_font_typewriter\">&lt;|sigh|&gt;</span>, <span class=\"ltx_text ltx_font_typewriter\">&lt;|breathing|&gt;</span>, <span class=\"ltx_text ltx_font_typewriter\">&lt;|coughing|&gt;</span>, and <span class=\"ltx_text ltx_font_typewriter\">&lt;throat_clearing&gt;</span>. The corresponding audio samples were subsequently synthesized using SoulX-Podcast in monologue speech synthesis mode.</p>\n\n",
                "matched_terms": [
                    "labels",
                    "laughter",
                    "sigh",
                    "breathing",
                    "paralinguistic",
                    "throatclearing",
                    "coughing"
                ]
            }
        ]
    },
    "S3.T4": {
        "caption": "Table 4: \nPerformance evaluation of SoulX-Podcast on TTS and dialogue generation across different dialects. Arrows indicate the desired direction (↓\\downarrow = lower is better, ↑\\uparrow = higher is better).",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\" style=\"padding-left:8.5pt;padding-right:8.5pt;\"><span class=\"ltx_text ltx_font_bold\">Dialect</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:8.5pt;padding-right:8.5pt;\"><span class=\"ltx_text ltx_font_bold\">Monologue Test</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:8.5pt;padding-right:8.5pt;\"><span class=\"ltx_text ltx_font_bold\">Dialogue Test</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">CER (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">SIM (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">CER (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">cpSIM (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">Sichuanese</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">3.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">0.704</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">15.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">0.641</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">Henanese</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">8.14</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">0.705</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">28.06</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">0.647</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">Cantonese</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">9.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">0.680</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">19.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">0.627</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "monologue",
            "direction",
            "dialogue",
            "evaluation",
            "↓downarrow",
            "desired",
            "tts",
            "cpsim",
            "sichuanese",
            "generation",
            "henanese",
            "soulxpodcast",
            "test",
            "performance",
            "across",
            "indicate",
            "dialects",
            "higher",
            "lower",
            "arrows",
            "↑uparrow",
            "cantonese",
            "dialect",
            "better",
            "sim",
            "cer",
            "different"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Dialect-specific ASR systems are used to compute CER, including Wenetspeech-Chuan-ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx41\" title=\"\">41</a>]</cite> for Sichuanese, TeleSpeech<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/Tele-AI/TeleSpeech-ASR\" title=\"\">https://github.com/Tele-AI/TeleSpeech-ASR</a></span></span></span>\nfor Henanese, and Wenetspeech-Yue-ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx40\" title=\"\">40</a>]</cite> for Cantonese. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#S3.T4\" title=\"Table 4 &#8227; 3.4 Dialect Generation &#8227; 3 Performance of SoulX-Podcast &#8227; SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and Paralinguistic Diversity\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, SoulX-Podcast achieves consistent speaker similarity across all three dialects, comparable to its performance on Mandarin and English. The relatively high CER values may partly arise from limitations of the ASR systems.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in text-to-speech (TTS) synthesis have significantly improved speech expressiveness and naturalness. However, most existing systems are tailored for single-speaker synthesis and fall short in generating coherent multi-speaker conversational speech. This technical report presents SoulX-Podcast, a system designed for podcast-style multi-turn, multi-speaker dialogic speech generation, while also achieving state-of-the-art performance in conventional text-to-speech (TTS) tasks.\nTo meet the higher naturalness demands of multi-turn spoken dialogue, SoulX-Podcast integrates a range of paralinguistic controls and supports both Mandarin and English, as well as several Chinese dialects, including Sichuanese, Henanese, and Cantonese, enabling more personalized podcast-style speech generation. Experimental results demonstrate that SoulX-Podcast can continuously produce over 90 minutes of conversation with stable speaker timbre and smooth speaker transitions. Moreover, speakers exhibit contextually adaptive prosody, reflecting natural rhythm and intonation changes as dialogues progress. Across multiple evaluation metrics, SoulX-Podcast achieves state-of-the-art performance in both monologue TTS and multi-turn conversational speech synthesis.</p>\n\n",
                "matched_terms": [
                    "across",
                    "tts",
                    "cantonese",
                    "monologue",
                    "sichuanese",
                    "dialogue",
                    "dialects",
                    "evaluation",
                    "generation",
                    "henanese",
                    "higher",
                    "soulxpodcast",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building upon the generative power of large language models (LLMs), modern text-to-speech (TTS) systems have reached a point where they can produce speech that is nearly indistinguishable from human voices, achieving remarkable naturalness and zero-shot voice cloning performance&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx5\" title=\"\">5</a>]</cite>. However, most previous work has primarily focused on single-speaker speech generation. These systems, while effective in isolated speech tasks, struggle to maintain fluency and naturalness in multi-speaker, multi-turn conversation scenarios. In response to this gap, this technical report introduces <span class=\"ltx_text ltx_font_bold\">SoulX-Podcast</span>, a speech synthesis model specifically designed for seamless multi-speaker, multi-turn dialogues. To further enhance conversational realism and diversity, SoulX-Podcast incorporates robust support for various paralinguistic features and dialects, ensuring a more dynamic and natural dialogue experience.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "dialogue",
                    "dialects",
                    "generation",
                    "soulxpodcast",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most of the aforementioned research primarily focuses on monologue-style speech generation, overlooking the challenges of multi-speaker, multi-turn conversational synthesis. In contrast, conversational speech generation places higher demands on natural prosodic and rhythmic variation to ensure smooth and coherent dialogue flow. Recently, several studies have begun to explore this direction. For example, Covomix&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx26\" title=\"\">26</a>]</cite> adopts a parallel-channel modeling strategy that simultaneously predicts the speech of different speakers through separate channels, while MoonCast&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx27\" title=\"\">27</a>]</cite> and MOSS-TTSD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx28\" title=\"\">28</a>]</cite> merge dialogue text with speaker labels to generate integrated multi-speaker conversations. The latest FireRedTTS-2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx25\" title=\"\">25</a>]</cite> model, on the other hand, produces speech from multiple speakers in an alternating manner. Although these methods achieve improved dialogue continuity and prosodic variation compared with standard TTS systems, their limited control over paralinguistic features still constrains the expressiveness and realism of the generated conversations.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "direction",
                    "dialogue",
                    "generation",
                    "higher",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce SoulX-Podcast, a large language model&#8211;driven framework for long-form, multi-speaker, and multi-dialect podcast speech synthesis. SoulX-Podcast is designed to generate stable, coherent, and expressive podcast-style dialogic speech by effectively modeling dialectal variation, paralinguistic cues, and context-dependent prosody. The framework represents interleaved text&#8211;speech sequences, where speaker-labeled text and corresponding speech tokens are chronologically aligned, thereby facilitating the generation of long-form conversational audio with consistent quality and speaker similarity. Experimental results demonstrate that SoulX-Podcast delivers superior performance in multi-turn dialogue synthesis and exhibits strong generalization to conventional TTS tasks, highlighting its versatility across diverse speech generation scenarios.</p>\n\n",
                "matched_terms": [
                    "across",
                    "tts",
                    "dialogue",
                    "generation",
                    "soulxpodcast",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It supports <span class=\"ltx_text ltx_font_bold\">long-form, natural dialogue speech generation</span> with a variety of <span class=\"ltx_text ltx_font_bold\">paralinguistic labels</span>, achieving high fluency and coherence across extended multi-turn dialogues.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "across",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to <span class=\"ltx_text ltx_font_bold\">Mandarin</span> and <span class=\"ltx_text ltx_font_bold\">English</span>, SoulX-Podcast provides robust support for several Chinese dialects, including <span class=\"ltx_text ltx_font_bold\">Sichuanese</span>, <span class=\"ltx_text ltx_font_bold\">Henanese</span>, and <span class=\"ltx_text ltx_font_bold\">Cantonese</span>, enabling more diverse and personalized voice generation. Importantly, all of these dialects support <span class=\"ltx_text ltx_font_bold\">Cross-dialectal, zero-shot voice cloning</span>, allowing a single audio prompt to generate speech in any of the supported dialects.</p>\n\n",
                "matched_terms": [
                    "cantonese",
                    "sichuanese",
                    "dialects",
                    "generation",
                    "henanese",
                    "soulxpodcast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SoulX-Podcast demonstrates superior performance not only in multi-turn conversational speech synthesis but also in conventional TTS tasks, such as <span class=\"ltx_text ltx_font_bold\">voice cloning</span>, highlighting its effectiveness and versatility across diverse speech synthesis scenarios.</p>\n\n",
                "matched_terms": [
                    "across",
                    "tts",
                    "soulxpodcast",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Dialogue text&#8211;speech paired data that contain speaker identity correspondence are a necessary prerequisite for building a dialogue speech synthesis system. This section first introduces the data processing methods used in this work, including the handling of dialogue data and the annotation of dialectal and paralinguistic information. Subsequently, we present the specific algorithm of SoulX-Podcast.</p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "soulxpodcast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to monologue speech, the processing of dialogue speech necessitates not only obtaining aligned transcripts but also distinguishing between speakers explicitly. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#S2.F2\" title=\"Figure 2 &#8227; 2.1 Data Processing &#8227; 2 Method &#8227; SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and Paralinguistic Diversity\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the overall workflow comprises speech enhancement, audio segmentation and speaker diarization, text transcription, and quality filtering. Additionally, to facilitate paralinguistic and dialectal controllability, further information is extracted and annotated.</p>\n\n",
                "matched_terms": [
                    "monologue",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate this issue, we first apply Voice Activity Detection (VAD)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx29\" title=\"\">29</a>]</cite> to segment long recordings into short utterances. These utterances are then concatenated into dialogue segments of approximately five minutes. During this process, we enforce a silence-duration constraint to prevent segment boundaries from crossing different sessions or long transitional silences: if the inter-utterance silence exceeds a predefined threshold, the adjacent utterances are treated as the end and start of separate segments.</p>\n\n",
                "matched_terms": [
                    "different",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This strategy maintains dialogue completeness and textual consistency while minimizing the adverse impact of transcription errors on speech synthesis training, thereby achieving a better trade-off between data retention and transcription reliability.</p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "better"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Paralinguistic cues, such as laughter and sighs, play a crucial role in enhancing the naturalness and expressiveness of dialogue. To enable controllable generation of such cues, we performed paralinguistic mining and annotation on the collected data. Moreover, most previous speech synthesis research has primarily focused on Mandarin Chinese, while major Chinese dialects such as Cantonese and Sichuanese have received limited attention. To facilitate effective dialectal controllability, we further annotated the collected data with dialectal labels, enabling the model to capture and reproduce dialect-specific characteristics.</p>\n\n",
                "matched_terms": [
                    "cantonese",
                    "sichuanese",
                    "dialogue",
                    "dialects",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dialectal Annotation.</span>\nTo efficiently collect dialectal speech data, we employed two complementary strategies. First, we collected publicly available recordings in specific Chinese dialects. Second, we trained a dialect identification model to retrieve and categorize dialectal utterances from the broader in-the-wild dataset.</p>\n\n",
                "matched_terms": [
                    "dialect",
                    "dialects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For transcription, we observed that our standard pipeline performed suboptimally on dialectal speech. Accordingly, we leveraged the commercial Seed-ASR API&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://docs.byteplus.com/zh-CN/docs/byteplusvoice/asrstreaming</span></span></span> to generate reliable transcripts. Using this approach, we obtained approximately 2,000 hours of Sichuanese, 1,000 hours of Cantonese, and 500 hours of Henanese speech.</p>\n\n",
                "matched_terms": [
                    "cantonese",
                    "sichuanese",
                    "henanese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable flexible, multi-turn dialogue generation, we adopt a text&#8211;speech interleaved sequence that allows sentence-by-sentence synthesis. Specifically, each speaker&#8217;s text tokens are followed by their corresponding speech tokens, which are then concatenated with the next speaker&#8217;s text and speech tokens in temporal order. Each utterance begins with a speaker token to indicate the speaker identity. Likewise, dialect control is achieved by inserting a dialect-specific token immediately after the speaker token, while paralinguistic cues (e.g., laughter, sighs) are treated as textual tokens and placed at their corresponding positions within the sequence.\nAn example with a dialect label is shown below:</p>\n\n",
                "matched_terms": [
                    "generation",
                    "dialect",
                    "dialogue",
                    "indicate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Dialogue speech data are relatively scarce compared to monologue speech. To effectively leverage heterogeneous data patterns and enhance performance in dialogue scenarios, we adopt a curriculum learning strategy.</p>\n\n",
                "matched_terms": [
                    "monologue",
                    "dialogue",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage, the LLM backbone is initialized from Qwen3-1.7B&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>https://huggingface.co/Qwen/Qwen3-1.7B</span></span></span> and trained on a mixture of monologue and dialogue data to acquire fundamental text-to-speech capabilities. Subsequently, the model is further trained on multi-speaker dialogue data in both Chinese and English, incorporating dialectal and paralinguistic elements. Since the amount of Chinese dialect data is significantly smaller than that of Mandarin and English, we perform additional fine-tuning on dialectal data to enhance the model&#8217;s dialectal capability, resulting in a podcast model specifically optimized for dialect generation.</p>\n\n",
                "matched_terms": [
                    "monologue",
                    "generation",
                    "dialogue",
                    "dialect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-dialectal Voice Cloning.</span> For dialect generation, our goal is to enable cross-dialectal voice cloning. However, this is nontrivial. Unlike the clear orthographic differences between Chinese and English, various Chinese dialects&#8212;particularly Mandarin, Henanese, and Sichuanese&#8212;share an identical written form. Even Cantonese, though linguistically more distinct, still exhibits substantial textual overlap with Mandarin. Consequently, when the target text is highly similar to Mandarin and the speech prompt is also in Mandarin, the dialectal control signal becomes weak.</p>\n\n",
                "matched_terms": [
                    "cantonese",
                    "generation",
                    "henanese",
                    "dialect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this issue and allow a Mandarin prompt to generate speech in any target dialect, we propose <span class=\"ltx_text ltx_font_bold\">Dialect-Guided Prompting (DGP)</span> inference strategy. Specifically, before generating a dialectal podcast, we prepend a short dialect-typical sentence&#8212;one that strongly reflects the target dialectal style&#8212;to the input text. This initial utterance effectively guides the model toward producing speech with the desired dialectal characteristics in subsequent generations.</p>\n\n",
                "matched_terms": [
                    "desired",
                    "dialect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although SoulX-Podcast is designed for multi-turn, multi-speaker dialogue synthesis, it is also capable of conventional monologue speech synthesis. Accordingly, we first compare its performance against SOTA TTS models on the standard monologue synthesis task. We then evaluate SoulX-Podcast&#8217;s capabilities in dialogue generation, as well as in paralinguistic and dialectal control.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "monologue",
                    "dialogue",
                    "generation",
                    "soulxpodcast",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the zero-shot (voice cloning) TTS capability of SoulX-Podcast, we assess its performance on Seed-TTS-eval and compare it with existing zero-shot TTS models. The results are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Monologue Speech Generation &#8227; 3 Performance of SoulX-Podcast &#8227; SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and Paralinguistic Diversity\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, where speech intelligibility is measured using CER for Chinese and WER for English, and speaker similarity (SIM) is quantified via the cosine similarity of speaker embeddings, following the Seed-TTS-eval protocol<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>https://github.com/BytedanceSpeech/seed-tts-eval</span></span></span>. As can been seen, SoulX-Podcast demonstrates significant superiority in intelligibility for zero-shot monologue TTS scenarios. Specifically, SoulX-Podcast achieves lowest CER in the Chinese test set. In the English test set, SoulX-Podcast only seconds to F5-TTS. In terms of speaker similarity, SoulX-Podcast also achieves strong results. Specifically, on both the Chinese and English test sets, it ranks just behind Seed-TTS and MaskGCT, demonstrating its excellent performance in conventional zero-shot TTS.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "monologue",
                    "sim",
                    "soulxpodcast",
                    "test",
                    "cer",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate multi-turn, multi-speaker dialogue generation, we compare SoulX-Podcast with representative dialogue TTS systems on the ZipVoice-Dia test set. This benchmark comprises natural multi-turn conversations, enabling assessment of both intelligibility and cross-speaker consistency (cpSIM) in long-form synthesis. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#S3.T2\" title=\"Table 2 &#8227; 3.2 Podcast Generation &#8227; 3 Performance of SoulX-Podcast &#8227; SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and Paralinguistic Diversity\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, SoulX-Podcast outperforms recent state-of-the-art models on both the Chinese and English subsets. Specifically, it achieves the lowest WER/CER and the highest cpSIM, while maintaining competitive UTMOS scores, demonstrating superior speaker coherence and perceived quality.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "cpsim",
                    "dialogue",
                    "generation",
                    "soulxpodcast",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the proposed model&#8217;s capability for controllable paralinguistic generation, we constructed a dedicated paralinguistic test set. Concretely, we employed GPT-5 to generate 20 test utterances for each of five paralinguistic labels, i.e., <span class=\"ltx_text ltx_font_typewriter\">&lt;|laughter|&gt;</span>, <span class=\"ltx_text ltx_font_typewriter\">&lt;|sigh|&gt;</span>, <span class=\"ltx_text ltx_font_typewriter\">&lt;|breathing|&gt;</span>, <span class=\"ltx_text ltx_font_typewriter\">&lt;|coughing|&gt;</span>, and <span class=\"ltx_text ltx_font_typewriter\">&lt;throat_clearing&gt;</span>. The corresponding audio samples were subsequently synthesized using SoulX-Podcast in monologue speech synthesis mode.</p>\n\n",
                "matched_terms": [
                    "monologue",
                    "generation",
                    "soulxpodcast",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SoulX-Podcast currently supports three major Chinese dialects: Sichuanese, Henanese, and Cantonese. We evaluate its performance on these dialects in both monologue TTS and dialogue generation settings. The monologue test set includes 1,000 samples per dialect, drawn from internal GPT-generated data as well as SeedTTS, Wenetspeech-Yue-eval&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx40\" title=\"\">40</a>]</cite>, and Wenetspeech-Chuan-eval&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23541v2#bib.bibx41\" title=\"\">41</a>]</cite>. The dialogue test set contains 100 GPT-generated items per dialect.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "cantonese",
                    "monologue",
                    "dialect",
                    "sichuanese",
                    "dialogue",
                    "dialects",
                    "generation",
                    "henanese",
                    "soulxpodcast",
                    "test",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced SoulX-Podcast, a large language model&#8211;driven framework for long-form, multi-speaker, and multi-dialect conversational speech synthesis. Through an interleaved text&#8211;speech modeling paradigm, SoulX-Podcast enables the generation of long-form, multi-turn conversational speech with consistent quality and coherence. Experimental results demonstrate that SoulX-Podcast not only excels in multi-turn dialogue synthesis but also generalizes effectively to zero-shot monologue TTS. Its capability to handle multiple Chinese dialects and paralinguistic cues further highlights its versatility and potential as a unified framework for speech generation.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "monologue",
                    "dialogue",
                    "dialects",
                    "generation",
                    "soulxpodcast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work focuses on advancing speech synthesis technology through the development of SoulX-Podcast, a large language model&#8211;driven framework for multi-speaker and multi-dialect conversational speech generation. All datasets used in this study were either publicly available or synthetically generated, and no personally identifiable information or private recordings were included.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "soulxpodcast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We acknowledge the potential risks associated with misuse of speech synthesis technology, such as voice spoofing, impersonation, or misinformation. To mitigate these risks, SoulX-Podcast is intended solely for research and responsible development of speech interfaces, and any downstream applications should incorporate appropriate speaker consent, watermarking, and misuse detection mechanisms. We advocate for the transparent, ethical, and human-centric use of speech generation technologies.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "soulxpodcast"
                ]
            }
        ]
    }
}