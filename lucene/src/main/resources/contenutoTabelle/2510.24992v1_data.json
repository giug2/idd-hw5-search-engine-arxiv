{
    "S4.T1": {
        "source_file": "POWSM: A Phonetic Open Whisper-Style Speech Foundation Model",
        "caption": "Table 1: Duration of the test sets for different tasks (in hours). Abbreviated datasets (in order): VoxAngeles, Tusom2021, DoReCo South-England, L2-ARCTIC, SpeechOcean762.",
        "body": "PR (In-domain)\n\n\neng\ndeu\nnld\nfra\nita\nspa\n\n\n10.58\n14.27\n12.76\n10.07\n5.27\n10.00\n\n\npor\npol\ntam\nkaz\ncmn\n\n\n\n3.74\n2.14\n16.58\n7.07\n10.02\n\n\n\nPR (Out-of-domain: Unseen languages)\n\n\nDoReCo\nVoxA.\nTusom.\n\n\n\n\n\n19.18\n1.58\n1.16\n\n\n\n\n\nPR (Out-of-domain: Language variation)\n\n\nBuckeye\nDRC-SE\nL2-ARC\nEpaDB\nSO762\n\n\n\n7.88\n0.77\n3.66\n2.74\n2.32\n\n\n\nASR (FLEURS)\n\n\nafr\norm\naze\npan\ntgk\nmkd\n\n\n0.66\n0.13\n2.37\n1.48\n1.96\n2.45\n\n\nbos\nslv\n\n\n\n\n\n\n2.45\n1.76",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" colspan=\"6\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">PR (In-domain)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">eng</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">deu</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">nld</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">fra</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">ita</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">spa</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">10.58</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">14.27</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">12.76</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">10.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">5.27</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">10.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">por</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">pol</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">tam</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">kaz</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">cmn</span></td>\n<td class=\"ltx_td\" style=\"padding:1pt 4.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">3.74</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">2.14</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">16.58</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">7.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">10.02</td>\n<td class=\"ltx_td\" style=\"padding:1pt 4.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<td class=\"ltx_td ltx_align_left\" colspan=\"6\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">PR (Out-of-domain: Unseen languages)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">DoReCo</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">VoxA.</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">Tusom.</td>\n<td class=\"ltx_td\" style=\"padding:1pt 4.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding:1pt 4.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding:1pt 4.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">19.18</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">1.58</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">1.16</td>\n<td class=\"ltx_td\" style=\"padding:1pt 4.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding:1pt 4.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding:1pt 4.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<td class=\"ltx_td ltx_align_left\" colspan=\"6\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">PR (Out-of-domain: Language variation)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">Buckeye</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">DRC-SE</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">L2-ARC</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">EpaDB</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">SO762</td>\n<td class=\"ltx_td\" style=\"padding:1pt 4.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">7.88</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">0.77</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">3.66</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">2.74</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">2.32</td>\n<td class=\"ltx_td\" style=\"padding:1pt 4.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" colspan=\"6\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">ASR (FLEURS)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">afr</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">orm</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">aze</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">pan</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">tgk</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">mkd</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">0.66</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">0.13</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">2.37</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">1.48</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">1.96</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">2.45</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">bos</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">slv</span></td>\n<td class=\"ltx_td\" style=\"padding:1pt 4.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding:1pt 4.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding:1pt 4.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding:1pt 4.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1pt 4.0pt;\">2.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1pt 4.0pt;\">1.76</td>\n<td class=\"ltx_td ltx_border_bb\" style=\"padding:1pt 4.0pt;\"/>\n<td class=\"ltx_td ltx_border_bb\" style=\"padding:1pt 4.0pt;\"/>\n<td class=\"ltx_td ltx_border_bb\" style=\"padding:1pt 4.0pt;\"/>\n<td class=\"ltx_td ltx_border_bb\" style=\"padding:1pt 4.0pt;\"/>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "sets",
            "tasks",
            "ita",
            "pan",
            "order",
            "l2arc",
            "orm",
            "eng",
            "hours",
            "datasets",
            "afr",
            "indomain",
            "mkd",
            "doreco",
            "test",
            "speechocean762",
            "outofdomain",
            "drcse",
            "so762",
            "variation",
            "pol",
            "buckeye",
            "bos",
            "slv",
            "deu",
            "aze",
            "language",
            "duration",
            "nld",
            "tusom2021",
            "spa",
            "voxa",
            "epadb",
            "kaz",
            "unseen",
            "asr",
            "southengland",
            "tusom",
            "abbreviated",
            "different",
            "por",
            "voxangeles",
            "l2arctic",
            "cmn",
            "fleurs",
            "languages",
            "tgk",
            "tam"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">See <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S4.T1\" title=\"Table 1 &#8227; Evaluation datasets &#8227; 4 Experimental Setup &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> for more details about our evaluation datasets.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in spoken language processing have led to substantial progress in phonetic tasks such as automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G). Despite their conceptual similarity, these tasks have largely been studied in isolation, each relying on task-specific architectures and datasets.\nIn this paper, we introduce POWSM (Phonetic Open Whisper-style Speech Model), the first unified framework capable of jointly performing multiple phone-related tasks.\nPOWSM enables seamless conversion between audio, text (graphemes), and phones, opening up new possibilities for universal and low-resource speech processing.\nOur model outperforms or matches specialized PR models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P, P2G, and ASR.\nOur training data, code<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/espnet</span></span></span> and models<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://huggingface.co/espnet/powsm</span></span></span> are released to foster open science.</p>\n\n",
                "matched_terms": [
                    "language",
                    "asr",
                    "datasets",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Phones are the smallest units of sound in speech. Unlike graphemes, phones are shared across languages and usually represented using the International Phonetic Alphabet (IPA) <cite class=\"ltx_cite ltx_citemacro_citep\">(International Phonetic Association, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib21\" title=\"\">1999</a>)</cite>, a unified transcription standard for all languages.\nBy providing a consistent representation of speech across languages, phone-level modeling allows fine-grained analysis and cross-lingual generalization, enabling tasks like atypical speech analysis (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, L2 speech <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib27\" title=\"\">2016</a>); Inceoglu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib20\" title=\"\">2023</a>)</cite> and pathological speech <cite class=\"ltx_cite ltx_citemacro_cite\">Choi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib9\" title=\"\">2025</a>); Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib26\" title=\"\">2025</a>)</cite>), endangered language documentation <cite class=\"ltx_cite ltx_citemacro_cite\">He et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib19\" title=\"\">2024</a>)</cite>, code-switched text-to-speech <cite class=\"ltx_cite ltx_citemacro_cite\">Zhou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib63\" title=\"\">2020</a>)</cite>, and cross-lingual transfer in speech-to-text <cite class=\"ltx_cite ltx_citemacro_cite\">Pratap et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib42\" title=\"\">2024</a>); Magoshi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib31\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "languages",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Four key phone-related tasks underpin phonetic spoken language processing: automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G).\n<span class=\"ltx_text ltx_font_italic\">ASR</span> learns implicit phonetic representations <cite class=\"ltx_cite ltx_citemacro_cite\">Belinkov and Glass (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib4\" title=\"\">2017</a>)</cite>, while <span class=\"ltx_text ltx_font_italic\">PR</span> offers explicit phone-level supervision.\n<span class=\"ltx_text ltx_font_italic\">G2P</span> and <span class=\"ltx_text ltx_font_italic\">P2G</span> bridge orthographic and phonetic spaces.\nCollectively, these tasks interact through shared phonetic representations, each addressing a different aspect of the relationship between audio, phones, phonemes, and graphemes.</p>\n\n",
                "matched_terms": [
                    "language",
                    "asr",
                    "different",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite their conceptual similarity, these tasks have traditionally been developed in isolation, using task-specific architectures and datasets.\nSuch systems are optimized for specific input-output mappings and cannot be easily extended to other phonetic tasks.\nThis fragmentation has hindered the development of general-purpose models for phonetic processing, necessitating a unified phonetic foundation model that can perform multiple phone-related tasks within a single, general framework for speech processing.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge this gap, we propose POWSM, a phonetic foundation model capable of performing four core phone-related tasks &#8212; PR, ASR, audio-guided G2P, and audio-guided P2G &#8212; within one unified architecture (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#S1.F1\" title=\"In 1 Introduction &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nTo construct this framework, we reformulate standard ASR datasets <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> into four task-specific formats, allowing the model to learn consistent mappings across audio, phoneme, and grapheme representations.\nIn addition, POWSM adopts an attention-based encoder-decoder (AED) architecture, following the design of large-scale speech foundation models such as Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite> and OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib39\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "datasets",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirically, POWSM outperforms previous PR models on both in-domain data and out-of-domain languages, achieves low-resource ASR performance comparable to web-scale multilingual foundation models, and can act as speech-grounded P2G and G2P across more than 70 languages.\nPOWSM offers a new unified paradigm for phone-level modeling, paving the way for inclusive and globally accessible speech technologies that transcend language boundaries and resource disparities.</p>\n\n",
                "matched_terms": [
                    "language",
                    "outofdomain",
                    "languages",
                    "indomain",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent speech foundation models such as Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite> and OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib39\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib38\" title=\"\">2024</a>)</cite> have driven progress in large-scale multilingual ASR and speech translation, but they do not explicitly address phoneme recognition or articulatory-level supervision.\nSubsequent work <cite class=\"ltx_cite ltx_citemacro_citep\">(Yusuyin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib60\" title=\"\">2025</a>; Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib11\" title=\"\">2025</a>)</cite> showed that incorporating phoneme-level objectives improves ASR for low-resource and long-tailed settings, while outputting phonemes as an intermediate benefited speech translation <cite class=\"ltx_cite ltx_citemacro_citep\">(G&#225;llego et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib18\" title=\"\">2025</a>)</cite>.\nPOWSM extends this line of work by being the first open foundation model jointly trained on phone recognition and related tasks, integrating multilinguality, phonetic supervision, and multi-task scalability within one framework.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ZIPA <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> scaled PR to 17,000+ hours of data and 88 languages using a Zipformer <cite class=\"ltx_cite ltx_citemacro_cite\">Yao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib59\" title=\"\">2024</a>)</cite> encoder and noisy-student training on 4,000+ languages, achieving state-of-the-art results. To construct its training corpus, ZIPA employed a G2P system to convert large-scale ASR transcriptions into phoneme sequences, effectively repurposing ASR datasets for PR. Building on this idea, POWSM leverages both the grapheme and the G2P-generated phoneme transcriptions, reformulating them into four task-specific forms: ASR, PR, G2P, and P2G.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "hours",
                    "datasets",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">G2P &amp; P2G</span> POWSM is the first model capable of both audio-guided G2P and audio-guided P2G.\nG2P conversion, sometimes called phonemization in the text-to-speech literature, can be accomplished with pronunciation dictionaries <cite class=\"ltx_cite ltx_citemacro_citep\">(Rudnicky, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib46\" title=\"\">1993</a>)</cite>, rules <cite class=\"ltx_cite ltx_citemacro_citep\">(Mortensen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib33\" title=\"\">2018</a>)</cite>, WFSTs <cite class=\"ltx_cite ltx_citemacro_citep\">(Black and Lenzo, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib5\" title=\"\">2001</a>)</cite>, or seq2seq neural methods to choose between different pronunciations of a word in context <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib65\" title=\"\">2022</a>)</cite>.\nText-based G2P, however, still cannot handle phonetic variation, enforcing a one-to-one mapping between orthography and transcription.\nIn contrast, audio-guided G2P can learn to map the different acoustic realizations of a phoneme across varieties of a language to a phone representation <cite class=\"ltx_cite ltx_citemacro_cite\">Route et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib45\" title=\"\">2019</a>)</cite>.\nIn particular, <cite class=\"ltx_cite ltx_citemacro_citet\">Mak et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib32\" title=\"\">2025</a>)</cite> observed a performance improvement in using audio-guided G2P versus text-based G2P alone for Cantonese.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Gao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib12\" title=\"\">2024</a>)</cite> similarly showed that joint learning of G2P, phone recognition, and forced alignment outperform a G2P teacher model. Similarly, <cite class=\"ltx_cite ltx_citemacro_citet\">Sun and Richmond (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib50\" title=\"\">2024</a>)</cite> jointly learned G2P and TTS.\nCompared to G2P, P2G conversion is less studied, with <cite class=\"ltx_cite ltx_citemacro_citet\">Lauc (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib25\" title=\"\">2024</a>)</cite> training a seq2seq model on 19 million language-grapheme-phoneme triplets.</p>\n\n",
                "matched_terms": [
                    "language",
                    "different",
                    "variation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">G2P-generated transcriptions have been manually inspected and cleaned. Following <cite class=\"ltx_cite ltx_citemacro_citet\">Samir et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib47\" title=\"\">2025</a>)</cite>, we remove Interlingua and 10 noisy FLEURS languages.\nUtterances longer than 300 phones are filtered out. IPA sequences are normalized to Unicode NFD (Canonical Decomposition); English G2P sequences are further refined with rule-based corrections to fix voice-onset time issues (see Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS1\" title=\"A.1 Refining English G2P &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.1</span></a>).</p>\n\n",
                "matched_terms": [
                    "fleurs",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model is trained on four tasks: PR, ASR, and audio-guided G2P and P2G. Each utterance is used once per task, with task-specific formatting as illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#S1.F1\" title=\"In 1 Introduction &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, including a text prompt, language token, task token, and target output. We leave the text prompt blank (token <span class=\"ltx_text ltx_font_typewriter\">&lt;na&gt;</span>) for PR and ASR, and provide graphemes and phones as prompts for G2P and P2G.</p>\n\n",
                "matched_terms": [
                    "language",
                    "asr",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For unseen languages, we evaluate on three datasets: DoReCo <cite class=\"ltx_cite ltx_citemacro_cite\">Paschen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib36\" title=\"\">2020</a>)</cite>, VoxAngeles <cite class=\"ltx_cite ltx_citemacro_cite\">Chodroff et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>)</cite>, and Tusom2021 <cite class=\"ltx_cite ltx_citemacro_cite\">Mortensen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib35\" title=\"\">2021</a>)</cite>.\nDoReCo is a dataset of 50+ languages (with broad transcriptions) intended for documentation of small or endangered languages; we use a 45-language subset.\nVoxAngeles <cite class=\"ltx_cite ltx_citemacro_citep\">(Chodroff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>)</cite> is a postprocessed version of the UCLA Phonetics Lab Archive <cite class=\"ltx_cite ltx_citemacro_citep\">(Ladefoged et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib24\" title=\"\">2009</a>)</cite> containing 95 languages.\nTusom is a low-data Tangkhulic language of India not included in the training data. Tusom2021 consists of narrow phonetic transcriptions (unlike the broad transcriptions from G2P on which POWSM was trained) of individual Tusom words.\nWe removed the tones.</p>\n\n",
                "matched_terms": [
                    "language",
                    "tusom2021",
                    "doreco",
                    "tusom",
                    "voxangeles",
                    "datasets",
                    "languages",
                    "unseen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also test on five datasets on varieties of English: the Buckeye Corpus <cite class=\"ltx_cite ltx_citemacro_cite\">Pitt et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib41\" title=\"\">2005</a>)</cite> and DoReCo South-England represent dialectal variation, while L2-ARCTIC <cite class=\"ltx_cite ltx_citemacro_cite\">Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib62\" title=\"\">2018</a>)</cite>, EpaDB <cite class=\"ltx_cite ltx_citemacro_cite\">Vidal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib54\" title=\"\">2019</a>)</cite>, and SpeechOcean762 <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib61\" title=\"\">2021</a>)</cite> contain L2 speakers.\nFor L2-ARCTIC, we used the manually annotated phoneme transcriptions (which <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> termed <span class=\"ltx_text ltx_font_italic\">L2-Perceived</span>) rather than G2P dictionary-based transcriptions. The manual transcriptions reflect what the speaker actually said, whereas the dictionary-based version enforces a single pronunciation variant.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>For instance, &#8220;crayon&#8221; in American English can be pronounced as\n<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/<span class=\"ltx_ERROR undefined\">\\textprimstress</span>k&#8290;r&#230;n/, <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/<span class=\"ltx_ERROR undefined\">\\textprimstress</span>k&#8290;rej.On/, or <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/<span class=\"ltx_ERROR undefined\">\\textprimstress</span>k&#8290;rej.6n/ <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaux and Golder, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib53\" title=\"\">2003</a>)</cite> (among others), but the CMU Pronouncing Dictionary <cite class=\"ltx_cite ltx_citemacro_citep\">(Rudnicky, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib46\" title=\"\">1993</a>)</cite> only lists one.</span></span></span>\nManual inspection by a trained phonologist further showed the L2-ARCTIC transcriptions to be of extremely poor quality.\nFor the five aforementioned datasets, we use preprocessed datasets from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://huggingface.co/anyspeech</span></span></span> and Koel Labs<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://huggingface.co/KoelLabs</span></span></span> for better transcription quality.</p>\n\n",
                "matched_terms": [
                    "southengland",
                    "doreco",
                    "test",
                    "speechocean762",
                    "variation",
                    "buckeye",
                    "l2arctic",
                    "datasets",
                    "epadb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then evaluated our model on in-domain data from IPAPack++, the dataset seen during training. We followed <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> in using LibriSpeech for English, AISHELL for Mandarin, and MLS for European languages, and additionally evaluated on IISc-MILE Tamil <cite class=\"ltx_cite ltx_citemacro_cite\">A et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib1\" title=\"\">2022</a>)</cite> for Tamil and KSC <cite class=\"ltx_cite ltx_citemacro_cite\">Khassanov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib22\" title=\"\">2021</a>)</cite> for Kazakh.\nFor ASR and P2G, we evaluate with FLEURS.</p>\n\n",
                "matched_terms": [
                    "indomain",
                    "languages",
                    "fleurs",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate all PR baselines without further training with IPAPack++. See Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS2\" title=\"A.2 Baseline Implementation &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.2</span></a> for more details about training data and language coverage.\nAllosaurus <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib28\" title=\"\">2020</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib29\" title=\"\">2021</a>)</cite> uses a phone-level CTC to train a language-agnostic model and applies language-specific allophone-to-phoneme mappings.\nWav2Vec2Phoneme <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib58\" title=\"\">2022</a>)</cite>, MultIPA <cite class=\"ltx_cite ltx_citemacro_cite\">Taguchi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib51\" title=\"\">2023</a>)</cite> and Allophant <cite class=\"ltx_cite ltx_citemacro_cite\">Glocker et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib14\" title=\"\">2023</a>)</cite> fine-tune XLS-R <cite class=\"ltx_cite ltx_citemacro_citep\">(Babu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib2\" title=\"\">2022</a>)</cite> with different objectives: Wav2Vec2Phoneme maps unseen phonemes using articulatory features, MultIPA leverages high-quality G2P data from seven languages, while Allophant decomposes phones into articulatory features and applies CTC losses for each.\nZIPA <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> trains ZipFormer <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib59\" title=\"\">2024</a>)</cite> from scratch on IPAPack++ using CR-CTC and also provides a variant trained with additional pseudo-labeled data (&#8220;ZIPA-CR-NS-Large&#8221;).</p>\n\n",
                "matched_terms": [
                    "language",
                    "languages",
                    "different",
                    "unseen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We found that POWSM&#8217;s performance on PR and ASR tasks is comparable or superior to competitive baselines.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results on the in-domain test sets are presented in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T2\" title=\"Table 2 &#8227; POWSM excels at in-domain phone recognition &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T3\" title=\"Table 3 &#8227; POWSM is comparable with web-scale ASR models on low-resource languages &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;3</span></a>. We provide further discussion of G2P and P2G in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.SS2\" title=\"6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;6.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "indomain",
                    "sets",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T2\" title=\"Table 2 &#8227; POWSM excels at in-domain phone recognition &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>, we see that POWSM achieves the lowest average PFER in phone recognition, due to the strong language modeling capability of the decoder.\nWe hypothesize that our English data cleaning (Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS1\" title=\"A.1 Refining English G2P &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.1</span></a>) may have negatively affected the PFER for Germanic languages due to a mismatch between training and test data.\nNevertheless, our approach fills this gap by achieving strong performance on other languages, outperforming models trained on larger datasets.</p>\n\n",
                "matched_terms": [
                    "language",
                    "datasets",
                    "test",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hypothesize that pre-training with phone recognition benefits low-resource ASR <cite class=\"ltx_cite ltx_citemacro_cite\">Yusuyin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib60\" title=\"\">2025</a>)</cite>.\nTo choose low-resource languages, we selected languages in IPAPack++ with less than 8 hours of speech in FLEURS to serve as the test set.\nSee <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS3\" title=\"A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.3</span></a> for details on the amount of data used by different models.\nFor a fair comparison with other multilingual ASR baselines without language-specific components, we use the same decoding hyperparameters <span class=\"ltx_text ltx_font_typewriter\">ctc=0.0, beam=1</span>.</p>\n\n",
                "matched_terms": [
                    "test",
                    "different",
                    "hours",
                    "fleurs",
                    "languages",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T4\" title=\"Table 4 &#8227; 5.2 POWSM generalizes well to unseen languages &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a> reports PFER on datasets with unseen languages and language variation.\nResults indicate that POWSM achieves strong performance across these datasets, and handles both dialectal and L2 variation effectively.\nNotably, our method outperforms ZIPA trained on the same data and even exceeds ZIPA trained with extra pseudo-labeled data, achieving the best results on unseen languages while performing three additional tasks.\nThis shows the effectiveness of our multi-task approach.\nWhile POWSM lags behind Wav2Vec2Phoneme on socio-phonetic variations, we attribute this to its self-supervised learning with over 60k hours of speech (from wav2vec 2.0 <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib3\" title=\"\">2020</a>)</cite>) prior to the supervised learning stage.</p>\n\n",
                "matched_terms": [
                    "language",
                    "tasks",
                    "variation",
                    "hours",
                    "datasets",
                    "languages",
                    "unseen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observed that mixing phones and orthography as encoder targets hindered training, because the same speech input would have different encoder CTC targets for different tasks.\nTherefore, we used phones as encoder targets, encouraging general representations of sounds to be shared across languages.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "different",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We run small-scale experiments on a 1k-hour subset of the multi-task data (250 hours of speech repeated across four tasks).\nWe use the validation CER of the encoder-CTC output as a proxy for training efficiency.\nAn earlier drop indicates that the encoder is learning a useful alignment early, which improves representations fed into the decoder and accelerates overall convergence.\nIn <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.F2\" title=\"Figure 2 &#8227; The CTC encoder prefers fine-grained phones without suprasegmentals &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;2</span></a>, PanPhon tokenization without suprasegmentals shows the earliest drop, suggesting that alignment with decoder units aids training, while\ncollapsing suprasegmental distinctions for CTC reduces confusion.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As in other encoder-decoder models <cite class=\"ltx_cite ltx_citemacro_cite\">Gong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib15\" title=\"\">2023</a>); Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>, we expect the encoder of POWSM to capture more general acoustic patterns, while the decoder handles language and task-specific output formats. Therefore,\nwe investigate whether emphasizing the encoder more during different stages of model development affects performance.\nTo balance data diversity with inference compute costs, we selected two smaller datasets from each category with distinct characteristics.</p>\n\n",
                "matched_terms": [
                    "language",
                    "different",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T5\" title=\"Table 5 &#8227; Increased encoder weights benefit PR on out-of-domain data &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;5</span></a>, higher CTC decoding weights improve PR performance on out-of-domain data but degrade it on in-domain data, as expected.\nThis echoes <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>&#8217;s finding that RNN-T <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves and Jaitly, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib17\" title=\"\">2014</a>)</cite>, an encoder-only speech-to-text model with an autoregressive text prediction network, hurts generalization to unseen patterns of phones (phonotactics).\nWe hypothesize that the decoder is performing implicit language modeling and &#8220;smooths&#8221; phonetic variation, as <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> described.</p>\n\n",
                "matched_terms": [
                    "language",
                    "outofdomain",
                    "variation",
                    "indomain",
                    "unseen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, we examine whether focusing more on the CTC loss through training widens this gap in performance between in-domain and out-of-domain data.\nWe find that fine-tuning with a higher CTC loss weight <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> after convergence does not improve out-of-domain performance and can even degrade it. Randomly varying <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> for each batch also shows no improvement.\nIn contrast, training with a higher <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> from the start benefits the out-of-domain distribution, achieving the lowest PFER on unseen languages with greedy decoding, while the PFER on in-domain data is comparatively higher.\nThese results suggest that assigning a higher weight to the encoder during training and inference improves PR, highlighting a common trade-off between in-domain performance and generalization.</p>\n\n",
                "matched_terms": [
                    "indomain",
                    "languages",
                    "outofdomain",
                    "unseen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare several P2G setups on the same set of low-resource languages from FLEURS, listed in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T7\" title=\"Table 7 &#8227; Audio-P2G effectively handles low-resource languages &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;7</span></a>. P2G significantly outperforms ASR, suggesting that it effectively leverages the provided phone context.\nHowever, since P2G uses gold phone labels, this comparison is not entirely fair. We therefore tested PR followed by P2G (PR-P2G), and found that performance improved for some languages but not for others.\nError propagation does not explain this variation in performance, as PFER trends from PR differ from the observed performance drops. Yet the PFER pattern aligns closely with ASR results, suggesting that phonotactic similarity to high-resource languages may play a role.</p>\n\n",
                "matched_terms": [
                    "fleurs",
                    "languages",
                    "variation",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test this, we run P2G with the language code set to English and post-process the output to match Cyrillic or Gurmukhi transcriptions with online conversion tools for certain languages.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.lexilogos.com\" title=\"\">https://www.lexilogos.com</a> for Macedonian and Tajik; <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://punjabi.indiatyping.com\" title=\"\">https://punjabi.indiatyping.com</a> for Panjabi.</span></span></span>\nThis approach often outperforms ASR and sometimes approaches P2G&#8217;s performance, indicating that P2G also relies heavily on speech input.\nLanguages with either comparibly low or high PFER did not benefit from this transliteration approach, possibly because the model already handled them well or had not yet learned them sufficiently.\nThis finding suggests a direction for further investigation in low-resource ASR.</p>\n\n",
                "matched_terms": [
                    "language",
                    "languages",
                    "test",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The language identification (LID) performance of POWSM on seen languages in FLEURS reaches 92.3% accuracy, as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.F3\" title=\"Figure 3 &#8227; A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;3</span></a>.\nTo see if the model implicitly learns phonotactic patterns and associates them with the language token, we evaluate PR on unseen languages by manipulating the language token at inference time.\nFor VoxAngeles and Tusom2021, the three most frequently assigned languages are Bashkir (42.6%, 25.1%), English (30.2%, 67.7%), and Kinyarwanda (14.5%, 2.3%), which are all relatively high-resource languages in IPAPack++.\n<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T8\" title=\"Table 8 &#8227; Language token captures phonotactics &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;8</span></a> shows that assigning the detected language token yields better performance than always using English, while setting the language as unknown performs best. This indicates that the language token influences PR by shifting the output distribution toward the assigned language.</p>\n\n",
                "matched_terms": [
                    "language",
                    "tusom2021",
                    "voxangeles",
                    "fleurs",
                    "languages",
                    "unseen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train a fully open-source phonetic speech foundation model POWSM using our scalable multi-task framework.\nOur model achieves state-of-the-art performance on PR while also supporting ASR across more than 70 languages.\nBeyond PR and ASR, the model&#8217;s ability to perform audio-guided G2P and P2G enables applications that require fine-grained linguistic analysis such as atypical speech assessment.\nOur analysis reveals that POWSM&#8217;s encoder benefits from phoneme-level CTC supervision and stronger encoder weighting, enhancing cross-lingual generalization.\nAdditionally, the model demonstrates interpretable multimodal and language-aware behaviors, effectively mediating between phonetic detail and standardized phonological patterns.\nTo conclude, POWSM not only provides a strong phone recognition foundation model for high-resource languages, but also acts as a versatile resource for unseen languages and socio-phonetic variation.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "languages",
                    "variation",
                    "unseen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">POWSM&#8217;s current decoder serves as a large phoneme-level phonotactic language model on which linguists could investigate hypotheses about phonetic universals <cite class=\"ltx_cite ltx_citemacro_citep\">(Chodroff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>; Chodroff, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib7\" title=\"\">2025</a>)</cite> and phonotactics <cite class=\"ltx_cite ltx_citemacro_citep\">(Shim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib48\" title=\"\">2024</a>; Pimentel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib40\" title=\"\">2020</a>)</cite>.\nIn the future, we seek to adapt to socio-phonetic variation either through (unsupervised) test-time adaptation <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib30\" title=\"\">2022</a>)</cite>, in-context learning <cite class=\"ltx_cite ltx_citemacro_citep\">(Roll et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib44\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib55\" title=\"\">2024</a>)</cite>, or mechanistic interpretability <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib52\" title=\"\">2024</a>)</cite>.\nFurthermore, since <cite class=\"ltx_cite ltx_citemacro_citet\">Shim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib49\" title=\"\">2025</a>)</cite> found that earlier encoder layers in Whisper preserve more phonetic detail, early exiting may mitigate the decoder&#8217;s tendencies to normalize socio-phonetic variation.</p>\n\n",
                "matched_terms": [
                    "language",
                    "variation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">POWSM has several limitations that we aim to address in future work.\nFirst, the model is neither strictly phonemic nor phonetic: its training data consist of cleaned and filtered phonemic transcriptions from multiple languages, which are not fully faithful to the phonetic or phonemic structure of the audio. Although phonemic transcriptions share similarities across languages, adding auxiliary tasks and language tokens may have reinforced language-specific biases. We also currently lack sufficient allophone-level data, which would provide more language-independent information.</p>\n\n",
                "matched_terms": [
                    "language",
                    "languages",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, the model still favors high-resource languages. Since we include a decoder for language modeling and language tokens, both of which function effectively, the model would inherently bias toward the seen distribution.</p>\n\n",
                "matched_terms": [
                    "language",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All of our data is ethically sourced, either through permissive licensing or through proper consent.\nWe are aware of the implicit prescriptivism and representational harms <cite class=\"ltx_cite ltx_citemacro_citep\">(Crawford, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib10\" title=\"\">2017</a>)</cite> that normalizing socio-phonetic variation in ASR or PR models can create.\nThis may threaten linguistic diversity instead of preserving it.\nWe also acknowledge that accurate modeling of socio-phonetic variation can enable demographic inference, as demographics and phonetic variation are deeply intertwined <cite class=\"ltx_cite ltx_citemacro_citep\">(Labov, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib23\" title=\"\">1963</a>)</cite>.\nWe stress that uses of POWSM must align with our vision: a future where advances in spoken language processing and NLP do not leave low-resource varieties behind.</p>\n\n",
                "matched_terms": [
                    "language",
                    "variation",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observed confusion in plosive voice-onset times on unseen languages in preliminary experiments, which is likely from English G2P data. For instance, broad phonemic transcription in English typically uses /b/ to transcribe the /b/ in /bat/, but its voice onset timing is actually voiceless in Mainstream American English and is closer to [p]. To mitigate this, we apply rule-based refinements to English G2P transcriptions, adjusting plosive voicing and aspiration, lateral velarization, and vowel nasalization.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "unseen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first filter out languages with more than 8 hours of training data in IPAPack++ <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>, keeping only those that are also present in FLEURS.\nThen, following the training data amounts reported in <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib6\" title=\"\">2025</a>)</cite>, we further identify the 50 lowest-resource languages to exclude any that may have other substantial sources not included in IPAPack++.\nThis process leaves us with nine languages. We finally exclude <span class=\"ltx_text ltx_font_typewriter\">ell</span>, as it is comparatively higher-resource and because there are already three other Balto-Slavic languages.\nNote that other models use strictly more data than ours&#8212;not only in terms of dataset count but also because IPAPack++ applies additional data-quality filtering. <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.T10\" title=\"Table 10 &#8227; A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;10</span></a> lists the amount of ASR training data for baselines.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "hours",
                    "fleurs",
                    "asr"
                ]
            }
        ]
    },
    "S5.T2": {
        "source_file": "POWSM: A Phonetic Open Whisper-Style Speech Foundation Model",
        "caption": "Table 2: PFER (\\downarrow) on the in-domain dataset, IPAPack++. Languages not supported by Allophant are left blank. Some languages were not seen by MultiIPA. Bold indicates the best performance.",
        "body": "Model\nParam.\neng\ndeu\nnld\nfra\nita\nspa\npor\npol\ntam\nkaz\ncmn\nAvg.\n\n\n\n\nAllosaurus\n11M\n6.89\n17.67\n19.19\n20.91\n19.02\n4.82\n19.61\n21.21\n12.01\n20.90\n15.28\n16.14\n\n\nAllophant\n300M\n10.26\n9.37\n18.39\n18.83\n7.82\n17.37\n15.44\n7.90\n19.32\n\n\n\n\n\nWav2Vec2Phoneme\n300M\n7.70\n7.89\n12.31\n17.73\n6.10\n3.67\n11.65\n9.57\n15.63\n15.30\n14.66\n11.11\n\n\nMultIPA\n300M\n15.81\n16.28\n18.97\n20.19\n7.20\n6.99\n15.04\n2.63\n10.54\n17.71\n21.10\n13.86\n\n\nZIPA-CR-Large\n300M\n1.63\n3.32\n3.03\n3.23\n3.24\n1.98\n4.01\n4.33\n4.59\n2.31\n1.25\n2.99\n\n\nZIPA-CR-NS-Large\n300M\n1.40\n3.17\n2.83\n2.92\n3.22\n1.53\n3.40\n4.31\n4.15\n1.87\n0.90\n2.70\n\n\nPOWSM\n350M\n2.85\n3.37\n5.14\n3.27\n1.81\n1.21\n2.90\n1.36\n3.56\n2.25\n1.15\n2.62",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Param.</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_typewriter\">eng</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_typewriter\">deu</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_typewriter\">nld</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_typewriter\">fra</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_typewriter\">ita</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_typewriter\">spa</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_typewriter\">por</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_typewriter\">pol</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_typewriter\">tam</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_typewriter\">kaz</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_typewriter\">cmn</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Avg.</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Allosaurus</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">11M</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">6.89</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">17.67</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">19.19</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">20.91</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">19.02</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">4.82</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">19.61</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">21.21</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">12.01</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">20.90</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">15.28</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">16.14</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Allophant</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">300M</th>\n<td class=\"ltx_td ltx_align_right\">10.26</td>\n<td class=\"ltx_td ltx_align_right\">9.37</td>\n<td class=\"ltx_td ltx_align_right\">18.39</td>\n<td class=\"ltx_td ltx_align_right\">18.83</td>\n<td class=\"ltx_td ltx_align_right\">7.82</td>\n<td class=\"ltx_td ltx_align_right\">17.37</td>\n<td class=\"ltx_td ltx_align_right\">15.44</td>\n<td class=\"ltx_td ltx_align_right\">7.90</td>\n<td class=\"ltx_td ltx_align_right\">19.32</td>\n<td class=\"ltx_td ltx_align_right\">&#8212;</td>\n<td class=\"ltx_td ltx_align_right\">&#8212;</td>\n<td class=\"ltx_td ltx_align_right\">&#8212;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Wav2Vec2Phoneme</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">300M</th>\n<td class=\"ltx_td ltx_align_right\">7.70</td>\n<td class=\"ltx_td ltx_align_right\">7.89</td>\n<td class=\"ltx_td ltx_align_right\">12.31</td>\n<td class=\"ltx_td ltx_align_right\">17.73</td>\n<td class=\"ltx_td ltx_align_right\">6.10</td>\n<td class=\"ltx_td ltx_align_right\">3.67</td>\n<td class=\"ltx_td ltx_align_right\">11.65</td>\n<td class=\"ltx_td ltx_align_right\">9.57</td>\n<td class=\"ltx_td ltx_align_right\">15.63</td>\n<td class=\"ltx_td ltx_align_right\">15.30</td>\n<td class=\"ltx_td ltx_align_right\">14.66</td>\n<td class=\"ltx_td ltx_align_right\">11.11</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MultIPA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">300M</th>\n<td class=\"ltx_td ltx_align_right\">15.81</td>\n<td class=\"ltx_td ltx_align_right\">16.28</td>\n<td class=\"ltx_td ltx_align_right\">18.97</td>\n<td class=\"ltx_td ltx_align_right\">20.19</td>\n<td class=\"ltx_td ltx_align_right\">7.20</td>\n<td class=\"ltx_td ltx_align_right\">6.99</td>\n<td class=\"ltx_td ltx_align_right\">15.04</td>\n<td class=\"ltx_td ltx_align_right\">2.63</td>\n<td class=\"ltx_td ltx_align_right\">10.54</td>\n<td class=\"ltx_td ltx_align_right\">17.71</td>\n<td class=\"ltx_td ltx_align_right\">21.10</td>\n<td class=\"ltx_td ltx_align_right\">13.86</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ZIPA-CR-Large</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">300M</th>\n<td class=\"ltx_td ltx_align_right\">1.63</td>\n<td class=\"ltx_td ltx_align_right\">3.32</td>\n<td class=\"ltx_td ltx_align_right\">3.03</td>\n<td class=\"ltx_td ltx_align_right\">3.23</td>\n<td class=\"ltx_td ltx_align_right\">3.24</td>\n<td class=\"ltx_td ltx_align_right\">1.98</td>\n<td class=\"ltx_td ltx_align_right\">4.01</td>\n<td class=\"ltx_td ltx_align_right\">4.33</td>\n<td class=\"ltx_td ltx_align_right\">4.59</td>\n<td class=\"ltx_td ltx_align_right\">2.31</td>\n<td class=\"ltx_td ltx_align_right\">1.25</td>\n<td class=\"ltx_td ltx_align_right\">2.99</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ZIPA-CR-NS-Large</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">300M</th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">1.40</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">3.17</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">2.83</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">2.92</span></td>\n<td class=\"ltx_td ltx_align_right\">3.22</td>\n<td class=\"ltx_td ltx_align_right\">1.53</td>\n<td class=\"ltx_td ltx_align_right\">3.40</td>\n<td class=\"ltx_td ltx_align_right\">4.31</td>\n<td class=\"ltx_td ltx_align_right\">4.15</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">1.87</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">0.90</span></td>\n<td class=\"ltx_td ltx_align_right\">2.70</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">POWSM</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t\">350M</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">2.85</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">3.37</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">5.14</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">3.27</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">1.81</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">1.21</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.90</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">1.36</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.56</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">2.25</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">1.15</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.62</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "ita",
            "downarrow",
            "blank",
            "eng",
            "avg",
            "powsm",
            "indomain",
            "supported",
            "left",
            "11m",
            "350m",
            "pol",
            "wav2vec2phoneme",
            "not",
            "allophant",
            "deu",
            "param",
            "allosaurus",
            "nld",
            "model",
            "ipapack",
            "spa",
            "300m",
            "dataset",
            "some",
            "seen",
            "zipacrlarge",
            "multiipa",
            "bold",
            "performance",
            "indicates",
            "kaz",
            "multipa",
            "tam",
            "por",
            "cmn",
            "best",
            "languages",
            "zipacrnslarge",
            "pfer"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Results on the in-domain test sets are presented in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T2\" title=\"Table 2 &#8227; POWSM excels at in-domain phone recognition &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T3\" title=\"Table 3 &#8227; POWSM is comparable with web-scale ASR models on low-resource languages &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;3</span></a>. We provide further discussion of G2P and P2G in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.SS2\" title=\"6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;6.2</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">From <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T2\" title=\"Table 2 &#8227; POWSM excels at in-domain phone recognition &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>, we see that POWSM achieves the lowest average PFER in phone recognition, due to the strong language modeling capability of the decoder.\nWe hypothesize that our English data cleaning (Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS1\" title=\"A.1 Refining English G2P &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.1</span></a>) may have negatively affected the PFER for Germanic languages due to a mismatch between training and test data.\nNevertheless, our approach fills this gap by achieving strong performance on other languages, outperforming models trained on larger datasets.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in spoken language processing have led to substantial progress in phonetic tasks such as automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G). Despite their conceptual similarity, these tasks have largely been studied in isolation, each relying on task-specific architectures and datasets.\nIn this paper, we introduce POWSM (Phonetic Open Whisper-style Speech Model), the first unified framework capable of jointly performing multiple phone-related tasks.\nPOWSM enables seamless conversion between audio, text (graphemes), and phones, opening up new possibilities for universal and low-resource speech processing.\nOur model outperforms or matches specialized PR models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P, P2G, and ASR.\nOur training data, code<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/espnet</span></span></span> and models<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://huggingface.co/espnet/powsm</span></span></span> are released to foster open science.</p>\n\n",
                "matched_terms": [
                    "model",
                    "wav2vec2phoneme",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">POWSM: A Phonetic Open Whisper-Style Speech Foundation Model</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge this gap, we propose POWSM, a phonetic foundation model capable of performing four core phone-related tasks &#8212; PR, ASR, audio-guided G2P, and audio-guided P2G &#8212; within one unified architecture (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#S1.F1\" title=\"In 1 Introduction &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nTo construct this framework, we reformulate standard ASR datasets <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> into four task-specific formats, allowing the model to learn consistent mappings across audio, phoneme, and grapheme representations.\nIn addition, POWSM adopts an attention-based encoder-decoder (AED) architecture, following the design of large-scale speech foundation models such as Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite> and OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib39\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirically, POWSM outperforms previous PR models on both in-domain data and out-of-domain languages, achieves low-resource ASR performance comparable to web-scale multilingual foundation models, and can act as speech-grounded P2G and G2P across more than 70 languages.\nPOWSM offers a new unified paradigm for phone-level modeling, paving the way for inclusive and globally accessible speech technologies that transcend language boundaries and resource disparities.</p>\n\n",
                "matched_terms": [
                    "indomain",
                    "performance",
                    "languages",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide POWSM, a large-scale foundation model that achieves state-of-the-art PR performance, and is capable of performing multiple fundamental phone-related tasks. Our model enables seamless conversion between speech, text (graphemes/orthography), and phones.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent speech foundation models such as Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite> and OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib39\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib38\" title=\"\">2024</a>)</cite> have driven progress in large-scale multilingual ASR and speech translation, but they do not explicitly address phoneme recognition or articulatory-level supervision.\nSubsequent work <cite class=\"ltx_cite ltx_citemacro_citep\">(Yusuyin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib60\" title=\"\">2025</a>; Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib11\" title=\"\">2025</a>)</cite> showed that incorporating phoneme-level objectives improves ASR for low-resource and long-tailed settings, while outputting phonemes as an intermediate benefited speech translation <cite class=\"ltx_cite ltx_citemacro_citep\">(G&#225;llego et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib18\" title=\"\">2025</a>)</cite>.\nPOWSM extends this line of work by being the first open foundation model jointly trained on phone recognition and related tasks, integrating multilinguality, phonetic supervision, and multi-task scalability within one framework.</p>\n\n",
                "matched_terms": [
                    "model",
                    "not",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior work in multilingual phone recognition can broadly be categorized into (1) language-specific models <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib13\" title=\"\">2021</a>)</cite> that rely on explicit phoneme <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib58\" title=\"\">2022</a>)</cite> or allophone inventories <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib28\" title=\"\">2020</a>)</cite> and (2) language-agnostic approaches that aim to generalize across languages without such resources <cite class=\"ltx_cite ltx_citemacro_citep\">(Taguchi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib51\" title=\"\">2023</a>; Glocker et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib14\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib29\" title=\"\">2021</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>.\nPOWSM follows the latter paradigm as a fully data-driven multilingual model that learns phone representations without predefined phoneme mappings.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">WhisperPPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Samir et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib47\" title=\"\">2025</a>)</cite> improved Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>&#8217;s performance through data cleaning but remained limited in data coverage and task diversity.\nHowever, Whisper is trained on a closed corpus and could display harmful biases for PR which cannot be fully removed by fine-tuning.\nPOWSM is trained from scratch on open datasets.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ZIPA <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> scaled PR to 17,000+ hours of data and 88 languages using a Zipformer <cite class=\"ltx_cite ltx_citemacro_cite\">Yao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib59\" title=\"\">2024</a>)</cite> encoder and noisy-student training on 4,000+ languages, achieving state-of-the-art results. To construct its training corpus, ZIPA employed a G2P system to convert large-scale ASR transcriptions into phoneme sequences, effectively repurposing ASR datasets for PR. Building on this idea, POWSM leverages both the grapheme and the G2P-generated phoneme transcriptions, reformulating them into four task-specific forms: ASR, PR, G2P, and P2G.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">G2P &amp; P2G</span> POWSM is the first model capable of both audio-guided G2P and audio-guided P2G.\nG2P conversion, sometimes called phonemization in the text-to-speech literature, can be accomplished with pronunciation dictionaries <cite class=\"ltx_cite ltx_citemacro_citep\">(Rudnicky, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib46\" title=\"\">1993</a>)</cite>, rules <cite class=\"ltx_cite ltx_citemacro_citep\">(Mortensen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib33\" title=\"\">2018</a>)</cite>, WFSTs <cite class=\"ltx_cite ltx_citemacro_citep\">(Black and Lenzo, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib5\" title=\"\">2001</a>)</cite>, or seq2seq neural methods to choose between different pronunciations of a word in context <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib65\" title=\"\">2022</a>)</cite>.\nText-based G2P, however, still cannot handle phonetic variation, enforcing a one-to-one mapping between orthography and transcription.\nIn contrast, audio-guided G2P can learn to map the different acoustic realizations of a phoneme across varieties of a language to a phone representation <cite class=\"ltx_cite ltx_citemacro_cite\">Route et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib45\" title=\"\">2019</a>)</cite>.\nIn particular, <cite class=\"ltx_cite ltx_citemacro_citet\">Mak et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib32\" title=\"\">2025</a>)</cite> observed a performance improvement in using audio-guided G2P versus text-based G2P alone for Cantonese.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Gao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib12\" title=\"\">2024</a>)</cite> similarly showed that joint learning of G2P, phone recognition, and forced alignment outperform a G2P teacher model. Similarly, <cite class=\"ltx_cite ltx_citemacro_citet\">Sun and Richmond (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib50\" title=\"\">2024</a>)</cite> jointly learned G2P and TTS.\nCompared to G2P, P2G conversion is less studied, with <cite class=\"ltx_cite ltx_citemacro_citet\">Lauc (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib25\" title=\"\">2024</a>)</cite> training a seq2seq model on 19 million language-grapheme-phoneme triplets.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use IPAPack++ <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> for training. It is an open source corpus of roughly 17,000 hours of multilingual speech with paired orthographic and phonemic transcriptions. We will release all data processing scripts to make POWSM fully reproducible.</p>\n\n",
                "matched_terms": [
                    "ipapack",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model is trained on four tasks: PR, ASR, and audio-guided G2P and P2G. Each utterance is used once per task, with task-specific formatting as illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#S1.F1\" title=\"In 1 Introduction &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, including a text prompt, language token, task token, and target output. We leave the text prompt blank (token <span class=\"ltx_text ltx_font_typewriter\">&lt;na&gt;</span>) for PR and ASR, and provide graphemes and phones as prompts for G2P and P2G.</p>\n\n",
                "matched_terms": [
                    "blank",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">POWSM adopts an attention-based encoder-decoder (AED) architecture, which flexibly models output sequences and allows the integration of additional tasks.\nSpecifically, we follow the OWSM v3.1 architecture <cite class=\"ltx_cite ltx_citemacro_cite\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib38\" title=\"\">2024</a>)</cite>, which employs an E-Branchformer encoder and a Transformer decoder, consistent with the general encoder-decoder structure of Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>. The model is trained from scratch using ESPnet <cite class=\"ltx_cite ltx_citemacro_cite\">Watanabe et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib56\" title=\"\">2018</a>)</cite> with a hybrid CTC/attention loss <cite class=\"ltx_cite ltx_citemacro_cite\">Watanabe et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib57\" title=\"\">2017</a>)</cite>, where we set the ratio <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> to 0.3:</p>\n\n",
                "matched_terms": [
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The encoder operates at the stride size of 40ms. Training uses a global batch size of 256. Speech inputs are 16kHz and padded to 20 seconds. The vocabulary consists of 40k tokens, including around 6k phone tokens, language and timestamp tokens, and BPE tokens from orthography.\nThe model has approximately 350M parameters with 9 layers for both the encoder and decoder and was trained on 4 H100 GPUs for 2 days.\nUsing a CTC loss <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib16\" title=\"\">2006</a>)</cite>, We align the encoder outputs with a simplified version of the phone token sequences. Unlike the decoder outputs, the phones in these sequences are stripped of break (<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/./, <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/*&#865;/)\nand length diacritics (<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/e<span class=\"ltx_ERROR undefined\">\\textlengthmark</span>/, /e<span class=\"ltx_ERROR undefined\">\\texthalflength</span>/, /&#283;/) to accelerate convergence. Additional details and analyses are provided in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.SS1.SSS0.Px1\" title=\"The CTC encoder prefers fine-grained phones without suprasegmentals &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;6.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "350m"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For unseen languages, we evaluate on three datasets: DoReCo <cite class=\"ltx_cite ltx_citemacro_cite\">Paschen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib36\" title=\"\">2020</a>)</cite>, VoxAngeles <cite class=\"ltx_cite ltx_citemacro_cite\">Chodroff et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>)</cite>, and Tusom2021 <cite class=\"ltx_cite ltx_citemacro_cite\">Mortensen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib35\" title=\"\">2021</a>)</cite>.\nDoReCo is a dataset of 50+ languages (with broad transcriptions) intended for documentation of small or endangered languages; we use a 45-language subset.\nVoxAngeles <cite class=\"ltx_cite ltx_citemacro_citep\">(Chodroff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>)</cite> is a postprocessed version of the UCLA Phonetics Lab Archive <cite class=\"ltx_cite ltx_citemacro_citep\">(Ladefoged et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib24\" title=\"\">2009</a>)</cite> containing 95 languages.\nTusom is a low-data Tangkhulic language of India not included in the training data. Tusom2021 consists of narrow phonetic transcriptions (unlike the broad transcriptions from G2P on which POWSM was trained) of individual Tusom words.\nWe removed the tones.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "not",
                    "dataset",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then evaluated our model on in-domain data from IPAPack++, the dataset seen during training. We followed <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> in using LibriSpeech for English, AISHELL for Mandarin, and MLS for European languages, and additionally evaluated on IISc-MILE Tamil <cite class=\"ltx_cite ltx_citemacro_cite\">A et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib1\" title=\"\">2022</a>)</cite> for Tamil and KSC <cite class=\"ltx_cite ltx_citemacro_cite\">Khassanov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib22\" title=\"\">2021</a>)</cite> for Kazakh.\nFor ASR and P2G, we evaluate with FLEURS.</p>\n\n",
                "matched_terms": [
                    "model",
                    "ipapack",
                    "seen",
                    "dataset",
                    "languages",
                    "indomain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate all PR baselines without further training with IPAPack++. See Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS2\" title=\"A.2 Baseline Implementation &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.2</span></a> for more details about training data and language coverage.\nAllosaurus <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib28\" title=\"\">2020</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib29\" title=\"\">2021</a>)</cite> uses a phone-level CTC to train a language-agnostic model and applies language-specific allophone-to-phoneme mappings.\nWav2Vec2Phoneme <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib58\" title=\"\">2022</a>)</cite>, MultIPA <cite class=\"ltx_cite ltx_citemacro_cite\">Taguchi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib51\" title=\"\">2023</a>)</cite> and Allophant <cite class=\"ltx_cite ltx_citemacro_cite\">Glocker et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib14\" title=\"\">2023</a>)</cite> fine-tune XLS-R <cite class=\"ltx_cite ltx_citemacro_citep\">(Babu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib2\" title=\"\">2022</a>)</cite> with different objectives: Wav2Vec2Phoneme maps unseen phonemes using articulatory features, MultIPA leverages high-quality G2P data from seven languages, while Allophant decomposes phones into articulatory features and applies CTC losses for each.\nZIPA <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> trains ZipFormer <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib59\" title=\"\">2024</a>)</cite> from scratch on IPAPack++ using CR-CTC and also provides a variant trained with additional pseudo-labeled data (&#8220;ZIPA-CR-NS-Large&#8221;).</p>\n\n",
                "matched_terms": [
                    "model",
                    "ipapack",
                    "multipa",
                    "wav2vec2phoneme",
                    "allophant",
                    "languages",
                    "allosaurus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR, we compare POWSM with two series of models: OWSM <cite class=\"ltx_cite ltx_citemacro_cite\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib37\" title=\"\">2025</a>)</cite> and OWLS <cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib6\" title=\"\">2025</a>)</cite>. We select OWSM-CTC v4 because it is the best-performing model in the series, featuring an encoder-CTC architecture that supports ASR, ST, and LID. For OWLS, we include models with comparable parameter sizes.</p>\n\n",
                "matched_terms": [
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hypothesize that pre-training with phone recognition benefits low-resource ASR <cite class=\"ltx_cite ltx_citemacro_cite\">Yusuyin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib60\" title=\"\">2025</a>)</cite>.\nTo choose low-resource languages, we selected languages in IPAPack++ with less than 8 hours of speech in FLEURS to serve as the test set.\nSee <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS3\" title=\"A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.3</span></a> for details on the amount of data used by different models.\nFor a fair comparison with other multilingual ASR baselines without language-specific components, we use the same decoding hyperparameters <span class=\"ltx_text ltx_font_typewriter\">ctc=0.0, beam=1</span>.</p>\n\n",
                "matched_terms": [
                    "ipapack",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T4\" title=\"Table 4 &#8227; 5.2 POWSM generalizes well to unseen languages &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a> reports PFER on datasets with unseen languages and language variation.\nResults indicate that POWSM achieves strong performance across these datasets, and handles both dialectal and L2 variation effectively.\nNotably, our method outperforms ZIPA trained on the same data and even exceeds ZIPA trained with extra pseudo-labeled data, achieving the best results on unseen languages while performing three additional tasks.\nThis shows the effectiveness of our multi-task approach.\nWhile POWSM lags behind Wav2Vec2Phoneme on socio-phonetic variations, we attribute this to its self-supervised learning with over 60k hours of speech (from wav2vec 2.0 <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib3\" title=\"\">2020</a>)</cite>) prior to the supervised learning stage.</p>\n\n",
                "matched_terms": [
                    "best",
                    "wav2vec2phoneme",
                    "languages",
                    "performance",
                    "powsm",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we analyze how POWSM works, focusing on the phonetic-aware encoder and task- and language-specific tokens, which are the defining features of the model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As in other encoder-decoder models <cite class=\"ltx_cite ltx_citemacro_cite\">Gong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib15\" title=\"\">2023</a>); Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>, we expect the encoder of POWSM to capture more general acoustic patterns, while the decoder handles language and task-specific output formats. Therefore,\nwe investigate whether emphasizing the encoder more during different stages of model development affects performance.\nTo balance data diversity with inference compute costs, we selected two smaller datasets from each category with distinct characteristics.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T5\" title=\"Table 5 &#8227; Increased encoder weights benefit PR on out-of-domain data &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;5</span></a>, higher CTC decoding weights improve PR performance on out-of-domain data but degrade it on in-domain data, as expected.\nThis echoes <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>&#8217;s finding that RNN-T <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves and Jaitly, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib17\" title=\"\">2014</a>)</cite>, an encoder-only speech-to-text model with an autoregressive text prediction network, hurts generalization to unseen patterns of phones (phonotactics).\nWe hypothesize that the decoder is performing implicit language modeling and &#8220;smooths&#8221; phonetic variation, as <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> described.</p>\n\n",
                "matched_terms": [
                    "indomain",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, we examine whether focusing more on the CTC loss through training widens this gap in performance between in-domain and out-of-domain data.\nWe find that fine-tuning with a higher CTC loss weight <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> after convergence does not improve out-of-domain performance and can even degrade it. Randomly varying <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> for each batch also shows no improvement.\nIn contrast, training with a higher <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> from the start benefits the out-of-domain distribution, achieving the lowest PFER on unseen languages with greedy decoding, while the PFER on in-domain data is comparatively higher.\nThese results suggest that assigning a higher weight to the encoder during training and inference improves PR, highlighting a common trade-off between in-domain performance and generalization.</p>\n\n",
                "matched_terms": [
                    "not",
                    "languages",
                    "indomain",
                    "performance",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand how POWSM integrates speech and text prompts, we analyze the relative influence of speech and text prompts in its G2P behavior.\nWe vary the G2P conditions from purely speech-based to purely text-based, as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T6\" title=\"Table 6 &#8227; Increased encoder weights benefit PR on out-of-domain data &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;6</span></a>, and evaluate the model on the Buckeye dataset.\nWhen only speech is provided, the performance is comparable to the PR setting, which differs only in the task token.\nAdding both speech and text prompts (the standard G2P setup) leads to degraded performance, with output showing standardized pronunciations.\nWhen the model relies solely on the text prompt, performance drops sharply and pronunciations become highly standardized as expected (just as <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> reported).</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model",
                    "dataset",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In other words, POWSM G2P responds to speech and text signals to controllably mediate between narrow and broad transcription.\nIn the multi-task setup, this effect may be stronger because the model is trained with G2P, which could bias it toward more standardized forms.</p>\n\n",
                "matched_terms": [
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare several P2G setups on the same set of low-resource languages from FLEURS, listed in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T7\" title=\"Table 7 &#8227; Audio-P2G effectively handles low-resource languages &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;7</span></a>. P2G significantly outperforms ASR, suggesting that it effectively leverages the provided phone context.\nHowever, since P2G uses gold phone labels, this comparison is not entirely fair. We therefore tested PR followed by P2G (PR-P2G), and found that performance improved for some languages but not for others.\nError propagation does not explain this variation in performance, as PFER trends from PR differ from the observed performance drops. Yet the PFER pattern aligns closely with ASR results, suggesting that phonotactic similarity to high-resource languages may play a role.</p>\n\n",
                "matched_terms": [
                    "some",
                    "not",
                    "languages",
                    "performance",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test this, we run P2G with the language code set to English and post-process the output to match Cyrillic or Gurmukhi transcriptions with online conversion tools for certain languages.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.lexilogos.com\" title=\"\">https://www.lexilogos.com</a> for Macedonian and Tajik; <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://punjabi.indiatyping.com\" title=\"\">https://punjabi.indiatyping.com</a> for Panjabi.</span></span></span>\nThis approach often outperforms ASR and sometimes approaches P2G&#8217;s performance, indicating that P2G also relies heavily on speech input.\nLanguages with either comparibly low or high PFER did not benefit from this transliteration approach, possibly because the model already handled them well or had not yet learned them sufficiently.\nThis finding suggests a direction for further investigation in low-resource ASR.</p>\n\n",
                "matched_terms": [
                    "model",
                    "not",
                    "languages",
                    "performance",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The language identification (LID) performance of POWSM on seen languages in FLEURS reaches 92.3% accuracy, as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.F3\" title=\"Figure 3 &#8227; A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;3</span></a>.\nTo see if the model implicitly learns phonotactic patterns and associates them with the language token, we evaluate PR on unseen languages by manipulating the language token at inference time.\nFor VoxAngeles and Tusom2021, the three most frequently assigned languages are Bashkir (42.6%, 25.1%), English (30.2%, 67.7%), and Kinyarwanda (14.5%, 2.3%), which are all relatively high-resource languages in IPAPack++.\n<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T8\" title=\"Table 8 &#8227; Language token captures phonotactics &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;8</span></a> shows that assigning the detected language token yields better performance than always using English, while setting the language as unknown performs best. This indicates that the language token influences PR by shifting the output distribution toward the assigned language.</p>\n\n",
                "matched_terms": [
                    "model",
                    "ipapack",
                    "languages",
                    "seen",
                    "best",
                    "powsm",
                    "performance",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train a fully open-source phonetic speech foundation model POWSM using our scalable multi-task framework.\nOur model achieves state-of-the-art performance on PR while also supporting ASR across more than 70 languages.\nBeyond PR and ASR, the model&#8217;s ability to perform audio-guided G2P and P2G enables applications that require fine-grained linguistic analysis such as atypical speech assessment.\nOur analysis reveals that POWSM&#8217;s encoder benefits from phoneme-level CTC supervision and stronger encoder weighting, enhancing cross-lingual generalization.\nAdditionally, the model demonstrates interpretable multimodal and language-aware behaviors, effectively mediating between phonetic detail and standardized phonological patterns.\nTo conclude, POWSM not only provides a strong phone recognition foundation model for high-resource languages, but also acts as a versatile resource for unseen languages and socio-phonetic variation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "not",
                    "languages",
                    "performance",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">POWSM has several limitations that we aim to address in future work.\nFirst, the model is neither strictly phonemic nor phonetic: its training data consist of cleaned and filtered phonemic transcriptions from multiple languages, which are not fully faithful to the phonetic or phonemic structure of the audio. Although phonemic transcriptions share similarities across languages, adding auxiliary tasks and language tokens may have reinforced language-specific biases. We also currently lack sufficient allophone-level data, which would provide more language-independent information.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "model",
                    "not",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, the model still favors high-resource languages. Since we include a decoder for language modeling and language tokens, both of which function effectively, the model would inherently bias toward the seen distribution.</p>\n\n",
                "matched_terms": [
                    "model",
                    "seen",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, the current AED architecture imposes engineering limitations. Inference is significantly slower than with encoder-only models, and the architecture does not easily support tone modeling, limiting its application to tonal languages.</p>\n\n",
                "matched_terms": [
                    "not",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All of our data is ethically sourced, either through permissive licensing or through proper consent.\nWe are aware of the implicit prescriptivism and representational harms <cite class=\"ltx_cite ltx_citemacro_citep\">(Crawford, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib10\" title=\"\">2017</a>)</cite> that normalizing socio-phonetic variation in ASR or PR models can create.\nThis may threaten linguistic diversity instead of preserving it.\nWe also acknowledge that accurate modeling of socio-phonetic variation can enable demographic inference, as demographics and phonetic variation are deeply intertwined <cite class=\"ltx_cite ltx_citemacro_citep\">(Labov, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib23\" title=\"\">1963</a>)</cite>.\nWe stress that uses of POWSM must align with our vision: a future where advances in spoken language processing and NLP do not leave low-resource varieties behind.</p>\n\n",
                "matched_terms": [
                    "not",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide the baselines&#8217; training data source, number of languages covered in the data, and links to model checkpoints or repository in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.T9\" title=\"Table 9 &#8227; A.2 Baseline Implementation &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;9</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first filter out languages with more than 8 hours of training data in IPAPack++ <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>, keeping only those that are also present in FLEURS.\nThen, following the training data amounts reported in <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib6\" title=\"\">2025</a>)</cite>, we further identify the 50 lowest-resource languages to exclude any that may have other substantial sources not included in IPAPack++.\nThis process leaves us with nine languages. We finally exclude <span class=\"ltx_text ltx_font_typewriter\">ell</span>, as it is comparatively higher-resource and because there are already three other Balto-Slavic languages.\nNote that other models use strictly more data than ours&#8212;not only in terms of dataset count but also because IPAPack++ applies additional data-quality filtering. <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.T10\" title=\"Table 10 &#8227; A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;10</span></a> lists the amount of ASR training data for baselines.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "not",
                    "ipapack",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multi-tasking may improve performance by tying acoustic signals to well-defined symbolic representations, yet it may distract the model if the relationships are not learned effectively.\nWe train POWSM with different data and model scales to examine how multitask learning interacts with the setup, and use <span class=\"ltx_text ltx_font_typewriter\">beam=1</span> during decoding to speed up inference.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model",
                    "not",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.T11\" title=\"Table 11 &#8227; A.4 Multi-tasking at Different Scales &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;11</span></a> shows that there is no clear trend regarding whether multitasking benefits PR performance. PR performance degrades when the model has excessive capacity relative to the available data (too little data), or when it is limited by size (too much data).</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "POWSM: A Phonetic Open Whisper-Style Speech Foundation Model",
        "caption": "Table 3: WER (\\downarrow) of ASR and PR-P2G on low-resource languages. PR-P2G uses phones predicted by PR as text prompts instead of gold phones. Bold indicates the best performance, and underline indicates the second-best.",
        "body": "Afroasiatic\nTurkic\nIndo-Iranian\nBalto-Slavic\n\n\nModel\nafr\norm\naze\npan\ntgk\nmkd\nbos\nslv\n\n\nOWLS 0.5B\n102.3\n89.0\n77.7\n59.3\n60.4\n54.2\n59.3\n58.6\n\n\nOWLS 1B\n95.7\n102.4\n67.5\n50.0\n50.7\n46.2\n50.0\n52.8\n\n\nOWSM-CTC v4 1B\n67.5\n92.7\n71.2\n88.7\n57.6\n51.2\n51.3\n60.4\n\n\nPOWSM 0.35B, ASR\n86.2\n125.3\n67.7\n83.1\n62.8\n56.0\n56.5\n64.5\n\n\nPOWSM 0.35B, PR-P2G\n68.8\n93.0\n66.7\n72.8\n51.0\n48.6\n56.9\n63.9",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding:2pt 4.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding:2pt 4.0pt;\">Afroasiatic</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:2pt 4.0pt;\">Turkic</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding:2pt 4.0pt;\">Indo-Iranian</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding:2pt 4.0pt;\">Balto-Slavic</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:2pt 4.0pt;\">Model</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">afr</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">orm</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">aze</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">pan</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">tgk</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">mkd</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">bos</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">slv</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:2pt 4.0pt;\">OWLS 0.5B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">102.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">89.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">77.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">59.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">60.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">54.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">59.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">58.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:2pt 4.0pt;\">OWLS 1B</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">95.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">102.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">67.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">50.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">50.7</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">46.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">50.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">52.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:2pt 4.0pt;\">OWSM-CTC v4 1B</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">67.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">92.7</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">71.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">88.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">57.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">51.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">51.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">60.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:2pt 4.0pt;\">POWSM 0.35B, ASR</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">86.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">125.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">67.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">83.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">62.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">56.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">56.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">64.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:2pt 4.0pt;\">POWSM 0.35B, PR-P2G</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">68.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:2pt 4.0pt;\">93.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">66.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:2pt 4.0pt;\">72.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">51.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">48.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:2pt 4.0pt;\">56.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:2pt 4.0pt;\">63.9</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "turkic",
            "text",
            "baltoslavic",
            "instead",
            "downarrow",
            "pan",
            "owls",
            "orm",
            "phones",
            "powsm",
            "afr",
            "mkd",
            "predicted",
            "prp2g",
            "lowresource",
            "bos",
            "slv",
            "wer",
            "underline",
            "aze",
            "gold",
            "model",
            "prompts",
            "bold",
            "performance",
            "owsmctc",
            "indoiranian",
            "indicates",
            "035b",
            "asr",
            "best",
            "afroasiatic",
            "05b",
            "languages",
            "tgk",
            "uses",
            "secondbest"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Results on the in-domain test sets are presented in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T2\" title=\"Table 2 &#8227; POWSM excels at in-domain phone recognition &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T3\" title=\"Table 3 &#8227; POWSM is comparable with web-scale ASR models on low-resource languages &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;3</span></a>. We provide further discussion of G2P and P2G in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.SS2\" title=\"6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;6.2</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T3\" title=\"Table 3 &#8227; POWSM is comparable with web-scale ASR models on low-resource languages &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;3</span></a>, POWSM (<span class=\"ltx_text ltx_font_typewriter\">POWSM 0.35B, ASR</span>) is often comparable to models of similar size trained on web-scale data for ASR (<span class=\"ltx_text ltx_font_typewriter\">OWLS 0.5B</span>).\nIncorporating phones obtained from PR as text prompts (PR-P2G) significantly decreases WER, making it comparable to or even better than these models.\nWhen using gold phone labels for P2G (see analysis in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.SS2.SSS0.Px2\" title=\"Audio-P2G effectively handles low-resource languages &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;6.2</span></a>), POWSM outperforms other ASR models by a large margin in most cases.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in spoken language processing have led to substantial progress in phonetic tasks such as automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G). Despite their conceptual similarity, these tasks have largely been studied in isolation, each relying on task-specific architectures and datasets.\nIn this paper, we introduce POWSM (Phonetic Open Whisper-style Speech Model), the first unified framework capable of jointly performing multiple phone-related tasks.\nPOWSM enables seamless conversion between audio, text (graphemes), and phones, opening up new possibilities for universal and low-resource speech processing.\nOur model outperforms or matches specialized PR models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P, P2G, and ASR.\nOur training data, code<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/espnet</span></span></span> and models<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://huggingface.co/espnet/powsm</span></span></span> are released to foster open science.</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "lowresource",
                    "phones",
                    "powsm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">POWSM: A Phonetic Open Whisper-Style Speech Foundation Model</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Phones are the smallest units of sound in speech. Unlike graphemes, phones are shared across languages and usually represented using the International Phonetic Alphabet (IPA) <cite class=\"ltx_cite ltx_citemacro_citep\">(International Phonetic Association, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib21\" title=\"\">1999</a>)</cite>, a unified transcription standard for all languages.\nBy providing a consistent representation of speech across languages, phone-level modeling allows fine-grained analysis and cross-lingual generalization, enabling tasks like atypical speech analysis (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, L2 speech <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib27\" title=\"\">2016</a>); Inceoglu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib20\" title=\"\">2023</a>)</cite> and pathological speech <cite class=\"ltx_cite ltx_citemacro_cite\">Choi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib9\" title=\"\">2025</a>); Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib26\" title=\"\">2025</a>)</cite>), endangered language documentation <cite class=\"ltx_cite ltx_citemacro_cite\">He et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib19\" title=\"\">2024</a>)</cite>, code-switched text-to-speech <cite class=\"ltx_cite ltx_citemacro_cite\">Zhou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib63\" title=\"\">2020</a>)</cite>, and cross-lingual transfer in speech-to-text <cite class=\"ltx_cite ltx_citemacro_cite\">Pratap et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib42\" title=\"\">2024</a>); Magoshi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib31\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "phones",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Four key phone-related tasks underpin phonetic spoken language processing: automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G).\n<span class=\"ltx_text ltx_font_italic\">ASR</span> learns implicit phonetic representations <cite class=\"ltx_cite ltx_citemacro_cite\">Belinkov and Glass (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib4\" title=\"\">2017</a>)</cite>, while <span class=\"ltx_text ltx_font_italic\">PR</span> offers explicit phone-level supervision.\n<span class=\"ltx_text ltx_font_italic\">G2P</span> and <span class=\"ltx_text ltx_font_italic\">P2G</span> bridge orthographic and phonetic spaces.\nCollectively, these tasks interact through shared phonetic representations, each addressing a different aspect of the relationship between audio, phones, phonemes, and graphemes.</p>\n\n",
                "matched_terms": [
                    "phones",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge this gap, we propose POWSM, a phonetic foundation model capable of performing four core phone-related tasks &#8212; PR, ASR, audio-guided G2P, and audio-guided P2G &#8212; within one unified architecture (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#S1.F1\" title=\"In 1 Introduction &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nTo construct this framework, we reformulate standard ASR datasets <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> into four task-specific formats, allowing the model to learn consistent mappings across audio, phoneme, and grapheme representations.\nIn addition, POWSM adopts an attention-based encoder-decoder (AED) architecture, following the design of large-scale speech foundation models such as Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite> and OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib39\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirically, POWSM outperforms previous PR models on both in-domain data and out-of-domain languages, achieves low-resource ASR performance comparable to web-scale multilingual foundation models, and can act as speech-grounded P2G and G2P across more than 70 languages.\nPOWSM offers a new unified paradigm for phone-level modeling, paving the way for inclusive and globally accessible speech technologies that transcend language boundaries and resource disparities.</p>\n\n",
                "matched_terms": [
                    "lowresource",
                    "languages",
                    "performance",
                    "powsm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide POWSM, a large-scale foundation model that achieves state-of-the-art PR performance, and is capable of performing multiple fundamental phone-related tasks. Our model enables seamless conversion between speech, text (graphemes/orthography), and phones.</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "phones",
                    "powsm",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent speech foundation models such as Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite> and OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib39\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib38\" title=\"\">2024</a>)</cite> have driven progress in large-scale multilingual ASR and speech translation, but they do not explicitly address phoneme recognition or articulatory-level supervision.\nSubsequent work <cite class=\"ltx_cite ltx_citemacro_citep\">(Yusuyin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib60\" title=\"\">2025</a>; Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib11\" title=\"\">2025</a>)</cite> showed that incorporating phoneme-level objectives improves ASR for low-resource and long-tailed settings, while outputting phonemes as an intermediate benefited speech translation <cite class=\"ltx_cite ltx_citemacro_citep\">(G&#225;llego et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib18\" title=\"\">2025</a>)</cite>.\nPOWSM extends this line of work by being the first open foundation model jointly trained on phone recognition and related tasks, integrating multilinguality, phonetic supervision, and multi-task scalability within one framework.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "model",
                    "lowresource",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior work in multilingual phone recognition can broadly be categorized into (1) language-specific models <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib13\" title=\"\">2021</a>)</cite> that rely on explicit phoneme <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib58\" title=\"\">2022</a>)</cite> or allophone inventories <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib28\" title=\"\">2020</a>)</cite> and (2) language-agnostic approaches that aim to generalize across languages without such resources <cite class=\"ltx_cite ltx_citemacro_citep\">(Taguchi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib51\" title=\"\">2023</a>; Glocker et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib14\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib29\" title=\"\">2021</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>.\nPOWSM follows the latter paradigm as a fully data-driven multilingual model that learns phone representations without predefined phoneme mappings.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">WhisperPPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Samir et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib47\" title=\"\">2025</a>)</cite> improved Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>&#8217;s performance through data cleaning but remained limited in data coverage and task diversity.\nHowever, Whisper is trained on a closed corpus and could display harmful biases for PR which cannot be fully removed by fine-tuning.\nPOWSM is trained from scratch on open datasets.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ZIPA <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> scaled PR to 17,000+ hours of data and 88 languages using a Zipformer <cite class=\"ltx_cite ltx_citemacro_cite\">Yao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib59\" title=\"\">2024</a>)</cite> encoder and noisy-student training on 4,000+ languages, achieving state-of-the-art results. To construct its training corpus, ZIPA employed a G2P system to convert large-scale ASR transcriptions into phoneme sequences, effectively repurposing ASR datasets for PR. Building on this idea, POWSM leverages both the grapheme and the G2P-generated phoneme transcriptions, reformulating them into four task-specific forms: ASR, PR, G2P, and P2G.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "languages",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">G2P &amp; P2G</span> POWSM is the first model capable of both audio-guided G2P and audio-guided P2G.\nG2P conversion, sometimes called phonemization in the text-to-speech literature, can be accomplished with pronunciation dictionaries <cite class=\"ltx_cite ltx_citemacro_citep\">(Rudnicky, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib46\" title=\"\">1993</a>)</cite>, rules <cite class=\"ltx_cite ltx_citemacro_citep\">(Mortensen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib33\" title=\"\">2018</a>)</cite>, WFSTs <cite class=\"ltx_cite ltx_citemacro_citep\">(Black and Lenzo, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib5\" title=\"\">2001</a>)</cite>, or seq2seq neural methods to choose between different pronunciations of a word in context <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib65\" title=\"\">2022</a>)</cite>.\nText-based G2P, however, still cannot handle phonetic variation, enforcing a one-to-one mapping between orthography and transcription.\nIn contrast, audio-guided G2P can learn to map the different acoustic realizations of a phoneme across varieties of a language to a phone representation <cite class=\"ltx_cite ltx_citemacro_cite\">Route et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib45\" title=\"\">2019</a>)</cite>.\nIn particular, <cite class=\"ltx_cite ltx_citemacro_citet\">Mak et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib32\" title=\"\">2025</a>)</cite> observed a performance improvement in using audio-guided G2P versus text-based G2P alone for Cantonese.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Gao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib12\" title=\"\">2024</a>)</cite> similarly showed that joint learning of G2P, phone recognition, and forced alignment outperform a G2P teacher model. Similarly, <cite class=\"ltx_cite ltx_citemacro_citet\">Sun and Richmond (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib50\" title=\"\">2024</a>)</cite> jointly learned G2P and TTS.\nCompared to G2P, P2G conversion is less studied, with <cite class=\"ltx_cite ltx_citemacro_citet\">Lauc (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib25\" title=\"\">2024</a>)</cite> training a seq2seq model on 19 million language-grapheme-phoneme triplets.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">G2P-generated transcriptions have been manually inspected and cleaned. Following <cite class=\"ltx_cite ltx_citemacro_citet\">Samir et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib47\" title=\"\">2025</a>)</cite>, we remove Interlingua and 10 noisy FLEURS languages.\nUtterances longer than 300 phones are filtered out. IPA sequences are normalized to Unicode NFD (Canonical Decomposition); English G2P sequences are further refined with rule-based corrections to fix voice-onset time issues (see Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS1\" title=\"A.1 Refining English G2P &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.1</span></a>).</p>\n\n",
                "matched_terms": [
                    "phones",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model is trained on four tasks: PR, ASR, and audio-guided G2P and P2G. Each utterance is used once per task, with task-specific formatting as illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#S1.F1\" title=\"In 1 Introduction &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, including a text prompt, language token, task token, and target output. We leave the text prompt blank (token <span class=\"ltx_text ltx_font_typewriter\">&lt;na&gt;</span>) for PR and ASR, and provide graphemes and phones as prompts for G2P and P2G.</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "phones",
                    "prompts",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">POWSM adopts an attention-based encoder-decoder (AED) architecture, which flexibly models output sequences and allows the integration of additional tasks.\nSpecifically, we follow the OWSM v3.1 architecture <cite class=\"ltx_cite ltx_citemacro_cite\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib38\" title=\"\">2024</a>)</cite>, which employs an E-Branchformer encoder and a Transformer decoder, consistent with the general encoder-decoder structure of Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>. The model is trained from scratch using ESPnet <cite class=\"ltx_cite ltx_citemacro_cite\">Watanabe et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib56\" title=\"\">2018</a>)</cite> with a hybrid CTC/attention loss <cite class=\"ltx_cite ltx_citemacro_cite\">Watanabe et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib57\" title=\"\">2017</a>)</cite>, where we set the ratio <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> to 0.3:</p>\n\n",
                "matched_terms": [
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The encoder operates at the stride size of 40ms. Training uses a global batch size of 256. Speech inputs are 16kHz and padded to 20 seconds. The vocabulary consists of 40k tokens, including around 6k phone tokens, language and timestamp tokens, and BPE tokens from orthography.\nThe model has approximately 350M parameters with 9 layers for both the encoder and decoder and was trained on 4 H100 GPUs for 2 days.\nUsing a CTC loss <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib16\" title=\"\">2006</a>)</cite>, We align the encoder outputs with a simplified version of the phone token sequences. Unlike the decoder outputs, the phones in these sequences are stripped of break (<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/./, <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/*&#865;/)\nand length diacritics (<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/e<span class=\"ltx_ERROR undefined\">\\textlengthmark</span>/, /e<span class=\"ltx_ERROR undefined\">\\texthalflength</span>/, /&#283;/) to accelerate convergence. Additional details and analyses are provided in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.SS1.SSS0.Px1\" title=\"The CTC encoder prefers fine-grained phones without suprasegmentals &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;6.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "uses",
                    "phones",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For unseen languages, we evaluate on three datasets: DoReCo <cite class=\"ltx_cite ltx_citemacro_cite\">Paschen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib36\" title=\"\">2020</a>)</cite>, VoxAngeles <cite class=\"ltx_cite ltx_citemacro_cite\">Chodroff et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>)</cite>, and Tusom2021 <cite class=\"ltx_cite ltx_citemacro_cite\">Mortensen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib35\" title=\"\">2021</a>)</cite>.\nDoReCo is a dataset of 50+ languages (with broad transcriptions) intended for documentation of small or endangered languages; we use a 45-language subset.\nVoxAngeles <cite class=\"ltx_cite ltx_citemacro_citep\">(Chodroff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>)</cite> is a postprocessed version of the UCLA Phonetics Lab Archive <cite class=\"ltx_cite ltx_citemacro_citep\">(Ladefoged et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib24\" title=\"\">2009</a>)</cite> containing 95 languages.\nTusom is a low-data Tangkhulic language of India not included in the training data. Tusom2021 consists of narrow phonetic transcriptions (unlike the broad transcriptions from G2P on which POWSM was trained) of individual Tusom words.\nWe removed the tones.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then evaluated our model on in-domain data from IPAPack++, the dataset seen during training. We followed <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> in using LibriSpeech for English, AISHELL for Mandarin, and MLS for European languages, and additionally evaluated on IISc-MILE Tamil <cite class=\"ltx_cite ltx_citemacro_cite\">A et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib1\" title=\"\">2022</a>)</cite> for Tamil and KSC <cite class=\"ltx_cite ltx_citemacro_cite\">Khassanov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib22\" title=\"\">2021</a>)</cite> for Kazakh.\nFor ASR and P2G, we evaluate with FLEURS.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate all PR baselines without further training with IPAPack++. See Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS2\" title=\"A.2 Baseline Implementation &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.2</span></a> for more details about training data and language coverage.\nAllosaurus <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib28\" title=\"\">2020</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib29\" title=\"\">2021</a>)</cite> uses a phone-level CTC to train a language-agnostic model and applies language-specific allophone-to-phoneme mappings.\nWav2Vec2Phoneme <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib58\" title=\"\">2022</a>)</cite>, MultIPA <cite class=\"ltx_cite ltx_citemacro_cite\">Taguchi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib51\" title=\"\">2023</a>)</cite> and Allophant <cite class=\"ltx_cite ltx_citemacro_cite\">Glocker et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib14\" title=\"\">2023</a>)</cite> fine-tune XLS-R <cite class=\"ltx_cite ltx_citemacro_citep\">(Babu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib2\" title=\"\">2022</a>)</cite> with different objectives: Wav2Vec2Phoneme maps unseen phonemes using articulatory features, MultIPA leverages high-quality G2P data from seven languages, while Allophant decomposes phones into articulatory features and applies CTC losses for each.\nZIPA <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> trains ZipFormer <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib59\" title=\"\">2024</a>)</cite> from scratch on IPAPack++ using CR-CTC and also provides a variant trained with additional pseudo-labeled data (&#8220;ZIPA-CR-NS-Large&#8221;).</p>\n\n",
                "matched_terms": [
                    "uses",
                    "phones",
                    "model",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR, we compare POWSM with two series of models: OWSM <cite class=\"ltx_cite ltx_citemacro_cite\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib37\" title=\"\">2025</a>)</cite> and OWLS <cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib6\" title=\"\">2025</a>)</cite>. We select OWSM-CTC v4 because it is the best-performing model in the series, featuring an encoder-CTC architecture that supports ASR, ST, and LID. For OWLS, we include models with comparable parameter sizes.</p>\n\n",
                "matched_terms": [
                    "model",
                    "owls",
                    "powsm",
                    "owsmctc",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We found that POWSM&#8217;s performance on PR and ASR tasks is comparable or superior to competitive baselines.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T2\" title=\"Table 2 &#8227; POWSM excels at in-domain phone recognition &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>, we see that POWSM achieves the lowest average PFER in phone recognition, due to the strong language modeling capability of the decoder.\nWe hypothesize that our English data cleaning (Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS1\" title=\"A.1 Refining English G2P &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.1</span></a>) may have negatively affected the PFER for Germanic languages due to a mismatch between training and test data.\nNevertheless, our approach fills this gap by achieving strong performance on other languages, outperforming models trained on larger datasets.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "languages",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hypothesize that pre-training with phone recognition benefits low-resource ASR <cite class=\"ltx_cite ltx_citemacro_cite\">Yusuyin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib60\" title=\"\">2025</a>)</cite>.\nTo choose low-resource languages, we selected languages in IPAPack++ with less than 8 hours of speech in FLEURS to serve as the test set.\nSee <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS3\" title=\"A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.3</span></a> for details on the amount of data used by different models.\nFor a fair comparison with other multilingual ASR baselines without language-specific components, we use the same decoding hyperparameters <span class=\"ltx_text ltx_font_typewriter\">ctc=0.0, beam=1</span>.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "lowresource",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T4\" title=\"Table 4 &#8227; 5.2 POWSM generalizes well to unseen languages &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a> reports PFER on datasets with unseen languages and language variation.\nResults indicate that POWSM achieves strong performance across these datasets, and handles both dialectal and L2 variation effectively.\nNotably, our method outperforms ZIPA trained on the same data and even exceeds ZIPA trained with extra pseudo-labeled data, achieving the best results on unseen languages while performing three additional tasks.\nThis shows the effectiveness of our multi-task approach.\nWhile POWSM lags behind Wav2Vec2Phoneme on socio-phonetic variations, we attribute this to its self-supervised learning with over 60k hours of speech (from wav2vec 2.0 <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib3\" title=\"\">2020</a>)</cite>) prior to the supervised learning stage.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "languages",
                    "best",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we analyze how POWSM works, focusing on the phonetic-aware encoder and task- and language-specific tokens, which are the defining features of the model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observed that mixing phones and orthography as encoder targets hindered training, because the same speech input would have different encoder CTC targets for different tasks.\nTherefore, we used phones as encoder targets, encouraging general representations of sounds to be shared across languages.</p>\n\n",
                "matched_terms": [
                    "phones",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To determine the most effective unit for the CTC encoder, we fix the decoder vocabulary to PanPhon phones and compared four encoder targets: (1) Unicode code points vs. PanPhon, and (2) sequences with vs. without suprasegmentals (length and break marks). Unicode code points offer simplicity and a smaller vocabulary but split phones into unnatural units (e.g. <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/p<sup class=\"ltx_sup\">h</sup>/) and increase sequence length, while PanPhon represents each phone-diacritic combination as a unit (e.g. <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/p<sup class=\"ltx_sup\">h</sup>/), yielding a more natural monotonic sequence at the expense of sparsity and potential out-of-vocabulary issues. Suprasegmentals such as <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/<span class=\"ltx_ERROR undefined\">\\textlengthmark</span>/, though phonemic in many languages, confuse PR models <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "phones",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As in other encoder-decoder models <cite class=\"ltx_cite ltx_citemacro_cite\">Gong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib15\" title=\"\">2023</a>); Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>, we expect the encoder of POWSM to capture more general acoustic patterns, while the decoder handles language and task-specific output formats. Therefore,\nwe investigate whether emphasizing the encoder more during different stages of model development affects performance.\nTo balance data diversity with inference compute costs, we selected two smaller datasets from each category with distinct characteristics.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T5\" title=\"Table 5 &#8227; Increased encoder weights benefit PR on out-of-domain data &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;5</span></a>, higher CTC decoding weights improve PR performance on out-of-domain data but degrade it on in-domain data, as expected.\nThis echoes <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>&#8217;s finding that RNN-T <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves and Jaitly, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib17\" title=\"\">2014</a>)</cite>, an encoder-only speech-to-text model with an autoregressive text prediction network, hurts generalization to unseen patterns of phones (phonotactics).\nWe hypothesize that the decoder is performing implicit language modeling and &#8220;smooths&#8221; phonetic variation, as <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> described.</p>\n\n",
                "matched_terms": [
                    "text",
                    "phones",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, we examine whether focusing more on the CTC loss through training widens this gap in performance between in-domain and out-of-domain data.\nWe find that fine-tuning with a higher CTC loss weight <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> after convergence does not improve out-of-domain performance and can even degrade it. Randomly varying <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> for each batch also shows no improvement.\nIn contrast, training with a higher <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> from the start benefits the out-of-domain distribution, achieving the lowest PFER on unseen languages with greedy decoding, while the PFER on in-domain data is comparatively higher.\nThese results suggest that assigning a higher weight to the encoder during training and inference improves PR, highlighting a common trade-off between in-domain performance and generalization.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand how POWSM integrates speech and text prompts, we analyze the relative influence of speech and text prompts in its G2P behavior.\nWe vary the G2P conditions from purely speech-based to purely text-based, as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T6\" title=\"Table 6 &#8227; Increased encoder weights benefit PR on out-of-domain data &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;6</span></a>, and evaluate the model on the Buckeye dataset.\nWhen only speech is provided, the performance is comparable to the PR setting, which differs only in the task token.\nAdding both speech and text prompts (the standard G2P setup) leads to degraded performance, with output showing standardized pronunciations.\nWhen the model relies solely on the text prompt, performance drops sharply and pronunciations become highly standardized as expected (just as <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> reported).</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "prompts",
                    "performance",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In other words, POWSM G2P responds to speech and text signals to controllably mediate between narrow and broad transcription.\nIn the multi-task setup, this effect may be stronger because the model is trained with G2P, which could bias it toward more standardized forms.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare several P2G setups on the same set of low-resource languages from FLEURS, listed in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T7\" title=\"Table 7 &#8227; Audio-P2G effectively handles low-resource languages &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;7</span></a>. P2G significantly outperforms ASR, suggesting that it effectively leverages the provided phone context.\nHowever, since P2G uses gold phone labels, this comparison is not entirely fair. We therefore tested PR followed by P2G (PR-P2G), and found that performance improved for some languages but not for others.\nError propagation does not explain this variation in performance, as PFER trends from PR differ from the observed performance drops. Yet the PFER pattern aligns closely with ASR results, suggesting that phonotactic similarity to high-resource languages may play a role.</p>\n\n",
                "matched_terms": [
                    "lowresource",
                    "prp2g",
                    "gold",
                    "languages",
                    "performance",
                    "uses",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test this, we run P2G with the language code set to English and post-process the output to match Cyrillic or Gurmukhi transcriptions with online conversion tools for certain languages.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.lexilogos.com\" title=\"\">https://www.lexilogos.com</a> for Macedonian and Tajik; <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://punjabi.indiatyping.com\" title=\"\">https://punjabi.indiatyping.com</a> for Panjabi.</span></span></span>\nThis approach often outperforms ASR and sometimes approaches P2G&#8217;s performance, indicating that P2G also relies heavily on speech input.\nLanguages with either comparibly low or high PFER did not benefit from this transliteration approach, possibly because the model already handled them well or had not yet learned them sufficiently.\nThis finding suggests a direction for further investigation in low-resource ASR.</p>\n\n",
                "matched_terms": [
                    "model",
                    "lowresource",
                    "languages",
                    "performance",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The language identification (LID) performance of POWSM on seen languages in FLEURS reaches 92.3% accuracy, as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.F3\" title=\"Figure 3 &#8227; A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;3</span></a>.\nTo see if the model implicitly learns phonotactic patterns and associates them with the language token, we evaluate PR on unseen languages by manipulating the language token at inference time.\nFor VoxAngeles and Tusom2021, the three most frequently assigned languages are Bashkir (42.6%, 25.1%), English (30.2%, 67.7%), and Kinyarwanda (14.5%, 2.3%), which are all relatively high-resource languages in IPAPack++.\n<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T8\" title=\"Table 8 &#8227; Language token captures phonotactics &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;8</span></a> shows that assigning the detected language token yields better performance than always using English, while setting the language as unknown performs best. This indicates that the language token influences PR by shifting the output distribution toward the assigned language.</p>\n\n",
                "matched_terms": [
                    "model",
                    "languages",
                    "best",
                    "powsm",
                    "performance",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train a fully open-source phonetic speech foundation model POWSM using our scalable multi-task framework.\nOur model achieves state-of-the-art performance on PR while also supporting ASR across more than 70 languages.\nBeyond PR and ASR, the model&#8217;s ability to perform audio-guided G2P and P2G enables applications that require fine-grained linguistic analysis such as atypical speech assessment.\nOur analysis reveals that POWSM&#8217;s encoder benefits from phoneme-level CTC supervision and stronger encoder weighting, enhancing cross-lingual generalization.\nAdditionally, the model demonstrates interpretable multimodal and language-aware behaviors, effectively mediating between phonetic detail and standardized phonological patterns.\nTo conclude, POWSM not only provides a strong phone recognition foundation model for high-resource languages, but also acts as a versatile resource for unseen languages and socio-phonetic variation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "languages",
                    "performance",
                    "powsm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">POWSM has several limitations that we aim to address in future work.\nFirst, the model is neither strictly phonemic nor phonetic: its training data consist of cleaned and filtered phonemic transcriptions from multiple languages, which are not fully faithful to the phonetic or phonemic structure of the audio. Although phonemic transcriptions share similarities across languages, adding auxiliary tasks and language tokens may have reinforced language-specific biases. We also currently lack sufficient allophone-level data, which would provide more language-independent information.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, the model still favors high-resource languages. Since we include a decoder for language modeling and language tokens, both of which function effectively, the model would inherently bias toward the seen distribution.</p>\n\n",
                "matched_terms": [
                    "model",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All of our data is ethically sourced, either through permissive licensing or through proper consent.\nWe are aware of the implicit prescriptivism and representational harms <cite class=\"ltx_cite ltx_citemacro_citep\">(Crawford, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib10\" title=\"\">2017</a>)</cite> that normalizing socio-phonetic variation in ASR or PR models can create.\nThis may threaten linguistic diversity instead of preserving it.\nWe also acknowledge that accurate modeling of socio-phonetic variation can enable demographic inference, as demographics and phonetic variation are deeply intertwined <cite class=\"ltx_cite ltx_citemacro_citep\">(Labov, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib23\" title=\"\">1963</a>)</cite>.\nWe stress that uses of POWSM must align with our vision: a future where advances in spoken language processing and NLP do not leave low-resource varieties behind.</p>\n\n",
                "matched_terms": [
                    "instead",
                    "lowresource",
                    "powsm",
                    "uses",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observed confusion in plosive voice-onset times on unseen languages in preliminary experiments, which is likely from English G2P data. For instance, broad phonemic transcription in English typically uses /b/ to transcribe the /b/ in /bat/, but its voice onset timing is actually voiceless in Mainstream American English and is closer to [p]. To mitigate this, we apply rule-based refinements to English G2P transcriptions, adjusting plosive voicing and aspiration, lateral velarization, and vowel nasalization.</p>\n\n",
                "matched_terms": [
                    "uses",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide the baselines&#8217; training data source, number of languages covered in the data, and links to model checkpoints or repository in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.T9\" title=\"Table 9 &#8227; A.2 Baseline Implementation &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;9</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first filter out languages with more than 8 hours of training data in IPAPack++ <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>, keeping only those that are also present in FLEURS.\nThen, following the training data amounts reported in <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib6\" title=\"\">2025</a>)</cite>, we further identify the 50 lowest-resource languages to exclude any that may have other substantial sources not included in IPAPack++.\nThis process leaves us with nine languages. We finally exclude <span class=\"ltx_text ltx_font_typewriter\">ell</span>, as it is comparatively higher-resource and because there are already three other Balto-Slavic languages.\nNote that other models use strictly more data than ours&#8212;not only in terms of dataset count but also because IPAPack++ applies additional data-quality filtering. <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.T10\" title=\"Table 10 &#8227; A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;10</span></a> lists the amount of ASR training data for baselines.</p>\n\n",
                "matched_terms": [
                    "baltoslavic",
                    "languages",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multi-tasking may improve performance by tying acoustic signals to well-defined symbolic representations, yet it may distract the model if the relationships are not learned effectively.\nWe train POWSM with different data and model scales to examine how multitask learning interacts with the setup, and use <span class=\"ltx_text ltx_font_typewriter\">beam=1</span> during decoding to speed up inference.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.T11\" title=\"Table 11 &#8227; A.4 Multi-tasking at Different Scales &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;11</span></a> shows that there is no clear trend regarding whether multitasking benefits PR performance. PR performance degrades when the model has excessive capacity relative to the available data (too little data), or when it is limited by size (too much data).</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model"
                ]
            }
        ]
    },
    "S5.T4": {
        "source_file": "POWSM: A Phonetic Open Whisper-Style Speech Foundation Model",
        "caption": "Table 4: PFER (\\downarrow) on out-of-domain data. DRC-SE stands for DoReCo South-England; L2-ARC stands for L2-ARCTIC; SO762 stands for SpeechOcean762. Unseen language datasets include languages not supported by Allophant; therefore, we do not report results for these datasets.",
        "body": "Unseen Languages\nLanguage Variation\n\n\nModel\nParam.\nDoReCo\nVoxAngeles\nTusom2021\nAvg.\nBuckeye\nDRC-SE\nL2-ARC\nEpaDB\nSO762\nAvg.\n\n\n\n\nAllosaurus\n11M\n24.71\n30.84\n42.02\n32.52\n15.24\n25.36\n13.39\n19.33\n21.61\n18.99\n\n\nAllophant\n300M\n\n\n\n\n16.05\n24.13\n11.91\n14.38\n18.28\n16.95\n\n\nWav2Vec2Phoneme\n300M\n17.25\n13.88\n31.92\n21.02\n12.50\n18.57\n9.86\n9.90\n13.60\n12.89\n\n\nMultIPA\n300M\n18.28\n15.23\n30.53\n21.35\n18.69\n23.31\n15.52\n15.64\n21.34\n18.90\n\n\nZIPA-CR-Large\n300M\n17.99\n16.95\n23.68\n19.54\n12.04\n17.89\n9.74\n17.38\n15.58\n14.53\n\n\nZIPA-CR-NS-Large\n300M\n16.82\n17.14\n23.08\n19.01\n12.05\n17.12\n9.69\n14.63\n18.20\n14.34\n\n\nPOWSM\n350M\n17.06\n17.11\n21.96\n18.71\n12.63\n18.33\n11.32\n11.86\n17.84\n14.40",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_tt\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\">Unseen Languages</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"6\">Language Variation</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Param.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">DoReCo</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">VoxAngeles</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Tusom2021</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">Avg.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Buckeye</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">DRC-SE</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\">L2-ARC</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\">EpaDB</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">SO762</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Avg.</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Allosaurus</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">11M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">24.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">30.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">42.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">32.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">15.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.36</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">13.39</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">19.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">21.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">18.99</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Allophant</td>\n<td class=\"ltx_td ltx_align_center\">300M</td>\n<td class=\"ltx_td ltx_align_center\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center\">16.05</td>\n<td class=\"ltx_td ltx_align_center\">24.13</td>\n<td class=\"ltx_td ltx_align_right\">11.91</td>\n<td class=\"ltx_td ltx_align_right\">14.38</td>\n<td class=\"ltx_td ltx_align_center\">18.28</td>\n<td class=\"ltx_td ltx_align_center\">16.95</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Wav2Vec2Phoneme</td>\n<td class=\"ltx_td ltx_align_center\">300M</td>\n<td class=\"ltx_td ltx_align_center\">17.25</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">13.88</span></td>\n<td class=\"ltx_td ltx_align_center\">31.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">21.02</td>\n<td class=\"ltx_td ltx_align_center\">12.50</td>\n<td class=\"ltx_td ltx_align_center\">18.57</td>\n<td class=\"ltx_td ltx_align_right\">9.86</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">9.90</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">13.60</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">12.89</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MultIPA</td>\n<td class=\"ltx_td ltx_align_center\">300M</td>\n<td class=\"ltx_td ltx_align_center\">18.28</td>\n<td class=\"ltx_td ltx_align_center\">15.23</td>\n<td class=\"ltx_td ltx_align_center\">30.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">21.35</td>\n<td class=\"ltx_td ltx_align_center\">18.69</td>\n<td class=\"ltx_td ltx_align_center\">23.31</td>\n<td class=\"ltx_td ltx_align_right\">15.52</td>\n<td class=\"ltx_td ltx_align_right\">15.64</td>\n<td class=\"ltx_td ltx_align_center\">21.34</td>\n<td class=\"ltx_td ltx_align_center\">18.90</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">ZIPA-CR-Large</td>\n<td class=\"ltx_td ltx_align_center\">300M</td>\n<td class=\"ltx_td ltx_align_center\">17.99</td>\n<td class=\"ltx_td ltx_align_center\">16.95</td>\n<td class=\"ltx_td ltx_align_center\">23.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">19.54</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">12.04</span></td>\n<td class=\"ltx_td ltx_align_center\">17.89</td>\n<td class=\"ltx_td ltx_align_right\">9.74</td>\n<td class=\"ltx_td ltx_align_right\">17.38</td>\n<td class=\"ltx_td ltx_align_center\">15.58</td>\n<td class=\"ltx_td ltx_align_center\">14.53</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">ZIPA-CR-NS-Large</td>\n<td class=\"ltx_td ltx_align_center\">300M</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">16.82</span></td>\n<td class=\"ltx_td ltx_align_center\">17.14</td>\n<td class=\"ltx_td ltx_align_center\">23.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">19.01</td>\n<td class=\"ltx_td ltx_align_center\">12.05</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">17.12</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">9.69</span></td>\n<td class=\"ltx_td ltx_align_right\">14.63</td>\n<td class=\"ltx_td ltx_align_center\">18.20</td>\n<td class=\"ltx_td ltx_align_center\">14.34</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">POWSM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">350M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">17.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">17.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">21.96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">18.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">12.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">18.33</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">11.32</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">11.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">17.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">14.40</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "downarrow",
            "zipacrnslarge",
            "l2arc",
            "avg",
            "datasets",
            "powsm",
            "supported",
            "doreco",
            "outofdomain",
            "speechocean762",
            "drcse",
            "so762",
            "11m",
            "350m",
            "variation",
            "buckeye",
            "wav2vec2phoneme",
            "include",
            "not",
            "allophant",
            "param",
            "results",
            "allosaurus",
            "language",
            "report",
            "model",
            "tusom2021",
            "stands",
            "300m",
            "zipacrlarge",
            "therefore",
            "l2arc",
            "epadb",
            "unseen",
            "drcse",
            "southengland",
            "multipa",
            "voxangeles",
            "l2arctic",
            "languages",
            "data",
            "so762",
            "pfer"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T4\" title=\"Table 4 &#8227; 5.2 POWSM generalizes well to unseen languages &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a> reports PFER on datasets with unseen languages and language variation.\nResults indicate that POWSM achieves strong performance across these datasets, and handles both dialectal and L2 variation effectively.\nNotably, our method outperforms ZIPA trained on the same data and even exceeds ZIPA trained with extra pseudo-labeled data, achieving the best results on unseen languages while performing three additional tasks.\nThis shows the effectiveness of our multi-task approach.\nWhile POWSM lags behind Wav2Vec2Phoneme on socio-phonetic variations, we attribute this to its self-supervised learning with over 60k hours of speech (from wav2vec 2.0 <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib3\" title=\"\">2020</a>)</cite>) prior to the supervised learning stage.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in spoken language processing have led to substantial progress in phonetic tasks such as automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G). Despite their conceptual similarity, these tasks have largely been studied in isolation, each relying on task-specific architectures and datasets.\nIn this paper, we introduce POWSM (Phonetic Open Whisper-style Speech Model), the first unified framework capable of jointly performing multiple phone-related tasks.\nPOWSM enables seamless conversion between audio, text (graphemes), and phones, opening up new possibilities for universal and low-resource speech processing.\nOur model outperforms or matches specialized PR models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P, P2G, and ASR.\nOur training data, code<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/espnet</span></span></span> and models<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://huggingface.co/espnet/powsm</span></span></span> are released to foster open science.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "wav2vec2phoneme",
                    "datasets",
                    "powsm",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">POWSM: A Phonetic Open Whisper-Style Speech Foundation Model</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Phones are the smallest units of sound in speech. Unlike graphemes, phones are shared across languages and usually represented using the International Phonetic Alphabet (IPA) <cite class=\"ltx_cite ltx_citemacro_citep\">(International Phonetic Association, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib21\" title=\"\">1999</a>)</cite>, a unified transcription standard for all languages.\nBy providing a consistent representation of speech across languages, phone-level modeling allows fine-grained analysis and cross-lingual generalization, enabling tasks like atypical speech analysis (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, L2 speech <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib27\" title=\"\">2016</a>); Inceoglu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib20\" title=\"\">2023</a>)</cite> and pathological speech <cite class=\"ltx_cite ltx_citemacro_cite\">Choi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib9\" title=\"\">2025</a>); Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib26\" title=\"\">2025</a>)</cite>), endangered language documentation <cite class=\"ltx_cite ltx_citemacro_cite\">He et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib19\" title=\"\">2024</a>)</cite>, code-switched text-to-speech <cite class=\"ltx_cite ltx_citemacro_cite\">Zhou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib63\" title=\"\">2020</a>)</cite>, and cross-lingual transfer in speech-to-text <cite class=\"ltx_cite ltx_citemacro_cite\">Pratap et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib42\" title=\"\">2024</a>); Magoshi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib31\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite their conceptual similarity, these tasks have traditionally been developed in isolation, using task-specific architectures and datasets.\nSuch systems are optimized for specific input-output mappings and cannot be easily extended to other phonetic tasks.\nThis fragmentation has hindered the development of general-purpose models for phonetic processing, necessitating a unified phonetic foundation model that can perform multiple phone-related tasks within a single, general framework for speech processing.</p>\n\n",
                "matched_terms": [
                    "model",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge this gap, we propose POWSM, a phonetic foundation model capable of performing four core phone-related tasks &#8212; PR, ASR, audio-guided G2P, and audio-guided P2G &#8212; within one unified architecture (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#S1.F1\" title=\"In 1 Introduction &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nTo construct this framework, we reformulate standard ASR datasets <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> into four task-specific formats, allowing the model to learn consistent mappings across audio, phoneme, and grapheme representations.\nIn addition, POWSM adopts an attention-based encoder-decoder (AED) architecture, following the design of large-scale speech foundation models such as Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite> and OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib39\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "datasets",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirically, POWSM outperforms previous PR models on both in-domain data and out-of-domain languages, achieves low-resource ASR performance comparable to web-scale multilingual foundation models, and can act as speech-grounded P2G and G2P across more than 70 languages.\nPOWSM offers a new unified paradigm for phone-level modeling, paving the way for inclusive and globally accessible speech technologies that transcend language boundaries and resource disparities.</p>\n\n",
                "matched_terms": [
                    "language",
                    "outofdomain",
                    "languages",
                    "data",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide POWSM, a large-scale foundation model that achieves state-of-the-art PR performance, and is capable of performing multiple fundamental phone-related tasks. Our model enables seamless conversion between speech, text (graphemes/orthography), and phones.</p>\n\n",
                "matched_terms": [
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fully open-source all our data preparation and evaluation scripts, model checkpoints and code to foster open science.</p>\n\n",
                "matched_terms": [
                    "data",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent speech foundation models such as Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite> and OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib39\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib38\" title=\"\">2024</a>)</cite> have driven progress in large-scale multilingual ASR and speech translation, but they do not explicitly address phoneme recognition or articulatory-level supervision.\nSubsequent work <cite class=\"ltx_cite ltx_citemacro_citep\">(Yusuyin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib60\" title=\"\">2025</a>; Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib11\" title=\"\">2025</a>)</cite> showed that incorporating phoneme-level objectives improves ASR for low-resource and long-tailed settings, while outputting phonemes as an intermediate benefited speech translation <cite class=\"ltx_cite ltx_citemacro_citep\">(G&#225;llego et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib18\" title=\"\">2025</a>)</cite>.\nPOWSM extends this line of work by being the first open foundation model jointly trained on phone recognition and related tasks, integrating multilinguality, phonetic supervision, and multi-task scalability within one framework.</p>\n\n",
                "matched_terms": [
                    "model",
                    "not",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior work in multilingual phone recognition can broadly be categorized into (1) language-specific models <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib13\" title=\"\">2021</a>)</cite> that rely on explicit phoneme <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib58\" title=\"\">2022</a>)</cite> or allophone inventories <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib28\" title=\"\">2020</a>)</cite> and (2) language-agnostic approaches that aim to generalize across languages without such resources <cite class=\"ltx_cite ltx_citemacro_citep\">(Taguchi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib51\" title=\"\">2023</a>; Glocker et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib14\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib29\" title=\"\">2021</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>.\nPOWSM follows the latter paradigm as a fully data-driven multilingual model that learns phone representations without predefined phoneme mappings.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">WhisperPPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Samir et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib47\" title=\"\">2025</a>)</cite> improved Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>&#8217;s performance through data cleaning but remained limited in data coverage and task diversity.\nHowever, Whisper is trained on a closed corpus and could display harmful biases for PR which cannot be fully removed by fine-tuning.\nPOWSM is trained from scratch on open datasets.</p>\n\n",
                "matched_terms": [
                    "data",
                    "datasets",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ZIPA <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> scaled PR to 17,000+ hours of data and 88 languages using a Zipformer <cite class=\"ltx_cite ltx_citemacro_cite\">Yao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib59\" title=\"\">2024</a>)</cite> encoder and noisy-student training on 4,000+ languages, achieving state-of-the-art results. To construct its training corpus, ZIPA employed a G2P system to convert large-scale ASR transcriptions into phoneme sequences, effectively repurposing ASR datasets for PR. Building on this idea, POWSM leverages both the grapheme and the G2P-generated phoneme transcriptions, reformulating them into four task-specific forms: ASR, PR, G2P, and P2G.</p>\n\n",
                "matched_terms": [
                    "powsm",
                    "datasets",
                    "languages",
                    "data",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">G2P &amp; P2G</span> POWSM is the first model capable of both audio-guided G2P and audio-guided P2G.\nG2P conversion, sometimes called phonemization in the text-to-speech literature, can be accomplished with pronunciation dictionaries <cite class=\"ltx_cite ltx_citemacro_citep\">(Rudnicky, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib46\" title=\"\">1993</a>)</cite>, rules <cite class=\"ltx_cite ltx_citemacro_citep\">(Mortensen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib33\" title=\"\">2018</a>)</cite>, WFSTs <cite class=\"ltx_cite ltx_citemacro_citep\">(Black and Lenzo, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib5\" title=\"\">2001</a>)</cite>, or seq2seq neural methods to choose between different pronunciations of a word in context <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib65\" title=\"\">2022</a>)</cite>.\nText-based G2P, however, still cannot handle phonetic variation, enforcing a one-to-one mapping between orthography and transcription.\nIn contrast, audio-guided G2P can learn to map the different acoustic realizations of a phoneme across varieties of a language to a phone representation <cite class=\"ltx_cite ltx_citemacro_cite\">Route et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib45\" title=\"\">2019</a>)</cite>.\nIn particular, <cite class=\"ltx_cite ltx_citemacro_citet\">Mak et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib32\" title=\"\">2025</a>)</cite> observed a performance improvement in using audio-guided G2P versus text-based G2P alone for Cantonese.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Gao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib12\" title=\"\">2024</a>)</cite> similarly showed that joint learning of G2P, phone recognition, and forced alignment outperform a G2P teacher model. Similarly, <cite class=\"ltx_cite ltx_citemacro_citet\">Sun and Richmond (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib50\" title=\"\">2024</a>)</cite> jointly learned G2P and TTS.\nCompared to G2P, P2G conversion is less studied, with <cite class=\"ltx_cite ltx_citemacro_citet\">Lauc (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib25\" title=\"\">2024</a>)</cite> training a seq2seq model on 19 million language-grapheme-phoneme triplets.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "variation",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use IPAPack++ <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> for training. It is an open source corpus of roughly 17,000 hours of multilingual speech with paired orthographic and phonemic transcriptions. We will release all data processing scripts to make POWSM fully reproducible.</p>\n\n",
                "matched_terms": [
                    "data",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model is trained on four tasks: PR, ASR, and audio-guided G2P and P2G. Each utterance is used once per task, with task-specific formatting as illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#S1.F1\" title=\"In 1 Introduction &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, including a text prompt, language token, task token, and target output. We leave the text prompt blank (token <span class=\"ltx_text ltx_font_typewriter\">&lt;na&gt;</span>) for PR and ASR, and provide graphemes and phones as prompts for G2P and P2G.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">POWSM adopts an attention-based encoder-decoder (AED) architecture, which flexibly models output sequences and allows the integration of additional tasks.\nSpecifically, we follow the OWSM v3.1 architecture <cite class=\"ltx_cite ltx_citemacro_cite\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib38\" title=\"\">2024</a>)</cite>, which employs an E-Branchformer encoder and a Transformer decoder, consistent with the general encoder-decoder structure of Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>. The model is trained from scratch using ESPnet <cite class=\"ltx_cite ltx_citemacro_cite\">Watanabe et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib56\" title=\"\">2018</a>)</cite> with a hybrid CTC/attention loss <cite class=\"ltx_cite ltx_citemacro_cite\">Watanabe et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib57\" title=\"\">2017</a>)</cite>, where we set the ratio <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> to 0.3:</p>\n\n",
                "matched_terms": [
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The encoder operates at the stride size of 40ms. Training uses a global batch size of 256. Speech inputs are 16kHz and padded to 20 seconds. The vocabulary consists of 40k tokens, including around 6k phone tokens, language and timestamp tokens, and BPE tokens from orthography.\nThe model has approximately 350M parameters with 9 layers for both the encoder and decoder and was trained on 4 H100 GPUs for 2 days.\nUsing a CTC loss <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib16\" title=\"\">2006</a>)</cite>, We align the encoder outputs with a simplified version of the phone token sequences. Unlike the decoder outputs, the phones in these sequences are stripped of break (<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/./, <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/*&#865;/)\nand length diacritics (<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/e<span class=\"ltx_ERROR undefined\">\\textlengthmark</span>/, /e<span class=\"ltx_ERROR undefined\">\\texthalflength</span>/, /&#283;/) to accelerate convergence. Additional details and analyses are provided in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.SS1.SSS0.Px1\" title=\"The CTC encoder prefers fine-grained phones without suprasegmentals &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;6.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "350m"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We report Phonetic Feature Error Rate (PFER), an edit distance using articulatory features from PanPhon <cite class=\"ltx_cite ltx_citemacro_citep\">(Mortensen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib34\" title=\"\">2016</a>)</cite>, averaged over the number of phones and computed as in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S4.E2\" title=\"Equation 2 &#8227; Evaluation metric &#8227; 4 Experimental Setup &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Equation&#160;2</span></a> for PR.\nEach feature contributes <math alttext=\"\\frac{1}{24}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mfrac><mn>1</mn><mn>24</mn></mfrac><annotation encoding=\"application/x-tex\">\\frac{1}{24}</annotation></semantics></math> distance unit, while insertion and deletion cost 1 unit.\nThe edit distance <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> grows linearly with the sequence length and has no upper bound.</p>\n\n",
                "matched_terms": [
                    "report",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For unseen languages, we evaluate on three datasets: DoReCo <cite class=\"ltx_cite ltx_citemacro_cite\">Paschen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib36\" title=\"\">2020</a>)</cite>, VoxAngeles <cite class=\"ltx_cite ltx_citemacro_cite\">Chodroff et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>)</cite>, and Tusom2021 <cite class=\"ltx_cite ltx_citemacro_cite\">Mortensen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib35\" title=\"\">2021</a>)</cite>.\nDoReCo is a dataset of 50+ languages (with broad transcriptions) intended for documentation of small or endangered languages; we use a 45-language subset.\nVoxAngeles <cite class=\"ltx_cite ltx_citemacro_citep\">(Chodroff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>)</cite> is a postprocessed version of the UCLA Phonetics Lab Archive <cite class=\"ltx_cite ltx_citemacro_citep\">(Ladefoged et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib24\" title=\"\">2009</a>)</cite> containing 95 languages.\nTusom is a low-data Tangkhulic language of India not included in the training data. Tusom2021 consists of narrow phonetic transcriptions (unlike the broad transcriptions from G2P on which POWSM was trained) of individual Tusom words.\nWe removed the tones.</p>\n\n",
                "matched_terms": [
                    "language",
                    "tusom2021",
                    "doreco",
                    "languages",
                    "voxangeles",
                    "not",
                    "datasets",
                    "powsm",
                    "data",
                    "unseen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also test on five datasets on varieties of English: the Buckeye Corpus <cite class=\"ltx_cite ltx_citemacro_cite\">Pitt et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib41\" title=\"\">2005</a>)</cite> and DoReCo South-England represent dialectal variation, while L2-ARCTIC <cite class=\"ltx_cite ltx_citemacro_cite\">Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib62\" title=\"\">2018</a>)</cite>, EpaDB <cite class=\"ltx_cite ltx_citemacro_cite\">Vidal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib54\" title=\"\">2019</a>)</cite>, and SpeechOcean762 <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib61\" title=\"\">2021</a>)</cite> contain L2 speakers.\nFor L2-ARCTIC, we used the manually annotated phoneme transcriptions (which <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> termed <span class=\"ltx_text ltx_font_italic\">L2-Perceived</span>) rather than G2P dictionary-based transcriptions. The manual transcriptions reflect what the speaker actually said, whereas the dictionary-based version enforces a single pronunciation variant.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>For instance, &#8220;crayon&#8221; in American English can be pronounced as\n<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/<span class=\"ltx_ERROR undefined\">\\textprimstress</span>k&#8290;r&#230;n/, <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/<span class=\"ltx_ERROR undefined\">\\textprimstress</span>k&#8290;rej.On/, or <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/<span class=\"ltx_ERROR undefined\">\\textprimstress</span>k&#8290;rej.6n/ <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaux and Golder, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib53\" title=\"\">2003</a>)</cite> (among others), but the CMU Pronouncing Dictionary <cite class=\"ltx_cite ltx_citemacro_citep\">(Rudnicky, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib46\" title=\"\">1993</a>)</cite> only lists one.</span></span></span>\nManual inspection by a trained phonologist further showed the L2-ARCTIC transcriptions to be of extremely poor quality.\nFor the five aforementioned datasets, we use preprocessed datasets from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://huggingface.co/anyspeech</span></span></span> and Koel Labs<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://huggingface.co/KoelLabs</span></span></span> for better transcription quality.</p>\n\n",
                "matched_terms": [
                    "southengland",
                    "doreco",
                    "speechocean762",
                    "variation",
                    "buckeye",
                    "l2arctic",
                    "datasets",
                    "epadb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then evaluated our model on in-domain data from IPAPack++, the dataset seen during training. We followed <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> in using LibriSpeech for English, AISHELL for Mandarin, and MLS for European languages, and additionally evaluated on IISc-MILE Tamil <cite class=\"ltx_cite ltx_citemacro_cite\">A et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib1\" title=\"\">2022</a>)</cite> for Tamil and KSC <cite class=\"ltx_cite ltx_citemacro_cite\">Khassanov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib22\" title=\"\">2021</a>)</cite> for Kazakh.\nFor ASR and P2G, we evaluate with FLEURS.</p>\n\n",
                "matched_terms": [
                    "data",
                    "model",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate all PR baselines without further training with IPAPack++. See Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS2\" title=\"A.2 Baseline Implementation &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.2</span></a> for more details about training data and language coverage.\nAllosaurus <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib28\" title=\"\">2020</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib29\" title=\"\">2021</a>)</cite> uses a phone-level CTC to train a language-agnostic model and applies language-specific allophone-to-phoneme mappings.\nWav2Vec2Phoneme <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib58\" title=\"\">2022</a>)</cite>, MultIPA <cite class=\"ltx_cite ltx_citemacro_cite\">Taguchi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib51\" title=\"\">2023</a>)</cite> and Allophant <cite class=\"ltx_cite ltx_citemacro_cite\">Glocker et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib14\" title=\"\">2023</a>)</cite> fine-tune XLS-R <cite class=\"ltx_cite ltx_citemacro_citep\">(Babu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib2\" title=\"\">2022</a>)</cite> with different objectives: Wav2Vec2Phoneme maps unseen phonemes using articulatory features, MultIPA leverages high-quality G2P data from seven languages, while Allophant decomposes phones into articulatory features and applies CTC losses for each.\nZIPA <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> trains ZipFormer <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib59\" title=\"\">2024</a>)</cite> from scratch on IPAPack++ using CR-CTC and also provides a variant trained with additional pseudo-labeled data (&#8220;ZIPA-CR-NS-Large&#8221;).</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "multipa",
                    "wav2vec2phoneme",
                    "allophant",
                    "languages",
                    "data",
                    "allosaurus",
                    "unseen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR, we compare POWSM with two series of models: OWSM <cite class=\"ltx_cite ltx_citemacro_cite\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib37\" title=\"\">2025</a>)</cite> and OWLS <cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib6\" title=\"\">2025</a>)</cite>. We select OWSM-CTC v4 because it is the best-performing model in the series, featuring an encoder-CTC architecture that supports ASR, ST, and LID. For OWLS, we include models with comparable parameter sizes.</p>\n\n",
                "matched_terms": [
                    "model",
                    "include",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T2\" title=\"Table 2 &#8227; POWSM excels at in-domain phone recognition &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>, we see that POWSM achieves the lowest average PFER in phone recognition, due to the strong language modeling capability of the decoder.\nWe hypothesize that our English data cleaning (Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS1\" title=\"A.1 Refining English G2P &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.1</span></a>) may have negatively affected the PFER for Germanic languages due to a mismatch between training and test data.\nNevertheless, our approach fills this gap by achieving strong performance on other languages, outperforming models trained on larger datasets.</p>\n\n",
                "matched_terms": [
                    "language",
                    "datasets",
                    "languages",
                    "data",
                    "powsm",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hypothesize that pre-training with phone recognition benefits low-resource ASR <cite class=\"ltx_cite ltx_citemacro_cite\">Yusuyin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib60\" title=\"\">2025</a>)</cite>.\nTo choose low-resource languages, we selected languages in IPAPack++ with less than 8 hours of speech in FLEURS to serve as the test set.\nSee <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS3\" title=\"A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.3</span></a> for details on the amount of data used by different models.\nFor a fair comparison with other multilingual ASR baselines without language-specific components, we use the same decoding hyperparameters <span class=\"ltx_text ltx_font_typewriter\">ctc=0.0, beam=1</span>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T3\" title=\"Table 3 &#8227; POWSM is comparable with web-scale ASR models on low-resource languages &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;3</span></a>, POWSM (<span class=\"ltx_text ltx_font_typewriter\">POWSM 0.35B, ASR</span>) is often comparable to models of similar size trained on web-scale data for ASR (<span class=\"ltx_text ltx_font_typewriter\">OWLS 0.5B</span>).\nIncorporating phones obtained from PR as text prompts (PR-P2G) significantly decreases WER, making it comparable to or even better than these models.\nWhen using gold phone labels for P2G (see analysis in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.SS2.SSS0.Px2\" title=\"Audio-P2G effectively handles low-resource languages &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;6.2</span></a>), POWSM outperforms other ASR models by a large margin in most cases.</p>\n\n",
                "matched_terms": [
                    "data",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we analyze how POWSM works, focusing on the phonetic-aware encoder and task- and language-specific tokens, which are the defining features of the model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observed that mixing phones and orthography as encoder targets hindered training, because the same speech input would have different encoder CTC targets for different tasks.\nTherefore, we used phones as encoder targets, encouraging general representations of sounds to be shared across languages.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "therefore"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As in other encoder-decoder models <cite class=\"ltx_cite ltx_citemacro_cite\">Gong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib15\" title=\"\">2023</a>); Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>, we expect the encoder of POWSM to capture more general acoustic patterns, while the decoder handles language and task-specific output formats. Therefore,\nwe investigate whether emphasizing the encoder more during different stages of model development affects performance.\nTo balance data diversity with inference compute costs, we selected two smaller datasets from each category with distinct characteristics.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "therefore",
                    "datasets",
                    "powsm",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T5\" title=\"Table 5 &#8227; Increased encoder weights benefit PR on out-of-domain data &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;5</span></a>, higher CTC decoding weights improve PR performance on out-of-domain data but degrade it on in-domain data, as expected.\nThis echoes <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>&#8217;s finding that RNN-T <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves and Jaitly, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib17\" title=\"\">2014</a>)</cite>, an encoder-only speech-to-text model with an autoregressive text prediction network, hurts generalization to unseen patterns of phones (phonotactics).\nWe hypothesize that the decoder is performing implicit language modeling and &#8220;smooths&#8221; phonetic variation, as <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> described.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "outofdomain",
                    "variation",
                    "data",
                    "unseen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, we examine whether focusing more on the CTC loss through training widens this gap in performance between in-domain and out-of-domain data.\nWe find that fine-tuning with a higher CTC loss weight <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> after convergence does not improve out-of-domain performance and can even degrade it. Randomly varying <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> for each batch also shows no improvement.\nIn contrast, training with a higher <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> from the start benefits the out-of-domain distribution, achieving the lowest PFER on unseen languages with greedy decoding, while the PFER on in-domain data is comparatively higher.\nThese results suggest that assigning a higher weight to the encoder during training and inference improves PR, highlighting a common trade-off between in-domain performance and generalization.</p>\n\n",
                "matched_terms": [
                    "outofdomain",
                    "pfer",
                    "not",
                    "languages",
                    "data",
                    "results",
                    "unseen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand how POWSM integrates speech and text prompts, we analyze the relative influence of speech and text prompts in its G2P behavior.\nWe vary the G2P conditions from purely speech-based to purely text-based, as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T6\" title=\"Table 6 &#8227; Increased encoder weights benefit PR on out-of-domain data &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;6</span></a>, and evaluate the model on the Buckeye dataset.\nWhen only speech is provided, the performance is comparable to the PR setting, which differs only in the task token.\nAdding both speech and text prompts (the standard G2P setup) leads to degraded performance, with output showing standardized pronunciations.\nWhen the model relies solely on the text prompt, performance drops sharply and pronunciations become highly standardized as expected (just as <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> reported).</p>\n\n",
                "matched_terms": [
                    "buckeye",
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In other words, POWSM G2P responds to speech and text signals to controllably mediate between narrow and broad transcription.\nIn the multi-task setup, this effect may be stronger because the model is trained with G2P, which could bias it toward more standardized forms.</p>\n\n",
                "matched_terms": [
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare several P2G setups on the same set of low-resource languages from FLEURS, listed in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T7\" title=\"Table 7 &#8227; Audio-P2G effectively handles low-resource languages &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;7</span></a>. P2G significantly outperforms ASR, suggesting that it effectively leverages the provided phone context.\nHowever, since P2G uses gold phone labels, this comparison is not entirely fair. We therefore tested PR followed by P2G (PR-P2G), and found that performance improved for some languages but not for others.\nError propagation does not explain this variation in performance, as PFER trends from PR differ from the observed performance drops. Yet the PFER pattern aligns closely with ASR results, suggesting that phonotactic similarity to high-resource languages may play a role.</p>\n\n",
                "matched_terms": [
                    "variation",
                    "therefore",
                    "not",
                    "languages",
                    "results",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test this, we run P2G with the language code set to English and post-process the output to match Cyrillic or Gurmukhi transcriptions with online conversion tools for certain languages.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.lexilogos.com\" title=\"\">https://www.lexilogos.com</a> for Macedonian and Tajik; <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://punjabi.indiatyping.com\" title=\"\">https://punjabi.indiatyping.com</a> for Panjabi.</span></span></span>\nThis approach often outperforms ASR and sometimes approaches P2G&#8217;s performance, indicating that P2G also relies heavily on speech input.\nLanguages with either comparibly low or high PFER did not benefit from this transliteration approach, possibly because the model already handled them well or had not yet learned them sufficiently.\nThis finding suggests a direction for further investigation in low-resource ASR.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "not",
                    "languages",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The language identification (LID) performance of POWSM on seen languages in FLEURS reaches 92.3% accuracy, as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.F3\" title=\"Figure 3 &#8227; A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;3</span></a>.\nTo see if the model implicitly learns phonotactic patterns and associates them with the language token, we evaluate PR on unseen languages by manipulating the language token at inference time.\nFor VoxAngeles and Tusom2021, the three most frequently assigned languages are Bashkir (42.6%, 25.1%), English (30.2%, 67.7%), and Kinyarwanda (14.5%, 2.3%), which are all relatively high-resource languages in IPAPack++.\n<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T8\" title=\"Table 8 &#8227; Language token captures phonotactics &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;8</span></a> shows that assigning the detected language token yields better performance than always using English, while setting the language as unknown performs best. This indicates that the language token influences PR by shifting the output distribution toward the assigned language.</p>\n\n",
                "matched_terms": [
                    "language",
                    "tusom2021",
                    "model",
                    "languages",
                    "voxangeles",
                    "powsm",
                    "unseen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train a fully open-source phonetic speech foundation model POWSM using our scalable multi-task framework.\nOur model achieves state-of-the-art performance on PR while also supporting ASR across more than 70 languages.\nBeyond PR and ASR, the model&#8217;s ability to perform audio-guided G2P and P2G enables applications that require fine-grained linguistic analysis such as atypical speech assessment.\nOur analysis reveals that POWSM&#8217;s encoder benefits from phoneme-level CTC supervision and stronger encoder weighting, enhancing cross-lingual generalization.\nAdditionally, the model demonstrates interpretable multimodal and language-aware behaviors, effectively mediating between phonetic detail and standardized phonological patterns.\nTo conclude, POWSM not only provides a strong phone recognition foundation model for high-resource languages, but also acts as a versatile resource for unseen languages and socio-phonetic variation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "languages",
                    "variation",
                    "not",
                    "powsm",
                    "unseen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">POWSM&#8217;s current decoder serves as a large phoneme-level phonotactic language model on which linguists could investigate hypotheses about phonetic universals <cite class=\"ltx_cite ltx_citemacro_citep\">(Chodroff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>; Chodroff, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib7\" title=\"\">2025</a>)</cite> and phonotactics <cite class=\"ltx_cite ltx_citemacro_citep\">(Shim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib48\" title=\"\">2024</a>; Pimentel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib40\" title=\"\">2020</a>)</cite>.\nIn the future, we seek to adapt to socio-phonetic variation either through (unsupervised) test-time adaptation <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib30\" title=\"\">2022</a>)</cite>, in-context learning <cite class=\"ltx_cite ltx_citemacro_citep\">(Roll et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib44\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib55\" title=\"\">2024</a>)</cite>, or mechanistic interpretability <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib52\" title=\"\">2024</a>)</cite>.\nFurthermore, since <cite class=\"ltx_cite ltx_citemacro_citet\">Shim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib49\" title=\"\">2025</a>)</cite> found that earlier encoder layers in Whisper preserve more phonetic detail, early exiting may mitigate the decoder&#8217;s tendencies to normalize socio-phonetic variation.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "variation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">POWSM has several limitations that we aim to address in future work.\nFirst, the model is neither strictly phonemic nor phonetic: its training data consist of cleaned and filtered phonemic transcriptions from multiple languages, which are not fully faithful to the phonetic or phonemic structure of the audio. Although phonemic transcriptions share similarities across languages, adding auxiliary tasks and language tokens may have reinforced language-specific biases. We also currently lack sufficient allophone-level data, which would provide more language-independent information.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "not",
                    "languages",
                    "data",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, the model still favors high-resource languages. Since we include a decoder for language modeling and language tokens, both of which function effectively, the model would inherently bias toward the seen distribution.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "include",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, the current AED architecture imposes engineering limitations. Inference is significantly slower than with encoder-only models, and the architecture does not easily support tone modeling, limiting its application to tonal languages.</p>\n\n",
                "matched_terms": [
                    "not",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All of our data is ethically sourced, either through permissive licensing or through proper consent.\nWe are aware of the implicit prescriptivism and representational harms <cite class=\"ltx_cite ltx_citemacro_citep\">(Crawford, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib10\" title=\"\">2017</a>)</cite> that normalizing socio-phonetic variation in ASR or PR models can create.\nThis may threaten linguistic diversity instead of preserving it.\nWe also acknowledge that accurate modeling of socio-phonetic variation can enable demographic inference, as demographics and phonetic variation are deeply intertwined <cite class=\"ltx_cite ltx_citemacro_citep\">(Labov, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib23\" title=\"\">1963</a>)</cite>.\nWe stress that uses of POWSM must align with our vision: a future where advances in spoken language processing and NLP do not leave low-resource varieties behind.</p>\n\n",
                "matched_terms": [
                    "language",
                    "variation",
                    "not",
                    "powsm",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observed confusion in plosive voice-onset times on unseen languages in preliminary experiments, which is likely from English G2P data. For instance, broad phonemic transcription in English typically uses /b/ to transcribe the /b/ in /bat/, but its voice onset timing is actually voiceless in Mainstream American English and is closer to [p]. To mitigate this, we apply rule-based refinements to English G2P transcriptions, adjusting plosive voicing and aspiration, lateral velarization, and vowel nasalization.</p>\n\n",
                "matched_terms": [
                    "data",
                    "languages",
                    "unseen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide the baselines&#8217; training data source, number of languages covered in the data, and links to model checkpoints or repository in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.T9\" title=\"Table 9 &#8227; A.2 Baseline Implementation &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;9</span></a>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "model",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first filter out languages with more than 8 hours of training data in IPAPack++ <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>, keeping only those that are also present in FLEURS.\nThen, following the training data amounts reported in <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib6\" title=\"\">2025</a>)</cite>, we further identify the 50 lowest-resource languages to exclude any that may have other substantial sources not included in IPAPack++.\nThis process leaves us with nine languages. We finally exclude <span class=\"ltx_text ltx_font_typewriter\">ell</span>, as it is comparatively higher-resource and because there are already three other Balto-Slavic languages.\nNote that other models use strictly more data than ours&#8212;not only in terms of dataset count but also because IPAPack++ applies additional data-quality filtering. <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.T10\" title=\"Table 10 &#8227; A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;10</span></a> lists the amount of ASR training data for baselines.</p>\n\n",
                "matched_terms": [
                    "data",
                    "not",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multi-tasking may improve performance by tying acoustic signals to well-defined symbolic representations, yet it may distract the model if the relationships are not learned effectively.\nWe train POWSM with different data and model scales to examine how multitask learning interacts with the setup, and use <span class=\"ltx_text ltx_font_typewriter\">beam=1</span> during decoding to speed up inference.</p>\n\n",
                "matched_terms": [
                    "data",
                    "model",
                    "not",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.T11\" title=\"Table 11 &#8227; A.4 Multi-tasking at Different Scales &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;11</span></a> shows that there is no clear trend regarding whether multitasking benefits PR performance. PR performance degrades when the model has excessive capacity relative to the available data (too little data), or when it is limited by size (too much data).</p>\n\n",
                "matched_terms": [
                    "data",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Further evidence is needed before concluding that phoneme recognition benefits less from scaling, as we currently lack sufficient data and large model capacity to test this thoroughly. Nevertheless, the model demonstrates the ability to multitask, which represents a promising direction for future work.</p>\n\n",
                "matched_terms": [
                    "data",
                    "model"
                ]
            }
        ]
    },
    "S6.T5": {
        "source_file": "POWSM: A Phonetic Open Whisper-Style Speech Foundation Model",
        "caption": "Table 5: PFER (\\downarrow) for different CTC weight settings. Ft denotes fine-tuning for 5 epochs from the checkpoint above. VoxAngeles and Tusom2021 are abbreviated.\nPre-training and fine-tuning rows use ctc=0.3. All setups use beam=1.",
        "body": "In-domain\nOut-of-domain\n\n\nSetup\nita\npol\nVoxA.\nTusom.\nDRC-SE\nEpaDB\n\n\n\n\nDecoding\n\n\n\n\n\n\nctc=0.3\n1.66\n1.36\n17.58\n33.52\n18.21\n11.88\n\n\nctc=0.7\n1.97\n1.37\n17.92\n24.29\n18.05\n11.82\n\n\nctc=0.9\n2.05\n1.38\n19.27\n22.94\n17.67\n11.80\n\n\nPre-training / Fine-tuning\n\n\n\n\n\n\n\nctc\\alpha_{\\text{ctc}}=0.3\n1.81\n1.60\n16.02\n22.47\n18.59\n11.66\n\n\n\n\\rightarrow Ft, ctc\\alpha_{\\text{ctc}}=0.5\n1.94\n1.53\n17.78\n23.72\n18.73\n11.82\n\n\n\nctc\\alpha_{\\text{ctc}}=0.7\n1.96\n1.65\n15.40\n22.10\n18.50\n11.62\n\n\n\n\\rightarrow Ft, ctc\\alpha_{\\text{ctc}}=0.5\n2.01\n1.57\n16.21\n22.93\n18.41\n11.47\n\n\n\nctc\\alpha_{\\text{ctc}}=UU(0.1,0.9)\n1.95\n1.62\n19.29\n25.11\n18.92\n11.33",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1pt 4.0pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding:1pt 4.0pt;\">In-domain</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\" style=\"padding:1pt 4.0pt;\">Out-of-domain</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column\" style=\"padding:1pt 4.0pt;\">Setup</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">ita</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">pol</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:1pt 4.0pt;\">VoxA.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:1pt 4.0pt;\">Tusom.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:1pt 4.0pt;\">DRC-SE</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:1pt 4.0pt;\">EpaDB</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" colspan=\"3\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Decoding</span></td>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:1pt 4.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:1pt 4.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:1pt 4.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:1pt 4.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">ctc=0.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">1.66</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">1.36</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">17.58</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">33.52</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">18.21</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">11.88</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">ctc=0.7</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">1.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:1pt 4.0pt;\">1.37</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">17.92</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">24.29</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">18.05</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">11.82</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">ctc=0.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">2.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:1pt 4.0pt;\">1.38</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">19.27</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">22.94</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">17.67</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">11.80</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" colspan=\"3\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Pre-training / Fine-tuning</span></td>\n<td class=\"ltx_td\" style=\"padding:1pt 4.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding:1pt 4.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding:1pt 4.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding:1pt 4.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 4.0pt;\">\n<math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T5.m1\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math>=0.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">1.81</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:1pt 4.0pt;\">1.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">16.02</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">22.47</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">18.59</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">11.66</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 4.0pt;\">\n<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Ft, <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T5.m3\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math>=0.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">1.94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">1.53</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">17.78</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">23.72</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">18.73</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">11.82</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 4.0pt;\">\n<math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T5.m4\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math>=0.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">1.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:1pt 4.0pt;\">1.65</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">15.40</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">22.10</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">18.50</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">11.62</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 4.0pt;\">\n<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T5.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Ft, <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T5.m6\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math>=0.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">2.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:1pt 4.0pt;\">1.57</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">16.21</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">22.93</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">18.41</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">11.47</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:1pt 4.0pt;\">\n<math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T5.m7\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math>=<math alttext=\"U\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T5.m8\" intent=\":literal\"><semantics><mi>U</mi><annotation encoding=\"application/x-tex\">U</annotation></semantics></math>(0.1,0.9)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1pt 4.0pt;\">1.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:1pt 4.0pt;\">1.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1pt 4.0pt;\">19.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1pt 4.0pt;\">25.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1pt 4.0pt;\">18.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">11.33</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "rows",
            "ita",
            "ctc07",
            "downarrow",
            "setups",
            "ctc03",
            "rightarrow",
            "indomain",
            "finetuning",
            "ctc",
            "drcse",
            "outofdomain",
            "ctcalphatextctcuu0109",
            "all",
            "ctcalphatextctc07",
            "pol",
            "from",
            "decoding",
            "ctcalphatextctc03",
            "tusom2021",
            "weight",
            "settings",
            "voxa",
            "checkpoint",
            "denotes",
            "above",
            "epadb",
            "ctcalphatextctc05",
            "epochs",
            "ctc09",
            "tusom",
            "abbreviated",
            "ft",
            "different",
            "beam1",
            "voxangeles",
            "use",
            "pretraining",
            "setup",
            "pfer"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T5\" title=\"Table 5 &#8227; Increased encoder weights benefit PR on out-of-domain data &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;5</span></a>, higher CTC decoding weights improve PR performance on out-of-domain data but degrade it on in-domain data, as expected.\nThis echoes <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>&#8217;s finding that RNN-T <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves and Jaitly, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib17\" title=\"\">2014</a>)</cite>, an encoder-only speech-to-text model with an autoregressive text prediction network, hurts generalization to unseen patterns of phones (phonotactics).\nWe hypothesize that the decoder is performing implicit language modeling and &#8220;smooths&#8221; phonetic variation, as <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> described.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Empirically, POWSM outperforms previous PR models on both in-domain data and out-of-domain languages, achieves low-resource ASR performance comparable to web-scale multilingual foundation models, and can act as speech-grounded P2G and G2P across more than 70 languages.\nPOWSM offers a new unified paradigm for phone-level modeling, paving the way for inclusive and globally accessible speech technologies that transcend language boundaries and resource disparities.</p>\n\n",
                "matched_terms": [
                    "indomain",
                    "outofdomain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">WhisperPPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Samir et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib47\" title=\"\">2025</a>)</cite> improved Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>&#8217;s performance through data cleaning but remained limited in data coverage and task diversity.\nHowever, Whisper is trained on a closed corpus and could display harmful biases for PR which cannot be fully removed by fine-tuning.\nPOWSM is trained from scratch on open datasets.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use IPAPack++ <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> for training. It is an open source corpus of roughly 17,000 hours of multilingual speech with paired orthographic and phonemic transcriptions. We will release all data processing scripts to make POWSM fully reproducible.</p>\n\n",
                "matched_terms": [
                    "all",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To prevent IPA tokens from being confused with graphemes, sequences are split into phone tokens with diacritics and modifiers attached, following PanPhon <cite class=\"ltx_cite ltx_citemacro_cite\">Mortensen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib34\" title=\"\">2016</a>)</cite>, and enclosed in slashes (e.g., <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/p<sup class=\"ltx_sup\">h</sup>Os@m/ <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/p<sup class=\"ltx_sup\">h</sup>/ /O/ /s/ /@/ /m/).</p>\n\n",
                "matched_terms": [
                    "from",
                    "rightarrow"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The encoder operates at the stride size of 40ms. Training uses a global batch size of 256. Speech inputs are 16kHz and padded to 20 seconds. The vocabulary consists of 40k tokens, including around 6k phone tokens, language and timestamp tokens, and BPE tokens from orthography.\nThe model has approximately 350M parameters with 9 layers for both the encoder and decoder and was trained on 4 H100 GPUs for 2 days.\nUsing a CTC loss <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib16\" title=\"\">2006</a>)</cite>, We align the encoder outputs with a simplified version of the phone token sequences. Unlike the decoder outputs, the phones in these sequences are stripped of break (<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/./, <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/*&#865;/)\nand length diacritics (<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/e<span class=\"ltx_ERROR undefined\">\\textlengthmark</span>/, /e<span class=\"ltx_ERROR undefined\">\\texthalflength</span>/, /&#283;/) to accelerate convergence. Additional details and analyses are provided in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.SS1.SSS0.Px1\" title=\"The CTC encoder prefers fine-grained phones without suprasegmentals &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;6.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "ctc",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We report Phonetic Feature Error Rate (PFER), an edit distance using articulatory features from PanPhon <cite class=\"ltx_cite ltx_citemacro_citep\">(Mortensen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib34\" title=\"\">2016</a>)</cite>, averaged over the number of phones and computed as in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S4.E2\" title=\"Equation 2 &#8227; Evaluation metric &#8227; 4 Experimental Setup &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Equation&#160;2</span></a> for PR.\nEach feature contributes <math alttext=\"\\frac{1}{24}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mfrac><mn>1</mn><mn>24</mn></mfrac><annotation encoding=\"application/x-tex\">\\frac{1}{24}</annotation></semantics></math> distance unit, while insertion and deletion cost 1 unit.\nThe edit distance <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> grows linearly with the sequence length and has no upper bound.</p>\n\n",
                "matched_terms": [
                    "from",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use a CTC weight (denoted as <span class=\"ltx_text ltx_font_typewriter\">ctc</span>) of 0.3 and a beam size (denoted as <span class=\"ltx_text ltx_font_typewriter\">beam</span>) of 3 during decoding for all reported numbers unless specified. Further details on the choice of hyperparameters are discussed in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.SS1.SSS0.Px2\" title=\"Increased encoder weights benefit PR on out-of-domain data &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;6.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "ctc",
                    "weight",
                    "all",
                    "use",
                    "decoding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For unseen languages, we evaluate on three datasets: DoReCo <cite class=\"ltx_cite ltx_citemacro_cite\">Paschen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib36\" title=\"\">2020</a>)</cite>, VoxAngeles <cite class=\"ltx_cite ltx_citemacro_cite\">Chodroff et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>)</cite>, and Tusom2021 <cite class=\"ltx_cite ltx_citemacro_cite\">Mortensen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib35\" title=\"\">2021</a>)</cite>.\nDoReCo is a dataset of 50+ languages (with broad transcriptions) intended for documentation of small or endangered languages; we use a 45-language subset.\nVoxAngeles <cite class=\"ltx_cite ltx_citemacro_citep\">(Chodroff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>)</cite> is a postprocessed version of the UCLA Phonetics Lab Archive <cite class=\"ltx_cite ltx_citemacro_citep\">(Ladefoged et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib24\" title=\"\">2009</a>)</cite> containing 95 languages.\nTusom is a low-data Tangkhulic language of India not included in the training data. Tusom2021 consists of narrow phonetic transcriptions (unlike the broad transcriptions from G2P on which POWSM was trained) of individual Tusom words.\nWe removed the tones.</p>\n\n",
                "matched_terms": [
                    "tusom2021",
                    "tusom",
                    "voxangeles",
                    "from",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also test on five datasets on varieties of English: the Buckeye Corpus <cite class=\"ltx_cite ltx_citemacro_cite\">Pitt et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib41\" title=\"\">2005</a>)</cite> and DoReCo South-England represent dialectal variation, while L2-ARCTIC <cite class=\"ltx_cite ltx_citemacro_cite\">Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib62\" title=\"\">2018</a>)</cite>, EpaDB <cite class=\"ltx_cite ltx_citemacro_cite\">Vidal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib54\" title=\"\">2019</a>)</cite>, and SpeechOcean762 <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib61\" title=\"\">2021</a>)</cite> contain L2 speakers.\nFor L2-ARCTIC, we used the manually annotated phoneme transcriptions (which <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> termed <span class=\"ltx_text ltx_font_italic\">L2-Perceived</span>) rather than G2P dictionary-based transcriptions. The manual transcriptions reflect what the speaker actually said, whereas the dictionary-based version enforces a single pronunciation variant.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>For instance, &#8220;crayon&#8221; in American English can be pronounced as\n<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/<span class=\"ltx_ERROR undefined\">\\textprimstress</span>k&#8290;r&#230;n/, <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/<span class=\"ltx_ERROR undefined\">\\textprimstress</span>k&#8290;rej.On/, or <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/<span class=\"ltx_ERROR undefined\">\\textprimstress</span>k&#8290;rej.6n/ <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaux and Golder, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib53\" title=\"\">2003</a>)</cite> (among others), but the CMU Pronouncing Dictionary <cite class=\"ltx_cite ltx_citemacro_citep\">(Rudnicky, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib46\" title=\"\">1993</a>)</cite> only lists one.</span></span></span>\nManual inspection by a trained phonologist further showed the L2-ARCTIC transcriptions to be of extremely poor quality.\nFor the five aforementioned datasets, we use preprocessed datasets from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://huggingface.co/anyspeech</span></span></span> and Koel Labs<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://huggingface.co/KoelLabs</span></span></span> for better transcription quality.</p>\n\n",
                "matched_terms": [
                    "from",
                    "use",
                    "epadb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then evaluated our model on in-domain data from IPAPack++, the dataset seen during training. We followed <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> in using LibriSpeech for English, AISHELL for Mandarin, and MLS for European languages, and additionally evaluated on IISc-MILE Tamil <cite class=\"ltx_cite ltx_citemacro_cite\">A et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib1\" title=\"\">2022</a>)</cite> for Tamil and KSC <cite class=\"ltx_cite ltx_citemacro_cite\">Khassanov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib22\" title=\"\">2021</a>)</cite> for Kazakh.\nFor ASR and P2G, we evaluate with FLEURS.</p>\n\n",
                "matched_terms": [
                    "indomain",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate all PR baselines without further training with IPAPack++. See Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS2\" title=\"A.2 Baseline Implementation &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.2</span></a> for more details about training data and language coverage.\nAllosaurus <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib28\" title=\"\">2020</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib29\" title=\"\">2021</a>)</cite> uses a phone-level CTC to train a language-agnostic model and applies language-specific allophone-to-phoneme mappings.\nWav2Vec2Phoneme <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib58\" title=\"\">2022</a>)</cite>, MultIPA <cite class=\"ltx_cite ltx_citemacro_cite\">Taguchi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib51\" title=\"\">2023</a>)</cite> and Allophant <cite class=\"ltx_cite ltx_citemacro_cite\">Glocker et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib14\" title=\"\">2023</a>)</cite> fine-tune XLS-R <cite class=\"ltx_cite ltx_citemacro_citep\">(Babu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib2\" title=\"\">2022</a>)</cite> with different objectives: Wav2Vec2Phoneme maps unseen phonemes using articulatory features, MultIPA leverages high-quality G2P data from seven languages, while Allophant decomposes phones into articulatory features and applies CTC losses for each.\nZIPA <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> trains ZipFormer <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib59\" title=\"\">2024</a>)</cite> from scratch on IPAPack++ using CR-CTC and also provides a variant trained with additional pseudo-labeled data (&#8220;ZIPA-CR-NS-Large&#8221;).</p>\n\n",
                "matched_terms": [
                    "all",
                    "ctc",
                    "from",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T2\" title=\"Table 2 &#8227; POWSM excels at in-domain phone recognition &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>, we see that POWSM achieves the lowest average PFER in phone recognition, due to the strong language modeling capability of the decoder.\nWe hypothesize that our English data cleaning (Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS1\" title=\"A.1 Refining English G2P &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.1</span></a>) may have negatively affected the PFER for Germanic languages due to a mismatch between training and test data.\nNevertheless, our approach fills this gap by achieving strong performance on other languages, outperforming models trained on larger datasets.</p>\n\n",
                "matched_terms": [
                    "from",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hypothesize that pre-training with phone recognition benefits low-resource ASR <cite class=\"ltx_cite ltx_citemacro_cite\">Yusuyin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib60\" title=\"\">2025</a>)</cite>.\nTo choose low-resource languages, we selected languages in IPAPack++ with less than 8 hours of speech in FLEURS to serve as the test set.\nSee <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS3\" title=\"A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.3</span></a> for details on the amount of data used by different models.\nFor a fair comparison with other multilingual ASR baselines without language-specific components, we use the same decoding hyperparameters <span class=\"ltx_text ltx_font_typewriter\">ctc=0.0, beam=1</span>.</p>\n\n",
                "matched_terms": [
                    "beam1",
                    "different",
                    "use",
                    "pretraining",
                    "decoding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T4\" title=\"Table 4 &#8227; 5.2 POWSM generalizes well to unseen languages &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a> reports PFER on datasets with unseen languages and language variation.\nResults indicate that POWSM achieves strong performance across these datasets, and handles both dialectal and L2 variation effectively.\nNotably, our method outperforms ZIPA trained on the same data and even exceeds ZIPA trained with extra pseudo-labeled data, achieving the best results on unseen languages while performing three additional tasks.\nThis shows the effectiveness of our multi-task approach.\nWhile POWSM lags behind Wav2Vec2Phoneme on socio-phonetic variations, we attribute this to its self-supervised learning with over 60k hours of speech (from wav2vec 2.0 <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib3\" title=\"\">2020</a>)</cite>) prior to the supervised learning stage.</p>\n\n",
                "matched_terms": [
                    "from",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observed that mixing phones and orthography as encoder targets hindered training, because the same speech input would have different encoder CTC targets for different tasks.\nTherefore, we used phones as encoder targets, encouraging general representations of sounds to be shared across languages.</p>\n\n",
                "matched_terms": [
                    "ctc",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We run small-scale experiments on a 1k-hour subset of the multi-task data (250 hours of speech repeated across four tasks).\nWe use the validation CER of the encoder-CTC output as a proxy for training efficiency.\nAn earlier drop indicates that the encoder is learning a useful alignment early, which improves representations fed into the decoder and accelerates overall convergence.\nIn <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.F2\" title=\"Figure 2 &#8227; The CTC encoder prefers fine-grained phones without suprasegmentals &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;2</span></a>, PanPhon tokenization without suprasegmentals shows the earliest drop, suggesting that alignment with decoder units aids training, while\ncollapsing suprasegmental distinctions for CTC reduces confusion.</p>\n\n",
                "matched_terms": [
                    "ctc",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As in other encoder-decoder models <cite class=\"ltx_cite ltx_citemacro_cite\">Gong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib15\" title=\"\">2023</a>); Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>, we expect the encoder of POWSM to capture more general acoustic patterns, while the decoder handles language and task-specific output formats. Therefore,\nwe investigate whether emphasizing the encoder more during different stages of model development affects performance.\nTo balance data diversity with inference compute costs, we selected two smaller datasets from each category with distinct characteristics.</p>\n\n",
                "matched_terms": [
                    "from",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, we examine whether focusing more on the CTC loss through training widens this gap in performance between in-domain and out-of-domain data.\nWe find that fine-tuning with a higher CTC loss weight <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> after convergence does not improve out-of-domain performance and can even degrade it. Randomly varying <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> for each batch also shows no improvement.\nIn contrast, training with a higher <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> from the start benefits the out-of-domain distribution, achieving the lowest PFER on unseen languages with greedy decoding, while the PFER on in-domain data is comparatively higher.\nThese results suggest that assigning a higher weight to the encoder during training and inference improves PR, highlighting a common trade-off between in-domain performance and generalization.</p>\n\n",
                "matched_terms": [
                    "ctc",
                    "outofdomain",
                    "weight",
                    "from",
                    "indomain",
                    "finetuning",
                    "decoding",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand how POWSM integrates speech and text prompts, we analyze the relative influence of speech and text prompts in its G2P behavior.\nWe vary the G2P conditions from purely speech-based to purely text-based, as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T6\" title=\"Table 6 &#8227; Increased encoder weights benefit PR on out-of-domain data &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;6</span></a>, and evaluate the model on the Buckeye dataset.\nWhen only speech is provided, the performance is comparable to the PR setting, which differs only in the task token.\nAdding both speech and text prompts (the standard G2P setup) leads to degraded performance, with output showing standardized pronunciations.\nWhen the model relies solely on the text prompt, performance drops sharply and pronunciations become highly standardized as expected (just as <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> reported).</p>\n\n",
                "matched_terms": [
                    "from",
                    "setup"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare several P2G setups on the same set of low-resource languages from FLEURS, listed in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T7\" title=\"Table 7 &#8227; Audio-P2G effectively handles low-resource languages &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;7</span></a>. P2G significantly outperforms ASR, suggesting that it effectively leverages the provided phone context.\nHowever, since P2G uses gold phone labels, this comparison is not entirely fair. We therefore tested PR followed by P2G (PR-P2G), and found that performance improved for some languages but not for others.\nError propagation does not explain this variation in performance, as PFER trends from PR differ from the observed performance drops. Yet the PFER pattern aligns closely with ASR results, suggesting that phonotactic similarity to high-resource languages may play a role.</p>\n\n",
                "matched_terms": [
                    "from",
                    "setups",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test this, we run P2G with the language code set to English and post-process the output to match Cyrillic or Gurmukhi transcriptions with online conversion tools for certain languages.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.lexilogos.com\" title=\"\">https://www.lexilogos.com</a> for Macedonian and Tajik; <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://punjabi.indiatyping.com\" title=\"\">https://punjabi.indiatyping.com</a> for Panjabi.</span></span></span>\nThis approach often outperforms ASR and sometimes approaches P2G&#8217;s performance, indicating that P2G also relies heavily on speech input.\nLanguages with either comparibly low or high PFER did not benefit from this transliteration approach, possibly because the model already handled them well or had not yet learned them sufficiently.\nThis finding suggests a direction for further investigation in low-resource ASR.</p>\n\n",
                "matched_terms": [
                    "from",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The language identification (LID) performance of POWSM on seen languages in FLEURS reaches 92.3% accuracy, as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.F3\" title=\"Figure 3 &#8227; A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;3</span></a>.\nTo see if the model implicitly learns phonotactic patterns and associates them with the language token, we evaluate PR on unseen languages by manipulating the language token at inference time.\nFor VoxAngeles and Tusom2021, the three most frequently assigned languages are Bashkir (42.6%, 25.1%), English (30.2%, 67.7%), and Kinyarwanda (14.5%, 2.3%), which are all relatively high-resource languages in IPAPack++.\n<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T8\" title=\"Table 8 &#8227; Language token captures phonotactics &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;8</span></a> shows that assigning the detected language token yields better performance than always using English, while setting the language as unknown performs best. This indicates that the language token influences PR by shifting the output distribution toward the assigned language.</p>\n\n",
                "matched_terms": [
                    "all",
                    "tusom2021",
                    "voxangeles"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train a fully open-source phonetic speech foundation model POWSM using our scalable multi-task framework.\nOur model achieves state-of-the-art performance on PR while also supporting ASR across more than 70 languages.\nBeyond PR and ASR, the model&#8217;s ability to perform audio-guided G2P and P2G enables applications that require fine-grained linguistic analysis such as atypical speech assessment.\nOur analysis reveals that POWSM&#8217;s encoder benefits from phoneme-level CTC supervision and stronger encoder weighting, enhancing cross-lingual generalization.\nAdditionally, the model demonstrates interpretable multimodal and language-aware behaviors, effectively mediating between phonetic detail and standardized phonological patterns.\nTo conclude, POWSM not only provides a strong phone recognition foundation model for high-resource languages, but also acts as a versatile resource for unseen languages and socio-phonetic variation.</p>\n\n",
                "matched_terms": [
                    "ctc",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We acknowledge the use of large language models (LLMs) to assist with grammar correction and clarity improvements in writing this paper. All conceptual, methodological, and experimental contributions were developed independently by the authors.</p>\n\n",
                "matched_terms": [
                    "all",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multi-tasking may improve performance by tying acoustic signals to well-defined symbolic representations, yet it may distract the model if the relationships are not learned effectively.\nWe train POWSM with different data and model scales to examine how multitask learning interacts with the setup, and use <span class=\"ltx_text ltx_font_typewriter\">beam=1</span> during decoding to speed up inference.</p>\n\n",
                "matched_terms": [
                    "beam1",
                    "different",
                    "use",
                    "setup",
                    "decoding"
                ]
            }
        ]
    },
    "S6.T6": {
        "source_file": "POWSM: A Phonetic Open Whisper-Style Speech Foundation Model",
        "caption": "Table 6: Comparing G2P with different available modalities with PFER (\\downarrow). Blue for correctly capturing mispronounced parts (\\tipaencoding/soU/), orange for error compared to other examples.",
        "body": "Task\nBuckeye\nExample\n\n\nASR Transcription\nany holidays at all they just kind of ignore\n\n\nPhonetic transcription\n\n\\tipaencoding/EnihAl2deIsERAlsoUDeIdZ2stkAr2vIgnOr/\n\n\n\nPR\n12.63\n\n\\tipaencoding/nihAl@deIztOlsoUeItS2stkhn@vIgnOr/\n\n\nG2P (speech)\n12.71\n\n\\tipaencoding/nihAl@deIztOlsoUeItS2stkh n@vIgnOr/\n\n\nG2P (both)\n16.38\n\n\\tipaencoding/nihAl@deIztOleItS2stkhnd@vIgnOr/\n\n\nG2P (text prompt)\n23.44\n\n\\tipaencoding/aIhoU\\textltildedaIztO\\textltildeeItSIsthnd@vIgn\\textrhookrevepsilon/",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding:2pt 4.0pt;\">Task</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:2pt 4.0pt;\">Buckeye</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:2pt 4.0pt;\">Example</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"2\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">ASR Transcription</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">any holidays at all they just kind of ignore</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<td class=\"ltx_td ltx_align_left\" colspan=\"2\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">Phonetic transcription</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">/EnihAl2deIsERAl<span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;--ltx-bg-color:#F2F2F2;\">soU</span>DeIdZ2stkAr&#771;2vIgnO&#8290;r/</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:2pt 4.0pt;\">PR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">12.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/&#7868;nihAl@deIz&#230;tOl<span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">soU</span>&#240;eItS2stk<sup class=\"ltx_sup\">h</sup>&#230;&#771;n@vIgnO&#8290;r/</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:2pt 4.0pt;\">G2P (speech)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">12.71</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/&#7868;nihAl@deIz&#230;tOl<span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">soU</span>&#240;eItS2stk<sup class=\"ltx_sup\">h</sup> &#230;&#771;n@vIgnO&#8290;r/</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:2pt 4.0pt;\">G2P (both)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">16.38</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/&#7868;nihAl@deIz&#230;tOl&#240;eItS2stk<sup class=\"ltx_sup\">h</sup><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF8000;\">&#296;</span>nd@vIgnO&#8290;r/</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:2pt 4.0pt;\">G2P (text prompt)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:2pt 4.0pt;\">23.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:2pt 4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF8000;\">aIhoU<span class=\"ltx_ERROR undefined\">\\textltilde</span></span>d<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF8000;\">a</span>Iz&#230;tO<span class=\"ltx_ERROR undefined\">\\textltilde</span>&#240;eItSIst<sup class=\"ltx_sup\">h</sup><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF8000;\">&#296;</span>nd@vIgn<span class=\"ltx_ERROR undefined\">\\textrhookrevepsilon</span>/</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "other",
            "mispronounced",
            "task",
            "orange",
            "holidays",
            "g2p",
            "text",
            "available",
            "downarrow",
            "tipaencodingaihoutextltildedaiztotextltildeeitsisthndvigntextrhookrevepsilon",
            "modalities",
            "they",
            "capturing",
            "tipaencodingnihaldeiztoleits2stkhndvignor",
            "blue",
            "speech",
            "example",
            "all",
            "nvignor",
            "buckeye",
            "both",
            "just",
            "transcription",
            "tipaencodingenihal2deiseralsoudeidz2stkar2vignor",
            "phonetic",
            "examples",
            "comparing",
            "correctly",
            "asr",
            "prompt",
            "tipaencodingsou",
            "tipaencodingnihaldeiztolsoueits2stkh",
            "ignore",
            "different",
            "compared",
            "any",
            "tipaencodingnihaldeiztolsoueits2stkhnvignor",
            "kind",
            "parts",
            "error",
            "pfer"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To better understand how POWSM integrates speech and text prompts, we analyze the relative influence of speech and text prompts in its G2P behavior.\nWe vary the G2P conditions from purely speech-based to purely text-based, as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T6\" title=\"Table 6 &#8227; Increased encoder weights benefit PR on out-of-domain data &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;6</span></a>, and evaluate the model on the Buckeye dataset.\nWhen only speech is provided, the performance is comparable to the PR setting, which differs only in the task token.\nAdding both speech and text prompts (the standard G2P setup) leads to degraded performance, with output showing standardized pronunciations.\nWhen the model relies solely on the text prompt, performance drops sharply and pronunciations become highly standardized as expected (just as <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> reported).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in spoken language processing have led to substantial progress in phonetic tasks such as automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G). Despite their conceptual similarity, these tasks have largely been studied in isolation, each relying on task-specific architectures and datasets.\nIn this paper, we introduce POWSM (Phonetic Open Whisper-style Speech Model), the first unified framework capable of jointly performing multiple phone-related tasks.\nPOWSM enables seamless conversion between audio, text (graphemes), and phones, opening up new possibilities for universal and low-resource speech processing.\nOur model outperforms or matches specialized PR models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P, P2G, and ASR.\nOur training data, code<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/espnet</span></span></span> and models<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://huggingface.co/espnet/powsm</span></span></span> are released to foster open science.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "g2p",
                    "phonetic",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">POWSM: A Phonetic Open Whisper-Style Speech Foundation Model</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "phonetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Phones are the smallest units of sound in speech. Unlike graphemes, phones are shared across languages and usually represented using the International Phonetic Alphabet (IPA) <cite class=\"ltx_cite ltx_citemacro_citep\">(International Phonetic Association, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib21\" title=\"\">1999</a>)</cite>, a unified transcription standard for all languages.\nBy providing a consistent representation of speech across languages, phone-level modeling allows fine-grained analysis and cross-lingual generalization, enabling tasks like atypical speech analysis (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, L2 speech <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib27\" title=\"\">2016</a>); Inceoglu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib20\" title=\"\">2023</a>)</cite> and pathological speech <cite class=\"ltx_cite ltx_citemacro_cite\">Choi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib9\" title=\"\">2025</a>); Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib26\" title=\"\">2025</a>)</cite>), endangered language documentation <cite class=\"ltx_cite ltx_citemacro_cite\">He et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib19\" title=\"\">2024</a>)</cite>, code-switched text-to-speech <cite class=\"ltx_cite ltx_citemacro_cite\">Zhou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib63\" title=\"\">2020</a>)</cite>, and cross-lingual transfer in speech-to-text <cite class=\"ltx_cite ltx_citemacro_cite\">Pratap et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib42\" title=\"\">2024</a>); Magoshi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib31\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "all",
                    "phonetic",
                    "transcription"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Four key phone-related tasks underpin phonetic spoken language processing: automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G).\n<span class=\"ltx_text ltx_font_italic\">ASR</span> learns implicit phonetic representations <cite class=\"ltx_cite ltx_citemacro_cite\">Belinkov and Glass (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib4\" title=\"\">2017</a>)</cite>, while <span class=\"ltx_text ltx_font_italic\">PR</span> offers explicit phone-level supervision.\n<span class=\"ltx_text ltx_font_italic\">G2P</span> and <span class=\"ltx_text ltx_font_italic\">P2G</span> bridge orthographic and phonetic spaces.\nCollectively, these tasks interact through shared phonetic representations, each addressing a different aspect of the relationship between audio, phones, phonemes, and graphemes.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "g2p",
                    "phonetic",
                    "different",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite their conceptual similarity, these tasks have traditionally been developed in isolation, using task-specific architectures and datasets.\nSuch systems are optimized for specific input-output mappings and cannot be easily extended to other phonetic tasks.\nThis fragmentation has hindered the development of general-purpose models for phonetic processing, necessitating a unified phonetic foundation model that can perform multiple phone-related tasks within a single, general framework for speech processing.</p>\n\n",
                "matched_terms": [
                    "other",
                    "speech",
                    "phonetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge this gap, we propose POWSM, a phonetic foundation model capable of performing four core phone-related tasks &#8212; PR, ASR, audio-guided G2P, and audio-guided P2G &#8212; within one unified architecture (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#S1.F1\" title=\"In 1 Introduction &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nTo construct this framework, we reformulate standard ASR datasets <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> into four task-specific formats, allowing the model to learn consistent mappings across audio, phoneme, and grapheme representations.\nIn addition, POWSM adopts an attention-based encoder-decoder (AED) architecture, following the design of large-scale speech foundation models such as Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite> and OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib39\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "g2p",
                    "phonetic",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirically, POWSM outperforms previous PR models on both in-domain data and out-of-domain languages, achieves low-resource ASR performance comparable to web-scale multilingual foundation models, and can act as speech-grounded P2G and G2P across more than 70 languages.\nPOWSM offers a new unified paradigm for phone-level modeling, paving the way for inclusive and globally accessible speech technologies that transcend language boundaries and resource disparities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "g2p",
                    "both",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide POWSM, a large-scale foundation model that achieves state-of-the-art PR performance, and is capable of performing multiple fundamental phone-related tasks. Our model enables seamless conversion between speech, text (graphemes/orthography), and phones.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent speech foundation models such as Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite> and OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib39\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib38\" title=\"\">2024</a>)</cite> have driven progress in large-scale multilingual ASR and speech translation, but they do not explicitly address phoneme recognition or articulatory-level supervision.\nSubsequent work <cite class=\"ltx_cite ltx_citemacro_citep\">(Yusuyin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib60\" title=\"\">2025</a>; Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib11\" title=\"\">2025</a>)</cite> showed that incorporating phoneme-level objectives improves ASR for low-resource and long-tailed settings, while outputting phonemes as an intermediate benefited speech translation <cite class=\"ltx_cite ltx_citemacro_citep\">(G&#225;llego et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib18\" title=\"\">2025</a>)</cite>.\nPOWSM extends this line of work by being the first open foundation model jointly trained on phone recognition and related tasks, integrating multilinguality, phonetic supervision, and multi-task scalability within one framework.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "they",
                    "phonetic",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ZIPA <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> scaled PR to 17,000+ hours of data and 88 languages using a Zipformer <cite class=\"ltx_cite ltx_citemacro_cite\">Yao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib59\" title=\"\">2024</a>)</cite> encoder and noisy-student training on 4,000+ languages, achieving state-of-the-art results. To construct its training corpus, ZIPA employed a G2P system to convert large-scale ASR transcriptions into phoneme sequences, effectively repurposing ASR datasets for PR. Building on this idea, POWSM leverages both the grapheme and the G2P-generated phoneme transcriptions, reformulating them into four task-specific forms: ASR, PR, G2P, and P2G.</p>\n\n",
                "matched_terms": [
                    "g2p",
                    "both",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">G2P &amp; P2G</span> POWSM is the first model capable of both audio-guided G2P and audio-guided P2G.\nG2P conversion, sometimes called phonemization in the text-to-speech literature, can be accomplished with pronunciation dictionaries <cite class=\"ltx_cite ltx_citemacro_citep\">(Rudnicky, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib46\" title=\"\">1993</a>)</cite>, rules <cite class=\"ltx_cite ltx_citemacro_citep\">(Mortensen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib33\" title=\"\">2018</a>)</cite>, WFSTs <cite class=\"ltx_cite ltx_citemacro_citep\">(Black and Lenzo, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib5\" title=\"\">2001</a>)</cite>, or seq2seq neural methods to choose between different pronunciations of a word in context <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib65\" title=\"\">2022</a>)</cite>.\nText-based G2P, however, still cannot handle phonetic variation, enforcing a one-to-one mapping between orthography and transcription.\nIn contrast, audio-guided G2P can learn to map the different acoustic realizations of a phoneme across varieties of a language to a phone representation <cite class=\"ltx_cite ltx_citemacro_cite\">Route et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib45\" title=\"\">2019</a>)</cite>.\nIn particular, <cite class=\"ltx_cite ltx_citemacro_citet\">Mak et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib32\" title=\"\">2025</a>)</cite> observed a performance improvement in using audio-guided G2P versus text-based G2P alone for Cantonese.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Gao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib12\" title=\"\">2024</a>)</cite> similarly showed that joint learning of G2P, phone recognition, and forced alignment outperform a G2P teacher model. Similarly, <cite class=\"ltx_cite ltx_citemacro_citet\">Sun and Richmond (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib50\" title=\"\">2024</a>)</cite> jointly learned G2P and TTS.\nCompared to G2P, P2G conversion is less studied, with <cite class=\"ltx_cite ltx_citemacro_citet\">Lauc (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib25\" title=\"\">2024</a>)</cite> training a seq2seq model on 19 million language-grapheme-phoneme triplets.</p>\n\n",
                "matched_terms": [
                    "transcription",
                    "g2p",
                    "phonetic",
                    "different",
                    "compared",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use IPAPack++ <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> for training. It is an open source corpus of roughly 17,000 hours of multilingual speech with paired orthographic and phonemic transcriptions. We will release all data processing scripts to make POWSM fully reproducible.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model is trained on four tasks: PR, ASR, and audio-guided G2P and P2G. Each utterance is used once per task, with task-specific formatting as illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#S1.F1\" title=\"In 1 Introduction &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, including a text prompt, language token, task token, and target output. We leave the text prompt blank (token <span class=\"ltx_text ltx_font_typewriter\">&lt;na&gt;</span>) for PR and ASR, and provide graphemes and phones as prompts for G2P and P2G.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "task",
                    "text",
                    "g2p",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The encoder operates at the stride size of 40ms. Training uses a global batch size of 256. Speech inputs are 16kHz and padded to 20 seconds. The vocabulary consists of 40k tokens, including around 6k phone tokens, language and timestamp tokens, and BPE tokens from orthography.\nThe model has approximately 350M parameters with 9 layers for both the encoder and decoder and was trained on 4 H100 GPUs for 2 days.\nUsing a CTC loss <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib16\" title=\"\">2006</a>)</cite>, We align the encoder outputs with a simplified version of the phone token sequences. Unlike the decoder outputs, the phones in these sequences are stripped of break (<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/./, <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/*&#865;/)\nand length diacritics (<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/e<span class=\"ltx_ERROR undefined\">\\textlengthmark</span>/, /e<span class=\"ltx_ERROR undefined\">\\texthalflength</span>/, /&#283;/) to accelerate convergence. Additional details and analyses are provided in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.SS1.SSS0.Px1\" title=\"The CTC encoder prefers fine-grained phones without suprasegmentals &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;6.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We report Phonetic Feature Error Rate (PFER), an edit distance using articulatory features from PanPhon <cite class=\"ltx_cite ltx_citemacro_citep\">(Mortensen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib34\" title=\"\">2016</a>)</cite>, averaged over the number of phones and computed as in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S4.E2\" title=\"Equation 2 &#8227; Evaluation metric &#8227; 4 Experimental Setup &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Equation&#160;2</span></a> for PR.\nEach feature contributes <math alttext=\"\\frac{1}{24}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mfrac><mn>1</mn><mn>24</mn></mfrac><annotation encoding=\"application/x-tex\">\\frac{1}{24}</annotation></semantics></math> distance unit, while insertion and deletion cost 1 unit.\nThe edit distance <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> grows linearly with the sequence length and has no upper bound.</p>\n\n",
                "matched_terms": [
                    "phonetic",
                    "error",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike Phone Error Rate (PER), which considers only exact phone matches, or Phone Token Error Rate (PTER), which treats diacritics and modifiers as separate tokens, PFER computes the edit distance in terms of articulatory features&#8212;interpretable subphone attributes (<span class=\"ltx_text ltx_font_italic\">e.g.</span> voicing)&#8212;capturing phonetic similarity in a fine-grained fashion.\nPrevious studies <cite class=\"ltx_cite ltx_citemacro_cite\">Taguchi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib51\" title=\"\">2023</a>); Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> define PFER as the mean articulatory feature edit distance over the evaluation set. In contrast, we normalize it by the number of phones in the reference transcription to measure the proportion of feature errors per phone.</p>\n\n",
                "matched_terms": [
                    "transcription",
                    "phonetic",
                    "error",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For unseen languages, we evaluate on three datasets: DoReCo <cite class=\"ltx_cite ltx_citemacro_cite\">Paschen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib36\" title=\"\">2020</a>)</cite>, VoxAngeles <cite class=\"ltx_cite ltx_citemacro_cite\">Chodroff et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>)</cite>, and Tusom2021 <cite class=\"ltx_cite ltx_citemacro_cite\">Mortensen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib35\" title=\"\">2021</a>)</cite>.\nDoReCo is a dataset of 50+ languages (with broad transcriptions) intended for documentation of small or endangered languages; we use a 45-language subset.\nVoxAngeles <cite class=\"ltx_cite ltx_citemacro_citep\">(Chodroff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>)</cite> is a postprocessed version of the UCLA Phonetics Lab Archive <cite class=\"ltx_cite ltx_citemacro_citep\">(Ladefoged et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib24\" title=\"\">2009</a>)</cite> containing 95 languages.\nTusom is a low-data Tangkhulic language of India not included in the training data. Tusom2021 consists of narrow phonetic transcriptions (unlike the broad transcriptions from G2P on which POWSM was trained) of individual Tusom words.\nWe removed the tones.</p>\n\n",
                "matched_terms": [
                    "g2p",
                    "phonetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also test on five datasets on varieties of English: the Buckeye Corpus <cite class=\"ltx_cite ltx_citemacro_cite\">Pitt et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib41\" title=\"\">2005</a>)</cite> and DoReCo South-England represent dialectal variation, while L2-ARCTIC <cite class=\"ltx_cite ltx_citemacro_cite\">Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib62\" title=\"\">2018</a>)</cite>, EpaDB <cite class=\"ltx_cite ltx_citemacro_cite\">Vidal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib54\" title=\"\">2019</a>)</cite>, and SpeechOcean762 <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib61\" title=\"\">2021</a>)</cite> contain L2 speakers.\nFor L2-ARCTIC, we used the manually annotated phoneme transcriptions (which <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> termed <span class=\"ltx_text ltx_font_italic\">L2-Perceived</span>) rather than G2P dictionary-based transcriptions. The manual transcriptions reflect what the speaker actually said, whereas the dictionary-based version enforces a single pronunciation variant.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>For instance, &#8220;crayon&#8221; in American English can be pronounced as\n<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/<span class=\"ltx_ERROR undefined\">\\textprimstress</span>k&#8290;r&#230;n/, <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/<span class=\"ltx_ERROR undefined\">\\textprimstress</span>k&#8290;rej.On/, or <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/<span class=\"ltx_ERROR undefined\">\\textprimstress</span>k&#8290;rej.6n/ <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaux and Golder, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib53\" title=\"\">2003</a>)</cite> (among others), but the CMU Pronouncing Dictionary <cite class=\"ltx_cite ltx_citemacro_citep\">(Rudnicky, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib46\" title=\"\">1993</a>)</cite> only lists one.</span></span></span>\nManual inspection by a trained phonologist further showed the L2-ARCTIC transcriptions to be of extremely poor quality.\nFor the five aforementioned datasets, we use preprocessed datasets from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://huggingface.co/anyspeech</span></span></span> and Koel Labs<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://huggingface.co/KoelLabs</span></span></span> for better transcription quality.</p>\n\n",
                "matched_terms": [
                    "buckeye",
                    "g2p",
                    "transcription"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate all PR baselines without further training with IPAPack++. See Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS2\" title=\"A.2 Baseline Implementation &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.2</span></a> for more details about training data and language coverage.\nAllosaurus <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib28\" title=\"\">2020</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib29\" title=\"\">2021</a>)</cite> uses a phone-level CTC to train a language-agnostic model and applies language-specific allophone-to-phoneme mappings.\nWav2Vec2Phoneme <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib58\" title=\"\">2022</a>)</cite>, MultIPA <cite class=\"ltx_cite ltx_citemacro_cite\">Taguchi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib51\" title=\"\">2023</a>)</cite> and Allophant <cite class=\"ltx_cite ltx_citemacro_cite\">Glocker et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib14\" title=\"\">2023</a>)</cite> fine-tune XLS-R <cite class=\"ltx_cite ltx_citemacro_citep\">(Babu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib2\" title=\"\">2022</a>)</cite> with different objectives: Wav2Vec2Phoneme maps unseen phonemes using articulatory features, MultIPA leverages high-quality G2P data from seven languages, while Allophant decomposes phones into articulatory features and applies CTC losses for each.\nZIPA <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> trains ZipFormer <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib59\" title=\"\">2024</a>)</cite> from scratch on IPAPack++ using CR-CTC and also provides a variant trained with additional pseudo-labeled data (&#8220;ZIPA-CR-NS-Large&#8221;).</p>\n\n",
                "matched_terms": [
                    "g2p",
                    "all",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T2\" title=\"Table 2 &#8227; POWSM excels at in-domain phone recognition &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>, we see that POWSM achieves the lowest average PFER in phone recognition, due to the strong language modeling capability of the decoder.\nWe hypothesize that our English data cleaning (Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS1\" title=\"A.1 Refining English G2P &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.1</span></a>) may have negatively affected the PFER for Germanic languages due to a mismatch between training and test data.\nNevertheless, our approach fills this gap by achieving strong performance on other languages, outperforming models trained on larger datasets.</p>\n\n",
                "matched_terms": [
                    "other",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hypothesize that pre-training with phone recognition benefits low-resource ASR <cite class=\"ltx_cite ltx_citemacro_cite\">Yusuyin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib60\" title=\"\">2025</a>)</cite>.\nTo choose low-resource languages, we selected languages in IPAPack++ with less than 8 hours of speech in FLEURS to serve as the test set.\nSee <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS3\" title=\"A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.3</span></a> for details on the amount of data used by different models.\nFor a fair comparison with other multilingual ASR baselines without language-specific components, we use the same decoding hyperparameters <span class=\"ltx_text ltx_font_typewriter\">ctc=0.0, beam=1</span>.</p>\n\n",
                "matched_terms": [
                    "other",
                    "speech",
                    "different",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T3\" title=\"Table 3 &#8227; POWSM is comparable with web-scale ASR models on low-resource languages &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;3</span></a>, POWSM (<span class=\"ltx_text ltx_font_typewriter\">POWSM 0.35B, ASR</span>) is often comparable to models of similar size trained on web-scale data for ASR (<span class=\"ltx_text ltx_font_typewriter\">OWLS 0.5B</span>).\nIncorporating phones obtained from PR as text prompts (PR-P2G) significantly decreases WER, making it comparable to or even better than these models.\nWhen using gold phone labels for P2G (see analysis in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.SS2.SSS0.Px2\" title=\"Audio-P2G effectively handles low-resource languages &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;6.2</span></a>), POWSM outperforms other ASR models by a large margin in most cases.</p>\n\n",
                "matched_terms": [
                    "other",
                    "text",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T4\" title=\"Table 4 &#8227; 5.2 POWSM generalizes well to unseen languages &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a> reports PFER on datasets with unseen languages and language variation.\nResults indicate that POWSM achieves strong performance across these datasets, and handles both dialectal and L2 variation effectively.\nNotably, our method outperforms ZIPA trained on the same data and even exceeds ZIPA trained with extra pseudo-labeled data, achieving the best results on unseen languages while performing three additional tasks.\nThis shows the effectiveness of our multi-task approach.\nWhile POWSM lags behind Wav2Vec2Phoneme on socio-phonetic variations, we attribute this to its self-supervised learning with over 60k hours of speech (from wav2vec 2.0 <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib3\" title=\"\">2020</a>)</cite>) prior to the supervised learning stage.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "both",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observed that mixing phones and orthography as encoder targets hindered training, because the same speech input would have different encoder CTC targets for different tasks.\nTherefore, we used phones as encoder targets, encouraging general representations of sounds to be shared across languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As in other encoder-decoder models <cite class=\"ltx_cite ltx_citemacro_cite\">Gong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib15\" title=\"\">2023</a>); Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>, we expect the encoder of POWSM to capture more general acoustic patterns, while the decoder handles language and task-specific output formats. Therefore,\nwe investigate whether emphasizing the encoder more during different stages of model development affects performance.\nTo balance data diversity with inference compute costs, we selected two smaller datasets from each category with distinct characteristics.</p>\n\n",
                "matched_terms": [
                    "other",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T5\" title=\"Table 5 &#8227; Increased encoder weights benefit PR on out-of-domain data &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;5</span></a>, higher CTC decoding weights improve PR performance on out-of-domain data but degrade it on in-domain data, as expected.\nThis echoes <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>&#8217;s finding that RNN-T <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves and Jaitly, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib17\" title=\"\">2014</a>)</cite>, an encoder-only speech-to-text model with an autoregressive text prediction network, hurts generalization to unseen patterns of phones (phonotactics).\nWe hypothesize that the decoder is performing implicit language modeling and &#8220;smooths&#8221; phonetic variation, as <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> described.</p>\n\n",
                "matched_terms": [
                    "text",
                    "phonetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In other words, POWSM G2P responds to speech and text signals to controllably mediate between narrow and broad transcription.\nIn the multi-task setup, this effect may be stronger because the model is trained with G2P, which could bias it toward more standardized forms.</p>\n\n",
                "matched_terms": [
                    "other",
                    "speech",
                    "transcription",
                    "text",
                    "g2p"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare several P2G setups on the same set of low-resource languages from FLEURS, listed in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T7\" title=\"Table 7 &#8227; Audio-P2G effectively handles low-resource languages &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;7</span></a>. P2G significantly outperforms ASR, suggesting that it effectively leverages the provided phone context.\nHowever, since P2G uses gold phone labels, this comparison is not entirely fair. We therefore tested PR followed by P2G (PR-P2G), and found that performance improved for some languages but not for others.\nError propagation does not explain this variation in performance, as PFER trends from PR differ from the observed performance drops. Yet the PFER pattern aligns closely with ASR results, suggesting that phonotactic similarity to high-resource languages may play a role.</p>\n\n",
                "matched_terms": [
                    "pfer",
                    "error",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test this, we run P2G with the language code set to English and post-process the output to match Cyrillic or Gurmukhi transcriptions with online conversion tools for certain languages.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.lexilogos.com\" title=\"\">https://www.lexilogos.com</a> for Macedonian and Tajik; <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://punjabi.indiatyping.com\" title=\"\">https://punjabi.indiatyping.com</a> for Panjabi.</span></span></span>\nThis approach often outperforms ASR and sometimes approaches P2G&#8217;s performance, indicating that P2G also relies heavily on speech input.\nLanguages with either comparibly low or high PFER did not benefit from this transliteration approach, possibly because the model already handled them well or had not yet learned them sufficiently.\nThis finding suggests a direction for further investigation in low-resource ASR.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "pfer",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train a fully open-source phonetic speech foundation model POWSM using our scalable multi-task framework.\nOur model achieves state-of-the-art performance on PR while also supporting ASR across more than 70 languages.\nBeyond PR and ASR, the model&#8217;s ability to perform audio-guided G2P and P2G enables applications that require fine-grained linguistic analysis such as atypical speech assessment.\nOur analysis reveals that POWSM&#8217;s encoder benefits from phoneme-level CTC supervision and stronger encoder weighting, enhancing cross-lingual generalization.\nAdditionally, the model demonstrates interpretable multimodal and language-aware behaviors, effectively mediating between phonetic detail and standardized phonological patterns.\nTo conclude, POWSM not only provides a strong phone recognition foundation model for high-resource languages, but also acts as a versatile resource for unseen languages and socio-phonetic variation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "g2p",
                    "phonetic",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All of our data is ethically sourced, either through permissive licensing or through proper consent.\nWe are aware of the implicit prescriptivism and representational harms <cite class=\"ltx_cite ltx_citemacro_citep\">(Crawford, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib10\" title=\"\">2017</a>)</cite> that normalizing socio-phonetic variation in ASR or PR models can create.\nThis may threaten linguistic diversity instead of preserving it.\nWe also acknowledge that accurate modeling of socio-phonetic variation can enable demographic inference, as demographics and phonetic variation are deeply intertwined <cite class=\"ltx_cite ltx_citemacro_citep\">(Labov, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib23\" title=\"\">1963</a>)</cite>.\nWe stress that uses of POWSM must align with our vision: a future where advances in spoken language processing and NLP do not leave low-resource varieties behind.</p>\n\n",
                "matched_terms": [
                    "all",
                    "phonetic",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observed confusion in plosive voice-onset times on unseen languages in preliminary experiments, which is likely from English G2P data. For instance, broad phonemic transcription in English typically uses /b/ to transcribe the /b/ in /bat/, but its voice onset timing is actually voiceless in Mainstream American English and is closer to [p]. To mitigate this, we apply rule-based refinements to English G2P transcriptions, adjusting plosive voicing and aspiration, lateral velarization, and vowel nasalization.</p>\n\n",
                "matched_terms": [
                    "transcription",
                    "g2p"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first filter out languages with more than 8 hours of training data in IPAPack++ <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>, keeping only those that are also present in FLEURS.\nThen, following the training data amounts reported in <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib6\" title=\"\">2025</a>)</cite>, we further identify the 50 lowest-resource languages to exclude any that may have other substantial sources not included in IPAPack++.\nThis process leaves us with nine languages. We finally exclude <span class=\"ltx_text ltx_font_typewriter\">ell</span>, as it is comparatively higher-resource and because there are already three other Balto-Slavic languages.\nNote that other models use strictly more data than ours&#8212;not only in terms of dataset count but also because IPAPack++ applies additional data-quality filtering. <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.T10\" title=\"Table 10 &#8227; A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;10</span></a> lists the amount of ASR training data for baselines.</p>\n\n",
                "matched_terms": [
                    "other",
                    "any",
                    "asr"
                ]
            }
        ]
    },
    "S6.T7": {
        "source_file": "POWSM: A Phonetic Open Whisper-Style Speech Foundation Model",
        "caption": "Table 7: WER (\\downarrow) of different P2G settings on low-resource languages Best stands for lowest WER in Table3 from ASR models.  indicates post-processed languages.",
        "body": "Afroasiatic\nTurkic\nIndo-Iranian\nBalto-Slavic\n\n\nTask\nafr\norm\naze\npan\ntgk\nmkd\nbos\nslv\n\n\nBest ASR\n67.5\n89.0\n67.5\n50.0\n50.7\n46.2\n50.0\n52.8\n\n\n\n\nASR\n86.2\n125.3\n67.7\n83.1\n62.8\n56\n56.5\n64.5\n\n\nP2G\n55.9\n88.0\n37.1\n52.6\n31.8\n36.9\n32.3\n40.3\n\n\nP2G, lang=<eng>\n\n60.4\n99.0\n64.2\n\n95.8\n\n74.0\n\n52.6\n39.6\n53.5\n\n\nPR-P2G\n68.8\n93.0\n66.7\n72.8\n51.0\n48.6\n56.9\n63.9\n\n\n PFER (\\downarrow)\n9.1\n12.9\n6.7\n7.9\n5.7\n3.3\n6.7\n6.9",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding:2pt 4.0pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding:2pt 4.0pt;\">Afroasiatic</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:2pt 4.0pt;\">Turkic</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding:2pt 4.0pt;\">Indo-Iranian</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" style=\"padding:2pt 4.0pt;\">Balto-Slavic</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" style=\"padding:2pt 4.0pt;\">Task</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">afr</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">orm</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">aze</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">pan</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">tgk</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">mkd</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">bos</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">slv</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding:2pt 4.0pt;\">Best ASR</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:2pt 4.0pt;\">67.5</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">89.0</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:2pt 4.0pt;\">67.5</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">50.0</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:2pt 4.0pt;\">50.7</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">46.2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:2pt 4.0pt;\">50.0</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">52.8</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:2pt 4.0pt;\">ASR</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">86.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">125.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">67.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">83.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">62.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">56.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">64.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:2pt 4.0pt;\">P2G</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">55.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">88.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">37.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">52.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">31.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">36.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">32.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">40.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:2pt 4.0pt;\">P2G, <span class=\"ltx_text ltx_font_typewriter\">lang=&lt;eng&gt;</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">60.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">99.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">64.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">\n<sup class=\"ltx_sup\">&#8727;</sup>95.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">\n<sup class=\"ltx_sup\">&#8727;</sup>74.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">\n<sup class=\"ltx_sup\">&#8727;</sup>52.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">39.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">53.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:2pt 4.0pt;\">PR-P2G</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">68.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">93.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">66.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">72.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">51.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">48.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">56.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:2pt 4.0pt;\">63.9</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">&#8196;&#8195; PFER (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T7.m4\" intent=\":literal\" style=\"--ltx-bg-color:#F2F2F2;\"><semantics><mo mathbackground=\"#F2F2F2\" stretchy=\"false\" style=\"--ltx-bg-color:#F2F2F2;\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">9.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">12.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">6.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">7.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">5.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">3.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">6.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">6.9</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "turkic",
            "task",
            "baltoslavic",
            "pan",
            "downarrow",
            "orm",
            "best",
            "afr",
            "mkd",
            "p2g",
            "lowresource",
            "prp2g",
            "langeng",
            "bos",
            "from",
            "slv",
            "wer",
            "aze",
            "postprocessed",
            "settings",
            "stands",
            "lowest",
            "indoiranian",
            "indicates",
            "asr",
            "526",
            "models",
            "different",
            "best",
            "afroasiatic",
            "740",
            "languages",
            "958",
            "tgk",
            "pfer"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We compare several P2G setups on the same set of low-resource languages from FLEURS, listed in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T7\" title=\"Table 7 &#8227; Audio-P2G effectively handles low-resource languages &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;7</span></a>. P2G significantly outperforms ASR, suggesting that it effectively leverages the provided phone context.\nHowever, since P2G uses gold phone labels, this comparison is not entirely fair. We therefore tested PR followed by P2G (PR-P2G), and found that performance improved for some languages but not for others.\nError propagation does not explain this variation in performance, as PFER trends from PR differ from the observed performance drops. Yet the PFER pattern aligns closely with ASR results, suggesting that phonotactic similarity to high-resource languages may play a role.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in spoken language processing have led to substantial progress in phonetic tasks such as automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G). Despite their conceptual similarity, these tasks have largely been studied in isolation, each relying on task-specific architectures and datasets.\nIn this paper, we introduce POWSM (Phonetic Open Whisper-style Speech Model), the first unified framework capable of jointly performing multiple phone-related tasks.\nPOWSM enables seamless conversion between audio, text (graphemes), and phones, opening up new possibilities for universal and low-resource speech processing.\nOur model outperforms or matches specialized PR models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P, P2G, and ASR.\nOur training data, code<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/espnet</span></span></span> and models<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://huggingface.co/espnet/powsm</span></span></span> are released to foster open science.</p>\n\n",
                "matched_terms": [
                    "models",
                    "p2g",
                    "lowresource",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Four key phone-related tasks underpin phonetic spoken language processing: automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G).\n<span class=\"ltx_text ltx_font_italic\">ASR</span> learns implicit phonetic representations <cite class=\"ltx_cite ltx_citemacro_cite\">Belinkov and Glass (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib4\" title=\"\">2017</a>)</cite>, while <span class=\"ltx_text ltx_font_italic\">PR</span> offers explicit phone-level supervision.\n<span class=\"ltx_text ltx_font_italic\">G2P</span> and <span class=\"ltx_text ltx_font_italic\">P2G</span> bridge orthographic and phonetic spaces.\nCollectively, these tasks interact through shared phonetic representations, each addressing a different aspect of the relationship between audio, phones, phonemes, and graphemes.</p>\n\n",
                "matched_terms": [
                    "different",
                    "p2g",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge this gap, we propose POWSM, a phonetic foundation model capable of performing four core phone-related tasks &#8212; PR, ASR, audio-guided G2P, and audio-guided P2G &#8212; within one unified architecture (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#S1.F1\" title=\"In 1 Introduction &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nTo construct this framework, we reformulate standard ASR datasets <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> into four task-specific formats, allowing the model to learn consistent mappings across audio, phoneme, and grapheme representations.\nIn addition, POWSM adopts an attention-based encoder-decoder (AED) architecture, following the design of large-scale speech foundation models such as Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite> and OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib39\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "p2g",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirically, POWSM outperforms previous PR models on both in-domain data and out-of-domain languages, achieves low-resource ASR performance comparable to web-scale multilingual foundation models, and can act as speech-grounded P2G and G2P across more than 70 languages.\nPOWSM offers a new unified paradigm for phone-level modeling, paving the way for inclusive and globally accessible speech technologies that transcend language boundaries and resource disparities.</p>\n\n",
                "matched_terms": [
                    "p2g",
                    "models",
                    "lowresource",
                    "languages",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent speech foundation models such as Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite> and OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib39\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib38\" title=\"\">2024</a>)</cite> have driven progress in large-scale multilingual ASR and speech translation, but they do not explicitly address phoneme recognition or articulatory-level supervision.\nSubsequent work <cite class=\"ltx_cite ltx_citemacro_citep\">(Yusuyin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib60\" title=\"\">2025</a>; Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib11\" title=\"\">2025</a>)</cite> showed that incorporating phoneme-level objectives improves ASR for low-resource and long-tailed settings, while outputting phonemes as an intermediate benefited speech translation <cite class=\"ltx_cite ltx_citemacro_citep\">(G&#225;llego et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib18\" title=\"\">2025</a>)</cite>.\nPOWSM extends this line of work by being the first open foundation model jointly trained on phone recognition and related tasks, integrating multilinguality, phonetic supervision, and multi-task scalability within one framework.</p>\n\n",
                "matched_terms": [
                    "settings",
                    "models",
                    "lowresource",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior work in multilingual phone recognition can broadly be categorized into (1) language-specific models <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib13\" title=\"\">2021</a>)</cite> that rely on explicit phoneme <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib58\" title=\"\">2022</a>)</cite> or allophone inventories <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib28\" title=\"\">2020</a>)</cite> and (2) language-agnostic approaches that aim to generalize across languages without such resources <cite class=\"ltx_cite ltx_citemacro_citep\">(Taguchi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib51\" title=\"\">2023</a>; Glocker et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib14\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib29\" title=\"\">2021</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>.\nPOWSM follows the latter paradigm as a fully data-driven multilingual model that learns phone representations without predefined phoneme mappings.</p>\n\n",
                "matched_terms": [
                    "models",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">WhisperPPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Samir et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib47\" title=\"\">2025</a>)</cite> improved Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>&#8217;s performance through data cleaning but remained limited in data coverage and task diversity.\nHowever, Whisper is trained on a closed corpus and could display harmful biases for PR which cannot be fully removed by fine-tuning.\nPOWSM is trained from scratch on open datasets.</p>\n\n",
                "matched_terms": [
                    "from",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ZIPA <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> scaled PR to 17,000+ hours of data and 88 languages using a Zipformer <cite class=\"ltx_cite ltx_citemacro_cite\">Yao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib59\" title=\"\">2024</a>)</cite> encoder and noisy-student training on 4,000+ languages, achieving state-of-the-art results. To construct its training corpus, ZIPA employed a G2P system to convert large-scale ASR transcriptions into phoneme sequences, effectively repurposing ASR datasets for PR. Building on this idea, POWSM leverages both the grapheme and the G2P-generated phoneme transcriptions, reformulating them into four task-specific forms: ASR, PR, G2P, and P2G.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "p2g",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">G2P &amp; P2G</span> POWSM is the first model capable of both audio-guided G2P and audio-guided P2G.\nG2P conversion, sometimes called phonemization in the text-to-speech literature, can be accomplished with pronunciation dictionaries <cite class=\"ltx_cite ltx_citemacro_citep\">(Rudnicky, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib46\" title=\"\">1993</a>)</cite>, rules <cite class=\"ltx_cite ltx_citemacro_citep\">(Mortensen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib33\" title=\"\">2018</a>)</cite>, WFSTs <cite class=\"ltx_cite ltx_citemacro_citep\">(Black and Lenzo, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib5\" title=\"\">2001</a>)</cite>, or seq2seq neural methods to choose between different pronunciations of a word in context <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib65\" title=\"\">2022</a>)</cite>.\nText-based G2P, however, still cannot handle phonetic variation, enforcing a one-to-one mapping between orthography and transcription.\nIn contrast, audio-guided G2P can learn to map the different acoustic realizations of a phoneme across varieties of a language to a phone representation <cite class=\"ltx_cite ltx_citemacro_cite\">Route et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib45\" title=\"\">2019</a>)</cite>.\nIn particular, <cite class=\"ltx_cite ltx_citemacro_citet\">Mak et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib32\" title=\"\">2025</a>)</cite> observed a performance improvement in using audio-guided G2P versus text-based G2P alone for Cantonese.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Gao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib12\" title=\"\">2024</a>)</cite> similarly showed that joint learning of G2P, phone recognition, and forced alignment outperform a G2P teacher model. Similarly, <cite class=\"ltx_cite ltx_citemacro_citet\">Sun and Richmond (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib50\" title=\"\">2024</a>)</cite> jointly learned G2P and TTS.\nCompared to G2P, P2G conversion is less studied, with <cite class=\"ltx_cite ltx_citemacro_citet\">Lauc (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib25\" title=\"\">2024</a>)</cite> training a seq2seq model on 19 million language-grapheme-phoneme triplets.</p>\n\n",
                "matched_terms": [
                    "different",
                    "p2g"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model is trained on four tasks: PR, ASR, and audio-guided G2P and P2G. Each utterance is used once per task, with task-specific formatting as illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#S1.F1\" title=\"In 1 Introduction &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, including a text prompt, language token, task token, and target output. We leave the text prompt blank (token <span class=\"ltx_text ltx_font_typewriter\">&lt;na&gt;</span>) for PR and ASR, and provide graphemes and phones as prompts for G2P and P2G.</p>\n\n",
                "matched_terms": [
                    "p2g",
                    "task",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">POWSM adopts an attention-based encoder-decoder (AED) architecture, which flexibly models output sequences and allows the integration of additional tasks.\nSpecifically, we follow the OWSM v3.1 architecture <cite class=\"ltx_cite ltx_citemacro_cite\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib38\" title=\"\">2024</a>)</cite>, which employs an E-Branchformer encoder and a Transformer decoder, consistent with the general encoder-decoder structure of Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>. The model is trained from scratch using ESPnet <cite class=\"ltx_cite ltx_citemacro_cite\">Watanabe et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib56\" title=\"\">2018</a>)</cite> with a hybrid CTC/attention loss <cite class=\"ltx_cite ltx_citemacro_cite\">Watanabe et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib57\" title=\"\">2017</a>)</cite>, where we set the ratio <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> to 0.3:</p>\n\n",
                "matched_terms": [
                    "models",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We report Phonetic Feature Error Rate (PFER), an edit distance using articulatory features from PanPhon <cite class=\"ltx_cite ltx_citemacro_citep\">(Mortensen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib34\" title=\"\">2016</a>)</cite>, averaged over the number of phones and computed as in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S4.E2\" title=\"Equation 2 &#8227; Evaluation metric &#8227; 4 Experimental Setup &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Equation&#160;2</span></a> for PR.\nEach feature contributes <math alttext=\"\\frac{1}{24}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mfrac><mn>1</mn><mn>24</mn></mfrac><annotation encoding=\"application/x-tex\">\\frac{1}{24}</annotation></semantics></math> distance unit, while insertion and deletion cost 1 unit.\nThe edit distance <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> grows linearly with the sequence length and has no upper bound.</p>\n\n",
                "matched_terms": [
                    "from",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For unseen languages, we evaluate on three datasets: DoReCo <cite class=\"ltx_cite ltx_citemacro_cite\">Paschen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib36\" title=\"\">2020</a>)</cite>, VoxAngeles <cite class=\"ltx_cite ltx_citemacro_cite\">Chodroff et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>)</cite>, and Tusom2021 <cite class=\"ltx_cite ltx_citemacro_cite\">Mortensen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib35\" title=\"\">2021</a>)</cite>.\nDoReCo is a dataset of 50+ languages (with broad transcriptions) intended for documentation of small or endangered languages; we use a 45-language subset.\nVoxAngeles <cite class=\"ltx_cite ltx_citemacro_citep\">(Chodroff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>)</cite> is a postprocessed version of the UCLA Phonetics Lab Archive <cite class=\"ltx_cite ltx_citemacro_citep\">(Ladefoged et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib24\" title=\"\">2009</a>)</cite> containing 95 languages.\nTusom is a low-data Tangkhulic language of India not included in the training data. Tusom2021 consists of narrow phonetic transcriptions (unlike the broad transcriptions from G2P on which POWSM was trained) of individual Tusom words.\nWe removed the tones.</p>\n\n",
                "matched_terms": [
                    "from",
                    "postprocessed",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then evaluated our model on in-domain data from IPAPack++, the dataset seen during training. We followed <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> in using LibriSpeech for English, AISHELL for Mandarin, and MLS for European languages, and additionally evaluated on IISc-MILE Tamil <cite class=\"ltx_cite ltx_citemacro_cite\">A et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib1\" title=\"\">2022</a>)</cite> for Tamil and KSC <cite class=\"ltx_cite ltx_citemacro_cite\">Khassanov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib22\" title=\"\">2021</a>)</cite> for Kazakh.\nFor ASR and P2G, we evaluate with FLEURS.</p>\n\n",
                "matched_terms": [
                    "from",
                    "languages",
                    "p2g",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate all PR baselines without further training with IPAPack++. See Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS2\" title=\"A.2 Baseline Implementation &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.2</span></a> for more details about training data and language coverage.\nAllosaurus <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib28\" title=\"\">2020</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib29\" title=\"\">2021</a>)</cite> uses a phone-level CTC to train a language-agnostic model and applies language-specific allophone-to-phoneme mappings.\nWav2Vec2Phoneme <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib58\" title=\"\">2022</a>)</cite>, MultIPA <cite class=\"ltx_cite ltx_citemacro_cite\">Taguchi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib51\" title=\"\">2023</a>)</cite> and Allophant <cite class=\"ltx_cite ltx_citemacro_cite\">Glocker et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib14\" title=\"\">2023</a>)</cite> fine-tune XLS-R <cite class=\"ltx_cite ltx_citemacro_citep\">(Babu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib2\" title=\"\">2022</a>)</cite> with different objectives: Wav2Vec2Phoneme maps unseen phonemes using articulatory features, MultIPA leverages high-quality G2P data from seven languages, while Allophant decomposes phones into articulatory features and applies CTC losses for each.\nZIPA <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> trains ZipFormer <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib59\" title=\"\">2024</a>)</cite> from scratch on IPAPack++ using CR-CTC and also provides a variant trained with additional pseudo-labeled data (&#8220;ZIPA-CR-NS-Large&#8221;).</p>\n\n",
                "matched_terms": [
                    "from",
                    "different",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR, we compare POWSM with two series of models: OWSM <cite class=\"ltx_cite ltx_citemacro_cite\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib37\" title=\"\">2025</a>)</cite> and OWLS <cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib6\" title=\"\">2025</a>)</cite>. We select OWSM-CTC v4 because it is the best-performing model in the series, featuring an encoder-CTC architecture that supports ASR, ST, and LID. For OWLS, we include models with comparable parameter sizes.</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T2\" title=\"Table 2 &#8227; POWSM excels at in-domain phone recognition &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>, we see that POWSM achieves the lowest average PFER in phone recognition, due to the strong language modeling capability of the decoder.\nWe hypothesize that our English data cleaning (Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS1\" title=\"A.1 Refining English G2P &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.1</span></a>) may have negatively affected the PFER for Germanic languages due to a mismatch between training and test data.\nNevertheless, our approach fills this gap by achieving strong performance on other languages, outperforming models trained on larger datasets.</p>\n\n",
                "matched_terms": [
                    "lowest",
                    "models",
                    "from",
                    "languages",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hypothesize that pre-training with phone recognition benefits low-resource ASR <cite class=\"ltx_cite ltx_citemacro_cite\">Yusuyin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib60\" title=\"\">2025</a>)</cite>.\nTo choose low-resource languages, we selected languages in IPAPack++ with less than 8 hours of speech in FLEURS to serve as the test set.\nSee <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS3\" title=\"A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.3</span></a> for details on the amount of data used by different models.\nFor a fair comparison with other multilingual ASR baselines without language-specific components, we use the same decoding hyperparameters <span class=\"ltx_text ltx_font_typewriter\">ctc=0.0, beam=1</span>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "different",
                    "lowresource",
                    "languages",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T3\" title=\"Table 3 &#8227; POWSM is comparable with web-scale ASR models on low-resource languages &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;3</span></a>, POWSM (<span class=\"ltx_text ltx_font_typewriter\">POWSM 0.35B, ASR</span>) is often comparable to models of similar size trained on web-scale data for ASR (<span class=\"ltx_text ltx_font_typewriter\">OWLS 0.5B</span>).\nIncorporating phones obtained from PR as text prompts (PR-P2G) significantly decreases WER, making it comparable to or even better than these models.\nWhen using gold phone labels for P2G (see analysis in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.SS2.SSS0.Px2\" title=\"Audio-P2G effectively handles low-resource languages &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;6.2</span></a>), POWSM outperforms other ASR models by a large margin in most cases.</p>\n\n",
                "matched_terms": [
                    "p2g",
                    "models",
                    "prp2g",
                    "from",
                    "wer",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T4\" title=\"Table 4 &#8227; 5.2 POWSM generalizes well to unseen languages &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a> reports PFER on datasets with unseen languages and language variation.\nResults indicate that POWSM achieves strong performance across these datasets, and handles both dialectal and L2 variation effectively.\nNotably, our method outperforms ZIPA trained on the same data and even exceeds ZIPA trained with extra pseudo-labeled data, achieving the best results on unseen languages while performing three additional tasks.\nThis shows the effectiveness of our multi-task approach.\nWhile POWSM lags behind Wav2Vec2Phoneme on socio-phonetic variations, we attribute this to its self-supervised learning with over 60k hours of speech (from wav2vec 2.0 <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib3\" title=\"\">2020</a>)</cite>) prior to the supervised learning stage.</p>\n\n",
                "matched_terms": [
                    "from",
                    "pfer",
                    "best",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observed that mixing phones and orthography as encoder targets hindered training, because the same speech input would have different encoder CTC targets for different tasks.\nTherefore, we used phones as encoder targets, encouraging general representations of sounds to be shared across languages.</p>\n\n",
                "matched_terms": [
                    "different",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To determine the most effective unit for the CTC encoder, we fix the decoder vocabulary to PanPhon phones and compared four encoder targets: (1) Unicode code points vs. PanPhon, and (2) sequences with vs. without suprasegmentals (length and break marks). Unicode code points offer simplicity and a smaller vocabulary but split phones into unnatural units (e.g. <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/p<sup class=\"ltx_sup\">h</sup>/) and increase sequence length, while PanPhon represents each phone-diacritic combination as a unit (e.g. <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/p<sup class=\"ltx_sup\">h</sup>/), yielding a more natural monotonic sequence at the expense of sparsity and potential out-of-vocabulary issues. Suprasegmentals such as <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/<span class=\"ltx_ERROR undefined\">\\textlengthmark</span>/, though phonemic in many languages, confuse PR models <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As in other encoder-decoder models <cite class=\"ltx_cite ltx_citemacro_cite\">Gong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib15\" title=\"\">2023</a>); Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>, we expect the encoder of POWSM to capture more general acoustic patterns, while the decoder handles language and task-specific output formats. Therefore,\nwe investigate whether emphasizing the encoder more during different stages of model development affects performance.\nTo balance data diversity with inference compute costs, we selected two smaller datasets from each category with distinct characteristics.</p>\n\n",
                "matched_terms": [
                    "models",
                    "from",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, we examine whether focusing more on the CTC loss through training widens this gap in performance between in-domain and out-of-domain data.\nWe find that fine-tuning with a higher CTC loss weight <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> after convergence does not improve out-of-domain performance and can even degrade it. Randomly varying <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> for each batch also shows no improvement.\nIn contrast, training with a higher <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> from the start benefits the out-of-domain distribution, achieving the lowest PFER on unseen languages with greedy decoding, while the PFER on in-domain data is comparatively higher.\nThese results suggest that assigning a higher weight to the encoder during training and inference improves PR, highlighting a common trade-off between in-domain performance and generalization.</p>\n\n",
                "matched_terms": [
                    "lowest",
                    "from",
                    "pfer",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand how POWSM integrates speech and text prompts, we analyze the relative influence of speech and text prompts in its G2P behavior.\nWe vary the G2P conditions from purely speech-based to purely text-based, as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T6\" title=\"Table 6 &#8227; Increased encoder weights benefit PR on out-of-domain data &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;6</span></a>, and evaluate the model on the Buckeye dataset.\nWhen only speech is provided, the performance is comparable to the PR setting, which differs only in the task token.\nAdding both speech and text prompts (the standard G2P setup) leads to degraded performance, with output showing standardized pronunciations.\nWhen the model relies solely on the text prompt, performance drops sharply and pronunciations become highly standardized as expected (just as <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> reported).</p>\n\n",
                "matched_terms": [
                    "from",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test this, we run P2G with the language code set to English and post-process the output to match Cyrillic or Gurmukhi transcriptions with online conversion tools for certain languages.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.lexilogos.com\" title=\"\">https://www.lexilogos.com</a> for Macedonian and Tajik; <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://punjabi.indiatyping.com\" title=\"\">https://punjabi.indiatyping.com</a> for Panjabi.</span></span></span>\nThis approach often outperforms ASR and sometimes approaches P2G&#8217;s performance, indicating that P2G also relies heavily on speech input.\nLanguages with either comparibly low or high PFER did not benefit from this transliteration approach, possibly because the model already handled them well or had not yet learned them sufficiently.\nThis finding suggests a direction for further investigation in low-resource ASR.</p>\n\n",
                "matched_terms": [
                    "p2g",
                    "pfer",
                    "lowresource",
                    "from",
                    "languages",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The language identification (LID) performance of POWSM on seen languages in FLEURS reaches 92.3% accuracy, as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.F3\" title=\"Figure 3 &#8227; A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;3</span></a>.\nTo see if the model implicitly learns phonotactic patterns and associates them with the language token, we evaluate PR on unseen languages by manipulating the language token at inference time.\nFor VoxAngeles and Tusom2021, the three most frequently assigned languages are Bashkir (42.6%, 25.1%), English (30.2%, 67.7%), and Kinyarwanda (14.5%, 2.3%), which are all relatively high-resource languages in IPAPack++.\n<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T8\" title=\"Table 8 &#8227; Language token captures phonotactics &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;8</span></a> shows that assigning the detected language token yields better performance than always using English, while setting the language as unknown performs best. This indicates that the language token influences PR by shifting the output distribution toward the assigned language.</p>\n\n",
                "matched_terms": [
                    "best",
                    "indicates",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train a fully open-source phonetic speech foundation model POWSM using our scalable multi-task framework.\nOur model achieves state-of-the-art performance on PR while also supporting ASR across more than 70 languages.\nBeyond PR and ASR, the model&#8217;s ability to perform audio-guided G2P and P2G enables applications that require fine-grained linguistic analysis such as atypical speech assessment.\nOur analysis reveals that POWSM&#8217;s encoder benefits from phoneme-level CTC supervision and stronger encoder weighting, enhancing cross-lingual generalization.\nAdditionally, the model demonstrates interpretable multimodal and language-aware behaviors, effectively mediating between phonetic detail and standardized phonological patterns.\nTo conclude, POWSM not only provides a strong phone recognition foundation model for high-resource languages, but also acts as a versatile resource for unseen languages and socio-phonetic variation.</p>\n\n",
                "matched_terms": [
                    "from",
                    "languages",
                    "p2g",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">POWSM has several limitations that we aim to address in future work.\nFirst, the model is neither strictly phonemic nor phonetic: its training data consist of cleaned and filtered phonemic transcriptions from multiple languages, which are not fully faithful to the phonetic or phonemic structure of the audio. Although phonemic transcriptions share similarities across languages, adding auxiliary tasks and language tokens may have reinforced language-specific biases. We also currently lack sufficient allophone-level data, which would provide more language-independent information.</p>\n\n",
                "matched_terms": [
                    "from",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, the current AED architecture imposes engineering limitations. Inference is significantly slower than with encoder-only models, and the architecture does not easily support tone modeling, limiting its application to tonal languages.</p>\n\n",
                "matched_terms": [
                    "models",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All of our data is ethically sourced, either through permissive licensing or through proper consent.\nWe are aware of the implicit prescriptivism and representational harms <cite class=\"ltx_cite ltx_citemacro_citep\">(Crawford, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib10\" title=\"\">2017</a>)</cite> that normalizing socio-phonetic variation in ASR or PR models can create.\nThis may threaten linguistic diversity instead of preserving it.\nWe also acknowledge that accurate modeling of socio-phonetic variation can enable demographic inference, as demographics and phonetic variation are deeply intertwined <cite class=\"ltx_cite ltx_citemacro_citep\">(Labov, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib23\" title=\"\">1963</a>)</cite>.\nWe stress that uses of POWSM must align with our vision: a future where advances in spoken language processing and NLP do not leave low-resource varieties behind.</p>\n\n",
                "matched_terms": [
                    "models",
                    "lowresource",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observed confusion in plosive voice-onset times on unseen languages in preliminary experiments, which is likely from English G2P data. For instance, broad phonemic transcription in English typically uses /b/ to transcribe the /b/ in /bat/, but its voice onset timing is actually voiceless in Mainstream American English and is closer to [p]. To mitigate this, we apply rule-based refinements to English G2P transcriptions, adjusting plosive voicing and aspiration, lateral velarization, and vowel nasalization.</p>\n\n",
                "matched_terms": [
                    "from",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first filter out languages with more than 8 hours of training data in IPAPack++ <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>, keeping only those that are also present in FLEURS.\nThen, following the training data amounts reported in <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib6\" title=\"\">2025</a>)</cite>, we further identify the 50 lowest-resource languages to exclude any that may have other substantial sources not included in IPAPack++.\nThis process leaves us with nine languages. We finally exclude <span class=\"ltx_text ltx_font_typewriter\">ell</span>, as it is comparatively higher-resource and because there are already three other Balto-Slavic languages.\nNote that other models use strictly more data than ours&#8212;not only in terms of dataset count but also because IPAPack++ applies additional data-quality filtering. <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.T10\" title=\"Table 10 &#8227; A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;10</span></a> lists the amount of ASR training data for baselines.</p>\n\n",
                "matched_terms": [
                    "models",
                    "baltoslavic",
                    "languages",
                    "asr"
                ]
            }
        ]
    },
    "S6.T8": {
        "source_file": "POWSM: A Phonetic Open Whisper-Style Speech Foundation Model",
        "caption": "Table 8: PFER (\\downarrow) of PR performance with different language token. The detected language in the example is <bak>. Blue for correct, orange for error compared to other examples.",
        "body": "Lang. token\nVoxangeles\nTusom2021\nExample\n\n\n\n\nPhonetic transcription\n\n\n\\tipaencoding/adZm3/\n\n\n\n<unk>\n17.11\n21.96\n\n\\tipaencoding/adZima/\n\n\nDetected\n17.55\n23.92\n\n\\tipaencoding/6jum/\n\n\n<eng>\n19.91\n24.21\n\n\\tipaencoding/aItimO/",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Lang. token</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Voxangeles</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Tusom2021</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Example</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"2\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">Phonetic transcription</span></td>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">/adZm3/</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_typewriter\">&lt;unk&gt;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">17.11</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">21.96</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/a<span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">d&#865;Z</span>ima/</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1pt;padding-bottom:1pt;\">Detected</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">17.55</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">23.92</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF8000;\">6ju</span>m/</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_typewriter\">&lt;eng&gt;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">19.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">24.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/a<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF8000;\">It</span>imO/</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "other",
            "tipaencodingadzima",
            "orange",
            "downarrow",
            "eng",
            "blue",
            "example",
            "transcription",
            "language",
            "detected",
            "tusom2021",
            "tipaencodingaitimo",
            "correct",
            "tipaencodingadzm3",
            "phonetic",
            "examples",
            "bak",
            "tipaencoding6jum",
            "token",
            "performance",
            "lang",
            "different",
            "unk",
            "compared",
            "voxangeles",
            "error",
            "pfer"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The language identification (LID) performance of POWSM on seen languages in FLEURS reaches 92.3% accuracy, as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.F3\" title=\"Figure 3 &#8227; A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;3</span></a>.\nTo see if the model implicitly learns phonotactic patterns and associates them with the language token, we evaluate PR on unseen languages by manipulating the language token at inference time.\nFor VoxAngeles and Tusom2021, the three most frequently assigned languages are Bashkir (42.6%, 25.1%), English (30.2%, 67.7%), and Kinyarwanda (14.5%, 2.3%), which are all relatively high-resource languages in IPAPack++.\n<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T8\" title=\"Table 8 &#8227; Language token captures phonotactics &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;8</span></a> shows that assigning the detected language token yields better performance than always using English, while setting the language as unknown performs best. This indicates that the language token influences PR by shifting the output distribution toward the assigned language.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in spoken language processing have led to substantial progress in phonetic tasks such as automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G). Despite their conceptual similarity, these tasks have largely been studied in isolation, each relying on task-specific architectures and datasets.\nIn this paper, we introduce POWSM (Phonetic Open Whisper-style Speech Model), the first unified framework capable of jointly performing multiple phone-related tasks.\nPOWSM enables seamless conversion between audio, text (graphemes), and phones, opening up new possibilities for universal and low-resource speech processing.\nOur model outperforms or matches specialized PR models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P, P2G, and ASR.\nOur training data, code<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/espnet</span></span></span> and models<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://huggingface.co/espnet/powsm</span></span></span> are released to foster open science.</p>\n\n",
                "matched_terms": [
                    "language",
                    "phonetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Phones are the smallest units of sound in speech. Unlike graphemes, phones are shared across languages and usually represented using the International Phonetic Alphabet (IPA) <cite class=\"ltx_cite ltx_citemacro_citep\">(International Phonetic Association, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib21\" title=\"\">1999</a>)</cite>, a unified transcription standard for all languages.\nBy providing a consistent representation of speech across languages, phone-level modeling allows fine-grained analysis and cross-lingual generalization, enabling tasks like atypical speech analysis (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, L2 speech <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib27\" title=\"\">2016</a>); Inceoglu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib20\" title=\"\">2023</a>)</cite> and pathological speech <cite class=\"ltx_cite ltx_citemacro_cite\">Choi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib9\" title=\"\">2025</a>); Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib26\" title=\"\">2025</a>)</cite>), endangered language documentation <cite class=\"ltx_cite ltx_citemacro_cite\">He et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib19\" title=\"\">2024</a>)</cite>, code-switched text-to-speech <cite class=\"ltx_cite ltx_citemacro_cite\">Zhou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib63\" title=\"\">2020</a>)</cite>, and cross-lingual transfer in speech-to-text <cite class=\"ltx_cite ltx_citemacro_cite\">Pratap et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib42\" title=\"\">2024</a>); Magoshi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib31\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "transcription",
                    "language",
                    "phonetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Four key phone-related tasks underpin phonetic spoken language processing: automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G).\n<span class=\"ltx_text ltx_font_italic\">ASR</span> learns implicit phonetic representations <cite class=\"ltx_cite ltx_citemacro_cite\">Belinkov and Glass (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib4\" title=\"\">2017</a>)</cite>, while <span class=\"ltx_text ltx_font_italic\">PR</span> offers explicit phone-level supervision.\n<span class=\"ltx_text ltx_font_italic\">G2P</span> and <span class=\"ltx_text ltx_font_italic\">P2G</span> bridge orthographic and phonetic spaces.\nCollectively, these tasks interact through shared phonetic representations, each addressing a different aspect of the relationship between audio, phones, phonemes, and graphemes.</p>\n\n",
                "matched_terms": [
                    "language",
                    "phonetic",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite their conceptual similarity, these tasks have traditionally been developed in isolation, using task-specific architectures and datasets.\nSuch systems are optimized for specific input-output mappings and cannot be easily extended to other phonetic tasks.\nThis fragmentation has hindered the development of general-purpose models for phonetic processing, necessitating a unified phonetic foundation model that can perform multiple phone-related tasks within a single, general framework for speech processing.</p>\n\n",
                "matched_terms": [
                    "other",
                    "phonetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirically, POWSM outperforms previous PR models on both in-domain data and out-of-domain languages, achieves low-resource ASR performance comparable to web-scale multilingual foundation models, and can act as speech-grounded P2G and G2P across more than 70 languages.\nPOWSM offers a new unified paradigm for phone-level modeling, paving the way for inclusive and globally accessible speech technologies that transcend language boundaries and resource disparities.</p>\n\n",
                "matched_terms": [
                    "language",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">G2P &amp; P2G</span> POWSM is the first model capable of both audio-guided G2P and audio-guided P2G.\nG2P conversion, sometimes called phonemization in the text-to-speech literature, can be accomplished with pronunciation dictionaries <cite class=\"ltx_cite ltx_citemacro_citep\">(Rudnicky, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib46\" title=\"\">1993</a>)</cite>, rules <cite class=\"ltx_cite ltx_citemacro_citep\">(Mortensen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib33\" title=\"\">2018</a>)</cite>, WFSTs <cite class=\"ltx_cite ltx_citemacro_citep\">(Black and Lenzo, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib5\" title=\"\">2001</a>)</cite>, or seq2seq neural methods to choose between different pronunciations of a word in context <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib65\" title=\"\">2022</a>)</cite>.\nText-based G2P, however, still cannot handle phonetic variation, enforcing a one-to-one mapping between orthography and transcription.\nIn contrast, audio-guided G2P can learn to map the different acoustic realizations of a phoneme across varieties of a language to a phone representation <cite class=\"ltx_cite ltx_citemacro_cite\">Route et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib45\" title=\"\">2019</a>)</cite>.\nIn particular, <cite class=\"ltx_cite ltx_citemacro_citet\">Mak et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib32\" title=\"\">2025</a>)</cite> observed a performance improvement in using audio-guided G2P versus text-based G2P alone for Cantonese.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Gao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib12\" title=\"\">2024</a>)</cite> similarly showed that joint learning of G2P, phone recognition, and forced alignment outperform a G2P teacher model. Similarly, <cite class=\"ltx_cite ltx_citemacro_citet\">Sun and Richmond (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib50\" title=\"\">2024</a>)</cite> jointly learned G2P and TTS.\nCompared to G2P, P2G conversion is less studied, with <cite class=\"ltx_cite ltx_citemacro_citet\">Lauc (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib25\" title=\"\">2024</a>)</cite> training a seq2seq model on 19 million language-grapheme-phoneme triplets.</p>\n\n",
                "matched_terms": [
                    "transcription",
                    "language",
                    "phonetic",
                    "different",
                    "compared",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model is trained on four tasks: PR, ASR, and audio-guided G2P and P2G. Each utterance is used once per task, with task-specific formatting as illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#S1.F1\" title=\"In 1 Introduction &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, including a text prompt, language token, task token, and target output. We leave the text prompt blank (token <span class=\"ltx_text ltx_font_typewriter\">&lt;na&gt;</span>) for PR and ASR, and provide graphemes and phones as prompts for G2P and P2G.</p>\n\n",
                "matched_terms": [
                    "language",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The encoder operates at the stride size of 40ms. Training uses a global batch size of 256. Speech inputs are 16kHz and padded to 20 seconds. The vocabulary consists of 40k tokens, including around 6k phone tokens, language and timestamp tokens, and BPE tokens from orthography.\nThe model has approximately 350M parameters with 9 layers for both the encoder and decoder and was trained on 4 H100 GPUs for 2 days.\nUsing a CTC loss <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib16\" title=\"\">2006</a>)</cite>, We align the encoder outputs with a simplified version of the phone token sequences. Unlike the decoder outputs, the phones in these sequences are stripped of break (<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/./, <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/*&#865;/)\nand length diacritics (<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/e<span class=\"ltx_ERROR undefined\">\\textlengthmark</span>/, /e<span class=\"ltx_ERROR undefined\">\\texthalflength</span>/, /&#283;/) to accelerate convergence. Additional details and analyses are provided in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.SS1.SSS0.Px1\" title=\"The CTC encoder prefers fine-grained phones without suprasegmentals &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;6.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We report Phonetic Feature Error Rate (PFER), an edit distance using articulatory features from PanPhon <cite class=\"ltx_cite ltx_citemacro_citep\">(Mortensen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib34\" title=\"\">2016</a>)</cite>, averaged over the number of phones and computed as in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S4.E2\" title=\"Equation 2 &#8227; Evaluation metric &#8227; 4 Experimental Setup &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Equation&#160;2</span></a> for PR.\nEach feature contributes <math alttext=\"\\frac{1}{24}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mfrac><mn>1</mn><mn>24</mn></mfrac><annotation encoding=\"application/x-tex\">\\frac{1}{24}</annotation></semantics></math> distance unit, while insertion and deletion cost 1 unit.\nThe edit distance <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> grows linearly with the sequence length and has no upper bound.</p>\n\n",
                "matched_terms": [
                    "phonetic",
                    "error",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike Phone Error Rate (PER), which considers only exact phone matches, or Phone Token Error Rate (PTER), which treats diacritics and modifiers as separate tokens, PFER computes the edit distance in terms of articulatory features&#8212;interpretable subphone attributes (<span class=\"ltx_text ltx_font_italic\">e.g.</span> voicing)&#8212;capturing phonetic similarity in a fine-grained fashion.\nPrevious studies <cite class=\"ltx_cite ltx_citemacro_cite\">Taguchi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib51\" title=\"\">2023</a>); Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> define PFER as the mean articulatory feature edit distance over the evaluation set. In contrast, we normalize it by the number of phones in the reference transcription to measure the proportion of feature errors per phone.</p>\n\n",
                "matched_terms": [
                    "transcription",
                    "phonetic",
                    "token",
                    "error",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For unseen languages, we evaluate on three datasets: DoReCo <cite class=\"ltx_cite ltx_citemacro_cite\">Paschen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib36\" title=\"\">2020</a>)</cite>, VoxAngeles <cite class=\"ltx_cite ltx_citemacro_cite\">Chodroff et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>)</cite>, and Tusom2021 <cite class=\"ltx_cite ltx_citemacro_cite\">Mortensen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib35\" title=\"\">2021</a>)</cite>.\nDoReCo is a dataset of 50+ languages (with broad transcriptions) intended for documentation of small or endangered languages; we use a 45-language subset.\nVoxAngeles <cite class=\"ltx_cite ltx_citemacro_citep\">(Chodroff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>)</cite> is a postprocessed version of the UCLA Phonetics Lab Archive <cite class=\"ltx_cite ltx_citemacro_citep\">(Ladefoged et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib24\" title=\"\">2009</a>)</cite> containing 95 languages.\nTusom is a low-data Tangkhulic language of India not included in the training data. Tusom2021 consists of narrow phonetic transcriptions (unlike the broad transcriptions from G2P on which POWSM was trained) of individual Tusom words.\nWe removed the tones.</p>\n\n",
                "matched_terms": [
                    "language",
                    "phonetic",
                    "tusom2021",
                    "voxangeles"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate all PR baselines without further training with IPAPack++. See Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS2\" title=\"A.2 Baseline Implementation &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.2</span></a> for more details about training data and language coverage.\nAllosaurus <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib28\" title=\"\">2020</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib29\" title=\"\">2021</a>)</cite> uses a phone-level CTC to train a language-agnostic model and applies language-specific allophone-to-phoneme mappings.\nWav2Vec2Phoneme <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib58\" title=\"\">2022</a>)</cite>, MultIPA <cite class=\"ltx_cite ltx_citemacro_cite\">Taguchi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib51\" title=\"\">2023</a>)</cite> and Allophant <cite class=\"ltx_cite ltx_citemacro_cite\">Glocker et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib14\" title=\"\">2023</a>)</cite> fine-tune XLS-R <cite class=\"ltx_cite ltx_citemacro_citep\">(Babu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib2\" title=\"\">2022</a>)</cite> with different objectives: Wav2Vec2Phoneme maps unseen phonemes using articulatory features, MultIPA leverages high-quality G2P data from seven languages, while Allophant decomposes phones into articulatory features and applies CTC losses for each.\nZIPA <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> trains ZipFormer <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib59\" title=\"\">2024</a>)</cite> from scratch on IPAPack++ using CR-CTC and also provides a variant trained with additional pseudo-labeled data (&#8220;ZIPA-CR-NS-Large&#8221;).</p>\n\n",
                "matched_terms": [
                    "language",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T2\" title=\"Table 2 &#8227; POWSM excels at in-domain phone recognition &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>, we see that POWSM achieves the lowest average PFER in phone recognition, due to the strong language modeling capability of the decoder.\nWe hypothesize that our English data cleaning (Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS1\" title=\"A.1 Refining English G2P &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.1</span></a>) may have negatively affected the PFER for Germanic languages due to a mismatch between training and test data.\nNevertheless, our approach fills this gap by achieving strong performance on other languages, outperforming models trained on larger datasets.</p>\n\n",
                "matched_terms": [
                    "other",
                    "language",
                    "performance",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hypothesize that pre-training with phone recognition benefits low-resource ASR <cite class=\"ltx_cite ltx_citemacro_cite\">Yusuyin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib60\" title=\"\">2025</a>)</cite>.\nTo choose low-resource languages, we selected languages in IPAPack++ with less than 8 hours of speech in FLEURS to serve as the test set.\nSee <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS3\" title=\"A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.3</span></a> for details on the amount of data used by different models.\nFor a fair comparison with other multilingual ASR baselines without language-specific components, we use the same decoding hyperparameters <span class=\"ltx_text ltx_font_typewriter\">ctc=0.0, beam=1</span>.</p>\n\n",
                "matched_terms": [
                    "other",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T4\" title=\"Table 4 &#8227; 5.2 POWSM generalizes well to unseen languages &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a> reports PFER on datasets with unseen languages and language variation.\nResults indicate that POWSM achieves strong performance across these datasets, and handles both dialectal and L2 variation effectively.\nNotably, our method outperforms ZIPA trained on the same data and even exceeds ZIPA trained with extra pseudo-labeled data, achieving the best results on unseen languages while performing three additional tasks.\nThis shows the effectiveness of our multi-task approach.\nWhile POWSM lags behind Wav2Vec2Phoneme on socio-phonetic variations, we attribute this to its self-supervised learning with over 60k hours of speech (from wav2vec 2.0 <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib3\" title=\"\">2020</a>)</cite>) prior to the supervised learning stage.</p>\n\n",
                "matched_terms": [
                    "language",
                    "performance",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As in other encoder-decoder models <cite class=\"ltx_cite ltx_citemacro_cite\">Gong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib15\" title=\"\">2023</a>); Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>, we expect the encoder of POWSM to capture more general acoustic patterns, while the decoder handles language and task-specific output formats. Therefore,\nwe investigate whether emphasizing the encoder more during different stages of model development affects performance.\nTo balance data diversity with inference compute costs, we selected two smaller datasets from each category with distinct characteristics.</p>\n\n",
                "matched_terms": [
                    "other",
                    "language",
                    "performance",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T5\" title=\"Table 5 &#8227; Increased encoder weights benefit PR on out-of-domain data &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;5</span></a>, higher CTC decoding weights improve PR performance on out-of-domain data but degrade it on in-domain data, as expected.\nThis echoes <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>&#8217;s finding that RNN-T <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves and Jaitly, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib17\" title=\"\">2014</a>)</cite>, an encoder-only speech-to-text model with an autoregressive text prediction network, hurts generalization to unseen patterns of phones (phonotactics).\nWe hypothesize that the decoder is performing implicit language modeling and &#8220;smooths&#8221; phonetic variation, as <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> described.</p>\n\n",
                "matched_terms": [
                    "language",
                    "performance",
                    "phonetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, we examine whether focusing more on the CTC loss through training widens this gap in performance between in-domain and out-of-domain data.\nWe find that fine-tuning with a higher CTC loss weight <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> after convergence does not improve out-of-domain performance and can even degrade it. Randomly varying <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> for each batch also shows no improvement.\nIn contrast, training with a higher <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> from the start benefits the out-of-domain distribution, achieving the lowest PFER on unseen languages with greedy decoding, while the PFER on in-domain data is comparatively higher.\nThese results suggest that assigning a higher weight to the encoder during training and inference improves PR, highlighting a common trade-off between in-domain performance and generalization.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand how POWSM integrates speech and text prompts, we analyze the relative influence of speech and text prompts in its G2P behavior.\nWe vary the G2P conditions from purely speech-based to purely text-based, as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T6\" title=\"Table 6 &#8227; Increased encoder weights benefit PR on out-of-domain data &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;6</span></a>, and evaluate the model on the Buckeye dataset.\nWhen only speech is provided, the performance is comparable to the PR setting, which differs only in the task token.\nAdding both speech and text prompts (the standard G2P setup) leads to degraded performance, with output showing standardized pronunciations.\nWhen the model relies solely on the text prompt, performance drops sharply and pronunciations become highly standardized as expected (just as <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> reported).</p>\n\n",
                "matched_terms": [
                    "performance",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In other words, POWSM G2P responds to speech and text signals to controllably mediate between narrow and broad transcription.\nIn the multi-task setup, this effect may be stronger because the model is trained with G2P, which could bias it toward more standardized forms.</p>\n\n",
                "matched_terms": [
                    "other",
                    "transcription"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare several P2G setups on the same set of low-resource languages from FLEURS, listed in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T7\" title=\"Table 7 &#8227; Audio-P2G effectively handles low-resource languages &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;7</span></a>. P2G significantly outperforms ASR, suggesting that it effectively leverages the provided phone context.\nHowever, since P2G uses gold phone labels, this comparison is not entirely fair. We therefore tested PR followed by P2G (PR-P2G), and found that performance improved for some languages but not for others.\nError propagation does not explain this variation in performance, as PFER trends from PR differ from the observed performance drops. Yet the PFER pattern aligns closely with ASR results, suggesting that phonotactic similarity to high-resource languages may play a role.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "error",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test this, we run P2G with the language code set to English and post-process the output to match Cyrillic or Gurmukhi transcriptions with online conversion tools for certain languages.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.lexilogos.com\" title=\"\">https://www.lexilogos.com</a> for Macedonian and Tajik; <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://punjabi.indiatyping.com\" title=\"\">https://punjabi.indiatyping.com</a> for Panjabi.</span></span></span>\nThis approach often outperforms ASR and sometimes approaches P2G&#8217;s performance, indicating that P2G also relies heavily on speech input.\nLanguages with either comparibly low or high PFER did not benefit from this transliteration approach, possibly because the model already handled them well or had not yet learned them sufficiently.\nThis finding suggests a direction for further investigation in low-resource ASR.</p>\n\n",
                "matched_terms": [
                    "language",
                    "performance",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train a fully open-source phonetic speech foundation model POWSM using our scalable multi-task framework.\nOur model achieves state-of-the-art performance on PR while also supporting ASR across more than 70 languages.\nBeyond PR and ASR, the model&#8217;s ability to perform audio-guided G2P and P2G enables applications that require fine-grained linguistic analysis such as atypical speech assessment.\nOur analysis reveals that POWSM&#8217;s encoder benefits from phoneme-level CTC supervision and stronger encoder weighting, enhancing cross-lingual generalization.\nAdditionally, the model demonstrates interpretable multimodal and language-aware behaviors, effectively mediating between phonetic detail and standardized phonological patterns.\nTo conclude, POWSM not only provides a strong phone recognition foundation model for high-resource languages, but also acts as a versatile resource for unseen languages and socio-phonetic variation.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "phonetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">POWSM&#8217;s current decoder serves as a large phoneme-level phonotactic language model on which linguists could investigate hypotheses about phonetic universals <cite class=\"ltx_cite ltx_citemacro_citep\">(Chodroff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>; Chodroff, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib7\" title=\"\">2025</a>)</cite> and phonotactics <cite class=\"ltx_cite ltx_citemacro_citep\">(Shim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib48\" title=\"\">2024</a>; Pimentel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib40\" title=\"\">2020</a>)</cite>.\nIn the future, we seek to adapt to socio-phonetic variation either through (unsupervised) test-time adaptation <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib30\" title=\"\">2022</a>)</cite>, in-context learning <cite class=\"ltx_cite ltx_citemacro_citep\">(Roll et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib44\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib55\" title=\"\">2024</a>)</cite>, or mechanistic interpretability <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib52\" title=\"\">2024</a>)</cite>.\nFurthermore, since <cite class=\"ltx_cite ltx_citemacro_citet\">Shim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib49\" title=\"\">2025</a>)</cite> found that earlier encoder layers in Whisper preserve more phonetic detail, early exiting may mitigate the decoder&#8217;s tendencies to normalize socio-phonetic variation.</p>\n\n",
                "matched_terms": [
                    "language",
                    "phonetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">POWSM has several limitations that we aim to address in future work.\nFirst, the model is neither strictly phonemic nor phonetic: its training data consist of cleaned and filtered phonemic transcriptions from multiple languages, which are not fully faithful to the phonetic or phonemic structure of the audio. Although phonemic transcriptions share similarities across languages, adding auxiliary tasks and language tokens may have reinforced language-specific biases. We also currently lack sufficient allophone-level data, which would provide more language-independent information.</p>\n\n",
                "matched_terms": [
                    "language",
                    "phonetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All of our data is ethically sourced, either through permissive licensing or through proper consent.\nWe are aware of the implicit prescriptivism and representational harms <cite class=\"ltx_cite ltx_citemacro_citep\">(Crawford, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib10\" title=\"\">2017</a>)</cite> that normalizing socio-phonetic variation in ASR or PR models can create.\nThis may threaten linguistic diversity instead of preserving it.\nWe also acknowledge that accurate modeling of socio-phonetic variation can enable demographic inference, as demographics and phonetic variation are deeply intertwined <cite class=\"ltx_cite ltx_citemacro_citep\">(Labov, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib23\" title=\"\">1963</a>)</cite>.\nWe stress that uses of POWSM must align with our vision: a future where advances in spoken language processing and NLP do not leave low-resource varieties behind.</p>\n\n",
                "matched_terms": [
                    "language",
                    "phonetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multi-tasking may improve performance by tying acoustic signals to well-defined symbolic representations, yet it may distract the model if the relationships are not learned effectively.\nWe train POWSM with different data and model scales to examine how multitask learning interacts with the setup, and use <span class=\"ltx_text ltx_font_typewriter\">beam=1</span> during decoding to speed up inference.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "different"
                ]
            }
        ]
    },
    "A1.T9": {
        "source_file": "POWSM: A Phonetic Open Whisper-Style Speech Foundation Model",
        "caption": "Table 9: Overview of the baselines for our work.",
        "body": "Model\nTraining Data Sources\nLanguage Coverage\nModel checkpoint / GitHub\n\n\nPR baselines\n\n\n\n\n\nAllosaurus\nVoxForge, Japanese CSJ, Hkust\n12\nxinjli/allosaurus\n\n\nLi etal. (2020, 2021)\nTedlium, Switchboard etc\n\n\n\nAllophant\nCommon Voice 10.0\n34\nkgnlp/allophant\n\n\nGlocker etal. (2023)\n\n\n\nWav2Vec2Phoneme\nMLS, Common Voice,\n40+\nfacebook/wav2vec2-xlsr-53-espeak-cv-ft\n\n\nXu etal. (2022)\nBabel\n\n\nMultIPA\nCommon Voice 11.0\n7\nctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\n\n\nTaguchi etal. (2023)\n\n\nZIPA\nIPAPack++\n88\nlingjzhu/zipa\n\n\nZhu etal. (2025)\nMMS ulab v2., VoxLingua-107 (Pseudo-label)\nanyspeech/zipa-large-crctc-ns-800k\n\n\nASR baselines\n\n\n\n\n\nOWSM-CTC v4\nOWSM v3.2, YODAS\n100+\nespnet/owsm_ctc_v4_1B\n\n\nPeng etal. (2025)\n\n\nOWLS\nOWSM v3.2, YODAS\n150\nespnet/owls-scaling-laws-for-speech-recognition-and-translation\n\n\nChen etal. (2025)",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Training Data Sources</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Language Coverage</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model checkpoint / GitHub</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">PR baselines</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Allosaurus</span></td>\n<td class=\"ltx_td ltx_align_left\">VoxForge, Japanese CSJ, Hkust</td>\n<td class=\"ltx_td ltx_align_center\" rowspan=\"2\">12</td>\n<td class=\"ltx_td ltx_align_left\"><a class=\"ltx_ref ltx_href\" href=\"https://github.com/xinjli/allosaurus\" title=\"\">xinjli/allosaurus</a></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib28\" title=\"\">2020</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib29\" title=\"\">2021</a>)</cite></td>\n<td class=\"ltx_td ltx_align_left\">Tedlium, Switchboard etc</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Allophant</span></td>\n<td class=\"ltx_td ltx_align_left\" rowspan=\"2\">Common Voice 10.0</td>\n<td class=\"ltx_td ltx_align_center\" rowspan=\"2\">34</td>\n<td class=\"ltx_td ltx_align_left\"><a class=\"ltx_ref ltx_href\" href=\"https://github.com/kgnlp/allophant\" title=\"\">kgnlp/allophant</a></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><cite class=\"ltx_cite ltx_citemacro_cite\">Glocker et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib14\" title=\"\">2023</a>)</cite></td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Wav2Vec2Phoneme</span></td>\n<td class=\"ltx_td ltx_align_left\">MLS, Common Voice,</td>\n<td class=\"ltx_td ltx_align_center\" rowspan=\"2\">40+</td>\n<td class=\"ltx_td ltx_align_left\" rowspan=\"2\"><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/facebook/wav2vec2-xlsr-53-espeak-cv-ft\" title=\"\">facebook/wav2vec2-xlsr-53-espeak-cv-ft</a></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><cite class=\"ltx_cite ltx_citemacro_cite\">Xu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib58\" title=\"\">2022</a>)</cite></td>\n<td class=\"ltx_td ltx_align_left\">Babel</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">MultIPA</span></td>\n<td class=\"ltx_td ltx_align_left\" rowspan=\"2\">Common Voice 11.0</td>\n<td class=\"ltx_td ltx_align_center\" rowspan=\"2\">7</td>\n<td class=\"ltx_td ltx_align_left\" rowspan=\"2\"><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\" title=\"\">ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns</a></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><cite class=\"ltx_cite ltx_citemacro_cite\">Taguchi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib51\" title=\"\">2023</a>)</cite></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">ZIPA</span></td>\n<td class=\"ltx_td ltx_align_left\">IPAPack++</td>\n<td class=\"ltx_td ltx_align_center\" rowspan=\"2\">88</td>\n<td class=\"ltx_td ltx_align_left\"><a class=\"ltx_ref ltx_href\" href=\"https://github.com/lingjzhu/zipa\" title=\"\">lingjzhu/zipa</a></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite></td>\n<td class=\"ltx_td ltx_align_left\">MMS ulab v2., VoxLingua-107 (Pseudo-label)</td>\n<td class=\"ltx_td ltx_align_left\"><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/anyspeech/zipa-large-crctc-ns-800k\" title=\"\">anyspeech/zipa-large-crctc-ns-800k</a></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">ASR baselines</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">OWSM-CTC v4</span></td>\n<td class=\"ltx_td ltx_align_left\" rowspan=\"2\">OWSM v3.2, YODAS</td>\n<td class=\"ltx_td ltx_align_center\" rowspan=\"2\">100+</td>\n<td class=\"ltx_td ltx_align_left\" rowspan=\"2\"><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/espnet/owsm_ctc_v4_1B\" title=\"\">espnet/owsm_ctc_v4_1B</a></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><cite class=\"ltx_cite ltx_citemacro_cite\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib37\" title=\"\">2025</a>)</cite></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">OWLS</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" rowspan=\"2\">OWSM v3.2, YODAS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" rowspan=\"2\">150</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" rowspan=\"2\"><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/collections/espnet/owls-scaling-laws-for-speech-recognition-and-translation-67ab7f991c194065f057ce8d\" title=\"\">espnet/owls-scaling-laws-for-speech-recognition-and-translation</a></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib6\" title=\"\">2025</a>)</cite></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "sources",
            "training",
            "peng",
            "owls",
            "overview",
            "etc",
            "facebookwav2vec2xlsr53espeakcvft",
            "owsm",
            "our",
            "anyspeechzipalargecrctcns800k",
            "voice",
            "hkust",
            "ctaguchiwav2vec2largexlsrjaplmthufieltaipa1000ns",
            "csj",
            "pseudolabel",
            "work",
            "wav2vec2phoneme",
            "lingjzhuzipa",
            "allophant",
            "allosaurus",
            "common",
            "xinjliallosaurus",
            "mms",
            "language",
            "japanese",
            "espnetowsmctcv41b",
            "model",
            "ipapack",
            "voxlingua107",
            "checkpoint",
            "github",
            "switchboard",
            "espnetowlsscalinglawsforspeechrecognitionandtranslation",
            "kgnlpallophant",
            "owsmctc",
            "ulab",
            "chen",
            "asr",
            "tedlium",
            "taguchi",
            "zhu",
            "multipa",
            "coverage",
            "mls",
            "v32",
            "babel",
            "zipa",
            "data",
            "yodas",
            "baselines",
            "glocker",
            "voxforge"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We provide the baselines&#8217; training data source, number of languages covered in the data, and links to model checkpoints or repository in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.T9\" title=\"Table 9 &#8227; A.2 Baseline Implementation &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;9</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in spoken language processing have led to substantial progress in phonetic tasks such as automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G). Despite their conceptual similarity, these tasks have largely been studied in isolation, each relying on task-specific architectures and datasets.\nIn this paper, we introduce POWSM (Phonetic Open Whisper-style Speech Model), the first unified framework capable of jointly performing multiple phone-related tasks.\nPOWSM enables seamless conversion between audio, text (graphemes), and phones, opening up new possibilities for universal and low-resource speech processing.\nOur model outperforms or matches specialized PR models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P, P2G, and ASR.\nOur training data, code<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/espnet</span></span></span> and models<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://huggingface.co/espnet/powsm</span></span></span> are released to foster open science.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "training",
                    "wav2vec2phoneme",
                    "zipa",
                    "data",
                    "our",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Four key phone-related tasks underpin phonetic spoken language processing: automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G).\n<span class=\"ltx_text ltx_font_italic\">ASR</span> learns implicit phonetic representations <cite class=\"ltx_cite ltx_citemacro_cite\">Belinkov and Glass (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib4\" title=\"\">2017</a>)</cite>, while <span class=\"ltx_text ltx_font_italic\">PR</span> offers explicit phone-level supervision.\n<span class=\"ltx_text ltx_font_italic\">G2P</span> and <span class=\"ltx_text ltx_font_italic\">P2G</span> bridge orthographic and phonetic spaces.\nCollectively, these tasks interact through shared phonetic representations, each addressing a different aspect of the relationship between audio, phones, phonemes, and graphemes.</p>\n\n",
                "matched_terms": [
                    "language",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge this gap, we propose POWSM, a phonetic foundation model capable of performing four core phone-related tasks &#8212; PR, ASR, audio-guided G2P, and audio-guided P2G &#8212; within one unified architecture (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#S1.F1\" title=\"In 1 Introduction &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nTo construct this framework, we reformulate standard ASR datasets <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> into four task-specific formats, allowing the model to learn consistent mappings across audio, phoneme, and grapheme representations.\nIn addition, POWSM adopts an attention-based encoder-decoder (AED) architecture, following the design of large-scale speech foundation models such as Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite> and OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib39\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "zhu",
                    "peng",
                    "owsm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirically, POWSM outperforms previous PR models on both in-domain data and out-of-domain languages, achieves low-resource ASR performance comparable to web-scale multilingual foundation models, and can act as speech-grounded P2G and G2P across more than 70 languages.\nPOWSM offers a new unified paradigm for phone-level modeling, paving the way for inclusive and globally accessible speech technologies that transcend language boundaries and resource disparities.</p>\n\n",
                "matched_terms": [
                    "language",
                    "data",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide POWSM, a large-scale foundation model that achieves state-of-the-art PR performance, and is capable of performing multiple fundamental phone-related tasks. Our model enables seamless conversion between speech, text (graphemes/orthography), and phones.</p>\n\n",
                "matched_terms": [
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fully open-source all our data preparation and evaluation scripts, model checkpoints and code to foster open science.</p>\n\n",
                "matched_terms": [
                    "data",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent speech foundation models such as Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite> and OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib39\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib38\" title=\"\">2024</a>)</cite> have driven progress in large-scale multilingual ASR and speech translation, but they do not explicitly address phoneme recognition or articulatory-level supervision.\nSubsequent work <cite class=\"ltx_cite ltx_citemacro_citep\">(Yusuyin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib60\" title=\"\">2025</a>; Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib11\" title=\"\">2025</a>)</cite> showed that incorporating phoneme-level objectives improves ASR for low-resource and long-tailed settings, while outputting phonemes as an intermediate benefited speech translation <cite class=\"ltx_cite ltx_citemacro_citep\">(G&#225;llego et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib18\" title=\"\">2025</a>)</cite>.\nPOWSM extends this line of work by being the first open foundation model jointly trained on phone recognition and related tasks, integrating multilinguality, phonetic supervision, and multi-task scalability within one framework.</p>\n\n",
                "matched_terms": [
                    "model",
                    "peng",
                    "work",
                    "owsm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior work in multilingual phone recognition can broadly be categorized into (1) language-specific models <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib13\" title=\"\">2021</a>)</cite> that rely on explicit phoneme <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib58\" title=\"\">2022</a>)</cite> or allophone inventories <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib28\" title=\"\">2020</a>)</cite> and (2) language-agnostic approaches that aim to generalize across languages without such resources <cite class=\"ltx_cite ltx_citemacro_citep\">(Taguchi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib51\" title=\"\">2023</a>; Glocker et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib14\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib29\" title=\"\">2021</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>.\nPOWSM follows the latter paradigm as a fully data-driven multilingual model that learns phone representations without predefined phoneme mappings.</p>\n\n",
                "matched_terms": [
                    "taguchi",
                    "model",
                    "zhu",
                    "work",
                    "glocker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">WhisperPPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Samir et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib47\" title=\"\">2025</a>)</cite> improved Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>&#8217;s performance through data cleaning but remained limited in data coverage and task diversity.\nHowever, Whisper is trained on a closed corpus and could display harmful biases for PR which cannot be fully removed by fine-tuning.\nPOWSM is trained from scratch on open datasets.</p>\n\n",
                "matched_terms": [
                    "coverage",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ZIPA <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> scaled PR to 17,000+ hours of data and 88 languages using a Zipformer <cite class=\"ltx_cite ltx_citemacro_cite\">Yao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib59\" title=\"\">2024</a>)</cite> encoder and noisy-student training on 4,000+ languages, achieving state-of-the-art results. To construct its training corpus, ZIPA employed a G2P system to convert large-scale ASR transcriptions into phoneme sequences, effectively repurposing ASR datasets for PR. Building on this idea, POWSM leverages both the grapheme and the G2P-generated phoneme transcriptions, reformulating them into four task-specific forms: ASR, PR, G2P, and P2G.</p>\n\n",
                "matched_terms": [
                    "zhu",
                    "training",
                    "zipa",
                    "data",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">G2P &amp; P2G</span> POWSM is the first model capable of both audio-guided G2P and audio-guided P2G.\nG2P conversion, sometimes called phonemization in the text-to-speech literature, can be accomplished with pronunciation dictionaries <cite class=\"ltx_cite ltx_citemacro_citep\">(Rudnicky, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib46\" title=\"\">1993</a>)</cite>, rules <cite class=\"ltx_cite ltx_citemacro_citep\">(Mortensen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib33\" title=\"\">2018</a>)</cite>, WFSTs <cite class=\"ltx_cite ltx_citemacro_citep\">(Black and Lenzo, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib5\" title=\"\">2001</a>)</cite>, or seq2seq neural methods to choose between different pronunciations of a word in context <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib65\" title=\"\">2022</a>)</cite>.\nText-based G2P, however, still cannot handle phonetic variation, enforcing a one-to-one mapping between orthography and transcription.\nIn contrast, audio-guided G2P can learn to map the different acoustic realizations of a phoneme across varieties of a language to a phone representation <cite class=\"ltx_cite ltx_citemacro_cite\">Route et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib45\" title=\"\">2019</a>)</cite>.\nIn particular, <cite class=\"ltx_cite ltx_citemacro_citet\">Mak et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib32\" title=\"\">2025</a>)</cite> observed a performance improvement in using audio-guided G2P versus text-based G2P alone for Cantonese.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Gao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib12\" title=\"\">2024</a>)</cite> similarly showed that joint learning of G2P, phone recognition, and forced alignment outperform a G2P teacher model. Similarly, <cite class=\"ltx_cite ltx_citemacro_citet\">Sun and Richmond (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib50\" title=\"\">2024</a>)</cite> jointly learned G2P and TTS.\nCompared to G2P, P2G conversion is less studied, with <cite class=\"ltx_cite ltx_citemacro_citet\">Lauc (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib25\" title=\"\">2024</a>)</cite> training a seq2seq model on 19 million language-grapheme-phoneme triplets.</p>\n\n",
                "matched_terms": [
                    "language",
                    "zhu",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use IPAPack++ <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> for training. It is an open source corpus of roughly 17,000 hours of multilingual speech with paired orthographic and phonemic transcriptions. We will release all data processing scripts to make POWSM fully reproducible.</p>\n\n",
                "matched_terms": [
                    "data",
                    "zhu",
                    "training",
                    "ipapack"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model is trained on four tasks: PR, ASR, and audio-guided G2P and P2G. Each utterance is used once per task, with task-specific formatting as illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#S1.F1\" title=\"In 1 Introduction &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, including a text prompt, language token, task token, and target output. We leave the text prompt blank (token <span class=\"ltx_text ltx_font_typewriter\">&lt;na&gt;</span>) for PR and ASR, and provide graphemes and phones as prompts for G2P and P2G.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "our",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">POWSM adopts an attention-based encoder-decoder (AED) architecture, which flexibly models output sequences and allows the integration of additional tasks.\nSpecifically, we follow the OWSM v3.1 architecture <cite class=\"ltx_cite ltx_citemacro_cite\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib38\" title=\"\">2024</a>)</cite>, which employs an E-Branchformer encoder and a Transformer decoder, consistent with the general encoder-decoder structure of Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>. The model is trained from scratch using ESPnet <cite class=\"ltx_cite ltx_citemacro_cite\">Watanabe et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib56\" title=\"\">2018</a>)</cite> with a hybrid CTC/attention loss <cite class=\"ltx_cite ltx_citemacro_cite\">Watanabe et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib57\" title=\"\">2017</a>)</cite>, where we set the ratio <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> to 0.3:</p>\n\n",
                "matched_terms": [
                    "owsm",
                    "model",
                    "peng"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The encoder operates at the stride size of 40ms. Training uses a global batch size of 256. Speech inputs are 16kHz and padded to 20 seconds. The vocabulary consists of 40k tokens, including around 6k phone tokens, language and timestamp tokens, and BPE tokens from orthography.\nThe model has approximately 350M parameters with 9 layers for both the encoder and decoder and was trained on 4 H100 GPUs for 2 days.\nUsing a CTC loss <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib16\" title=\"\">2006</a>)</cite>, We align the encoder outputs with a simplified version of the phone token sequences. Unlike the decoder outputs, the phones in these sequences are stripped of break (<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/./, <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/*&#865;/)\nand length diacritics (<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/e<span class=\"ltx_ERROR undefined\">\\textlengthmark</span>/, /e<span class=\"ltx_ERROR undefined\">\\texthalflength</span>/, /&#283;/) to accelerate convergence. Additional details and analyses are provided in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.SS1.SSS0.Px1\" title=\"The CTC encoder prefers fine-grained phones without suprasegmentals &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;6.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike Phone Error Rate (PER), which considers only exact phone matches, or Phone Token Error Rate (PTER), which treats diacritics and modifiers as separate tokens, PFER computes the edit distance in terms of articulatory features&#8212;interpretable subphone attributes (<span class=\"ltx_text ltx_font_italic\">e.g.</span> voicing)&#8212;capturing phonetic similarity in a fine-grained fashion.\nPrevious studies <cite class=\"ltx_cite ltx_citemacro_cite\">Taguchi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib51\" title=\"\">2023</a>); Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> define PFER as the mean articulatory feature edit distance over the evaluation set. In contrast, we normalize it by the number of phones in the reference transcription to measure the proportion of feature errors per phone.</p>\n\n",
                "matched_terms": [
                    "taguchi",
                    "zhu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For unseen languages, we evaluate on three datasets: DoReCo <cite class=\"ltx_cite ltx_citemacro_cite\">Paschen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib36\" title=\"\">2020</a>)</cite>, VoxAngeles <cite class=\"ltx_cite ltx_citemacro_cite\">Chodroff et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>)</cite>, and Tusom2021 <cite class=\"ltx_cite ltx_citemacro_cite\">Mortensen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib35\" title=\"\">2021</a>)</cite>.\nDoReCo is a dataset of 50+ languages (with broad transcriptions) intended for documentation of small or endangered languages; we use a 45-language subset.\nVoxAngeles <cite class=\"ltx_cite ltx_citemacro_citep\">(Chodroff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>)</cite> is a postprocessed version of the UCLA Phonetics Lab Archive <cite class=\"ltx_cite ltx_citemacro_citep\">(Ladefoged et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib24\" title=\"\">2009</a>)</cite> containing 95 languages.\nTusom is a low-data Tangkhulic language of India not included in the training data. Tusom2021 consists of narrow phonetic transcriptions (unlike the broad transcriptions from G2P on which POWSM was trained) of individual Tusom words.\nWe removed the tones.</p>\n\n",
                "matched_terms": [
                    "language",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then evaluated our model on in-domain data from IPAPack++, the dataset seen during training. We followed <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> in using LibriSpeech for English, AISHELL for Mandarin, and MLS for European languages, and additionally evaluated on IISc-MILE Tamil <cite class=\"ltx_cite ltx_citemacro_cite\">A et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib1\" title=\"\">2022</a>)</cite> for Tamil and KSC <cite class=\"ltx_cite ltx_citemacro_cite\">Khassanov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib22\" title=\"\">2021</a>)</cite> for Kazakh.\nFor ASR and P2G, we evaluate with FLEURS.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "ipapack",
                    "zhu",
                    "mls",
                    "data",
                    "our",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate all PR baselines without further training with IPAPack++. See Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS2\" title=\"A.2 Baseline Implementation &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.2</span></a> for more details about training data and language coverage.\nAllosaurus <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib28\" title=\"\">2020</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib29\" title=\"\">2021</a>)</cite> uses a phone-level CTC to train a language-agnostic model and applies language-specific allophone-to-phoneme mappings.\nWav2Vec2Phoneme <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib58\" title=\"\">2022</a>)</cite>, MultIPA <cite class=\"ltx_cite ltx_citemacro_cite\">Taguchi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib51\" title=\"\">2023</a>)</cite> and Allophant <cite class=\"ltx_cite ltx_citemacro_cite\">Glocker et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib14\" title=\"\">2023</a>)</cite> fine-tune XLS-R <cite class=\"ltx_cite ltx_citemacro_citep\">(Babu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib2\" title=\"\">2022</a>)</cite> with different objectives: Wav2Vec2Phoneme maps unseen phonemes using articulatory features, MultIPA leverages high-quality G2P data from seven languages, while Allophant decomposes phones into articulatory features and applies CTC losses for each.\nZIPA <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> trains ZipFormer <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib59\" title=\"\">2024</a>)</cite> from scratch on IPAPack++ using CR-CTC and also provides a variant trained with additional pseudo-labeled data (&#8220;ZIPA-CR-NS-Large&#8221;).</p>\n\n",
                "matched_terms": [
                    "language",
                    "taguchi",
                    "model",
                    "training",
                    "ipapack",
                    "zhu",
                    "multipa",
                    "coverage",
                    "wav2vec2phoneme",
                    "allophant",
                    "zipa",
                    "data",
                    "baselines",
                    "glocker",
                    "allosaurus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR, we compare POWSM with two series of models: OWSM <cite class=\"ltx_cite ltx_citemacro_cite\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib37\" title=\"\">2025</a>)</cite> and OWLS <cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib6\" title=\"\">2025</a>)</cite>. We select OWSM-CTC v4 because it is the best-performing model in the series, featuring an encoder-CTC architecture that supports ASR, ST, and LID. For OWLS, we include models with comparable parameter sizes.</p>\n\n",
                "matched_terms": [
                    "model",
                    "peng",
                    "owls",
                    "owsmctc",
                    "owsm",
                    "chen",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We found that POWSM&#8217;s performance on PR and ASR tasks is comparable or superior to competitive baselines.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T2\" title=\"Table 2 &#8227; POWSM excels at in-domain phone recognition &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>, we see that POWSM achieves the lowest average PFER in phone recognition, due to the strong language modeling capability of the decoder.\nWe hypothesize that our English data cleaning (Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS1\" title=\"A.1 Refining English G2P &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.1</span></a>) may have negatively affected the PFER for Germanic languages due to a mismatch between training and test data.\nNevertheless, our approach fills this gap by achieving strong performance on other languages, outperforming models trained on larger datasets.</p>\n\n",
                "matched_terms": [
                    "language",
                    "training",
                    "our",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hypothesize that pre-training with phone recognition benefits low-resource ASR <cite class=\"ltx_cite ltx_citemacro_cite\">Yusuyin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib60\" title=\"\">2025</a>)</cite>.\nTo choose low-resource languages, we selected languages in IPAPack++ with less than 8 hours of speech in FLEURS to serve as the test set.\nSee <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS3\" title=\"A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.3</span></a> for details on the amount of data used by different models.\nFor a fair comparison with other multilingual ASR baselines without language-specific components, we use the same decoding hyperparameters <span class=\"ltx_text ltx_font_typewriter\">ctc=0.0, beam=1</span>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "baselines",
                    "ipapack",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T3\" title=\"Table 3 &#8227; POWSM is comparable with web-scale ASR models on low-resource languages &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;3</span></a>, POWSM (<span class=\"ltx_text ltx_font_typewriter\">POWSM 0.35B, ASR</span>) is often comparable to models of similar size trained on web-scale data for ASR (<span class=\"ltx_text ltx_font_typewriter\">OWLS 0.5B</span>).\nIncorporating phones obtained from PR as text prompts (PR-P2G) significantly decreases WER, making it comparable to or even better than these models.\nWhen using gold phone labels for P2G (see analysis in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.SS2.SSS0.Px2\" title=\"Audio-P2G effectively handles low-resource languages &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;6.2</span></a>), POWSM outperforms other ASR models by a large margin in most cases.</p>\n\n",
                "matched_terms": [
                    "data",
                    "owls",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T4\" title=\"Table 4 &#8227; 5.2 POWSM generalizes well to unseen languages &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a> reports PFER on datasets with unseen languages and language variation.\nResults indicate that POWSM achieves strong performance across these datasets, and handles both dialectal and L2 variation effectively.\nNotably, our method outperforms ZIPA trained on the same data and even exceeds ZIPA trained with extra pseudo-labeled data, achieving the best results on unseen languages while performing three additional tasks.\nThis shows the effectiveness of our multi-task approach.\nWhile POWSM lags behind Wav2Vec2Phoneme on socio-phonetic variations, we attribute this to its self-supervised learning with over 60k hours of speech (from wav2vec 2.0 <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib3\" title=\"\">2020</a>)</cite>) prior to the supervised learning stage.</p>\n\n",
                "matched_terms": [
                    "language",
                    "wav2vec2phoneme",
                    "zipa",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We run small-scale experiments on a 1k-hour subset of the multi-task data (250 hours of speech repeated across four tasks).\nWe use the validation CER of the encoder-CTC output as a proxy for training efficiency.\nAn earlier drop indicates that the encoder is learning a useful alignment early, which improves representations fed into the decoder and accelerates overall convergence.\nIn <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.F2\" title=\"Figure 2 &#8227; The CTC encoder prefers fine-grained phones without suprasegmentals &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;2</span></a>, PanPhon tokenization without suprasegmentals shows the earliest drop, suggesting that alignment with decoder units aids training, while\ncollapsing suprasegmental distinctions for CTC reduces confusion.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As in other encoder-decoder models <cite class=\"ltx_cite ltx_citemacro_cite\">Gong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib15\" title=\"\">2023</a>); Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>, we expect the encoder of POWSM to capture more general acoustic patterns, while the decoder handles language and task-specific output formats. Therefore,\nwe investigate whether emphasizing the encoder more during different stages of model development affects performance.\nTo balance data diversity with inference compute costs, we selected two smaller datasets from each category with distinct characteristics.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T5\" title=\"Table 5 &#8227; Increased encoder weights benefit PR on out-of-domain data &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;5</span></a>, higher CTC decoding weights improve PR performance on out-of-domain data but degrade it on in-domain data, as expected.\nThis echoes <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>&#8217;s finding that RNN-T <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves and Jaitly, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib17\" title=\"\">2014</a>)</cite>, an encoder-only speech-to-text model with an autoregressive text prediction network, hurts generalization to unseen patterns of phones (phonotactics).\nWe hypothesize that the decoder is performing implicit language modeling and &#8220;smooths&#8221; phonetic variation, as <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> described.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "zhu",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, we examine whether focusing more on the CTC loss through training widens this gap in performance between in-domain and out-of-domain data.\nWe find that fine-tuning with a higher CTC loss weight <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> after convergence does not improve out-of-domain performance and can even degrade it. Randomly varying <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> for each batch also shows no improvement.\nIn contrast, training with a higher <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> from the start benefits the out-of-domain distribution, achieving the lowest PFER on unseen languages with greedy decoding, while the PFER on in-domain data is comparatively higher.\nThese results suggest that assigning a higher weight to the encoder during training and inference improves PR, highlighting a common trade-off between in-domain performance and generalization.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training",
                    "common"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand how POWSM integrates speech and text prompts, we analyze the relative influence of speech and text prompts in its G2P behavior.\nWe vary the G2P conditions from purely speech-based to purely text-based, as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T6\" title=\"Table 6 &#8227; Increased encoder weights benefit PR on out-of-domain data &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;6</span></a>, and evaluate the model on the Buckeye dataset.\nWhen only speech is provided, the performance is comparable to the PR setting, which differs only in the task token.\nAdding both speech and text prompts (the standard G2P setup) leads to degraded performance, with output showing standardized pronunciations.\nWhen the model relies solely on the text prompt, performance drops sharply and pronunciations become highly standardized as expected (just as <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> reported).</p>\n\n",
                "matched_terms": [
                    "model",
                    "zhu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test this, we run P2G with the language code set to English and post-process the output to match Cyrillic or Gurmukhi transcriptions with online conversion tools for certain languages.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.lexilogos.com\" title=\"\">https://www.lexilogos.com</a> for Macedonian and Tajik; <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://punjabi.indiatyping.com\" title=\"\">https://punjabi.indiatyping.com</a> for Panjabi.</span></span></span>\nThis approach often outperforms ASR and sometimes approaches P2G&#8217;s performance, indicating that P2G also relies heavily on speech input.\nLanguages with either comparibly low or high PFER did not benefit from this transliteration approach, possibly because the model already handled them well or had not yet learned them sufficiently.\nThis finding suggests a direction for further investigation in low-resource ASR.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The language identification (LID) performance of POWSM on seen languages in FLEURS reaches 92.3% accuracy, as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.F3\" title=\"Figure 3 &#8227; A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;3</span></a>.\nTo see if the model implicitly learns phonotactic patterns and associates them with the language token, we evaluate PR on unseen languages by manipulating the language token at inference time.\nFor VoxAngeles and Tusom2021, the three most frequently assigned languages are Bashkir (42.6%, 25.1%), English (30.2%, 67.7%), and Kinyarwanda (14.5%, 2.3%), which are all relatively high-resource languages in IPAPack++.\n<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T8\" title=\"Table 8 &#8227; Language token captures phonotactics &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;8</span></a> shows that assigning the detected language token yields better performance than always using English, while setting the language as unknown performs best. This indicates that the language token influences PR by shifting the output distribution toward the assigned language.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "ipapack"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train a fully open-source phonetic speech foundation model POWSM using our scalable multi-task framework.\nOur model achieves state-of-the-art performance on PR while also supporting ASR across more than 70 languages.\nBeyond PR and ASR, the model&#8217;s ability to perform audio-guided G2P and P2G enables applications that require fine-grained linguistic analysis such as atypical speech assessment.\nOur analysis reveals that POWSM&#8217;s encoder benefits from phoneme-level CTC supervision and stronger encoder weighting, enhancing cross-lingual generalization.\nAdditionally, the model demonstrates interpretable multimodal and language-aware behaviors, effectively mediating between phonetic detail and standardized phonological patterns.\nTo conclude, POWSM not only provides a strong phone recognition foundation model for high-resource languages, but also acts as a versatile resource for unseen languages and socio-phonetic variation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "our",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">POWSM&#8217;s current decoder serves as a large phoneme-level phonotactic language model on which linguists could investigate hypotheses about phonetic universals <cite class=\"ltx_cite ltx_citemacro_citep\">(Chodroff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>; Chodroff, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib7\" title=\"\">2025</a>)</cite> and phonotactics <cite class=\"ltx_cite ltx_citemacro_citep\">(Shim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib48\" title=\"\">2024</a>; Pimentel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib40\" title=\"\">2020</a>)</cite>.\nIn the future, we seek to adapt to socio-phonetic variation either through (unsupervised) test-time adaptation <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib30\" title=\"\">2022</a>)</cite>, in-context learning <cite class=\"ltx_cite ltx_citemacro_citep\">(Roll et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib44\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib55\" title=\"\">2024</a>)</cite>, or mechanistic interpretability <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib52\" title=\"\">2024</a>)</cite>.\nFurthermore, since <cite class=\"ltx_cite ltx_citemacro_citet\">Shim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib49\" title=\"\">2025</a>)</cite> found that earlier encoder layers in Whisper preserve more phonetic detail, early exiting may mitigate the decoder&#8217;s tendencies to normalize socio-phonetic variation.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">POWSM has several limitations that we aim to address in future work.\nFirst, the model is neither strictly phonemic nor phonetic: its training data consist of cleaned and filtered phonemic transcriptions from multiple languages, which are not fully faithful to the phonetic or phonemic structure of the audio. Although phonemic transcriptions share similarities across languages, adding auxiliary tasks and language tokens may have reinforced language-specific biases. We also currently lack sufficient allophone-level data, which would provide more language-independent information.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "training",
                    "work",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, the model still favors high-resource languages. Since we include a decoder for language modeling and language tokens, both of which function effectively, the model would inherently bias toward the seen distribution.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All of our data is ethically sourced, either through permissive licensing or through proper consent.\nWe are aware of the implicit prescriptivism and representational harms <cite class=\"ltx_cite ltx_citemacro_citep\">(Crawford, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib10\" title=\"\">2017</a>)</cite> that normalizing socio-phonetic variation in ASR or PR models can create.\nThis may threaten linguistic diversity instead of preserving it.\nWe also acknowledge that accurate modeling of socio-phonetic variation can enable demographic inference, as demographics and phonetic variation are deeply intertwined <cite class=\"ltx_cite ltx_citemacro_citep\">(Labov, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib23\" title=\"\">1963</a>)</cite>.\nWe stress that uses of POWSM must align with our vision: a future where advances in spoken language processing and NLP do not leave low-resource varieties behind.</p>\n\n",
                "matched_terms": [
                    "language",
                    "data",
                    "our",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observed confusion in plosive voice-onset times on unseen languages in preliminary experiments, which is likely from English G2P data. For instance, broad phonemic transcription in English typically uses /b/ to transcribe the /b/ in /bat/, but its voice onset timing is actually voiceless in Mainstream American English and is closer to [p]. To mitigate this, we apply rule-based refinements to English G2P transcriptions, adjusting plosive voicing and aspiration, lateral velarization, and vowel nasalization.</p>\n\n",
                "matched_terms": [
                    "data",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first filter out languages with more than 8 hours of training data in IPAPack++ <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>, keeping only those that are also present in FLEURS.\nThen, following the training data amounts reported in <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib6\" title=\"\">2025</a>)</cite>, we further identify the 50 lowest-resource languages to exclude any that may have other substantial sources not included in IPAPack++.\nThis process leaves us with nine languages. We finally exclude <span class=\"ltx_text ltx_font_typewriter\">ell</span>, as it is comparatively higher-resource and because there are already three other Balto-Slavic languages.\nNote that other models use strictly more data than ours&#8212;not only in terms of dataset count but also because IPAPack++ applies additional data-quality filtering. <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.T10\" title=\"Table 10 &#8227; A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;10</span></a> lists the amount of ASR training data for baselines.</p>\n\n",
                "matched_terms": [
                    "sources",
                    "training",
                    "ipapack",
                    "zhu",
                    "data",
                    "baselines",
                    "chen",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multi-tasking may improve performance by tying acoustic signals to well-defined symbolic representations, yet it may distract the model if the relationships are not learned effectively.\nWe train POWSM with different data and model scales to examine how multitask learning interacts with the setup, and use <span class=\"ltx_text ltx_font_typewriter\">beam=1</span> during decoding to speed up inference.</p>\n\n",
                "matched_terms": [
                    "data",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.T11\" title=\"Table 11 &#8227; A.4 Multi-tasking at Different Scales &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;11</span></a> shows that there is no clear trend regarding whether multitasking benefits PR performance. PR performance degrades when the model has excessive capacity relative to the available data (too little data), or when it is limited by size (too much data).</p>\n\n",
                "matched_terms": [
                    "data",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Further evidence is needed before concluding that phoneme recognition benefits less from scaling, as we currently lack sufficient data and large model capacity to test this thoroughly. Nevertheless, the model demonstrates the ability to multitask, which represents a promising direction for future work.</p>\n\n",
                "matched_terms": [
                    "data",
                    "work",
                    "model"
                ]
            }
        ]
    },
    "A1.T10": {
        "source_file": "POWSM: A Phonetic Open Whisper-Style Speech Foundation Model",
        "caption": "Table 10: Amount of ASR training data for languages included in ASR comparison (in hours), according to Zhu etal. (2025) and Chen etal. (2025).",
        "body": "Afroasiatic\nTurkic\nIndo-Iranian\nBalto-Slavic\n\n\nModel\nafr\norm\naze\npan\ntgk\nmkd\nbos\nslv\n\n\n\n\nPOWSM\n2.71\n5.11\n6.89\n4.96\n6.52\n5.14\n7.57\n7.19\n\n\nOWSM-CTC v4\n5.54\n6.50\n10.69\n8.30\n8.03\n8.4\n9.96\n26.00\n\n\n\n\nOWLS",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding:2pt 4.0pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding:2pt 4.0pt;\">Afroasiatic</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:2pt 4.0pt;\">Turkic</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding:2pt 4.0pt;\">Indo-Iranian</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" style=\"padding:2pt 4.0pt;\">Balto-Slavic</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" style=\"padding:2pt 4.0pt;\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">afr</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">orm</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">aze</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">pan</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">tgk</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">mkd</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">bos</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:2pt 4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">slv</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:2pt 4.0pt;\">POWSM</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">2.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">5.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">6.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">4.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">6.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">5.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">7.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:2pt 4.0pt;\">7.19</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:2pt 4.0pt;\">OWSM-CTC v4</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" rowspan=\"2\" style=\"padding:2pt 4.0pt;\">5.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" rowspan=\"2\" style=\"padding:2pt 4.0pt;\">6.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" rowspan=\"2\" style=\"padding:2pt 4.0pt;\">10.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" rowspan=\"2\" style=\"padding:2pt 4.0pt;\">8.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" rowspan=\"2\" style=\"padding:2pt 4.0pt;\">8.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" rowspan=\"2\" style=\"padding:2pt 4.0pt;\">8.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" rowspan=\"2\" style=\"padding:2pt 4.0pt;\">9.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" rowspan=\"2\" style=\"padding:2pt 4.0pt;\">26.00</td>\n</tr>\n</tbody>\n<tfoot class=\"ltx_tfoot\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:2pt 4.0pt;\">OWLS</th>\n</tr>\n</tfoot>\n</table>\n\n",
        "informative_terms_identified": [
            "turkic",
            "training",
            "baltoslavic",
            "pan",
            "owls",
            "orm",
            "hours",
            "powsm",
            "afr",
            "mkd",
            "bos",
            "slv",
            "aze",
            "model",
            "according",
            "owsmctc",
            "indoiranian",
            "chen",
            "asr",
            "zhu",
            "amount",
            "afroasiatic",
            "languages",
            "data",
            "tgk",
            "comparison",
            "included"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We first filter out languages with more than 8 hours of training data in IPAPack++ <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>, keeping only those that are also present in FLEURS.\nThen, following the training data amounts reported in <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib6\" title=\"\">2025</a>)</cite>, we further identify the 50 lowest-resource languages to exclude any that may have other substantial sources not included in IPAPack++.\nThis process leaves us with nine languages. We finally exclude <span class=\"ltx_text ltx_font_typewriter\">ell</span>, as it is comparatively higher-resource and because there are already three other Balto-Slavic languages.\nNote that other models use strictly more data than ours&#8212;not only in terms of dataset count but also because IPAPack++ applies additional data-quality filtering. <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.T10\" title=\"Table 10 &#8227; A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;10</span></a> lists the amount of ASR training data for baselines.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in spoken language processing have led to substantial progress in phonetic tasks such as automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G). Despite their conceptual similarity, these tasks have largely been studied in isolation, each relying on task-specific architectures and datasets.\nIn this paper, we introduce POWSM (Phonetic Open Whisper-style Speech Model), the first unified framework capable of jointly performing multiple phone-related tasks.\nPOWSM enables seamless conversion between audio, text (graphemes), and phones, opening up new possibilities for universal and low-resource speech processing.\nOur model outperforms or matches specialized PR models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P, P2G, and ASR.\nOur training data, code<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/espnet</span></span></span> and models<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://huggingface.co/espnet/powsm</span></span></span> are released to foster open science.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "powsm",
                    "data",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">POWSM: A Phonetic Open Whisper-Style Speech Foundation Model</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge this gap, we propose POWSM, a phonetic foundation model capable of performing four core phone-related tasks &#8212; PR, ASR, audio-guided G2P, and audio-guided P2G &#8212; within one unified architecture (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#S1.F1\" title=\"In 1 Introduction &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nTo construct this framework, we reformulate standard ASR datasets <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> into four task-specific formats, allowing the model to learn consistent mappings across audio, phoneme, and grapheme representations.\nIn addition, POWSM adopts an attention-based encoder-decoder (AED) architecture, following the design of large-scale speech foundation models such as Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite> and OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib39\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "model",
                    "zhu",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirically, POWSM outperforms previous PR models on both in-domain data and out-of-domain languages, achieves low-resource ASR performance comparable to web-scale multilingual foundation models, and can act as speech-grounded P2G and G2P across more than 70 languages.\nPOWSM offers a new unified paradigm for phone-level modeling, paving the way for inclusive and globally accessible speech technologies that transcend language boundaries and resource disparities.</p>\n\n",
                "matched_terms": [
                    "data",
                    "asr",
                    "languages",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide POWSM, a large-scale foundation model that achieves state-of-the-art PR performance, and is capable of performing multiple fundamental phone-related tasks. Our model enables seamless conversion between speech, text (graphemes/orthography), and phones.</p>\n\n",
                "matched_terms": [
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fully open-source all our data preparation and evaluation scripts, model checkpoints and code to foster open science.</p>\n\n",
                "matched_terms": [
                    "data",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent speech foundation models such as Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite> and OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib39\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib38\" title=\"\">2024</a>)</cite> have driven progress in large-scale multilingual ASR and speech translation, but they do not explicitly address phoneme recognition or articulatory-level supervision.\nSubsequent work <cite class=\"ltx_cite ltx_citemacro_citep\">(Yusuyin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib60\" title=\"\">2025</a>; Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib11\" title=\"\">2025</a>)</cite> showed that incorporating phoneme-level objectives improves ASR for low-resource and long-tailed settings, while outputting phonemes as an intermediate benefited speech translation <cite class=\"ltx_cite ltx_citemacro_citep\">(G&#225;llego et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib18\" title=\"\">2025</a>)</cite>.\nPOWSM extends this line of work by being the first open foundation model jointly trained on phone recognition and related tasks, integrating multilinguality, phonetic supervision, and multi-task scalability within one framework.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior work in multilingual phone recognition can broadly be categorized into (1) language-specific models <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib13\" title=\"\">2021</a>)</cite> that rely on explicit phoneme <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib58\" title=\"\">2022</a>)</cite> or allophone inventories <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib28\" title=\"\">2020</a>)</cite> and (2) language-agnostic approaches that aim to generalize across languages without such resources <cite class=\"ltx_cite ltx_citemacro_citep\">(Taguchi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib51\" title=\"\">2023</a>; Glocker et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib14\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib29\" title=\"\">2021</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>.\nPOWSM follows the latter paradigm as a fully data-driven multilingual model that learns phone representations without predefined phoneme mappings.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "model",
                    "zhu",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">WhisperPPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Samir et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib47\" title=\"\">2025</a>)</cite> improved Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>&#8217;s performance through data cleaning but remained limited in data coverage and task diversity.\nHowever, Whisper is trained on a closed corpus and could display harmful biases for PR which cannot be fully removed by fine-tuning.\nPOWSM is trained from scratch on open datasets.</p>\n\n",
                "matched_terms": [
                    "data",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ZIPA <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> scaled PR to 17,000+ hours of data and 88 languages using a Zipformer <cite class=\"ltx_cite ltx_citemacro_cite\">Yao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib59\" title=\"\">2024</a>)</cite> encoder and noisy-student training on 4,000+ languages, achieving state-of-the-art results. To construct its training corpus, ZIPA employed a G2P system to convert large-scale ASR transcriptions into phoneme sequences, effectively repurposing ASR datasets for PR. Building on this idea, POWSM leverages both the grapheme and the G2P-generated phoneme transcriptions, reformulating them into four task-specific forms: ASR, PR, G2P, and P2G.</p>\n\n",
                "matched_terms": [
                    "zhu",
                    "training",
                    "languages",
                    "hours",
                    "powsm",
                    "data",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">G2P &amp; P2G</span> POWSM is the first model capable of both audio-guided G2P and audio-guided P2G.\nG2P conversion, sometimes called phonemization in the text-to-speech literature, can be accomplished with pronunciation dictionaries <cite class=\"ltx_cite ltx_citemacro_citep\">(Rudnicky, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib46\" title=\"\">1993</a>)</cite>, rules <cite class=\"ltx_cite ltx_citemacro_citep\">(Mortensen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib33\" title=\"\">2018</a>)</cite>, WFSTs <cite class=\"ltx_cite ltx_citemacro_citep\">(Black and Lenzo, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib5\" title=\"\">2001</a>)</cite>, or seq2seq neural methods to choose between different pronunciations of a word in context <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib65\" title=\"\">2022</a>)</cite>.\nText-based G2P, however, still cannot handle phonetic variation, enforcing a one-to-one mapping between orthography and transcription.\nIn contrast, audio-guided G2P can learn to map the different acoustic realizations of a phoneme across varieties of a language to a phone representation <cite class=\"ltx_cite ltx_citemacro_cite\">Route et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib45\" title=\"\">2019</a>)</cite>.\nIn particular, <cite class=\"ltx_cite ltx_citemacro_citet\">Mak et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib32\" title=\"\">2025</a>)</cite> observed a performance improvement in using audio-guided G2P versus text-based G2P alone for Cantonese.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Gao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib12\" title=\"\">2024</a>)</cite> similarly showed that joint learning of G2P, phone recognition, and forced alignment outperform a G2P teacher model. Similarly, <cite class=\"ltx_cite ltx_citemacro_citet\">Sun and Richmond (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib50\" title=\"\">2024</a>)</cite> jointly learned G2P and TTS.\nCompared to G2P, P2G conversion is less studied, with <cite class=\"ltx_cite ltx_citemacro_citet\">Lauc (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib25\" title=\"\">2024</a>)</cite> training a seq2seq model on 19 million language-grapheme-phoneme triplets.</p>\n\n",
                "matched_terms": [
                    "zhu",
                    "model",
                    "training",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use IPAPack++ <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> for training. It is an open source corpus of roughly 17,000 hours of multilingual speech with paired orthographic and phonemic transcriptions. We will release all data processing scripts to make POWSM fully reproducible.</p>\n\n",
                "matched_terms": [
                    "zhu",
                    "training",
                    "hours",
                    "powsm",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model is trained on four tasks: PR, ASR, and audio-guided G2P and P2G. Each utterance is used once per task, with task-specific formatting as illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#S1.F1\" title=\"In 1 Introduction &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, including a text prompt, language token, task token, and target output. We leave the text prompt blank (token <span class=\"ltx_text ltx_font_typewriter\">&lt;na&gt;</span>) for PR and ASR, and provide graphemes and phones as prompts for G2P and P2G.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">POWSM adopts an attention-based encoder-decoder (AED) architecture, which flexibly models output sequences and allows the integration of additional tasks.\nSpecifically, we follow the OWSM v3.1 architecture <cite class=\"ltx_cite ltx_citemacro_cite\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib38\" title=\"\">2024</a>)</cite>, which employs an E-Branchformer encoder and a Transformer decoder, consistent with the general encoder-decoder structure of Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>. The model is trained from scratch using ESPnet <cite class=\"ltx_cite ltx_citemacro_cite\">Watanabe et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib56\" title=\"\">2018</a>)</cite> with a hybrid CTC/attention loss <cite class=\"ltx_cite ltx_citemacro_cite\">Watanabe et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib57\" title=\"\">2017</a>)</cite>, where we set the ratio <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> to 0.3:</p>\n\n",
                "matched_terms": [
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The encoder operates at the stride size of 40ms. Training uses a global batch size of 256. Speech inputs are 16kHz and padded to 20 seconds. The vocabulary consists of 40k tokens, including around 6k phone tokens, language and timestamp tokens, and BPE tokens from orthography.\nThe model has approximately 350M parameters with 9 layers for both the encoder and decoder and was trained on 4 H100 GPUs for 2 days.\nUsing a CTC loss <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib16\" title=\"\">2006</a>)</cite>, We align the encoder outputs with a simplified version of the phone token sequences. Unlike the decoder outputs, the phones in these sequences are stripped of break (<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/./, <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/*&#865;/)\nand length diacritics (<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/e<span class=\"ltx_ERROR undefined\">\\textlengthmark</span>/, /e<span class=\"ltx_ERROR undefined\">\\texthalflength</span>/, /&#283;/) to accelerate convergence. Additional details and analyses are provided in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.SS1.SSS0.Px1\" title=\"The CTC encoder prefers fine-grained phones without suprasegmentals &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;6.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For unseen languages, we evaluate on three datasets: DoReCo <cite class=\"ltx_cite ltx_citemacro_cite\">Paschen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib36\" title=\"\">2020</a>)</cite>, VoxAngeles <cite class=\"ltx_cite ltx_citemacro_cite\">Chodroff et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>)</cite>, and Tusom2021 <cite class=\"ltx_cite ltx_citemacro_cite\">Mortensen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib35\" title=\"\">2021</a>)</cite>.\nDoReCo is a dataset of 50+ languages (with broad transcriptions) intended for documentation of small or endangered languages; we use a 45-language subset.\nVoxAngeles <cite class=\"ltx_cite ltx_citemacro_citep\">(Chodroff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>)</cite> is a postprocessed version of the UCLA Phonetics Lab Archive <cite class=\"ltx_cite ltx_citemacro_citep\">(Ladefoged et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib24\" title=\"\">2009</a>)</cite> containing 95 languages.\nTusom is a low-data Tangkhulic language of India not included in the training data. Tusom2021 consists of narrow phonetic transcriptions (unlike the broad transcriptions from G2P on which POWSM was trained) of individual Tusom words.\nWe removed the tones.</p>\n\n",
                "matched_terms": [
                    "training",
                    "languages",
                    "data",
                    "powsm",
                    "included"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then evaluated our model on in-domain data from IPAPack++, the dataset seen during training. We followed <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> in using LibriSpeech for English, AISHELL for Mandarin, and MLS for European languages, and additionally evaluated on IISc-MILE Tamil <cite class=\"ltx_cite ltx_citemacro_cite\">A et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib1\" title=\"\">2022</a>)</cite> for Tamil and KSC <cite class=\"ltx_cite ltx_citemacro_cite\">Khassanov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib22\" title=\"\">2021</a>)</cite> for Kazakh.\nFor ASR and P2G, we evaluate with FLEURS.</p>\n\n",
                "matched_terms": [
                    "model",
                    "zhu",
                    "training",
                    "languages",
                    "data",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate all PR baselines without further training with IPAPack++. See Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS2\" title=\"A.2 Baseline Implementation &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.2</span></a> for more details about training data and language coverage.\nAllosaurus <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib28\" title=\"\">2020</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib29\" title=\"\">2021</a>)</cite> uses a phone-level CTC to train a language-agnostic model and applies language-specific allophone-to-phoneme mappings.\nWav2Vec2Phoneme <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib58\" title=\"\">2022</a>)</cite>, MultIPA <cite class=\"ltx_cite ltx_citemacro_cite\">Taguchi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib51\" title=\"\">2023</a>)</cite> and Allophant <cite class=\"ltx_cite ltx_citemacro_cite\">Glocker et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib14\" title=\"\">2023</a>)</cite> fine-tune XLS-R <cite class=\"ltx_cite ltx_citemacro_citep\">(Babu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib2\" title=\"\">2022</a>)</cite> with different objectives: Wav2Vec2Phoneme maps unseen phonemes using articulatory features, MultIPA leverages high-quality G2P data from seven languages, while Allophant decomposes phones into articulatory features and applies CTC losses for each.\nZIPA <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> trains ZipFormer <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib59\" title=\"\">2024</a>)</cite> from scratch on IPAPack++ using CR-CTC and also provides a variant trained with additional pseudo-labeled data (&#8220;ZIPA-CR-NS-Large&#8221;).</p>\n\n",
                "matched_terms": [
                    "model",
                    "zhu",
                    "training",
                    "languages",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR, we compare POWSM with two series of models: OWSM <cite class=\"ltx_cite ltx_citemacro_cite\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib37\" title=\"\">2025</a>)</cite> and OWLS <cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib6\" title=\"\">2025</a>)</cite>. We select OWSM-CTC v4 because it is the best-performing model in the series, featuring an encoder-CTC architecture that supports ASR, ST, and LID. For OWLS, we include models with comparable parameter sizes.</p>\n\n",
                "matched_terms": [
                    "model",
                    "owls",
                    "powsm",
                    "owsmctc",
                    "chen",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T2\" title=\"Table 2 &#8227; POWSM excels at in-domain phone recognition &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>, we see that POWSM achieves the lowest average PFER in phone recognition, due to the strong language modeling capability of the decoder.\nWe hypothesize that our English data cleaning (Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS1\" title=\"A.1 Refining English G2P &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.1</span></a>) may have negatively affected the PFER for Germanic languages due to a mismatch between training and test data.\nNevertheless, our approach fills this gap by achieving strong performance on other languages, outperforming models trained on larger datasets.</p>\n\n",
                "matched_terms": [
                    "data",
                    "languages",
                    "training",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hypothesize that pre-training with phone recognition benefits low-resource ASR <cite class=\"ltx_cite ltx_citemacro_cite\">Yusuyin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib60\" title=\"\">2025</a>)</cite>.\nTo choose low-resource languages, we selected languages in IPAPack++ with less than 8 hours of speech in FLEURS to serve as the test set.\nSee <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS3\" title=\"A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.3</span></a> for details on the amount of data used by different models.\nFor a fair comparison with other multilingual ASR baselines without language-specific components, we use the same decoding hyperparameters <span class=\"ltx_text ltx_font_typewriter\">ctc=0.0, beam=1</span>.</p>\n\n",
                "matched_terms": [
                    "amount",
                    "hours",
                    "languages",
                    "data",
                    "comparison",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T3\" title=\"Table 3 &#8227; POWSM is comparable with web-scale ASR models on low-resource languages &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;3</span></a>, POWSM (<span class=\"ltx_text ltx_font_typewriter\">POWSM 0.35B, ASR</span>) is often comparable to models of similar size trained on web-scale data for ASR (<span class=\"ltx_text ltx_font_typewriter\">OWLS 0.5B</span>).\nIncorporating phones obtained from PR as text prompts (PR-P2G) significantly decreases WER, making it comparable to or even better than these models.\nWhen using gold phone labels for P2G (see analysis in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.SS2.SSS0.Px2\" title=\"Audio-P2G effectively handles low-resource languages &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;6.2</span></a>), POWSM outperforms other ASR models by a large margin in most cases.</p>\n\n",
                "matched_terms": [
                    "data",
                    "asr",
                    "owls",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T4\" title=\"Table 4 &#8227; 5.2 POWSM generalizes well to unseen languages &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a> reports PFER on datasets with unseen languages and language variation.\nResults indicate that POWSM achieves strong performance across these datasets, and handles both dialectal and L2 variation effectively.\nNotably, our method outperforms ZIPA trained on the same data and even exceeds ZIPA trained with extra pseudo-labeled data, achieving the best results on unseen languages while performing three additional tasks.\nThis shows the effectiveness of our multi-task approach.\nWhile POWSM lags behind Wav2Vec2Phoneme on socio-phonetic variations, we attribute this to its self-supervised learning with over 60k hours of speech (from wav2vec 2.0 <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib3\" title=\"\">2020</a>)</cite>) prior to the supervised learning stage.</p>\n\n",
                "matched_terms": [
                    "data",
                    "languages",
                    "hours",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we analyze how POWSM works, focusing on the phonetic-aware encoder and task- and language-specific tokens, which are the defining features of the model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observed that mixing phones and orthography as encoder targets hindered training, because the same speech input would have different encoder CTC targets for different tasks.\nTherefore, we used phones as encoder targets, encouraging general representations of sounds to be shared across languages.</p>\n\n",
                "matched_terms": [
                    "training",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To determine the most effective unit for the CTC encoder, we fix the decoder vocabulary to PanPhon phones and compared four encoder targets: (1) Unicode code points vs. PanPhon, and (2) sequences with vs. without suprasegmentals (length and break marks). Unicode code points offer simplicity and a smaller vocabulary but split phones into unnatural units (e.g. <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/p<sup class=\"ltx_sup\">h</sup>/) and increase sequence length, while PanPhon represents each phone-diacritic combination as a unit (e.g. <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/p<sup class=\"ltx_sup\">h</sup>/), yielding a more natural monotonic sequence at the expense of sparsity and potential out-of-vocabulary issues. Suprasegmentals such as <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/<span class=\"ltx_ERROR undefined\">\\textlengthmark</span>/, though phonemic in many languages, confuse PR models <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "zhu",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We run small-scale experiments on a 1k-hour subset of the multi-task data (250 hours of speech repeated across four tasks).\nWe use the validation CER of the encoder-CTC output as a proxy for training efficiency.\nAn earlier drop indicates that the encoder is learning a useful alignment early, which improves representations fed into the decoder and accelerates overall convergence.\nIn <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.F2\" title=\"Figure 2 &#8227; The CTC encoder prefers fine-grained phones without suprasegmentals &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;2</span></a>, PanPhon tokenization without suprasegmentals shows the earliest drop, suggesting that alignment with decoder units aids training, while\ncollapsing suprasegmental distinctions for CTC reduces confusion.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As in other encoder-decoder models <cite class=\"ltx_cite ltx_citemacro_cite\">Gong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib15\" title=\"\">2023</a>); Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>, we expect the encoder of POWSM to capture more general acoustic patterns, while the decoder handles language and task-specific output formats. Therefore,\nwe investigate whether emphasizing the encoder more during different stages of model development affects performance.\nTo balance data diversity with inference compute costs, we selected two smaller datasets from each category with distinct characteristics.</p>\n\n",
                "matched_terms": [
                    "data",
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T5\" title=\"Table 5 &#8227; Increased encoder weights benefit PR on out-of-domain data &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;5</span></a>, higher CTC decoding weights improve PR performance on out-of-domain data but degrade it on in-domain data, as expected.\nThis echoes <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>&#8217;s finding that RNN-T <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves and Jaitly, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib17\" title=\"\">2014</a>)</cite>, an encoder-only speech-to-text model with an autoregressive text prediction network, hurts generalization to unseen patterns of phones (phonotactics).\nWe hypothesize that the decoder is performing implicit language modeling and &#8220;smooths&#8221; phonetic variation, as <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> described.</p>\n\n",
                "matched_terms": [
                    "data",
                    "model",
                    "zhu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, we examine whether focusing more on the CTC loss through training widens this gap in performance between in-domain and out-of-domain data.\nWe find that fine-tuning with a higher CTC loss weight <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> after convergence does not improve out-of-domain performance and can even degrade it. Randomly varying <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> for each batch also shows no improvement.\nIn contrast, training with a higher <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> from the start benefits the out-of-domain distribution, achieving the lowest PFER on unseen languages with greedy decoding, while the PFER on in-domain data is comparatively higher.\nThese results suggest that assigning a higher weight to the encoder during training and inference improves PR, highlighting a common trade-off between in-domain performance and generalization.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand how POWSM integrates speech and text prompts, we analyze the relative influence of speech and text prompts in its G2P behavior.\nWe vary the G2P conditions from purely speech-based to purely text-based, as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T6\" title=\"Table 6 &#8227; Increased encoder weights benefit PR on out-of-domain data &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;6</span></a>, and evaluate the model on the Buckeye dataset.\nWhen only speech is provided, the performance is comparable to the PR setting, which differs only in the task token.\nAdding both speech and text prompts (the standard G2P setup) leads to degraded performance, with output showing standardized pronunciations.\nWhen the model relies solely on the text prompt, performance drops sharply and pronunciations become highly standardized as expected (just as <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> reported).</p>\n\n",
                "matched_terms": [
                    "model",
                    "zhu",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In other words, POWSM G2P responds to speech and text signals to controllably mediate between narrow and broad transcription.\nIn the multi-task setup, this effect may be stronger because the model is trained with G2P, which could bias it toward more standardized forms.</p>\n\n",
                "matched_terms": [
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare several P2G setups on the same set of low-resource languages from FLEURS, listed in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T7\" title=\"Table 7 &#8227; Audio-P2G effectively handles low-resource languages &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;7</span></a>. P2G significantly outperforms ASR, suggesting that it effectively leverages the provided phone context.\nHowever, since P2G uses gold phone labels, this comparison is not entirely fair. We therefore tested PR followed by P2G (PR-P2G), and found that performance improved for some languages but not for others.\nError propagation does not explain this variation in performance, as PFER trends from PR differ from the observed performance drops. Yet the PFER pattern aligns closely with ASR results, suggesting that phonotactic similarity to high-resource languages may play a role.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "comparison",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test this, we run P2G with the language code set to English and post-process the output to match Cyrillic or Gurmukhi transcriptions with online conversion tools for certain languages.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.lexilogos.com\" title=\"\">https://www.lexilogos.com</a> for Macedonian and Tajik; <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://punjabi.indiatyping.com\" title=\"\">https://punjabi.indiatyping.com</a> for Panjabi.</span></span></span>\nThis approach often outperforms ASR and sometimes approaches P2G&#8217;s performance, indicating that P2G also relies heavily on speech input.\nLanguages with either comparibly low or high PFER did not benefit from this transliteration approach, possibly because the model already handled them well or had not yet learned them sufficiently.\nThis finding suggests a direction for further investigation in low-resource ASR.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The language identification (LID) performance of POWSM on seen languages in FLEURS reaches 92.3% accuracy, as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.F3\" title=\"Figure 3 &#8227; A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;3</span></a>.\nTo see if the model implicitly learns phonotactic patterns and associates them with the language token, we evaluate PR on unseen languages by manipulating the language token at inference time.\nFor VoxAngeles and Tusom2021, the three most frequently assigned languages are Bashkir (42.6%, 25.1%), English (30.2%, 67.7%), and Kinyarwanda (14.5%, 2.3%), which are all relatively high-resource languages in IPAPack++.\n<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T8\" title=\"Table 8 &#8227; Language token captures phonotactics &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;8</span></a> shows that assigning the detected language token yields better performance than always using English, while setting the language as unknown performs best. This indicates that the language token influences PR by shifting the output distribution toward the assigned language.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train a fully open-source phonetic speech foundation model POWSM using our scalable multi-task framework.\nOur model achieves state-of-the-art performance on PR while also supporting ASR across more than 70 languages.\nBeyond PR and ASR, the model&#8217;s ability to perform audio-guided G2P and P2G enables applications that require fine-grained linguistic analysis such as atypical speech assessment.\nOur analysis reveals that POWSM&#8217;s encoder benefits from phoneme-level CTC supervision and stronger encoder weighting, enhancing cross-lingual generalization.\nAdditionally, the model demonstrates interpretable multimodal and language-aware behaviors, effectively mediating between phonetic detail and standardized phonological patterns.\nTo conclude, POWSM not only provides a strong phone recognition foundation model for high-resource languages, but also acts as a versatile resource for unseen languages and socio-phonetic variation.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "languages",
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">POWSM has several limitations that we aim to address in future work.\nFirst, the model is neither strictly phonemic nor phonetic: its training data consist of cleaned and filtered phonemic transcriptions from multiple languages, which are not fully faithful to the phonetic or phonemic structure of the audio. Although phonemic transcriptions share similarities across languages, adding auxiliary tasks and language tokens may have reinforced language-specific biases. We also currently lack sufficient allophone-level data, which would provide more language-independent information.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "languages",
                    "data",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, the model still favors high-resource languages. Since we include a decoder for language modeling and language tokens, both of which function effectively, the model would inherently bias toward the seen distribution.</p>\n\n",
                "matched_terms": [
                    "model",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All of our data is ethically sourced, either through permissive licensing or through proper consent.\nWe are aware of the implicit prescriptivism and representational harms <cite class=\"ltx_cite ltx_citemacro_citep\">(Crawford, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib10\" title=\"\">2017</a>)</cite> that normalizing socio-phonetic variation in ASR or PR models can create.\nThis may threaten linguistic diversity instead of preserving it.\nWe also acknowledge that accurate modeling of socio-phonetic variation can enable demographic inference, as demographics and phonetic variation are deeply intertwined <cite class=\"ltx_cite ltx_citemacro_citep\">(Labov, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib23\" title=\"\">1963</a>)</cite>.\nWe stress that uses of POWSM must align with our vision: a future where advances in spoken language processing and NLP do not leave low-resource varieties behind.</p>\n\n",
                "matched_terms": [
                    "data",
                    "asr",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observed confusion in plosive voice-onset times on unseen languages in preliminary experiments, which is likely from English G2P data. For instance, broad phonemic transcription in English typically uses /b/ to transcribe the /b/ in /bat/, but its voice onset timing is actually voiceless in Mainstream American English and is closer to [p]. To mitigate this, we apply rule-based refinements to English G2P transcriptions, adjusting plosive voicing and aspiration, lateral velarization, and vowel nasalization.</p>\n\n",
                "matched_terms": [
                    "data",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide the baselines&#8217; training data source, number of languages covered in the data, and links to model checkpoints or repository in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.T9\" title=\"Table 9 &#8227; A.2 Baseline Implementation &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;9</span></a>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "model",
                    "training",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multi-tasking may improve performance by tying acoustic signals to well-defined symbolic representations, yet it may distract the model if the relationships are not learned effectively.\nWe train POWSM with different data and model scales to examine how multitask learning interacts with the setup, and use <span class=\"ltx_text ltx_font_typewriter\">beam=1</span> during decoding to speed up inference.</p>\n\n",
                "matched_terms": [
                    "data",
                    "model",
                    "powsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.T11\" title=\"Table 11 &#8227; A.4 Multi-tasking at Different Scales &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;11</span></a> shows that there is no clear trend regarding whether multitasking benefits PR performance. PR performance degrades when the model has excessive capacity relative to the available data (too little data), or when it is limited by size (too much data).</p>\n\n",
                "matched_terms": [
                    "data",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Further evidence is needed before concluding that phoneme recognition benefits less from scaling, as we currently lack sufficient data and large model capacity to test this thoroughly. Nevertheless, the model demonstrates the ability to multitask, which represents a promising direction for future work.</p>\n\n",
                "matched_terms": [
                    "data",
                    "model"
                ]
            }
        ]
    },
    "A1.T11": {
        "source_file": "POWSM: A Phonetic Open Whisper-Style Speech Foundation Model",
        "caption": "Table 11: Comparison of PFER (\\downarrow) on different seting of tasks and data. 1 task refers to PR, 2 tasks refer to PR+ASR, and 4 tasks include PR, ASR, P2G, and G2P.",
        "body": "Data ( khr)\nTasks\nParams.\nVoxAngeles\nTusom2021\nL2-Arctic\n\n\n0.25\n1\n100M\n27.22\n32.59\n13.50\n\n\n0.25\n4\n100M\n26.30\n30.32\n13.40\n\n\n0.25\n1\n300M\n20.63\n25.83\n12.88\n\n\n0.25\n4\n300M\n23.81\n25.91\n14.14\n\n\n17\n1\n100M\n17.88\n26.68\n11.76\n\n\n17\n2\n100M\n24.69\n49.28\n11.35\n\n\n17\n4\n100M\n30.07\n61.89\n12.37\n\n\n17\n1\n300M\n17.08\n25.20\n10.50\n\n\n17\n2\n300M\n17.17\n23.70\n10.47\n\n\n17\n4\n300M\n17.58\n33.52\n10.54",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" style=\"padding:1pt 4.0pt;\">Data ( khr)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" style=\"padding:1pt 4.0pt;\">Tasks</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:1pt 4.0pt;\">Params.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:1pt 4.0pt;\">VoxAngeles</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:1pt 4.0pt;\">Tusom2021</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:1pt 4.0pt;\">L2-Arctic</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding:1pt 4.0pt;\">0.25</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding:1pt 4.0pt;\">1</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 4.0pt;\">100M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 4.0pt;\">27.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 4.0pt;\">32.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 4.0pt;\">13.50</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:1pt 4.0pt;\">0.25</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:1pt 4.0pt;\">4</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">100M</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">26.30</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">30.32</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">13.40</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:1pt 4.0pt;\">0.25</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:1pt 4.0pt;\">1</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">300M</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">20.63</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">25.83</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">12.88</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:1pt 4.0pt;\">0.25</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:1pt 4.0pt;\">4</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">300M</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">23.81</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">25.91</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">14.14</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding:1pt 4.0pt;\">17</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding:1pt 4.0pt;\">1</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 4.0pt;\">100M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 4.0pt;\">17.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 4.0pt;\">26.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 4.0pt;\">11.76</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:1pt 4.0pt;\">17</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:1pt 4.0pt;\">2</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">100M</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">24.69</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">49.28</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">11.35</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:1pt 4.0pt;\">17</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:1pt 4.0pt;\">4</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">100M</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">30.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">61.89</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">12.37</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:1pt 4.0pt;\">17</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:1pt 4.0pt;\">1</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">300M</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">17.08</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">25.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">10.50</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:1pt 4.0pt;\">17</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding:1pt 4.0pt;\">2</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">300M</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">17.17</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">23.70</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 4.0pt;\">10.47</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" style=\"padding:1pt 4.0pt;\">17</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" style=\"padding:1pt 4.0pt;\">4</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1pt 4.0pt;\">300M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1pt 4.0pt;\">17.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1pt 4.0pt;\">33.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1pt 4.0pt;\">10.54</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "task",
            "tasks",
            "g2p",
            "refer",
            "downarrow",
            "refers",
            "prasr",
            "p2g",
            "include",
            "seting",
            "tusom2021",
            "params",
            "300m",
            "asr",
            "different",
            "voxangeles",
            "l2arctic",
            "100m",
            "data",
            "khr",
            "comparison",
            "pfer"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.T11\" title=\"Table 11 &#8227; A.4 Multi-tasking at Different Scales &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;11</span></a> shows that there is no clear trend regarding whether multitasking benefits PR performance. PR performance degrades when the model has excessive capacity relative to the available data (too little data), or when it is limited by size (too much data).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in spoken language processing have led to substantial progress in phonetic tasks such as automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G). Despite their conceptual similarity, these tasks have largely been studied in isolation, each relying on task-specific architectures and datasets.\nIn this paper, we introduce POWSM (Phonetic Open Whisper-style Speech Model), the first unified framework capable of jointly performing multiple phone-related tasks.\nPOWSM enables seamless conversion between audio, text (graphemes), and phones, opening up new possibilities for universal and low-resource speech processing.\nOur model outperforms or matches specialized PR models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P, P2G, and ASR.\nOur training data, code<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/espnet</span></span></span> and models<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://huggingface.co/espnet/powsm</span></span></span> are released to foster open science.</p>\n\n",
                "matched_terms": [
                    "p2g",
                    "tasks",
                    "g2p",
                    "data",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Four key phone-related tasks underpin phonetic spoken language processing: automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G).\n<span class=\"ltx_text ltx_font_italic\">ASR</span> learns implicit phonetic representations <cite class=\"ltx_cite ltx_citemacro_cite\">Belinkov and Glass (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib4\" title=\"\">2017</a>)</cite>, while <span class=\"ltx_text ltx_font_italic\">PR</span> offers explicit phone-level supervision.\n<span class=\"ltx_text ltx_font_italic\">G2P</span> and <span class=\"ltx_text ltx_font_italic\">P2G</span> bridge orthographic and phonetic spaces.\nCollectively, these tasks interact through shared phonetic representations, each addressing a different aspect of the relationship between audio, phones, phonemes, and graphemes.</p>\n\n",
                "matched_terms": [
                    "p2g",
                    "tasks",
                    "g2p",
                    "different",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge this gap, we propose POWSM, a phonetic foundation model capable of performing four core phone-related tasks &#8212; PR, ASR, audio-guided G2P, and audio-guided P2G &#8212; within one unified architecture (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#S1.F1\" title=\"In 1 Introduction &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nTo construct this framework, we reformulate standard ASR datasets <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> into four task-specific formats, allowing the model to learn consistent mappings across audio, phoneme, and grapheme representations.\nIn addition, POWSM adopts an attention-based encoder-decoder (AED) architecture, following the design of large-scale speech foundation models such as Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite> and OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib39\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "g2p",
                    "asr",
                    "p2g",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirically, POWSM outperforms previous PR models on both in-domain data and out-of-domain languages, achieves low-resource ASR performance comparable to web-scale multilingual foundation models, and can act as speech-grounded P2G and G2P across more than 70 languages.\nPOWSM offers a new unified paradigm for phone-level modeling, paving the way for inclusive and globally accessible speech technologies that transcend language boundaries and resource disparities.</p>\n\n",
                "matched_terms": [
                    "g2p",
                    "p2g",
                    "data",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent speech foundation models such as Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite> and OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib39\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib38\" title=\"\">2024</a>)</cite> have driven progress in large-scale multilingual ASR and speech translation, but they do not explicitly address phoneme recognition or articulatory-level supervision.\nSubsequent work <cite class=\"ltx_cite ltx_citemacro_citep\">(Yusuyin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib60\" title=\"\">2025</a>; Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib11\" title=\"\">2025</a>)</cite> showed that incorporating phoneme-level objectives improves ASR for low-resource and long-tailed settings, while outputting phonemes as an intermediate benefited speech translation <cite class=\"ltx_cite ltx_citemacro_citep\">(G&#225;llego et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib18\" title=\"\">2025</a>)</cite>.\nPOWSM extends this line of work by being the first open foundation model jointly trained on phone recognition and related tasks, integrating multilinguality, phonetic supervision, and multi-task scalability within one framework.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">WhisperPPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Samir et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib47\" title=\"\">2025</a>)</cite> improved Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>&#8217;s performance through data cleaning but remained limited in data coverage and task diversity.\nHowever, Whisper is trained on a closed corpus and could display harmful biases for PR which cannot be fully removed by fine-tuning.\nPOWSM is trained from scratch on open datasets.</p>\n\n",
                "matched_terms": [
                    "data",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ZIPA <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> scaled PR to 17,000+ hours of data and 88 languages using a Zipformer <cite class=\"ltx_cite ltx_citemacro_cite\">Yao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib59\" title=\"\">2024</a>)</cite> encoder and noisy-student training on 4,000+ languages, achieving state-of-the-art results. To construct its training corpus, ZIPA employed a G2P system to convert large-scale ASR transcriptions into phoneme sequences, effectively repurposing ASR datasets for PR. Building on this idea, POWSM leverages both the grapheme and the G2P-generated phoneme transcriptions, reformulating them into four task-specific forms: ASR, PR, G2P, and P2G.</p>\n\n",
                "matched_terms": [
                    "g2p",
                    "p2g",
                    "data",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">G2P &amp; P2G</span> POWSM is the first model capable of both audio-guided G2P and audio-guided P2G.\nG2P conversion, sometimes called phonemization in the text-to-speech literature, can be accomplished with pronunciation dictionaries <cite class=\"ltx_cite ltx_citemacro_citep\">(Rudnicky, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib46\" title=\"\">1993</a>)</cite>, rules <cite class=\"ltx_cite ltx_citemacro_citep\">(Mortensen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib33\" title=\"\">2018</a>)</cite>, WFSTs <cite class=\"ltx_cite ltx_citemacro_citep\">(Black and Lenzo, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib5\" title=\"\">2001</a>)</cite>, or seq2seq neural methods to choose between different pronunciations of a word in context <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib65\" title=\"\">2022</a>)</cite>.\nText-based G2P, however, still cannot handle phonetic variation, enforcing a one-to-one mapping between orthography and transcription.\nIn contrast, audio-guided G2P can learn to map the different acoustic realizations of a phoneme across varieties of a language to a phone representation <cite class=\"ltx_cite ltx_citemacro_cite\">Route et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib45\" title=\"\">2019</a>)</cite>.\nIn particular, <cite class=\"ltx_cite ltx_citemacro_citet\">Mak et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib32\" title=\"\">2025</a>)</cite> observed a performance improvement in using audio-guided G2P versus text-based G2P alone for Cantonese.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Gao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib12\" title=\"\">2024</a>)</cite> similarly showed that joint learning of G2P, phone recognition, and forced alignment outperform a G2P teacher model. Similarly, <cite class=\"ltx_cite ltx_citemacro_citet\">Sun and Richmond (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib50\" title=\"\">2024</a>)</cite> jointly learned G2P and TTS.\nCompared to G2P, P2G conversion is less studied, with <cite class=\"ltx_cite ltx_citemacro_citet\">Lauc (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib25\" title=\"\">2024</a>)</cite> training a seq2seq model on 19 million language-grapheme-phoneme triplets.</p>\n\n",
                "matched_terms": [
                    "different",
                    "g2p",
                    "p2g"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model is trained on four tasks: PR, ASR, and audio-guided G2P and P2G. Each utterance is used once per task, with task-specific formatting as illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#S1.F1\" title=\"In 1 Introduction &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, including a text prompt, language token, task token, and target output. We leave the text prompt blank (token <span class=\"ltx_text ltx_font_typewriter\">&lt;na&gt;</span>) for PR and ASR, and provide graphemes and phones as prompts for G2P and P2G.</p>\n\n",
                "matched_terms": [
                    "p2g",
                    "task",
                    "tasks",
                    "g2p",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For unseen languages, we evaluate on three datasets: DoReCo <cite class=\"ltx_cite ltx_citemacro_cite\">Paschen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib36\" title=\"\">2020</a>)</cite>, VoxAngeles <cite class=\"ltx_cite ltx_citemacro_cite\">Chodroff et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>)</cite>, and Tusom2021 <cite class=\"ltx_cite ltx_citemacro_cite\">Mortensen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib35\" title=\"\">2021</a>)</cite>.\nDoReCo is a dataset of 50+ languages (with broad transcriptions) intended for documentation of small or endangered languages; we use a 45-language subset.\nVoxAngeles <cite class=\"ltx_cite ltx_citemacro_citep\">(Chodroff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib8\" title=\"\">2024</a>)</cite> is a postprocessed version of the UCLA Phonetics Lab Archive <cite class=\"ltx_cite ltx_citemacro_citep\">(Ladefoged et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib24\" title=\"\">2009</a>)</cite> containing 95 languages.\nTusom is a low-data Tangkhulic language of India not included in the training data. Tusom2021 consists of narrow phonetic transcriptions (unlike the broad transcriptions from G2P on which POWSM was trained) of individual Tusom words.\nWe removed the tones.</p>\n\n",
                "matched_terms": [
                    "g2p",
                    "tusom2021",
                    "data",
                    "voxangeles"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also test on five datasets on varieties of English: the Buckeye Corpus <cite class=\"ltx_cite ltx_citemacro_cite\">Pitt et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib41\" title=\"\">2005</a>)</cite> and DoReCo South-England represent dialectal variation, while L2-ARCTIC <cite class=\"ltx_cite ltx_citemacro_cite\">Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib62\" title=\"\">2018</a>)</cite>, EpaDB <cite class=\"ltx_cite ltx_citemacro_cite\">Vidal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib54\" title=\"\">2019</a>)</cite>, and SpeechOcean762 <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib61\" title=\"\">2021</a>)</cite> contain L2 speakers.\nFor L2-ARCTIC, we used the manually annotated phoneme transcriptions (which <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> termed <span class=\"ltx_text ltx_font_italic\">L2-Perceived</span>) rather than G2P dictionary-based transcriptions. The manual transcriptions reflect what the speaker actually said, whereas the dictionary-based version enforces a single pronunciation variant.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>For instance, &#8220;crayon&#8221; in American English can be pronounced as\n<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/<span class=\"ltx_ERROR undefined\">\\textprimstress</span>k&#8290;r&#230;n/, <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/<span class=\"ltx_ERROR undefined\">\\textprimstress</span>k&#8290;rej.On/, or <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/<span class=\"ltx_ERROR undefined\">\\textprimstress</span>k&#8290;rej.6n/ <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaux and Golder, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib53\" title=\"\">2003</a>)</cite> (among others), but the CMU Pronouncing Dictionary <cite class=\"ltx_cite ltx_citemacro_citep\">(Rudnicky, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib46\" title=\"\">1993</a>)</cite> only lists one.</span></span></span>\nManual inspection by a trained phonologist further showed the L2-ARCTIC transcriptions to be of extremely poor quality.\nFor the five aforementioned datasets, we use preprocessed datasets from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://huggingface.co/anyspeech</span></span></span> and Koel Labs<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://huggingface.co/KoelLabs</span></span></span> for better transcription quality.</p>\n\n",
                "matched_terms": [
                    "l2arctic",
                    "g2p"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then evaluated our model on in-domain data from IPAPack++, the dataset seen during training. We followed <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> in using LibriSpeech for English, AISHELL for Mandarin, and MLS for European languages, and additionally evaluated on IISc-MILE Tamil <cite class=\"ltx_cite ltx_citemacro_cite\">A et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib1\" title=\"\">2022</a>)</cite> for Tamil and KSC <cite class=\"ltx_cite ltx_citemacro_cite\">Khassanov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib22\" title=\"\">2021</a>)</cite> for Kazakh.\nFor ASR and P2G, we evaluate with FLEURS.</p>\n\n",
                "matched_terms": [
                    "data",
                    "p2g",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate all PR baselines without further training with IPAPack++. See Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS2\" title=\"A.2 Baseline Implementation &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.2</span></a> for more details about training data and language coverage.\nAllosaurus <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib28\" title=\"\">2020</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib29\" title=\"\">2021</a>)</cite> uses a phone-level CTC to train a language-agnostic model and applies language-specific allophone-to-phoneme mappings.\nWav2Vec2Phoneme <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib58\" title=\"\">2022</a>)</cite>, MultIPA <cite class=\"ltx_cite ltx_citemacro_cite\">Taguchi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib51\" title=\"\">2023</a>)</cite> and Allophant <cite class=\"ltx_cite ltx_citemacro_cite\">Glocker et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib14\" title=\"\">2023</a>)</cite> fine-tune XLS-R <cite class=\"ltx_cite ltx_citemacro_citep\">(Babu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib2\" title=\"\">2022</a>)</cite> with different objectives: Wav2Vec2Phoneme maps unseen phonemes using articulatory features, MultIPA leverages high-quality G2P data from seven languages, while Allophant decomposes phones into articulatory features and applies CTC losses for each.\nZIPA <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> trains ZipFormer <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib59\" title=\"\">2024</a>)</cite> from scratch on IPAPack++ using CR-CTC and also provides a variant trained with additional pseudo-labeled data (&#8220;ZIPA-CR-NS-Large&#8221;).</p>\n\n",
                "matched_terms": [
                    "g2p",
                    "different",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR, we compare POWSM with two series of models: OWSM <cite class=\"ltx_cite ltx_citemacro_cite\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib37\" title=\"\">2025</a>)</cite> and OWLS <cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib6\" title=\"\">2025</a>)</cite>. We select OWSM-CTC v4 because it is the best-performing model in the series, featuring an encoder-CTC architecture that supports ASR, ST, and LID. For OWLS, we include models with comparable parameter sizes.</p>\n\n",
                "matched_terms": [
                    "include",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We found that POWSM&#8217;s performance on PR and ASR tasks is comparable or superior to competitive baselines.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results on the in-domain test sets are presented in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T2\" title=\"Table 2 &#8227; POWSM excels at in-domain phone recognition &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T3\" title=\"Table 3 &#8227; POWSM is comparable with web-scale ASR models on low-resource languages &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;3</span></a>. We provide further discussion of G2P and P2G in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.SS2\" title=\"6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;6.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "g2p",
                    "p2g"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T2\" title=\"Table 2 &#8227; POWSM excels at in-domain phone recognition &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>, we see that POWSM achieves the lowest average PFER in phone recognition, due to the strong language modeling capability of the decoder.\nWe hypothesize that our English data cleaning (Appendix <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS1\" title=\"A.1 Refining English G2P &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.1</span></a>) may have negatively affected the PFER for Germanic languages due to a mismatch between training and test data.\nNevertheless, our approach fills this gap by achieving strong performance on other languages, outperforming models trained on larger datasets.</p>\n\n",
                "matched_terms": [
                    "data",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hypothesize that pre-training with phone recognition benefits low-resource ASR <cite class=\"ltx_cite ltx_citemacro_cite\">Yusuyin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib60\" title=\"\">2025</a>)</cite>.\nTo choose low-resource languages, we selected languages in IPAPack++ with less than 8 hours of speech in FLEURS to serve as the test set.\nSee <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.SS3\" title=\"A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;A.3</span></a> for details on the amount of data used by different models.\nFor a fair comparison with other multilingual ASR baselines without language-specific components, we use the same decoding hyperparameters <span class=\"ltx_text ltx_font_typewriter\">ctc=0.0, beam=1</span>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "comparison",
                    "different",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T3\" title=\"Table 3 &#8227; POWSM is comparable with web-scale ASR models on low-resource languages &#8227; 5.1 Multi-task performance &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;3</span></a>, POWSM (<span class=\"ltx_text ltx_font_typewriter\">POWSM 0.35B, ASR</span>) is often comparable to models of similar size trained on web-scale data for ASR (<span class=\"ltx_text ltx_font_typewriter\">OWLS 0.5B</span>).\nIncorporating phones obtained from PR as text prompts (PR-P2G) significantly decreases WER, making it comparable to or even better than these models.\nWhen using gold phone labels for P2G (see analysis in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.SS2.SSS0.Px2\" title=\"Audio-P2G effectively handles low-resource languages &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">&#167;&#160;6.2</span></a>), POWSM outperforms other ASR models by a large margin in most cases.</p>\n\n",
                "matched_terms": [
                    "data",
                    "p2g",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S5.T4\" title=\"Table 4 &#8227; 5.2 POWSM generalizes well to unseen languages &#8227; 5 Results &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a> reports PFER on datasets with unseen languages and language variation.\nResults indicate that POWSM achieves strong performance across these datasets, and handles both dialectal and L2 variation effectively.\nNotably, our method outperforms ZIPA trained on the same data and even exceeds ZIPA trained with extra pseudo-labeled data, achieving the best results on unseen languages while performing three additional tasks.\nThis shows the effectiveness of our multi-task approach.\nWhile POWSM lags behind Wav2Vec2Phoneme on socio-phonetic variations, we attribute this to its self-supervised learning with over 60k hours of speech (from wav2vec 2.0 <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib3\" title=\"\">2020</a>)</cite>) prior to the supervised learning stage.</p>\n\n",
                "matched_terms": [
                    "data",
                    "pfer",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observed that mixing phones and orthography as encoder targets hindered training, because the same speech input would have different encoder CTC targets for different tasks.\nTherefore, we used phones as encoder targets, encouraging general representations of sounds to be shared across languages.</p>\n\n",
                "matched_terms": [
                    "different",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We run small-scale experiments on a 1k-hour subset of the multi-task data (250 hours of speech repeated across four tasks).\nWe use the validation CER of the encoder-CTC output as a proxy for training efficiency.\nAn earlier drop indicates that the encoder is learning a useful alignment early, which improves representations fed into the decoder and accelerates overall convergence.\nIn <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.F2\" title=\"Figure 2 &#8227; The CTC encoder prefers fine-grained phones without suprasegmentals &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;2</span></a>, PanPhon tokenization without suprasegmentals shows the earliest drop, suggesting that alignment with decoder units aids training, while\ncollapsing suprasegmental distinctions for CTC reduces confusion.</p>\n\n",
                "matched_terms": [
                    "data",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As in other encoder-decoder models <cite class=\"ltx_cite ltx_citemacro_cite\">Gong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib15\" title=\"\">2023</a>); Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib43\" title=\"\">2023</a>)</cite>, we expect the encoder of POWSM to capture more general acoustic patterns, while the decoder handles language and task-specific output formats. Therefore,\nwe investigate whether emphasizing the encoder more during different stages of model development affects performance.\nTo balance data diversity with inference compute costs, we selected two smaller datasets from each category with distinct characteristics.</p>\n\n",
                "matched_terms": [
                    "data",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, we examine whether focusing more on the CTC loss through training widens this gap in performance between in-domain and out-of-domain data.\nWe find that fine-tuning with a higher CTC loss weight <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> after convergence does not improve out-of-domain performance and can even degrade it. Randomly varying <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> for each batch also shows no improvement.\nIn contrast, training with a higher <math alttext=\"\\alpha_{\\text{ctc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mtext>ctc</mtext></msub><annotation encoding=\"application/x-tex\">\\alpha_{\\text{ctc}}</annotation></semantics></math> from the start benefits the out-of-domain distribution, achieving the lowest PFER on unseen languages with greedy decoding, while the PFER on in-domain data is comparatively higher.\nThese results suggest that assigning a higher weight to the encoder during training and inference improves PR, highlighting a common trade-off between in-domain performance and generalization.</p>\n\n",
                "matched_terms": [
                    "data",
                    "pfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand how POWSM integrates speech and text prompts, we analyze the relative influence of speech and text prompts in its G2P behavior.\nWe vary the G2P conditions from purely speech-based to purely text-based, as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T6\" title=\"Table 6 &#8227; Increased encoder weights benefit PR on out-of-domain data &#8227; 6.1 Behavior of the speech encoder &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;6</span></a>, and evaluate the model on the Buckeye dataset.\nWhen only speech is provided, the performance is comparable to the PR setting, which differs only in the task token.\nAdding both speech and text prompts (the standard G2P setup) leads to degraded performance, with output showing standardized pronunciations.\nWhen the model relies solely on the text prompt, performance drops sharply and pronunciations become highly standardized as expected (just as <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite> reported).</p>\n\n",
                "matched_terms": [
                    "g2p",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare several P2G setups on the same set of low-resource languages from FLEURS, listed in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T7\" title=\"Table 7 &#8227; Audio-P2G effectively handles low-resource languages &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;7</span></a>. P2G significantly outperforms ASR, suggesting that it effectively leverages the provided phone context.\nHowever, since P2G uses gold phone labels, this comparison is not entirely fair. We therefore tested PR followed by P2G (PR-P2G), and found that performance improved for some languages but not for others.\nError propagation does not explain this variation in performance, as PFER trends from PR differ from the observed performance drops. Yet the PFER pattern aligns closely with ASR results, suggesting that phonotactic similarity to high-resource languages may play a role.</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "pfer",
                    "p2g",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test this, we run P2G with the language code set to English and post-process the output to match Cyrillic or Gurmukhi transcriptions with online conversion tools for certain languages.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.lexilogos.com\" title=\"\">https://www.lexilogos.com</a> for Macedonian and Tajik; <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://punjabi.indiatyping.com\" title=\"\">https://punjabi.indiatyping.com</a> for Panjabi.</span></span></span>\nThis approach often outperforms ASR and sometimes approaches P2G&#8217;s performance, indicating that P2G also relies heavily on speech input.\nLanguages with either comparibly low or high PFER did not benefit from this transliteration approach, possibly because the model already handled them well or had not yet learned them sufficiently.\nThis finding suggests a direction for further investigation in low-resource ASR.</p>\n\n",
                "matched_terms": [
                    "pfer",
                    "p2g",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The language identification (LID) performance of POWSM on seen languages in FLEURS reaches 92.3% accuracy, as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.F3\" title=\"Figure 3 &#8227; A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;3</span></a>.\nTo see if the model implicitly learns phonotactic patterns and associates them with the language token, we evaluate PR on unseen languages by manipulating the language token at inference time.\nFor VoxAngeles and Tusom2021, the three most frequently assigned languages are Bashkir (42.6%, 25.1%), English (30.2%, 67.7%), and Kinyarwanda (14.5%, 2.3%), which are all relatively high-resource languages in IPAPack++.\n<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#S6.T8\" title=\"Table 8 &#8227; Language token captures phonotactics &#8227; 6.2 Inspecting Task and Language Tokens &#8227; 6 Analysis &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;8</span></a> shows that assigning the detected language token yields better performance than always using English, while setting the language as unknown performs best. This indicates that the language token influences PR by shifting the output distribution toward the assigned language.</p>\n\n",
                "matched_terms": [
                    "tusom2021",
                    "voxangeles"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train a fully open-source phonetic speech foundation model POWSM using our scalable multi-task framework.\nOur model achieves state-of-the-art performance on PR while also supporting ASR across more than 70 languages.\nBeyond PR and ASR, the model&#8217;s ability to perform audio-guided G2P and P2G enables applications that require fine-grained linguistic analysis such as atypical speech assessment.\nOur analysis reveals that POWSM&#8217;s encoder benefits from phoneme-level CTC supervision and stronger encoder weighting, enhancing cross-lingual generalization.\nAdditionally, the model demonstrates interpretable multimodal and language-aware behaviors, effectively mediating between phonetic detail and standardized phonological patterns.\nTo conclude, POWSM not only provides a strong phone recognition foundation model for high-resource languages, but also acts as a versatile resource for unseen languages and socio-phonetic variation.</p>\n\n",
                "matched_terms": [
                    "g2p",
                    "p2g",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">POWSM has several limitations that we aim to address in future work.\nFirst, the model is neither strictly phonemic nor phonetic: its training data consist of cleaned and filtered phonemic transcriptions from multiple languages, which are not fully faithful to the phonetic or phonemic structure of the audio. Although phonemic transcriptions share similarities across languages, adding auxiliary tasks and language tokens may have reinforced language-specific biases. We also currently lack sufficient allophone-level data, which would provide more language-independent information.</p>\n\n",
                "matched_terms": [
                    "data",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All of our data is ethically sourced, either through permissive licensing or through proper consent.\nWe are aware of the implicit prescriptivism and representational harms <cite class=\"ltx_cite ltx_citemacro_citep\">(Crawford, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib10\" title=\"\">2017</a>)</cite> that normalizing socio-phonetic variation in ASR or PR models can create.\nThis may threaten linguistic diversity instead of preserving it.\nWe also acknowledge that accurate modeling of socio-phonetic variation can enable demographic inference, as demographics and phonetic variation are deeply intertwined <cite class=\"ltx_cite ltx_citemacro_citep\">(Labov, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib23\" title=\"\">1963</a>)</cite>.\nWe stress that uses of POWSM must align with our vision: a future where advances in spoken language processing and NLP do not leave low-resource varieties behind.</p>\n\n",
                "matched_terms": [
                    "data",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observed confusion in plosive voice-onset times on unseen languages in preliminary experiments, which is likely from English G2P data. For instance, broad phonemic transcription in English typically uses /b/ to transcribe the /b/ in /bat/, but its voice onset timing is actually voiceless in Mainstream American English and is closer to [p]. To mitigate this, we apply rule-based refinements to English G2P transcriptions, adjusting plosive voicing and aspiration, lateral velarization, and vowel nasalization.</p>\n\n",
                "matched_terms": [
                    "g2p",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first filter out languages with more than 8 hours of training data in IPAPack++ <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib64\" title=\"\">2025</a>)</cite>, keeping only those that are also present in FLEURS.\nThen, following the training data amounts reported in <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24992v1#bib.bib6\" title=\"\">2025</a>)</cite>, we further identify the 50 lowest-resource languages to exclude any that may have other substantial sources not included in IPAPack++.\nThis process leaves us with nine languages. We finally exclude <span class=\"ltx_text ltx_font_typewriter\">ell</span>, as it is comparatively higher-resource and because there are already three other Balto-Slavic languages.\nNote that other models use strictly more data than ours&#8212;not only in terms of dataset count but also because IPAPack++ applies additional data-quality filtering. <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.24992v1#A1.T10\" title=\"Table 10 &#8227; A.3 FLEURS language selection for ASR &#8227; Appendix A Appendix &#8227; POWSM: A Phonetic Open Whisper-Style Speech Foundation Model\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;10</span></a> lists the amount of ASR training data for baselines.</p>\n\n",
                "matched_terms": [
                    "data",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multi-tasking may improve performance by tying acoustic signals to well-defined symbolic representations, yet it may distract the model if the relationships are not learned effectively.\nWe train POWSM with different data and model scales to examine how multitask learning interacts with the setup, and use <span class=\"ltx_text ltx_font_typewriter\">beam=1</span> during decoding to speed up inference.</p>\n\n",
                "matched_terms": [
                    "data",
                    "different"
                ]
            }
        ]
    }
}