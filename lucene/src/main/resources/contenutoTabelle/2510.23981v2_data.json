{
    "S1.T1": {
        "caption": "Table 1: Related benchmarks for TeleEgo. Comparison across duration, egocentricity, streaming protocol, modality coverage, and long-memory QA. TeleEgo (ours) uniquely satisfies all dimensions.\n✓denotes supported, ✗  not applicable, and Partial  partial support.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F0F0F0;\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F0F0F0;\">Benchmark</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F0F0F0;\">Focus Scene</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F0F0F0;\">Video Duration</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F0F0F0;\">Tasks</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F0F0F0;\">Omni-Modal</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F0F0F0;\">Streaming</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F0F0F0;\">Egocentric</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F0F0F0;\">Long-Mem QA</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F7F7F7;\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">EgoExoLearn <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib10\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">10</span></a>]</cite></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">Ego&#8211;Exo Skill Assessment</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">avg. 13.4 min (ego)</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">avg. 4.5 min (exo)</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">Cross-view Tasks</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CC6600;--ltx-bg-color:#F7F7F7;\">Partial</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">&#10007;</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFFF;\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">EgoThink <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib3\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></a>]</cite></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">First-Person Thinking</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Object, Activity, Localization, Reasoning,</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Forecasting and planning</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10007;</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F7F7F7;\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">OVBench <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>]</cite></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">Online Video Understanding</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">30 s-1 h</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">Spatiotemporal Understanding and Interpretation</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">&#10007;</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFFF;\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">StreamingBench <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">Online Video Understanding</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">3 s-24 min</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Real-Time Visual, Omni-Source</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">and Contextual Understanding</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10007;</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F7F7F7;\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">EgoTextVQA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>]</cite></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">Egocentric Scene-text</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">avg. 101.7 s</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">Identification and Reasoning</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">&#10007;</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFFF;\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">X-LeBench <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib26\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">26</span></a>]</cite></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">Extra Long Egocentric data</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">23 min&#8211;16.4 h</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Temporal Localization, Summarization,</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Counting and Ordering</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10003;</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F7F7F7;\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">VStream-QA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib24\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">24</span></a>]</cite></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">Online Video Stream</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">avg. 40 min</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">Event and Scene Understanding</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CC6600;--ltx-bg-color:#F7F7F7;\">Partial</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">&#10007;</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFFF;\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">OVO-Bench <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>]</cite></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">Online Video Reasoning</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">avg. 263.42s</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Backward Tracing, Real-Time Visual</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Perception, and Forward Active Responding</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10007;</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F7F7F7;\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">ODV-Bench <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">Online Driving Video</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">5-90 s</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">Realtime Perception and Prediction</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">&#10007;</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFFF;\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">EgoLife <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib21\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">Egocentric Assistant</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">avg. 44.3 h</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">EntityLog, EventRecall, HabitInsight,</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">RelationMap and TaskMaster</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10003;</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F7F7F7;\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F7F7F7;\">TeleEgo (Ours)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">Real-world Settings</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">avg. 14.4 h</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">Memory, Understanding and Cross-Memory Reasoning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F7F7;\">&#10003;</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "applicable",
            "egoexolearn",
            "stream",
            "settings",
            "activity",
            "interpretation",
            "extra",
            "assessment",
            "realworld",
            "related",
            "forward",
            "ordering",
            "responding",
            "reasoning",
            "planning",
            "ours",
            "exo",
            "duration",
            "supported",
            "spatiotemporal",
            "event",
            "crossmemory",
            "not",
            "streaming",
            "eventrecall",
            "across",
            "avg",
            "teleego",
            "longmem",
            "ovobench",
            "online",
            "omnimodal",
            "benchmark",
            "min–164",
            "skill",
            "entitylog",
            "active",
            "object",
            "egothink",
            "satisfies",
            "taskmaster",
            "odvbench",
            "modality",
            "understanding",
            "tasks",
            "scene",
            "driving",
            "thinking",
            "egotextvqa",
            "memory",
            "ego–exo",
            "summarization",
            "video",
            "contextual",
            "backward",
            "perception",
            "support",
            "uniquely",
            "all",
            "data",
            "egocentricity",
            "counting",
            "benchmarks",
            "egocentric",
            "temporal",
            "streamingbench",
            "visual",
            "partial",
            "comparison",
            "assistant",
            "localization",
            "egolife",
            "identification",
            "relationmap",
            "prediction",
            "long",
            "ego",
            "omnisource",
            "26342s",
            "habitinsight",
            "protocol",
            "longmemory",
            "min",
            "scenetext",
            "✓denotes",
            "s24",
            "realtime",
            "vstreamqa",
            "crossview",
            "ovbench",
            "dimensions",
            "xlebench",
            "tracing",
            "forecasting",
            "focus",
            "coverage",
            "firstperson"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Despite these requirements, existing benchmarks evaluate these ability in isolation or in simplified settings (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; TeleEgo: Benchmarking Egocentric AI Assistants in the Wild\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nFirst, some focus on offline long-term memory (e.g., X-LeBench&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib26\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">26</span></a>]</cite>), while others test short-window streaming (e.g., StreamingBench&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite>, VStream-QA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib24\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">24</span></a>]</cite>), making it hard to assess the trade-off between memory and real-time performance.\nSecond, true egocentric streaming evaluation is rare. Most datasets use third-person or static videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>]</cite>, avoiding challenges like self-motion and viewpoint shifts. Some exceptions (e.g., ODV-Bench&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite>) use first-person footage, but with short sequences and limited multimodality.\nThird, few datasets offer long, continuous, real-world recordings. Many are short clips or image sets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib3\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>]</cite>. While X-LeBench and EgoLife&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib21\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite> are longer, the former stitches clips, and the latter is recorded in closed and controlled environments.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Egocentric AI assistants in real-world settings must process multi-modal inputs (video, audio, text), respond in real time, and retain evolving long-term memory. However, existing benchmarks typically evaluate these abilities in isolation, lack realistic streaming scenarios, or support only short-term tasks.\nWe introduce <span class=\"ltx_text ltx_font_bold\">TeleEgo</span>, a long-duration, streaming, omni-modal benchmark for evaluating egocentric AI assistants in realistic daily contexts. The dataset features over 14 hours per participant of synchronized egocentric video, audio, and text across four domains: work &amp; study, lifestyle &amp; routines, social activities, and outings &amp; culture. All data is aligned on a unified global timeline and includes high-quality visual narrations and speech transcripts, curated through human refinement.\nTeleEgo defines 12 diagnostic subtasks across three core capabilities: Memory (recalling past events), Understanding (interpreting the current moment), and Cross-Memory Reasoning (linking distant events). It contains 3,291 human-verified QA items spanning multiple question formats (single-choice, binary, multi-choice, and open-ended), evaluated strictly in a streaming setting.\nWe propose two key metrics &#8212; Real-Time Accuracy and Memory Persistence Time &#8212; to jointly assess correctness, temporal responsiveness, and long-term retention. TeleEgo provides a realistic and comprehensive evaluation to advance the development of practical AI assistants.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "settings",
                    "crossmemory",
                    "benchmarks",
                    "understanding",
                    "streaming",
                    "tasks",
                    "across",
                    "realworld",
                    "teleego",
                    "memory",
                    "realtime",
                    "reasoning",
                    "omnimodal",
                    "benchmark",
                    "video",
                    "support",
                    "all",
                    "data",
                    "egocentric",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the rapid advancement of artificial intelligence, egocentric AI assistants&#8212;those operating from a first-person perspective&#8212;are gradually transitioning from controlled experimental settings to real-world applications. To function effectively in such scenarios, these assistants must exhibit three tightly integrated capability: memory, streaming decision-making, and multimodal understanding. They must be able to retain and recall growing streams of past information; make timely judgments in the context of continuous audio-visual inputs; and interpret, in a unified manner, what the camera sees, what the microphone hears, and what the user expresses through language.\nImportantly, these capability are not exercised in isolation&#8212;they must work together in harmony. A model that remembers past events but acts at the wrong moment can still fail. Similarly, a system that processes the current frame but cannot identify the speaker or key objects is unlikely to succeed. Effective real-world AI assistants must therefore reason not only about what is happening, but also when and how to respond, based on long-term context and multimodal cues.</p>\n\n",
                "matched_terms": [
                    "realworld",
                    "firstperson",
                    "memory",
                    "understanding",
                    "streaming",
                    "settings",
                    "egocentric",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges, we present TeleEgo: a long-duration, streaming, and fully multimodal benchmark grounded in real-world scenarios, purpose-built to evaluate egocentric AI assistants (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; TeleEgo: Benchmarking Egocentric AI Assistants in the Wild\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). TeleEgo consists of synchronized video, audio, and textual data collected from multiple participants, each contributing more than 14 hours of recordings. The dataset spans four major domains: work &amp; study, lifestyle &amp; routines, social activities, and outings &amp; culture. All data streams are precisely aligned to a unified global timeline and enriched with manually curated speech transcripts and visual narrations to ensure high quality and semantic clarity.\nBuilding on this foundation, TeleEgo introduces 12 diagnostic tasks covering three core ability: <span class=\"ltx_text ltx_font_italic\">memory</span> (recalling past events), <span class=\"ltx_text ltx_font_italic\">understanding</span> (interpreting the present), and <span class=\"ltx_text ltx_font_italic\">cross-memory reasoning</span> (connecting distant moments). These tasks include a total of 3,291 human-verified QA items across various formats, including single-choice, binary, multiple-choice, and open-ended questions. Each task is tied to a specific time point and a decision window, requiring models not only to answer correctly, but also to respond at the right time. All evaluations are conducted under streaming conditions, and a task is considered successful only if the model&#8217;s first correct answer falls within the allowed window. This setup prevents models from guessing too early or answering too late, offering a more accurate measure of real-time decision-making and responsiveness.\nWe further introduce two key evaluation metrics: <span class=\"ltx_text ltx_font_italic\">Real-Time Accuracy</span>, which captures the model&#8217;s ability to produce correct responses within the given time constraint, and <span class=\"ltx_text ltx_font_italic\">Memory Persistence Time</span>, which measures how long a model can retain past information without re-encountering the original context. These metrics offer a comprehensive view of not just correctness, but also temporal responsiveness and long-term memory retention.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "crossmemory",
                    "not",
                    "understanding",
                    "streaming",
                    "tasks",
                    "long",
                    "across",
                    "realworld",
                    "teleego",
                    "memory",
                    "realtime",
                    "reasoning",
                    "benchmark",
                    "video",
                    "all",
                    "data",
                    "egocentric",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A unified, streaming, multimodal benchmark aligned to a global timeline with rich, real-world data across multiple participants and scenarios.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "across",
                    "realworld",
                    "streaming",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A diagnostic task suite covering memory, understanding, and cross-memory reasoning, with timestamped, multimodal evidence for interpretability.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "understanding",
                    "crossmemory",
                    "memory"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An evaluation protocol tailored to streaming inputs, using two core metrics, real-time accuracy and memory persistence, to assess comprehensive performance.</p>\n\n",
                "matched_terms": [
                    "protocol",
                    "streaming",
                    "realtime",
                    "memory"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Egocentric Models and Benchmarks.</span>\nEgocentric (first-person) vision research has grown significantly, evolving from early single-user recordings to diverse, large-scale datasets capturing daily life from a first-person perspective. Pioneering work such as EPIC-KITCHENS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib5\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></a>]</cite> and its extensions like VISOR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite> introduced large-scale, object-rich video datasets in home environments. Later, Ego4D and Ego-Exo4D <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib8\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib9\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">9</span></a>]</cite> broadened the scope to include tasks like episodic memory, future prediction, and skill learning, incorporating both egocentric and exocentric views.\nRecent datasets have begun to explore more complex cognitive tasks and assistant-like interactions. EgoLife <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib21\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite> captures long-duration recordings from a single household, while EgoThink <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib3\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></a>]</cite> and EgoExoLearn <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib10\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">10</span></a>]</cite> focus on segment-level reasoning and teaching-following dynamics. Other work like MM-Ego <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib22\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">22</span></a>]</cite> and EgoTextVQA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>]</cite> targets memory and text-based understanding. However, most of these evaluations remain offline and task-specific.\nTeleEgo advances egocentric benchmarking by combining multi-day, multi-role, and multi-theme recordings with dual text annotations aligned to a unified global timeline. Its online evaluation protocol measures not only real-time decision-making, but also the persistence of memory over time&#8212;enabling more realistic and comprehensive evaluations of assistant capabilities.</p>\n\n",
                "matched_terms": [
                    "egoexolearn",
                    "not",
                    "egothink",
                    "assistant",
                    "benchmarks",
                    "egolife",
                    "prediction",
                    "understanding",
                    "tasks",
                    "protocol",
                    "egotextvqa",
                    "teleego",
                    "memory",
                    "realtime",
                    "reasoning",
                    "online",
                    "video",
                    "skill",
                    "focus",
                    "egocentric",
                    "firstperson"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Streaming Video Understanding Benchmarks.</span>\nTo assess assistants in time-sensitive and dynamic environments, several benchmarks have emerged that focus on streaming video understanding. StreamingBench <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite>, OVBench, and OVO-Bench <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>]</cite> support online task formats, but typically span short episodes and lack sustained memory testing across events. ODV-Bench <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite> emphasizes driving tasks and short-term prediction, prioritizing perception over memory.\nSome datasets, such as X-LeBench <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib26\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">26</span></a>]</cite>, extend video QA to longer contexts, but still operate in offline settings without real-time constraints.\nTeleEgo fills this gap by offering continuous, multi-day egocentric video streams with temporally grounded question-answer pairs. Its evaluation protocol emphasizes real-time responses and long-term memory recall, supporting deeper analyses of assistant performance in realistic, ever-evolving scenarios.</p>\n\n",
                "matched_terms": [
                    "streamingbench",
                    "settings",
                    "assistant",
                    "egocentric",
                    "odvbench",
                    "prediction",
                    "understanding",
                    "streaming",
                    "tasks",
                    "across",
                    "driving",
                    "protocol",
                    "teleego",
                    "memory",
                    "ovobench",
                    "realtime",
                    "online",
                    "video",
                    "perception",
                    "support",
                    "ovbench",
                    "xlebench",
                    "focus",
                    "benchmarks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Omni-Modal Assistants.</span>\nRecent advancements in omni-modal models aim to unify understanding across multiple input types&#8212;such as text, vision, audio, and speech&#8212;enabling more flexible and human-like assistants. Closed-source systems like GPT-4o <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> and Gemini 1.5/2.5 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>]</cite> support end-to-end speech and video processing with long multimodal context windows, allowing for rich, coherent interactions.\nOpen-source progress has also accelerated. Models such as LLaVA&#8211;OneVision <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib13\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">13</span></a>]</cite> and InternVL-2.5 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib2\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></a>]</cite> support multi-image and video understanding. Others like Qwen2.5&#8211;Omni <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a>]</cite>, MiniCPM&#8211;o <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>]</cite>, VITA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">7</span></a>]</cite>, and Baichuan&#8211;Omni <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib14\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">14</span></a>]</cite> are optimized for real-time, streaming audiovisual input and response.\nDespite their strong performance, these models are typically evaluated in offline, task-specific settings. Few benchmarks assess whether assistants can respond accurately within time-sensitive decision windows or retain relevant information across long temporal spans.\nBy combining egocentric recordings, streaming video contexts, and real-time memory evaluation, TeleEgo provides a unified testbed to study omni-modal assistant performance in realistic environments&#8212;bridging the gap between research in egocentric datasets, streaming benchmarks, and multimodal models.</p>\n\n",
                "matched_terms": [
                    "across",
                    "egocentric",
                    "assistant",
                    "temporal",
                    "video",
                    "teleego",
                    "memory",
                    "understanding",
                    "streaming",
                    "support",
                    "settings",
                    "long",
                    "realtime",
                    "benchmarks",
                    "omnimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure broad diversity and real-world relevance, TeleEgo uses a carefully designed data collection protocol that spans multiple roles, themes, and tasks. We recruited five participants with balanced gender representation and a wide range of cultural, regional, and personality backgrounds, aiming to reflect a representative slice of the general population.\nEach participant wore a first-person camera over three consecutive days and recorded egocentric video following a set of predefined scenarios. The recordings include both solo activities and group interactions, taking place in diverse indoor and outdoor environments and across various social contexts. This approach goes beyond traditional single-household or uniform-group datasets, improving the generalizability and practical value of the benchmark.\nTo capture the richness of everyday human experience, we structured the data around four common life themes (see Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#S3.F2\" title=\"Figure 2 &#8227; 3 TeleEgo &#8227; TeleEgo: Benchmarking Egocentric AI Assistants in the Wild\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). These themes cover different cognitive demands, social situations, environmental settings, and physical activities.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "across",
                    "realworld",
                    "protocol",
                    "video",
                    "teleego",
                    "settings",
                    "tasks",
                    "data",
                    "egocentric",
                    "firstperson"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Lifestyle &amp; Routines</span> (e.g., shopping, exercising, walking, cooking):\nThis theme involves semi-structured daily activities that combine object handling and movement in dynamic yet familiar environments. It supports research on long-term activity recognition, task progression, and inferring higher-level states such as fatigue or task completion.</p>\n\n",
                "matched_terms": [
                    "activity",
                    "object"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Over the course of three days, each participant recorded a wide range of egocentric videos covering all four themes. The result is a rich, multi-role, multi-theme, multi-day dataset that supports research on long-term memory, context carryover, and generalization across different situations&#8212;key challenges in real-world perception.</p>\n\n",
                "matched_terms": [
                    "across",
                    "realworld",
                    "perception",
                    "memory",
                    "all",
                    "egocentric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure privacy and ethical use, all collected recordings go through a careful de-identification process. This includes blurring faces, removing speech from non-participants, and masking any sensitive visual or audio content. These steps preserve participant privacy while keeping the recordings natural and realistic, reflecting everyday first-person experiences. To support deeper multimodal understanding beyond raw audio and video, the TeleEgo dataset includes two types of time-aligned textual annotations.</p>\n\n",
                "matched_terms": [
                    "video",
                    "teleego",
                    "visual",
                    "understanding",
                    "support",
                    "all",
                    "firstperson"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech transcripts</span> captures all verbal communication in multi-person settings. Spoken content is automatically transcribed, then manually verified and annotated with speaker identities, yielding temporally aligned conversational transcripts that preserve discourse structure and interaction dynamics. This produces complete dialogue corpora suitable for studying social cognition, turn-taking, and multimodal grounding in egocentric contexts.</p>\n\n",
                "matched_terms": [
                    "all",
                    "settings",
                    "egocentric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Visual narrations</span> consists of participants&#8217; self-reported verbal descriptions of their ongoing activities and salient environmental details. When explicit actions are absent, narrations focus on attentional targets and key scene elements, providing semantic coverage of visual content such as object interactions, spatial relations, and contextual cues. Each narration is timestamped and aligned with the corresponding video segment, forming a natural-language layer that parallels the perceptual stream.</p>\n\n",
                "matched_terms": [
                    "stream",
                    "contextual",
                    "video",
                    "visual",
                    "scene",
                    "coverage",
                    "focus",
                    "object"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both streams are precisely synchronized with the video timeline, producing dual-layer annotations for visual and linguistic events. This structure enables rich cross-modal grounding. The processed multimodal data is then used by powerful AI tools to generate candidate QA items, which are further refined through human verification to build our benchmark system (see Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#S3.F3\" title=\"Figure 3 &#8227; 3.1 Dataset Overview &#8227; 3 TeleEgo &#8227; TeleEgo: Benchmarking Egocentric AI Assistants in the Wild\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Collectively, TeleEgo offers real-world recordings that combine perception, language, and memory, furnishing comprehensive multimodal material for evaluating AI systems&#8217; capacity to understand and retain complex first-person experiences.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "realworld",
                    "video",
                    "perception",
                    "teleego",
                    "memory",
                    "visual",
                    "data",
                    "firstperson"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate multidimensional cognitive abilities in egocentric video understanding, we introduce a benchmark that spans three key cognitive dimensions: memory, comprehension, and cross-memory reasoning (see Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#S3.F4\" title=\"Figure 4 &#8227; 3.2 Raw Data Processing &#8227; 3 TeleEgo &#8227; TeleEgo: Benchmarking Egocentric AI Assistants in the Wild\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). These dimensions form a hierarchical structure that reflects different levels of cognition, ranging from momentary perception to long-term reasoning. This framework enables a systematic distinction between information retention, semantic understanding, and integrative reasoning across time and entities. To support this evaluation, we design 12 fine-grained question-answering subtasks, each corresponding to one of the three dimensions, allowing us to assess model performance across a wide range of cognitive scenarios. Examples of these subtasks are illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#S3.F5\" title=\"Figure 5 &#8227; 3.3 Benchmark Task Design &#8227; 3 TeleEgo &#8227; TeleEgo: Benchmarking Egocentric AI Assistants in the Wild\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "across",
                    "video",
                    "reasoning",
                    "perception",
                    "support",
                    "memory",
                    "understanding",
                    "dimensions",
                    "crossmemory",
                    "egocentric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">Memory</span> focuses on temporally grounded recall, assessing a model&#8217;s ability to retain, retrieve, and compare events over different time spans. Tasks range from short-term recall of transient object states and actions, to long-term and ultra-long memory over extended episodes, as well as continuous entity tracking and temporal interval reasoning. These tasks evaluate how well models maintain temporal coherence and represent evolving dynamics in first-person experiences.</p>\n\n",
                "matched_terms": [
                    "memory",
                    "tasks",
                    "reasoning",
                    "object",
                    "firstperson",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">Understanding</span> dimension measures a model&#8217;s capacity to grasp meaning and coherence within complex, context-rich scenarios. It goes beyond surface perception to assess how well a model understands causal structures and human intent. This includes recognizing cause-effect relationships, inferring latent motivations, and constructing a unified interpretation from temporally or spatially dispersed cues. It also requires integrating multimodal inputs into coherent semantic representations.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "interpretation",
                    "perception"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">Cross-Memory Reasoning</span> dimension challenges models to combine information across disjoint time periods and entity contexts. Tasks require building global narrative structures, linking distant events into causal chains, inferring relational dynamics between interacting agents, and synthesizing long temporal sequences into structured, meaningful processes. This dimension represents the most complex aspect of egocentric cognition, requiring reasoning over long-range dependencies in continuous experiences.</p>\n\n",
                "matched_terms": [
                    "across",
                    "reasoning",
                    "tasks",
                    "long",
                    "crossmemory",
                    "egocentric",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt four complementary QA formats: single-choice (mc_single), multiple-choice (mc_multi), binary, and open-ended. Each format serves a distinct purpose. The mc_single format allows for precise evaluation through carefully crafted distractors and unambiguous correct answers. The mc_multi format captures complex or uncertain scenarios by permitting multiple correct options. Binary questions offer high-precision evaluation at low annotation cost. Open-ended questions encourage free-form reasoning and compositional thinking, complementing the more structured formats. Together, these formats strike a balance between standardization and expressiveness, enabling scalable evaluation while supporting fine-grained behavioral probing. Our QA generation process begins with post-processed, time-aligned transcripts of speech and narration. We use state-of-the-art large language models (GPT-5-Thinking and Gemini-2.5-Pro) to draft initial QA candidates. For the Ultra-Long Memory subcategory, the models ingest full dual-stream transcripts and generate questions grounded in evidence spanning 10&#8211;60 minutes. For the remaining eleven subcategories, we segment each recording into 30-minute windows and prompt the models to generate QA pairs with evidence evenly distributed across each window. Human annotators then verify factual alignment with the source video, correct timestamps, and remove ambiguous or low-quality items.</p>\n\n",
                "matched_terms": [
                    "across",
                    "video",
                    "thinking",
                    "memory",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#S3.F4\" title=\"Figure 4 &#8227; 3.2 Raw Data Processing &#8227; 3 TeleEgo &#8227; TeleEgo: Benchmarking Egocentric AI Assistants in the Wild\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the TeleEgo benchmark is hierarchically structured along three key cognitive dimensions&#8212;memory, understanding, and reasoning&#8212;with twelve subcategories providing finer granularity. The final dataset contains 3,291 verified QA instances across all four formats (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Benchmark Task Design &#8227; 3 TeleEgo &#8227; TeleEgo: Benchmarking Egocentric AI Assistants in the Wild\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Subcategory distributions demonstrate balanced coverage across temporal, causal, and semantic reasoning challenges. Collectively, these design choices make TeleEgo a robust and discriminative benchmark for evaluating embodied video understanding models.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "across",
                    "video",
                    "teleego",
                    "understanding",
                    "all",
                    "reasoning",
                    "coverage",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:70%;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:718.4pt;height:121.9pt;vertical-align:-59.2pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#F6F8FC;\">\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F6F8FC;\">Method</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F6F8FC;\">Params</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F6F8FC;\">Omni</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F6F8FC;\">Streaming</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_6\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">Memory (%)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_5\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">Understanding (%)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_4\" style=\"--ltx-bg-color:#FFF3E7;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFF3E7;\">Cross&#8211;Memory Reasoning (%)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F6F8FC;\">Overall</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">UlM</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">StM</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">ET</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">TCI</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">LtM</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">All</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">II</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">CU</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">CmU</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">MsR</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">All</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFF3E7;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFF3E7;\">CeR</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFF3E7;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFF3E7;\">TCU</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFF3E7;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFF3E7;\">CtC</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#FFF3E7;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFF3E7;\">All</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#F4F4F6;\">\n<span class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_20\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FCFCFC;\">Proprietary MLLMs</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">GPT-4o <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#8211;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#10003;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#8211;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">47.71</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">47.68</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">34.05</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">39.76</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">34.87</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">42.69</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">78.40</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">60.09</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">53.59</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">47.30</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">60.92</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">33.11</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">57.69</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">58.73</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">45.87</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">48.04</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FCFCFC;\">\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">Gemini-2.5-Pro <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#8211;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10003;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#8211;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">46.52</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">49.63</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">30.47</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">35.34</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">39.92</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">42.23</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">71.36</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">60.56</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">49.28</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">47.30</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">57.98</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">28.48</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">50.00</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">52.38</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">40.26</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">46.35</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#F4F4F6;\">\n<span class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_20\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFFFFF;\">Open&#8211;Source MLLMs</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FCFCFC;\">\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">Qwen2.5-VL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">8B</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10007;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10007;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">34.63</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">39.85</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">27.24</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">37.35</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">28.57</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">34.24</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">43.66</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">44.60</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">28.71</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">22.30</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">35.89</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">21.85</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">26.92</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">34.13</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">27.39</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">33.96</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">VideoChat-Online <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">4B</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#10007;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#10003;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">32.26</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">31.78</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">23.66</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">26.51</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">24.37</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">28.91</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">57.28</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">44.13</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">34.93</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">25.68</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">41.76</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">18.54</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">42.06</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">26.92</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">29.04</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">32.46</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FCFCFC;\">\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">7B</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10003;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10007;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">26.15</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">29.58</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">21.15</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">26.10</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">20.17</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">25.34</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">32.86</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">32.39</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">23.44</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">17.57</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">27.33</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">15.89</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">23.08</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">24.60</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">20.13</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">25.33</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">MiniCPM-o <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">8B</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#10007;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#10003;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">43.63</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">44.01</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">29.75</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">39.36</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">39.50</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">40.36</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">67.14</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">51.17</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">40.67</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">37.84</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">50.19</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">25.83</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">26.92</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">55.56</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">38.28</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">42.84</span></span></span>\n</span></span>\n</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "memory",
                    "understanding",
                    "streaming",
                    "all",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:70%;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:701.9pt;height:121.9pt;vertical-align:-59.2pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#F6F8FC;\">\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F6F8FC;\">Method</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F6F8FC;\">Params</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F6F8FC;\">Omni</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F6F8FC;\">Streaming</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_6\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">Memory (min)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_5\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">Understanding (min)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_4\" style=\"--ltx-bg-color:#FFF3E7;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFF3E7;\">Cross&#8211;Memory Reasoning (min)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F6F8FC;\">Overall</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">UlM</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">StM</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">ET</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">TCI</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">LtM</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">All</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">II</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">CU</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">CmU</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">MsR</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">All</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFF3E7;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFF3E7;\">CeR</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFF3E7;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFF3E7;\">TCU</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFF3E7;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFF3E7;\">CtC</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#FFF3E7;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFF3E7;\">All</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#F4F4F6;\">\n<span class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_20\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FCFCFC;\">Proprietary MLLMs</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">GPT-4o <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#8211;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#10003;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#8211;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">3.41</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">2.66</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.36</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">2.06</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.82</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">2.60</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">6.27</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">4.36</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">2.80</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">3.14</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">4.49</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.60</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">4.04</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">3.82</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">3.06</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">3.01</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FCFCFC;\">\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">Gemini-2.5-Pro <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#8211;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10003;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#8211;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">3.17</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">2.22</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.13</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.94</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.89</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">2.37</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">5.70</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">4.58</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">2.34</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">3.11</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">4.22</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.15</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">4.19</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">3.26</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">2.62</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">2.76</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#F4F4F6;\">\n<span class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_20\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFFFFF;\">Open&#8211;Source MLLMs</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FCFCFC;\">\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">Qwen2.5-VL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">8B</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10007;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10007;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.80</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.77</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.39</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.76</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.11</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.66</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">2.09</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">2.29</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.22</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">0.87</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.83</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">0.97</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.42</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.56</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.31</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.60</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">VideoChat-Online <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">4B</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#10007;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#10003;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.30</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.03</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">0.65</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">0.84</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">0.82</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.03</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">3.72</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">2.17</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.25</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.11</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">2.41</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">0.46</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">0.62</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.87</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.32</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.33</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FCFCFC;\">\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">7B</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10003;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10007;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.06</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.09</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">0.80</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.20</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">0.61</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.00</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.37</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.32</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.02</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">0.74</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.20</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">0.48</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.04</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.02</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">0.81</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.00</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">MiniCPM-o <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">8B</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#10007;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#10003;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">2.27</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.58</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.16</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.79</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.66</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.82</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">4.58</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">3.43</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">2.03</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">2.18</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">3.37</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">0.99</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.77</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">3.46</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">2.53</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">2.19</span></span></span>\n</span></span>\n</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "min",
                    "memory",
                    "understanding",
                    "streaming",
                    "all",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All evaluations adhere to the streaming protocol. Video frames and audio are timestamped to a single global clock. For question-answer (QA) instance <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, we define a decision window <math alttext=\"[t^{(i)}_{\\mathrm{s}},\\,t^{(i)}_{\\mathrm{s}}+T]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>t</mi><mi mathvariant=\"normal\">s</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo rspace=\"0.337em\">,</mo><mrow><msubsup><mi>t</mi><mi mathvariant=\"normal\">s</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>+</mo><mi>T</mi></mrow><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[t^{(i)}_{\\mathrm{s}},\\,t^{(i)}_{\\mathrm{s}}+T]</annotation></semantics></math>, where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> denotes the time margin. At test time, the model must respond within this window. We set <math alttext=\"T=5\\,\\mathrm{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><mn>5</mn><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">s</mi></mrow></mrow><annotation encoding=\"application/x-tex\">T=5\\,\\mathrm{s}</annotation></semantics></math> in all experiments. We report two primary metrics:</p>\n\n",
                "matched_terms": [
                    "protocol",
                    "streaming",
                    "video",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2) Memory Persistence Time (MPT, minutes)</span>: For each item correctly answered at time <math alttext=\"t^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p3.m1\" intent=\":literal\"><semantics><msup><mi>t</mi><mo>&#8902;</mo></msup><annotation encoding=\"application/x-tex\">t^{\\star}</annotation></semantics></math>, we continue streaming without repeating the original evidence. At regular intervals, we re-query the same item. MPT is the time from <math alttext=\"t^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p3.m2\" intent=\":literal\"><semantics><msup><mi>t</mi><mo>&#8902;</mo></msup><annotation encoding=\"application/x-tex\">t^{\\star}</annotation></semantics></math> until the first failed recall. If the item is never answered correctly, MPT<math alttext=\"\\,{=}\\,0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p3.m3\" intent=\":literal\"><semantics><mrow><mi/><mo lspace=\"0.448em\">=</mo><mn>&#8201;0</mn></mrow><annotation encoding=\"application/x-tex\">\\,{=}\\,0</annotation></semantics></math>. If it is never forgotten within the probing range, it is right-censored at the last probe. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#S3.F6\" title=\"Figure 6 &#8227; 3.4 Benchmark QA Annotation &#8227; 3 TeleEgo &#8227; TeleEgo: Benchmarking Egocentric AI Assistants in the Wild\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> provides a schematic overview of the pipeline used to compute Memory Persistence Time (MPT).</p>\n\n",
                "matched_terms": [
                    "streaming",
                    "memory"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate six state-of-the-art vision-language or omni-modal models, covering both proprietary and open-source systems, as well as streaming-specialized designs:\nGemini-2.5-Pro <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>]</cite>, GPT-4o <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite>, Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a>]</cite>, Videochat-Online <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>]</cite>, Qwen2.5-VL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite>, and MiniCPM-o <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>]</cite>. All models receive synchronized video, audio, and text inputs unless stated otherwise. For models lacking built-in speech recognition, we attach a Whisper-style ASR component <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib17\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">17</span></a>]</cite> to transcribe audio.</p>\n\n",
                "matched_terms": [
                    "all",
                    "video",
                    "omnimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All experiments are conducted on a single NVIDIA H200 GPU (140 GB). At inference time, for each role we order all videos by their start timestamps and concatenate them into one continuous stream, mirroring realistic personal-assistant usage where the assistant is invoked intermittently around task- or context-specific segments rather than running continuously.</p>\n\n",
                "matched_terms": [
                    "all",
                    "assistant",
                    "stream"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each item correctly answered at time <math alttext=\"t^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><msup><mi>t</mi><mo>&#8902;</mo></msup><annotation encoding=\"application/x-tex\">t^{\\star}</annotation></semantics></math>, we schedule up to ten recall evaluations at <math alttext=\"t^{\\star}+r\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mrow><msup><mi>t</mi><mo>&#8902;</mo></msup><mo>+</mo><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#916;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">t^{\\star}+r\\Delta</annotation></semantics></math> (<math alttext=\"\\Delta=60\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo>=</mo><mn>60</mn></mrow><annotation encoding=\"application/x-tex\">\\Delta=60</annotation></semantics></math>&#8201;s; <math alttext=\"r=1,\\dots,10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mn>10</mn></mrow></mrow><annotation encoding=\"application/x-tex\">r=1,\\dots,10</annotation></semantics></math>). At each evaluation, the original evidence is not replayed; only the ongoing stream is available. If an item fails an evaluation, it is removed from subsequent rounds; its horizon is the elapsed time from <math alttext=\"t^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><msup><mi>t</mi><mo>&#8902;</mo></msup><annotation encoding=\"application/x-tex\">t^{\\star}</annotation></semantics></math> to the first failed evaluation.</p>\n\n",
                "matched_terms": [
                    "stream",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#S3.T3\" title=\"Table 3 &#8227; 3.4 Benchmark QA Annotation &#8227; 3 TeleEgo &#8227; TeleEgo: Benchmarking Egocentric AI Assistants in the Wild\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#S3.T4\" title=\"Table 4 &#8227; 3.4 Benchmark QA Annotation &#8227; 3 TeleEgo &#8227; TeleEgo: Benchmarking Egocentric AI Assistants in the Wild\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, under the strict streaming protocol and evidence-compliance constraints of TeleEgo, the results reveal a clear and structured pattern. Proprietary multimodal assistants (e.g., GPT-4o, Gemini-2.5-Pro) achieve strong overall performance in both RTA and MPT. However, their advantage is concentrated in the Understanding axis (e.g., GPT-4o reaches 61% in Understanding-All), while performance drops significantly in tasks requiring fine-grained temporal binding and cross-modal attribution (43% in Memory-All, 46% in Cross-Memory-All). This &#8220;semantic-strong but temporally-weak&#8221; trend is consistent across subtasks: intent inference approaches near-ceiling accuracy, whereas entity tracking and cross-entity relation inference remain the weakest, indicating that current systems heavily rely on semantic priors but struggle with timestamp alignment and instance-level grounding.</p>\n\n",
                "matched_terms": [
                    "across",
                    "protocol",
                    "teleego",
                    "understanding",
                    "streaming",
                    "tasks",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Interestingly, some open-source models with native streaming designs (e.g., MiniCPM-o) significantly close the RTA gap with proprietary systems, despite having fewer parameters. This suggests that managing temporal states and controlling output emissions may matter more than broad multimodal coverage. Conversely, models with audio-visual-text fusion but without streaming mechanisms show limited benefit in TeleEgo&#8217;s &#8220;correct-then-timed-and-verifiable&#8221; setting. This highlights that latency handling, cache scheduling, and alignment logic are the true drivers of real-time accuracy.</p>\n\n",
                "matched_terms": [
                    "streaming",
                    "coverage",
                    "realtime",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From a temporal persistence perspective, MPT further reveals a disconnect between what models remember and how long they remember it. Proprietary models sustain longer persistence on understanding-oriented tasks (e.g., GPT-4o achieves 6.3 minutes MPT on intent inference) but only 2&#8211;3 minutes on memory-centric tasks. Open-source models show shorter persistence across the board. This suggests that while models can compress long experiences into abstract semantic representations, they struggle to retain auditable, time-anchored evidence and dynamic entity states.</p>\n\n",
                "matched_terms": [
                    "across",
                    "tasks",
                    "long",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The divergence between RTA and MPT under a 5-second decision window and evidence-overlap constraints points to two complementary optimization directions: 1) Timestamp-aware temporal learning &#8212; where decoding conditioned on timestamps and calibrated silence policies improve when to respond; 2) Structured long-term memory architectures &#8212; integrating clock-indexed event keys with multimodal anchors to improve how to substantiate outputs.\nOverall, TeleEgo tightly couples correctness, grounding, and timing, shifting the primary bottleneck of egocentric assistants from sheer context length to verifiable alignment and real-time temporal control. This establishes a concrete and actionable frontier for future research.</p>\n\n",
                "matched_terms": [
                    "teleego",
                    "memory",
                    "realtime",
                    "event",
                    "egocentric",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, we present TeleEgo, an online, omni-modal, first-person benchmark grounded in real-world use, built from continuous, multi-participant, multi-scene, multi-day recordings that align video, ambient speech/dialogue, and dual textual timelines under a unified clock, and equipped with a contract-based annotation scheme that binds each query to its required modalities and precise, time-stamped evidence spans for auditable attribution. Centered on three capability axes, Memory, Understanding, and Cross-Memory Reasoning, TeleEgo offers a fine-grained task suite and a strict streaming-only evaluation protocol: responses receive credit only if they arrive within task-specific decision windows and satisfy evidence compliance. Two complementary metrics, Real-time Accuracy, and Memory Persistence Time, jointly assess correctness, response timing, and long-horizon memory, while failure cases are decomposed into retention, retrieval, alignment, and timing to yield actionable diagnostics. We envision TeleEgo as an ecologically valid, diagnostically informative, and reproducible foundation for building first-person assistants that must remember, align, and act in real time.</p>\n\n",
                "matched_terms": [
                    "crossmemory",
                    "benchmark",
                    "realworld",
                    "protocol",
                    "video",
                    "teleego",
                    "memory",
                    "understanding",
                    "realtime",
                    "reasoning",
                    "firstperson",
                    "online",
                    "omnimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We thank the AI Glasses product team at China Telecom for their enthusiastic support and valuable assistance.\nWe are grateful to Professor Haiwei Wu&#8217;s research group at the University of Electronic Science and Technology of China for their great help in data annotation, validation, and the construction of QA pairs.</p>\n\n",
                "matched_terms": [
                    "data",
                    "support"
                ]
            }
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Statistics of TeleEgo benchmark. Left: Overview of task categories and QA types, showing a balanced mixture across Memory, Understanding, and Cross-Memory Reasoning dimensions. Right: Subcategory-level distribution over twelve cognitive tasks, encompassing a total of 3,291 QA instances. The benchmark spans short- to ultra-long memory, causal and intent reasoning, and cross-temporal integration, providing a comprehensive foundation for evaluating multimodal and embodied intelligence models.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">Subcategory</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">Count</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">Share (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#FFCCCC;\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFCCCC;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Ultra-long Memory</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"722\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m15\" intent=\":literal\" style=\"--ltx-bg-color:#FFCCCC;\"><semantics><mn mathbackground=\"#FFCCCC\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#FFCCCC;\">722</mn><annotation encoding=\"application/x-tex\">722</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"21.9\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m16\" intent=\":literal\" style=\"--ltx-bg-color:#FFCCCC;\"><semantics><mn mathbackground=\"#FFCCCC\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#FFCCCC;\">21.9</mn><annotation encoding=\"application/x-tex\">21.9</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#FFCCCC;\">\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFCCCC;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Short-term Memory</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"414\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m17\" intent=\":literal\" style=\"--ltx-bg-color:#FFCCCC;\"><semantics><mn mathbackground=\"#FFCCCC\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#FFCCCC;\">414</mn><annotation encoding=\"application/x-tex\">414</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"12.6\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m18\" intent=\":literal\" style=\"--ltx-bg-color:#FFCCCC;\"><semantics><mn mathbackground=\"#FFCCCC\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#FFCCCC;\">12.6</mn><annotation encoding=\"application/x-tex\">12.6</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#FFCCCC;\">\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFCCCC;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Entity Tracking</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"289\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m19\" intent=\":literal\" style=\"--ltx-bg-color:#FFCCCC;\"><semantics><mn mathbackground=\"#FFCCCC\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#FFCCCC;\">289</mn><annotation encoding=\"application/x-tex\">289</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"8.8\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m20\" intent=\":literal\" style=\"--ltx-bg-color:#FFCCCC;\"><semantics><mn mathbackground=\"#FFCCCC\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#FFCCCC;\">8.8</mn><annotation encoding=\"application/x-tex\">8.8</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#FFCCCC;\">\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFCCCC;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Temporal Comparison &amp; Interval</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"259\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m21\" intent=\":literal\" style=\"--ltx-bg-color:#FFCCCC;\"><semantics><mn mathbackground=\"#FFCCCC\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#FFCCCC;\">259</mn><annotation encoding=\"application/x-tex\">259</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"7.9\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m22\" intent=\":literal\" style=\"--ltx-bg-color:#FFCCCC;\"><semantics><mn mathbackground=\"#FFCCCC\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#FFCCCC;\">7.9</mn><annotation encoding=\"application/x-tex\">7.9</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#FFCCCC;\">\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFCCCC;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Long-term Memory</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"253\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m23\" intent=\":literal\" style=\"--ltx-bg-color:#FFCCCC;\"><semantics><mn mathbackground=\"#FFCCCC\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#FFCCCC;\">253</mn><annotation encoding=\"application/x-tex\">253</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"7.7\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m24\" intent=\":literal\" style=\"--ltx-bg-color:#FFCCCC;\"><semantics><mn mathbackground=\"#FFCCCC\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#FFCCCC;\">7.7</mn><annotation encoding=\"application/x-tex\">7.7</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFCC;\">\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFCC;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Intent Inference</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"238\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m25\" intent=\":literal\" style=\"--ltx-bg-color:#FFFFCC;\"><semantics><mn mathbackground=\"#FFFFCC\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#FFFFCC;\">238</mn><annotation encoding=\"application/x-tex\">238</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"7.2\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m26\" intent=\":literal\" style=\"--ltx-bg-color:#FFFFCC;\"><semantics><mn mathbackground=\"#FFFFCC\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#FFFFCC;\">7.2</mn><annotation encoding=\"application/x-tex\">7.2</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFCC;\">\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFCC;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Causal Understanding</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"225\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m27\" intent=\":literal\" style=\"--ltx-bg-color:#FFFFCC;\"><semantics><mn mathbackground=\"#FFFFCC\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#FFFFCC;\">225</mn><annotation encoding=\"application/x-tex\">225</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"6.9\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m28\" intent=\":literal\" style=\"--ltx-bg-color:#FFFFCC;\"><semantics><mn mathbackground=\"#FFFFCC\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#FFFFCC;\">6.9</mn><annotation encoding=\"application/x-tex\">6.9</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFCC;\">\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFCC;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Cross-modal Understanding</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"219\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m29\" intent=\":literal\" style=\"--ltx-bg-color:#FFFFCC;\"><semantics><mn mathbackground=\"#FFFFCC\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#FFFFCC;\">219</mn><annotation encoding=\"application/x-tex\">219</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"6.8\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m30\" intent=\":literal\" style=\"--ltx-bg-color:#FFFFCC;\"><semantics><mn mathbackground=\"#FFFFCC\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#FFFFCC;\">6.8</mn><annotation encoding=\"application/x-tex\">6.8</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFCC;\">\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFCC;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Multi-step Reasoning</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"215\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m31\" intent=\":literal\" style=\"--ltx-bg-color:#FFFFCC;\"><semantics><mn mathbackground=\"#FFFFCC\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#FFFFCC;\">215</mn><annotation encoding=\"application/x-tex\">215</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"6.5\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m32\" intent=\":literal\" style=\"--ltx-bg-color:#FFFFCC;\"><semantics><mn mathbackground=\"#FFFFCC\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#FFFFCC;\">6.5</mn><annotation encoding=\"application/x-tex\">6.5</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6FF;\">\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#E6E6FF;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Cross-entity Relation</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"159\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m33\" intent=\":literal\" style=\"--ltx-bg-color:#E6E6FF;\"><semantics><mn mathbackground=\"#E6E6FF\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#E6E6FF;\">159</mn><annotation encoding=\"application/x-tex\">159</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"4.8\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m34\" intent=\":literal\" style=\"--ltx-bg-color:#E6E6FF;\"><semantics><mn mathbackground=\"#E6E6FF\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#E6E6FF;\">4.8</mn><annotation encoding=\"application/x-tex\">4.8</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6FF;\">\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#E6E6FF;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Temporal Chain Understanding</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"152\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m35\" intent=\":literal\" style=\"--ltx-bg-color:#E6E6FF;\"><semantics><mn mathbackground=\"#E6E6FF\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#E6E6FF;\">152</mn><annotation encoding=\"application/x-tex\">152</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"4.6\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m36\" intent=\":literal\" style=\"--ltx-bg-color:#E6E6FF;\"><semantics><mn mathbackground=\"#E6E6FF\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#E6E6FF;\">4.6</mn><annotation encoding=\"application/x-tex\">4.6</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6FF;\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#E6E6FF;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Cross-temporal Causality</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"146\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m37\" intent=\":literal\" style=\"--ltx-bg-color:#E6E6FF;\"><semantics><mn mathbackground=\"#E6E6FF\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#E6E6FF;\">146</mn><annotation encoding=\"application/x-tex\">146</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"4.4\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T2.m38\" intent=\":literal\" style=\"--ltx-bg-color:#E6E6FF;\"><semantics><mn mathbackground=\"#E6E6FF\" mathsize=\"0.900em\" style=\"--ltx-bg-color:#E6E6FF;\">4.4</mn><annotation encoding=\"application/x-tex\">4.4</annotation></semantics></math></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "categories",
            "models",
            "balanced",
            "tracking",
            "foundation",
            "left",
            "inference",
            "integration",
            "crossmemory",
            "interval",
            "comparison",
            "evaluating",
            "subcategorylevel",
            "mixture",
            "subcategory",
            "showing",
            "types",
            "statistics",
            "understanding",
            "short",
            "tasks",
            "distribution",
            "over",
            "across",
            "count",
            "multistep",
            "instances",
            "causal",
            "temporal",
            "crossentity",
            "causality",
            "shortterm",
            "relation",
            "twelve",
            "teleego",
            "memory",
            "intent",
            "reasoning",
            "overview",
            "spans",
            "intelligence",
            "comprehensive",
            "encompassing",
            "right",
            "benchmark",
            "multimodal",
            "cognitive",
            "ultralong",
            "task",
            "total",
            "share",
            "longterm",
            "dimensions",
            "crosstemporal",
            "entity",
            "embodied",
            "crossmodal",
            "chain",
            "providing"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#S3.F4\" title=\"Figure 4 &#8227; 3.2 Raw Data Processing &#8227; 3 TeleEgo &#8227; TeleEgo: Benchmarking Egocentric AI Assistants in the Wild\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the TeleEgo benchmark is hierarchically structured along three key cognitive dimensions&#8212;memory, understanding, and reasoning&#8212;with twelve subcategories providing finer granularity. The final dataset contains 3,291 verified QA instances across all four formats (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Benchmark Task Design &#8227; 3 TeleEgo &#8227; TeleEgo: Benchmarking Egocentric AI Assistants in the Wild\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Subcategory distributions demonstrate balanced coverage across temporal, causal, and semantic reasoning challenges. Collectively, these design choices make TeleEgo a robust and discriminative benchmark for evaluating embodied video understanding models.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Egocentric AI assistants in real-world settings must process multi-modal inputs (video, audio, text), respond in real time, and retain evolving long-term memory. However, existing benchmarks typically evaluate these abilities in isolation, lack realistic streaming scenarios, or support only short-term tasks.\nWe introduce <span class=\"ltx_text ltx_font_bold\">TeleEgo</span>, a long-duration, streaming, omni-modal benchmark for evaluating egocentric AI assistants in realistic daily contexts. The dataset features over 14 hours per participant of synchronized egocentric video, audio, and text across four domains: work &amp; study, lifestyle &amp; routines, social activities, and outings &amp; culture. All data is aligned on a unified global timeline and includes high-quality visual narrations and speech transcripts, curated through human refinement.\nTeleEgo defines 12 diagnostic subtasks across three core capabilities: Memory (recalling past events), Understanding (interpreting the current moment), and Cross-Memory Reasoning (linking distant events). It contains 3,291 human-verified QA items spanning multiple question formats (single-choice, binary, multi-choice, and open-ended), evaluated strictly in a streaming setting.\nWe propose two key metrics &#8212; Real-Time Accuracy and Memory Persistence Time &#8212; to jointly assess correctness, temporal responsiveness, and long-term retention. TeleEgo provides a realistic and comprehensive evaluation to advance the development of practical AI assistants.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "over",
                    "across",
                    "multimodal",
                    "temporal",
                    "shortterm",
                    "reasoning",
                    "longterm",
                    "teleego",
                    "memory",
                    "understanding",
                    "tasks",
                    "crossmemory",
                    "comprehensive",
                    "evaluating"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the rapid advancement of artificial intelligence, egocentric AI assistants&#8212;those operating from a first-person perspective&#8212;are gradually transitioning from controlled experimental settings to real-world applications. To function effectively in such scenarios, these assistants must exhibit three tightly integrated capability: memory, streaming decision-making, and multimodal understanding. They must be able to retain and recall growing streams of past information; make timely judgments in the context of continuous audio-visual inputs; and interpret, in a unified manner, what the camera sees, what the microphone hears, and what the user expresses through language.\nImportantly, these capability are not exercised in isolation&#8212;they must work together in harmony. A model that remembers past events but acts at the wrong moment can still fail. Similarly, a system that processes the current frame but cannot identify the speaker or key objects is unlikely to succeed. Effective real-world AI assistants must therefore reason not only about what is happening, but also when and how to respond, based on long-term context and multimodal cues.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "longterm",
                    "memory",
                    "understanding",
                    "intelligence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these requirements, existing benchmarks evaluate these ability in isolation or in simplified settings (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; TeleEgo: Benchmarking Egocentric AI Assistants in the Wild\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nFirst, some focus on offline long-term memory (e.g., X-LeBench&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib26\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">26</span></a>]</cite>), while others test short-window streaming (e.g., StreamingBench&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite>, VStream-QA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib24\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">24</span></a>]</cite>), making it hard to assess the trade-off between memory and real-time performance.\nSecond, true egocentric streaming evaluation is rare. Most datasets use third-person or static videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>]</cite>, avoiding challenges like self-motion and viewpoint shifts. Some exceptions (e.g., ODV-Bench&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite>) use first-person footage, but with short sequences and limited multimodality.\nThird, few datasets offer long, continuous, real-world recordings. Many are short clips or image sets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib3\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>]</cite>. While X-LeBench and EgoLife&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib21\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite> are longer, the former stitches clips, and the latter is recorded in closed and controlled environments.</p>\n\n",
                "matched_terms": [
                    "short",
                    "longterm",
                    "memory"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges, we present TeleEgo: a long-duration, streaming, and fully multimodal benchmark grounded in real-world scenarios, purpose-built to evaluate egocentric AI assistants (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; TeleEgo: Benchmarking Egocentric AI Assistants in the Wild\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). TeleEgo consists of synchronized video, audio, and textual data collected from multiple participants, each contributing more than 14 hours of recordings. The dataset spans four major domains: work &amp; study, lifestyle &amp; routines, social activities, and outings &amp; culture. All data streams are precisely aligned to a unified global timeline and enriched with manually curated speech transcripts and visual narrations to ensure high quality and semantic clarity.\nBuilding on this foundation, TeleEgo introduces 12 diagnostic tasks covering three core ability: <span class=\"ltx_text ltx_font_italic\">memory</span> (recalling past events), <span class=\"ltx_text ltx_font_italic\">understanding</span> (interpreting the present), and <span class=\"ltx_text ltx_font_italic\">cross-memory reasoning</span> (connecting distant moments). These tasks include a total of 3,291 human-verified QA items across various formats, including single-choice, binary, multiple-choice, and open-ended questions. Each task is tied to a specific time point and a decision window, requiring models not only to answer correctly, but also to respond at the right time. All evaluations are conducted under streaming conditions, and a task is considered successful only if the model&#8217;s first correct answer falls within the allowed window. This setup prevents models from guessing too early or answering too late, offering a more accurate measure of real-time decision-making and responsiveness.\nWe further introduce two key evaluation metrics: <span class=\"ltx_text ltx_font_italic\">Real-Time Accuracy</span>, which captures the model&#8217;s ability to produce correct responses within the given time constraint, and <span class=\"ltx_text ltx_font_italic\">Memory Persistence Time</span>, which measures how long a model can retain past information without re-encountering the original context. These metrics offer a comprehensive view of not just correctness, but also temporal responsiveness and long-term memory retention.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "models",
                    "across",
                    "multimodal",
                    "task",
                    "total",
                    "reasoning",
                    "foundation",
                    "longterm",
                    "teleego",
                    "memory",
                    "understanding",
                    "tasks",
                    "right",
                    "crossmemory",
                    "spans",
                    "comprehensive",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A unified, streaming, multimodal benchmark aligned to a global timeline with rich, real-world data across multiple participants and scenarios.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "across",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A diagnostic task suite covering memory, understanding, and cross-memory reasoning, with timestamped, multimodal evidence for interpretability.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "task",
                    "memory",
                    "understanding",
                    "reasoning",
                    "crossmemory"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An evaluation protocol tailored to streaming inputs, using two core metrics, real-time accuracy and memory persistence, to assess comprehensive performance.</p>\n\n",
                "matched_terms": [
                    "comprehensive",
                    "memory"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Egocentric Models and Benchmarks.</span>\nEgocentric (first-person) vision research has grown significantly, evolving from early single-user recordings to diverse, large-scale datasets capturing daily life from a first-person perspective. Pioneering work such as EPIC-KITCHENS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib5\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></a>]</cite> and its extensions like VISOR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite> introduced large-scale, object-rich video datasets in home environments. Later, Ego4D and Ego-Exo4D <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib8\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib9\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">9</span></a>]</cite> broadened the scope to include tasks like episodic memory, future prediction, and skill learning, incorporating both egocentric and exocentric views.\nRecent datasets have begun to explore more complex cognitive tasks and assistant-like interactions. EgoLife <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib21\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite> captures long-duration recordings from a single household, while EgoThink <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib3\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></a>]</cite> and EgoExoLearn <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib10\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">10</span></a>]</cite> focus on segment-level reasoning and teaching-following dynamics. Other work like MM-Ego <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib22\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">22</span></a>]</cite> and EgoTextVQA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>]</cite> targets memory and text-based understanding. However, most of these evaluations remain offline and task-specific.\nTeleEgo advances egocentric benchmarking by combining multi-day, multi-role, and multi-theme recordings with dual text annotations aligned to a unified global timeline. Its online evaluation protocol measures not only real-time decision-making, but also the persistence of memory over time&#8212;enabling more realistic and comprehensive evaluations of assistant capabilities.</p>\n\n",
                "matched_terms": [
                    "models",
                    "over",
                    "cognitive",
                    "teleego",
                    "memory",
                    "understanding",
                    "tasks",
                    "reasoning",
                    "comprehensive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Streaming Video Understanding Benchmarks.</span>\nTo assess assistants in time-sensitive and dynamic environments, several benchmarks have emerged that focus on streaming video understanding. StreamingBench <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite>, OVBench, and OVO-Bench <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>]</cite> support online task formats, but typically span short episodes and lack sustained memory testing across events. ODV-Bench <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite> emphasizes driving tasks and short-term prediction, prioritizing perception over memory.\nSome datasets, such as X-LeBench <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib26\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">26</span></a>]</cite>, extend video QA to longer contexts, but still operate in offline settings without real-time constraints.\nTeleEgo fills this gap by offering continuous, multi-day egocentric video streams with temporally grounded question-answer pairs. Its evaluation protocol emphasizes real-time responses and long-term memory recall, supporting deeper analyses of assistant performance in realistic, ever-evolving scenarios.</p>\n\n",
                "matched_terms": [
                    "over",
                    "across",
                    "task",
                    "shortterm",
                    "longterm",
                    "teleego",
                    "memory",
                    "understanding",
                    "short",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Omni-Modal Assistants.</span>\nRecent advancements in omni-modal models aim to unify understanding across multiple input types&#8212;such as text, vision, audio, and speech&#8212;enabling more flexible and human-like assistants. Closed-source systems like GPT-4o <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> and Gemini 1.5/2.5 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>]</cite> support end-to-end speech and video processing with long multimodal context windows, allowing for rich, coherent interactions.\nOpen-source progress has also accelerated. Models such as LLaVA&#8211;OneVision <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib13\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">13</span></a>]</cite> and InternVL-2.5 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib2\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></a>]</cite> support multi-image and video understanding. Others like Qwen2.5&#8211;Omni <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a>]</cite>, MiniCPM&#8211;o <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>]</cite>, VITA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">7</span></a>]</cite>, and Baichuan&#8211;Omni <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib14\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">14</span></a>]</cite> are optimized for real-time, streaming audiovisual input and response.\nDespite their strong performance, these models are typically evaluated in offline, task-specific settings. Few benchmarks assess whether assistants can respond accurately within time-sensitive decision windows or retain relevant information across long temporal spans.\nBy combining egocentric recordings, streaming video contexts, and real-time memory evaluation, TeleEgo provides a unified testbed to study omni-modal assistant performance in realistic environments&#8212;bridging the gap between research in egocentric datasets, streaming benchmarks, and multimodal models.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "models",
                    "across",
                    "teleego",
                    "memory",
                    "understanding",
                    "spans",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure broad diversity and real-world relevance, TeleEgo uses a carefully designed data collection protocol that spans multiple roles, themes, and tasks. We recruited five participants with balanced gender representation and a wide range of cultural, regional, and personality backgrounds, aiming to reflect a representative slice of the general population.\nEach participant wore a first-person camera over three consecutive days and recorded egocentric video following a set of predefined scenarios. The recordings include both solo activities and group interactions, taking place in diverse indoor and outdoor environments and across various social contexts. This approach goes beyond traditional single-household or uniform-group datasets, improving the generalizability and practical value of the benchmark.\nTo capture the richness of everyday human experience, we structured the data around four common life themes (see Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#S3.F2\" title=\"Figure 2 &#8227; 3 TeleEgo &#8227; TeleEgo: Benchmarking Egocentric AI Assistants in the Wild\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). These themes cover different cognitive demands, social situations, environmental settings, and physical activities.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "over",
                    "across",
                    "balanced",
                    "cognitive",
                    "teleego",
                    "tasks",
                    "spans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Work &amp; Study</span> (e.g., giving a presentation, meetings, coding, learning, reception tasks): This category includes knowledge-based and goal-driven tasks that often involve tools, screens, and structured interactions. These activities require focused attention, task switching, and formal turn-taking, making them ideal for evaluating cognitive workload and procedural behaviors.</p>\n\n",
                "matched_terms": [
                    "cognitive",
                    "task",
                    "tasks",
                    "evaluating"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Lifestyle &amp; Routines</span> (e.g., shopping, exercising, walking, cooking):\nThis theme involves semi-structured daily activities that combine object handling and movement in dynamic yet familiar environments. It supports research on long-term activity recognition, task progression, and inferring higher-level states such as fatigue or task completion.</p>\n\n",
                "matched_terms": [
                    "task",
                    "longterm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Outings &amp; Culture</span> (dining out, dating, visiting museums):\nThese scenarios occur in complex public environments with varying lighting, noise levels, and crowd density. They also involve subtle social norms and cultural practices. This category helps evaluate model robustness to occlusion, background noise, and unfamiliar contexts, while enabling understanding of social intent and etiquette.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "intent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Over the course of three days, each participant recorded a wide range of egocentric videos covering all four themes. The result is a rich, multi-role, multi-theme, multi-day dataset that supports research on long-term memory, context carryover, and generalization across different situations&#8212;key challenges in real-world perception.</p>\n\n",
                "matched_terms": [
                    "over",
                    "across",
                    "longterm",
                    "memory"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure privacy and ethical use, all collected recordings go through a careful de-identification process. This includes blurring faces, removing speech from non-participants, and masking any sensitive visual or audio content. These steps preserve participant privacy while keeping the recordings natural and realistic, reflecting everyday first-person experiences. To support deeper multimodal understanding beyond raw audio and video, the TeleEgo dataset includes two types of time-aligned textual annotations.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "multimodal",
                    "types",
                    "teleego"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both streams are precisely synchronized with the video timeline, producing dual-layer annotations for visual and linguistic events. This structure enables rich cross-modal grounding. The processed multimodal data is then used by powerful AI tools to generate candidate QA items, which are further refined through human verification to build our benchmark system (see Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#S3.F3\" title=\"Figure 3 &#8227; 3.1 Dataset Overview &#8227; 3 TeleEgo &#8227; TeleEgo: Benchmarking Egocentric AI Assistants in the Wild\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Collectively, TeleEgo offers real-world recordings that combine perception, language, and memory, furnishing comprehensive multimodal material for evaluating AI systems&#8217; capacity to understand and retain complex first-person experiences.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "benchmark",
                    "teleego",
                    "memory",
                    "crossmodal",
                    "comprehensive",
                    "evaluating"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate multidimensional cognitive abilities in egocentric video understanding, we introduce a benchmark that spans three key cognitive dimensions: memory, comprehension, and cross-memory reasoning (see Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#S3.F4\" title=\"Figure 4 &#8227; 3.2 Raw Data Processing &#8227; 3 TeleEgo &#8227; TeleEgo: Benchmarking Egocentric AI Assistants in the Wild\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). These dimensions form a hierarchical structure that reflects different levels of cognition, ranging from momentary perception to long-term reasoning. This framework enables a systematic distinction between information retention, semantic understanding, and integrative reasoning across time and entities. To support this evaluation, we design 12 fine-grained question-answering subtasks, each corresponding to one of the three dimensions, allowing us to assess model performance across a wide range of cognitive scenarios. Examples of these subtasks are illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#S3.F5\" title=\"Figure 5 &#8227; 3.3 Benchmark Task Design &#8227; 3 TeleEgo &#8227; TeleEgo: Benchmarking Egocentric AI Assistants in the Wild\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "across",
                    "cognitive",
                    "reasoning",
                    "longterm",
                    "memory",
                    "understanding",
                    "dimensions",
                    "crossmemory",
                    "spans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">Memory</span> focuses on temporally grounded recall, assessing a model&#8217;s ability to retain, retrieve, and compare events over different time spans. Tasks range from short-term recall of transient object states and actions, to long-term and ultra-long memory over extended episodes, as well as continuous entity tracking and temporal interval reasoning. These tasks evaluate how well models maintain temporal coherence and represent evolving dynamics in first-person experiences.</p>\n\n",
                "matched_terms": [
                    "over",
                    "models",
                    "tracking",
                    "ultralong",
                    "shortterm",
                    "longterm",
                    "memory",
                    "tasks",
                    "entity",
                    "reasoning",
                    "interval",
                    "spans",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Subcategory Distribution</span>\n</p>\n\n",
                "matched_terms": [
                    "subcategory",
                    "distribution"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">Understanding</span> dimension measures a model&#8217;s capacity to grasp meaning and coherence within complex, context-rich scenarios. It goes beyond surface perception to assess how well a model understands causal structures and human intent. This includes recognizing cause-effect relationships, inferring latent motivations, and constructing a unified interpretation from temporally or spatially dispersed cues. It also requires integrating multimodal inputs into coherent semantic representations.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "intent",
                    "causal",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">Cross-Memory Reasoning</span> dimension challenges models to combine information across disjoint time periods and entity contexts. Tasks require building global narrative structures, linking distant events into causal chains, inferring relational dynamics between interacting agents, and synthesizing long temporal sequences into structured, meaningful processes. This dimension represents the most complex aspect of egocentric cognition, requiring reasoning over long-range dependencies in continuous experiences.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "over",
                    "causal",
                    "reasoning",
                    "tasks",
                    "entity",
                    "crossmemory",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt four complementary QA formats: single-choice (mc_single), multiple-choice (mc_multi), binary, and open-ended. Each format serves a distinct purpose. The mc_single format allows for precise evaluation through carefully crafted distractors and unambiguous correct answers. The mc_multi format captures complex or uncertain scenarios by permitting multiple correct options. Binary questions offer high-precision evaluation at low annotation cost. Open-ended questions encourage free-form reasoning and compositional thinking, complementing the more structured formats. Together, these formats strike a balance between standardization and expressiveness, enabling scalable evaluation while supporting fine-grained behavioral probing. Our QA generation process begins with post-processed, time-aligned transcripts of speech and narration. We use state-of-the-art large language models (GPT-5-Thinking and Gemini-2.5-Pro) to draft initial QA candidates. For the Ultra-Long Memory subcategory, the models ingest full dual-stream transcripts and generate questions grounded in evidence spanning 10&#8211;60 minutes. For the remaining eleven subcategories, we segment each recording into 30-minute windows and prompt the models to generate QA pairs with evidence evenly distributed across each window. Human annotators then verify factual alignment with the source video, correct timestamps, and remove ambiguous or low-quality items.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "ultralong",
                    "subcategory",
                    "memory",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:70%;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:718.4pt;height:121.9pt;vertical-align:-59.2pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#F6F8FC;\">\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F6F8FC;\">Method</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F6F8FC;\">Params</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F6F8FC;\">Omni</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F6F8FC;\">Streaming</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_6\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">Memory (%)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_5\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">Understanding (%)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_4\" style=\"--ltx-bg-color:#FFF3E7;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFF3E7;\">Cross&#8211;Memory Reasoning (%)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F6F8FC;\">Overall</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">UlM</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">StM</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">ET</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">TCI</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">LtM</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">All</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">II</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">CU</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">CmU</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">MsR</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">All</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFF3E7;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFF3E7;\">CeR</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFF3E7;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFF3E7;\">TCU</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFF3E7;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFF3E7;\">CtC</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#FFF3E7;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFF3E7;\">All</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#F4F4F6;\">\n<span class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_20\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FCFCFC;\">Proprietary MLLMs</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">GPT-4o <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#8211;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#10003;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#8211;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">47.71</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">47.68</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">34.05</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">39.76</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">34.87</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">42.69</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">78.40</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">60.09</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">53.59</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">47.30</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">60.92</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">33.11</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">57.69</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">58.73</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">45.87</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">48.04</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FCFCFC;\">\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">Gemini-2.5-Pro <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#8211;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10003;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#8211;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">46.52</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">49.63</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">30.47</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">35.34</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">39.92</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">42.23</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">71.36</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">60.56</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">49.28</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">47.30</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">57.98</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">28.48</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">50.00</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">52.38</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">40.26</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">46.35</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#F4F4F6;\">\n<span class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_20\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFFFFF;\">Open&#8211;Source MLLMs</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FCFCFC;\">\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">Qwen2.5-VL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">8B</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10007;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10007;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">34.63</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">39.85</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">27.24</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">37.35</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">28.57</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">34.24</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">43.66</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">44.60</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">28.71</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">22.30</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">35.89</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">21.85</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">26.92</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">34.13</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">27.39</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">33.96</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">VideoChat-Online <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">4B</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#10007;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#10003;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">32.26</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">31.78</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">23.66</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">26.51</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">24.37</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">28.91</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">57.28</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">44.13</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">34.93</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">25.68</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">41.76</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">18.54</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">42.06</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">26.92</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">29.04</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">32.46</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FCFCFC;\">\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">7B</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10003;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10007;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">26.15</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">29.58</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">21.15</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">26.10</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">20.17</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">25.34</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">32.86</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">32.39</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">23.44</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">17.57</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">27.33</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">15.89</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">23.08</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">24.60</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">20.13</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">25.33</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">MiniCPM-o <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">8B</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#10007;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#10003;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">43.63</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">44.01</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">29.75</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">39.36</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">39.50</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">40.36</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">67.14</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">51.17</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">40.67</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">37.84</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">50.19</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">25.83</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">26.92</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">55.56</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">38.28</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">42.84</span></span></span>\n</span></span>\n</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "reasoning",
                    "memory"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:70%;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:701.9pt;height:121.9pt;vertical-align:-59.2pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#F6F8FC;\">\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F6F8FC;\">Method</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F6F8FC;\">Params</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F6F8FC;\">Omni</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F6F8FC;\">Streaming</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_6\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">Memory (min)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_5\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">Understanding (min)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_4\" style=\"--ltx-bg-color:#FFF3E7;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFF3E7;\">Cross&#8211;Memory Reasoning (min)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F6F8FC;\">Overall</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">UlM</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">StM</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">ET</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">TCI</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">LtM</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#E8F3FF;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E8F3FF;\">All</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">II</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">CU</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">CmU</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">MsR</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#EBFBF2;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#EBFBF2;\">All</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFF3E7;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFF3E7;\">CeR</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFF3E7;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFF3E7;\">TCU</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFF3E7;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFF3E7;\">CtC</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#FFF3E7;padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFF3E7;\">All</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#F4F4F6;\">\n<span class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_20\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FCFCFC;\">Proprietary MLLMs</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">GPT-4o <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#8211;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#10003;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#8211;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">3.41</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">2.66</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.36</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">2.06</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.82</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">2.60</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">6.27</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">4.36</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">2.80</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">3.14</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">4.49</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.60</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">4.04</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">3.82</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">3.06</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">3.01</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FCFCFC;\">\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">Gemini-2.5-Pro <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#8211;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10003;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#8211;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">3.17</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">2.22</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.13</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.94</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.89</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">2.37</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">5.70</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">4.58</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">2.34</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">3.11</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">4.22</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.15</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">4.19</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">3.26</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">2.62</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">2.76</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#F4F4F6;\">\n<span class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_20\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFFFFF;\">Open&#8211;Source MLLMs</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FCFCFC;\">\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">Qwen2.5-VL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">8B</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10007;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10007;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.80</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.77</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.39</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.76</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.11</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.66</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">2.09</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">2.29</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.22</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">0.87</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.83</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">0.97</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.42</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.56</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.31</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.60</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">VideoChat-Online <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">4B</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#10007;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#10003;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.30</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.03</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">0.65</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">0.84</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">0.82</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.03</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">3.72</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">2.17</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.25</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.11</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">2.41</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">0.46</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">0.62</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.87</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.32</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.33</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FCFCFC;\">\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">7B</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10003;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">&#10007;</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.06</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.09</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">0.80</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.20</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">0.61</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.00</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.37</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.32</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.02</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">0.74</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.20</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">0.48</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.04</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.02</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">0.81</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCFC;\">1.00</span></span></span>\n<span class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">MiniCPM-o <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>]</cite></span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">8B</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#10007;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">&#10003;</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">2.27</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.58</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.16</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.79</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.66</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.82</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">4.58</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">3.43</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">2.03</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">2.18</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">3.37</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">0.99</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">1.77</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">3.46</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">2.53</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFFF;\">2.19</span></span></span>\n</span></span>\n</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "reasoning",
                    "memory"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2) Memory Persistence Time (MPT, minutes)</span>: For each item correctly answered at time <math alttext=\"t^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p3.m1\" intent=\":literal\"><semantics><msup><mi>t</mi><mo>&#8902;</mo></msup><annotation encoding=\"application/x-tex\">t^{\\star}</annotation></semantics></math>, we continue streaming without repeating the original evidence. At regular intervals, we re-query the same item. MPT is the time from <math alttext=\"t^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p3.m2\" intent=\":literal\"><semantics><msup><mi>t</mi><mo>&#8902;</mo></msup><annotation encoding=\"application/x-tex\">t^{\\star}</annotation></semantics></math> until the first failed recall. If the item is never answered correctly, MPT<math alttext=\"\\,{=}\\,0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p3.m3\" intent=\":literal\"><semantics><mrow><mi/><mo lspace=\"0.448em\">=</mo><mn>&#8201;0</mn></mrow><annotation encoding=\"application/x-tex\">\\,{=}\\,0</annotation></semantics></math>. If it is never forgotten within the probing range, it is right-censored at the last probe. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#S3.F6\" title=\"Figure 6 &#8227; 3.4 Benchmark QA Annotation &#8227; 3 TeleEgo &#8227; TeleEgo: Benchmarking Egocentric AI Assistants in the Wild\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> provides a schematic overview of the pipeline used to compute Memory Persistence Time (MPT).</p>\n\n",
                "matched_terms": [
                    "overview",
                    "memory"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All experiments are conducted on a single NVIDIA H200 GPU (140 GB). At inference time, for each role we order all videos by their start timestamps and concatenate them into one continuous stream, mirroring realistic personal-assistant usage where the assistant is invoked intermittently around task- or context-specific segments rather than running continuously.</p>\n\n",
                "matched_terms": [
                    "task",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#S3.T3\" title=\"Table 3 &#8227; 3.4 Benchmark QA Annotation &#8227; 3 TeleEgo &#8227; TeleEgo: Benchmarking Egocentric AI Assistants in the Wild\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23981v2#S3.T4\" title=\"Table 4 &#8227; 3.4 Benchmark QA Annotation &#8227; 3 TeleEgo &#8227; TeleEgo: Benchmarking Egocentric AI Assistants in the Wild\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, under the strict streaming protocol and evidence-compliance constraints of TeleEgo, the results reveal a clear and structured pattern. Proprietary multimodal assistants (e.g., GPT-4o, Gemini-2.5-Pro) achieve strong overall performance in both RTA and MPT. However, their advantage is concentrated in the Understanding axis (e.g., GPT-4o reaches 61% in Understanding-All), while performance drops significantly in tasks requiring fine-grained temporal binding and cross-modal attribution (43% in Memory-All, 46% in Cross-Memory-All). This &#8220;semantic-strong but temporally-weak&#8221; trend is consistent across subtasks: intent inference approaches near-ceiling accuracy, whereas entity tracking and cross-entity relation inference remain the weakest, indicating that current systems heavily rely on semantic priors but struggle with timestamp alignment and instance-level grounding.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "across",
                    "tracking",
                    "crossentity",
                    "relation",
                    "teleego",
                    "understanding",
                    "intent",
                    "inference",
                    "tasks",
                    "entity",
                    "crossmodal",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Interestingly, some open-source models with native streaming designs (e.g., MiniCPM-o) significantly close the RTA gap with proprietary systems, despite having fewer parameters. This suggests that managing temporal states and controlling output emissions may matter more than broad multimodal coverage. Conversely, models with audio-visual-text fusion but without streaming mechanisms show limited benefit in TeleEgo&#8217;s &#8220;correct-then-timed-and-verifiable&#8221; setting. This highlights that latency handling, cache scheduling, and alignment logic are the true drivers of real-time accuracy.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "models",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From a temporal persistence perspective, MPT further reveals a disconnect between what models remember and how long they remember it. Proprietary models sustain longer persistence on understanding-oriented tasks (e.g., GPT-4o achieves 6.3 minutes MPT on intent inference) but only 2&#8211;3 minutes on memory-centric tasks. Open-source models show shorter persistence across the board. This suggests that while models can compress long experiences into abstract semantic representations, they struggle to retain auditable, time-anchored evidence and dynamic entity states.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "intent",
                    "inference",
                    "tasks",
                    "entity",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The divergence between RTA and MPT under a 5-second decision window and evidence-overlap constraints points to two complementary optimization directions: 1) Timestamp-aware temporal learning &#8212; where decoding conditioned on timestamps and calibrated silence policies improve when to respond; 2) Structured long-term memory architectures &#8212; integrating clock-indexed event keys with multimodal anchors to improve how to substantiate outputs.\nOverall, TeleEgo tightly couples correctness, grounding, and timing, shifting the primary bottleneck of egocentric assistants from sheer context length to verifiable alignment and real-time temporal control. This establishes a concrete and actionable frontier for future research.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "longterm",
                    "teleego",
                    "memory",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, we present TeleEgo, an online, omni-modal, first-person benchmark grounded in real-world use, built from continuous, multi-participant, multi-scene, multi-day recordings that align video, ambient speech/dialogue, and dual textual timelines under a unified clock, and equipped with a contract-based annotation scheme that binds each query to its required modalities and precise, time-stamped evidence spans for auditable attribution. Centered on three capability axes, Memory, Understanding, and Cross-Memory Reasoning, TeleEgo offers a fine-grained task suite and a strict streaming-only evaluation protocol: responses receive credit only if they arrive within task-specific decision windows and satisfy evidence compliance. Two complementary metrics, Real-time Accuracy, and Memory Persistence Time, jointly assess correctness, response timing, and long-horizon memory, while failure cases are decomposed into retention, retrieval, alignment, and timing to yield actionable diagnostics. We envision TeleEgo as an ecologically valid, diagnostically informative, and reproducible foundation for building first-person assistants that must remember, align, and act in real time.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "task",
                    "reasoning",
                    "foundation",
                    "teleego",
                    "memory",
                    "understanding",
                    "crossmemory",
                    "spans"
                ]
            }
        ]
    }
}