{
    "S4.T1": {
        "source_file": "Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis",
        "caption": "Table 1: Objective results on English and Chinese test sets for TTS with word-level emotion and speaking rate control. The best results for each metric are in bold, and the second-best are underlined.",
        "body": "Method\n\n\n\nWER/CER\n ↓\\downarrow\nDNSV↓\\downarrow\nS-SIM↑\\uparrow\n\n\n\nAutoPCP\n ↑\\uparrow\n\nEmotion↑\\uparrow\n\n\n\n\nEmo2v.\nAro.\n\n\n\n\nEnglish\n\nIndex-TTS\n2.611\n8.967\n0.387\n2.436\n0.858\n0.434\n\n\nF5-TTS\n2.954\n8.972\n0.453\n2.417\n0.869\n0.447\n\n\nSpark-TTS\n2.787\n8.637\n0.374\n2.560\n0.861\n0.440\n\n\nCosyVoice2\n3.185\n7.894\n0.521\n2.525\n0.866\n0.446\n\n\nWeSCon (1st)\n3.204\n4.577\n0.531\n2.689\n0.879\n0.463\n\n\nWeSCon (2nd)\n3.192\n4.361\n0.532\n2.707\n0.882\n0.468\n\n\n\n\nChinese\n\nIndex-TTS\n1.834\n8.521\n0.490\n2.470\n0.838\n0.514\n\n\nF5-TTS\n1.965\n9.134\n0.478\n2.541\n0.847\n0.510\n\n\nSpark-TTS\n1.897\n8.633\n0.441\n2.518\n0.848\n0.530\n\n\nCosyVoice2\n2.119\n7.612\n0.581\n2.514\n0.843\n0.537\n\n\nWeSCon (1st)\n2.129\n4.980\n0.595\n2.650\n0.866\n0.551\n\n\nWeSCon (2nd)\n2.122\n4.210\n0.599\n2.663\n0.872\n0.556",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">WER/CER</span></span>\n</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">DNSV<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">S-SIM<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">AutoPCP</span></span>\n</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Emotion</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Emo2v.</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Aro.</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"6\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.0pt;height:30pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:30.0pt;transform:translate(-11.0pt,-11.0pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\">English</span>\n</span></span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Index-TTS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.611</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.967</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.387</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.436</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.858</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.434</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">F5-TTS</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.954</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.972</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.453</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.417</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.869</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.447</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Spark-TTS</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">2.787</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.637</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.374</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.560</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.861</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.440</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.185</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.894</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.521</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.525</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.866</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.446</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">WeSCon (1st)</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.204</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">4.577</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.531</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">2.689</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.879</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.463</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">WeSCon (2nd)</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.192</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.361</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.532</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.707</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.882</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.468</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" rowspan=\"6\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:6.3pt;height:31.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:31.4pt;transform:translate(-12.6pt,-12.6pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\">Chinese</span>\n</span></span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Index-TTS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.834</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.521</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.490</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.470</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.838</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.514</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">F5-TTS</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.965</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.134</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.478</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.541</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.847</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.510</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Spark-TTS</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">1.897</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.633</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.441</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.518</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.848</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.530</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.119</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.612</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.581</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.514</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.843</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.537</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">WeSCon (1st)</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.129</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">4.980</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.595</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">2.650</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.866</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.551</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">WeSCon (2nd)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.122</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.210</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.599</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.663</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.872</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.556</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "emo2v",
            "sets",
            "indextts",
            "rate",
            "↓downarrow",
            "f5tts",
            "2nd",
            "dnsv↓downarrow",
            "objective",
            "wescon",
            "each",
            "test",
            "english",
            "tts",
            "ssim↑uparrow",
            "wordlevel",
            "results",
            "wercer",
            "chinese",
            "bold",
            "↑uparrow",
            "aro",
            "cosyvoice2",
            "speaking",
            "underlined",
            "sparktts",
            "1st",
            "emotion↑uparrow",
            "metric",
            "emotion",
            "best",
            "method",
            "autopcp",
            "control",
            "secondbest"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluate our method on word-level emotion and speaking rate control in both English and Chinese TTS. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, WeSCon (1st-stage) and WeSCon (2nd-stage) consistently outperform baselines on expressive metrics. Notably, the 2nd-stage model achieves the highest Emo2V. and Aro. scores in both languages, demonstrating strong word-level emotional expressiveness enabled by our self-training framework.\nRegarding transition smoothness, our models significantly reduce DNSV compared to CosyVoice2, with values dropping from 7.894 to 4.361 in English and from 7.612 to 4.210 in Chinese. This highlights the effectiveness of our smoothing mechanism and the end-to-end continuous inference in the 2nd-stage model in mitigating acoustic discontinuities across transitions.\nWhile the character error rate is slightly higher than baselines, it remains comparable to CosyVoice2, our backbone model.\nFinally, the 2nd-stage model slightly surpasses the 1st-stage model, benefiting from self-training with selective filtering that retains high-quality supervision from the teacher. Overall, our approach consistently improves upon CosyVoice2 and achieves SOTA performance in key aspects of word-level expressive controllable TTS.</p>\n\n",
            "<p class=\"ltx_p\">In addition to introducing word-level controllability, we evaluate the performance of our method on the standard zero-shot TTS task using the SEED test set (test-zh)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib60\" title=\"\">anastassiou2024seedttsfamilyhighqualityversatile </a></cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T3\" title=\"Table 3 &#8227; Capability on Zero-shot TTS &#8227; 4.2.1 Comparison with Reference Models &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the WeSCon (1st) model yields results identical to CosyVoice2, as the backbone TTS is frozen during this stage. The 2nd-stage model also achieves comparable results. Together with the findings in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, these results demonstrate that our method enables word-level emotion and speaking rate control without significantly degrading the original zero-shot TTS performance of the pretrained model.</p>\n\n",
            "<p class=\"ltx_p\">To evaluate the generalization ability of our approach under an out-of-domain dataset, we conduct word-level emotion and speaking rate control experiments on the CASIA dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib68\" title=\"\">68</a>]</cite>, as organized according to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6\" title=\"Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. The CASIA corpus is a Mandarin emotional speech dataset recorded by four native speakers and covers six emotion categories: neutral, angry, fear, happy, sad, and surprise. Some of these emotions are not seen during training, which makes CASIA suitable for testing the cross-domain robustness of controllable speech synthesis.\nThe results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A7.T5\" title=\"Table 5 &#8227; Appendix G Out-of-Domain Evaluation &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nOur method, WeSCon, demonstrates strong performance across nearly all evaluation metrics, achieving lower DNSV and higher speaker similarity (S-SIM), emotional similarity (Emo2vec), and arousal scores compared to other baselines. The overall results are consistent with those in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, further confirming that our method generalizes well to unseen speakers and novel emotional patterns.</p>\n\n",
            "<p class=\"ltx_p\">Compared to Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the student model (WeSCon 2nd) shows slightly weaker performance than the teacher model on the out-of-domain test set, in some metrics. This degradation is primarily caused by the data filtering strategy adopted during self-training, which improves performance on in-domain speakers and emotions but may introduce subtle biases, resulting in mild overfitting. Nevertheless, such performance fluctuations are acceptable given that the second-stage model significantly simplifies the inference process.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">While emotional text-to-speech (TTS) has made significant progress, most existing research remains limited to utterance-level emotional expression and fails to support word-level control. Achieving word-level expressive control poses fundamental challenges, primarily due to the complexity of modeling multi-emotion transitions and the scarcity of annotated datasets that capture intra-sentence emotional and prosodic variation.\nIn this paper, we propose WeSCon, the first self-training framework that enables word-level control of both emotion and speaking rate in a pretrained zero-shot TTS model, without relying on datasets containing intra-sentence emotion or speed transitions.\nOur method introduces a transition-smoothing strategy and a dynamic speed control mechanism to guide the pretrained TTS model in performing word-level expressive synthesis through a multi-round inference process. To further simplify the inference, we incorporate a dynamic emotional attention bias mechanism and fine-tune the model via self-training, thereby activating its ability for word-level expressive control in an end-to-end manner.\nExperimental results show that WeSCon effectively overcomes data scarcity, achieving state-of-the-art performance in word-level emotional expression control while preserving the strong zero-shot synthesis capabilities of the original TTS model.\n</p>\n\n",
                "matched_terms": [
                    "wordlevel",
                    "emotion",
                    "tts",
                    "rate",
                    "method",
                    "control",
                    "results",
                    "wescon",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans possess the ability to regulate emotional expression during speech flexibly&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib1\" title=\"\">scherer1995expression </a></cite>. To simulate this expressive capability, recent advances in text-to-speech synthesis (TTS) have increasingly focused on controllable generation of various aspects of speech, such as timbre, emotion, and speaking rate&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib2\" title=\"\">xie2024towards </a></cite>. Such control is a key objective in the development of human-like and expressive TTS.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "tts",
                    "rate",
                    "control",
                    "objective",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most current TTS models exhibit zero-shot capabilities, enabling them to synthesize speech from text while cloning attributes such as timbre, emotion, and speaking rate from a reference speech sample&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib3\" title=\"\">kharitonov2023speak </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib4\" title=\"\">wang2023neural </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib5\" title=\"\">ju2024naturalspeech </a></cite>.\nDespite these advances, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, emotional and speaking rate control in current models is typically limited to the utterance level.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "tts",
                    "rate",
                    "control",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This differs significantly from how humans naturally express emotion in speech. <span class=\"ltx_text ltx_font_bold\">Unlike global speaker identity, emotional expression and speaking rate are dynamic and often vary within a single sentence</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib6\" title=\"\">mozziconacci2002prosody </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib7\" title=\"\">guan2018speech </a></cite>. Therefore, word-level control of these factors is essential for achieving more natural and expressive speech synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib8\" title=\"\">9746323 </a></cite>.\nTo address this limitation, some approaches have proposed phoneme-level emotion prediction from target text to guide expressive synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib9\" title=\"\">tang2024ed </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib10\" title=\"\">9383524 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib11\" title=\"\">du21b_interspeech </a></cite>. While these methods show potential for word-level emotion control, relying solely on text makes it difficult to capture essential acoustic cues such as prosody and intensity, which are vital to emotional expression control&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib12\" title=\"\">chen2022fine </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib13\" title=\"\">chandra2024exploring </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib14\" title=\"\">zhao2023emotion </a></cite>.\nTo address this limitation, recent studies such as ELaTE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib15\" title=\"\">kanda2024making </a></cite> and EmoCtrl-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib16\" title=\"\">wu2024laugh </a></cite> have demonstrated that reference speech with emotional content can support intra-utterance control of time-varying expressive patterns, such as transitions from laughter to crying.\nThese works reflect a growing interest in TTS with word-level control over both emotion and speaking rate, but they also underscore several fundamental challenges.\nFirst, word-level expression control requires multiple emotional speech prompts, which introduces the challenge of guiding the model to attend to the appropriate emotion at each word.\nIn addition, current methods for fine-grained expression control rely on large-scale emotional speech datasets with time-aligned emotion transitions. However, such datasets are limited in both scale and accessibility&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib17\" title=\"\">rathi2024analyzing </a></cite>, making fine-grained control even more difficult to realize in practice.\nThese challenges lead us to ask:\n<span class=\"ltx_text ltx_font_bold\">Is it possible to achieve effective word-level control of both emotion and speaking rate without relying on speech datasets containing emotion or speed transitions?</span></p>\n\n",
                "matched_terms": [
                    "each",
                    "emotion",
                    "tts",
                    "rate",
                    "control",
                    "wordlevel",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, motivated by the zero-shot potential of pretrained TTS models, we propose WeSCon, a two-stage self-training framework that achieves <span class=\"ltx_text ltx_font_bold\">W</span>ord-level <span class=\"ltx_text ltx_font_bold\">E</span>motion and <span class=\"ltx_text ltx_font_bold\">S</span>peed <span class=\"ltx_text ltx_font_bold\">Con</span>trol for TTS using only a small amount of public speech data without emotion or speed transitions.\nIn the first stage, we design a multi-round inference framework that incorporates a transition-smoothing module and a dynamic speed control mechanism. Without relying on any emotional training data, this approach enables a pretrained zero-shot TTS model to perform high-quality word-level emotional expression control in TTS. In the second stage, the original TTS model is repurposed as a student and trained under the supervision of the 1st-stage teacher. A dynamic emotional attention bias is introduced, enabling the student to acquire word-level control of emotion and speed through a simplified end-to-end inference process, without the need for complex iterative generation or smoothing.\nExperimental results show that WeSCon achieves state-of-the-art performance on the task of word-level emotional expression control in TTS, while preserving the zero-shot generalization and generation capabilities of the pretrained TTS model.\nOur contributions are summarized as follows:</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "tts",
                    "control",
                    "wordlevel",
                    "wescon",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a multi-round inference mechanism equipped with transition smoothing and dynamic speaking rate control, which is the first to achieve word-level control of both emotion and speaking rate in TTS without relying on any emotional training data.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "tts",
                    "rate",
                    "control",
                    "wordlevel",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further introduce a novel self-training framework with a dynamic emotional attention bias mechanism that empowers a pretrained TTS model with end-to-end word-level emotion and speaking rate control, using limited data without intra-sentence emotion or speed transitions.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "tts",
                    "rate",
                    "control",
                    "wordlevel",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct comprehensive experiments to validate the effectiveness of our proposed framework. Results show that our method enables a pretrained zero-shot TTS model to achieve SOTA performance in word-level emotional expression control, while preserving its original zero-shot capabilities. Ablation studies further confirm the contribution of each key design component. Our samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymousdemo999.github.io/\" title=\"\">https://anonymousdemo999.github.io/</a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "tts",
                    "method",
                    "control",
                    "wordlevel",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development of controllable TTS, particularly for emotional expression control, depends heavily on high-quality emotional speech datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib18\" title=\"\">zhu2024metts_emodata </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib19\" title=\"\">guo2023emodiff_emodata </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib20\" title=\"\">yang2025emovoicellmbasedemotionaltexttospeech_emodata </a></cite>. While public corpora such as ESD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib21\" title=\"\">zhou2021seen </a></cite>, IEMOCAP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib22\" title=\"\">busso2008iemocap </a></cite>, and CREMA-D&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib23\" title=\"\">cao2014crema </a></cite> are available, they primarily provide utterance-level annotations and lack word-level or time-aligned emotional labels. These datasets are also limited in size and diversity, often consisting of scripted speech and covering a narrow range of emotions and speakers&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib24\" title=\"\">ma24b_interspeech </a></cite>.\nMore importantly, emotional datasets with intra-sentence variation, which are essential for learning word-level control, remain extremely scarce and are typically restricted to private use&#160; <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib15\" title=\"\">kanda2024making </a></cite>. Creating such datasets is expensive, requiring detailed word- or frame-level annotation and subjective emotional labeling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib25\" title=\"\">liu2024emphasisrenderingconversationaltexttospeech </a></cite>. This lack of fine-grained emotional data poses a major challenge for training models capable of word-level expressive TTS.</p>\n\n",
                "matched_terms": [
                    "control",
                    "tts",
                    "wordlevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most controllable TTS systems support only utterance-level control, where a single label or reference speech governs the entire sentence&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib26\" title=\"\">anastassiou2024seed </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib27\" title=\"\">du2024cosyvoice </a></cite>. To achieve word-level control, some methods attempt to predict frame- or phoneme-level emotional indicators from text alone&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib28\" title=\"\">phone_level_tts0 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib29\" title=\"\">phone_level_tts1 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib30\" title=\"\">phone_level_tts2 </a></cite>, but they often fail to capture expressive variability due to the lack of acoustic cues such as intensity and prosody&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib9\" title=\"\">tang2024ed </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib10\" title=\"\">9383524 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib11\" title=\"\">du21b_interspeech </a></cite>.\nOther approaches, such as ELaTE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib15\" title=\"\">kanda2024making </a></cite> and EmoCtrl-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib16\" title=\"\">wu2024laugh </a></cite>, introduce emotional reference speech to enable intra-utterance control of specific expressive patterns like laughter or crying. While these represent progress, they are typically limited in expressiveness or rely on large-scale emotional datasets that are rarely publicly available. Consequently, achieving general and flexible word-level control over both emotion and speaking rate remains a major challenge.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "tts",
                    "rate",
                    "control",
                    "wordlevel",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Self-training has become a promising approach for low-resource speech signal processing, enabling knowledge transfer without fine-grained datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib31\" title=\"\">zoph2020rethinking </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib32\" title=\"\">amini2025self </a></cite>. While it has been applied to tasks like speaker adaptation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib33\" title=\"\">khurana2021unsupervised </a></cite>, paralinguistic modeling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib34\" title=\"\">yang2024frame </a></cite>, and speech translation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib35\" title=\"\">pino20_interspeech </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib36\" title=\"\">fang-etal-2022-stemm </a></cite>, its use for fine-grained emotional control in TTS remains unexplored, especially without detailed expressive labels.\nTo address the scarcity of fine-grained datasets for word-level expressive control, we propose a self-training framework where a teacher model with multi-round inference, transition smoothing, and dynamic speed control generates expressive pseudo-labels. A student model, sharing the teacher&#8217;s backbone, is then fine-tuned under its supervision to perform word-level emotion and speaking rate control through a simplified end-to-end inference process, using only a small public dataset without intra-sentence emotion or speed transitions.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "tts",
                    "rate",
                    "control",
                    "wordlevel",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">WeSCon is a two-stage self-training framework that enables word-level control of emotion and speaking rate in a pretrained zero-shot TTS model, using only a small amount of emotional speech data without intra-sentence emotion transitions as prompts. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S2.F2\" title=\"Figure 2 &#8227; Self-Training under Data Scarcity &#8227; 2 Related Work &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, in the first stage, we introduce a multi-round inference process with transition smoothing and dynamic speaking rate control to generate speech with word-level expression variations. In the second stage, the 1st-stage model acts as a teacher to guide the original TTS model, equipped with a dynamic emotional attention bias (DEAB), toward word-level control through a simplified end-to-end inference. Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.SS2\" title=\"3.2 Teacher Model &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.SS3\" title=\"3.3 Self-Training &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a> describe the two stages, and Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.SS4\" title=\"3.4 Detail Training Setup &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a> provides the training details.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "tts",
                    "rate",
                    "control",
                    "wordlevel",
                    "wescon",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S2\" title=\"2 Related Work &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, current TTS models can perform utterance-level emotion and speaker cloning. Building on this, we adopt the high-performance CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">du2024cosyvoice2 </a></cite> as our backbone (details of the backbone architecture are provided in <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A1\" title=\"Appendix A Details of CosyVoice2 &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">A</span></a></span>) and propose a multi-round inference strategy, where the model synthesizes multiple segments using different emotional prompts to achieve word-level emotion control.\nWhile this approach enables flexible emotional modulation, it often causes unnatural acoustic discontinuities at segment boundaries. To address this, we introduce a transition-smoothing mechanism that improves coherence across inference rounds, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F3\" title=\"Figure 3 &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nWithout modifying CosyVoice2, we append a lightweight content aligner, composed of non-causal Transformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib38\" title=\"\">vaswani2017attention </a></cite> and convolutional layers. Trained on ASR data, this module predicts the corresponding text token for each speech token and requires no emotional supervision.\nDuring inference, the input text is segmented based on a user-defined emotion plan. At each inference round, the final text and speech tokens from the previous round are appended to the current prompt, forming an explicit tail-to-head linkage. This aligns naturally with CosyVoice2&#8217;s continuation-style generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib39\" title=\"\">borsos2023audiolm </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib40\" title=\"\">valle </a></cite>, enabling smooth and coherent emotional transitions.</p>\n\n",
                "matched_terms": [
                    "each",
                    "emotion",
                    "tts",
                    "control",
                    "cosyvoice2",
                    "wordlevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In CosyVoice2, utterance-level temporal prosody, including speaking rate and duration, is entirely determined by the reference speech prompt. To support more flexible and word-level control of speaking rate within a single utterance, we introduce a dynamic speed control mechanism as part of our multi-round inference framework, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F3\" title=\"Figure 3 &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The core idea is to adjust the prompt speech tokens using either nearest-neighbor interpolation or downsampling. Interpolation extends the prompt length, which slows down the generated speech, while downsampling shortens the prompt, resulting in a faster speaking rate. As demonstrated in <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A2\" title=\"Appendix B Speed Control &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">B</span></a></span>, this resampling method provides effective global prosody control. By integrating it into the multi-round inference process, the speaking rate can be dynamically controlled at the word level as needed.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "method",
                    "control",
                    "cosyvoice2",
                    "wordlevel",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the previous section, we enabled word-level control of emotion and speaking rate by introducing a multi-round inference framework for CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">du2024cosyvoice2 </a></cite>. However, components such as the non-causal content aligner, multi-round inference, and tail-to-head linkage introduce significant inference complexity. To reduce this overhead while preserving controllability, we adopt a self-training strategy. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F4\" title=\"Figure 4 &#8227; 3.3 Self-Training &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the enhanced first-stage model serves as a teacher to supervise the original TTS model. The student model, equipped with a dynamic emotional attention bias, learns to achieve word-level emotion and speaking rate control through a simplified end-to-end inference.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "tts",
                    "rate",
                    "control",
                    "cosyvoice2",
                    "wordlevel",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our teacher model achieves word-level control of emotion and speaking rate without modifying the original TTS parameters, relying instead on a complex inference pipeline with dynamic speed control and multi-round generation. To transfer this fine-grained control ability to a simplified end-to-end model, we propose a self-training strategy. Specifically, the 1st-stage teacher model guides the student model to learn word-level controllability. We first use GPT-4o&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib41\" title=\"\">hurst2024gpt </a></cite> to generate emotion-transition text sequences (details are shown in <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A3\" title=\"Appendix C Generation of Emotionally Varying Texts &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">C</span></a></span>), which are paired with public emotional speech samples (without emotion transitions) as prompts. The teacher then synthesizes speech with word-level variation in emotion and speaking rate. These outputs are filtered based on character accuracy and expressive similarity (<span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">details are introduced in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A4\" title=\"Appendix D Data Filtering in Self-Training &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">D</span></a></span>), and the student model is fine-tuned on the filtered supervisions with a small learning rate. This enables word-level emotional expression control during inference without requiring multi-round generation or dynamic concatenation.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "tts",
                    "rate",
                    "control",
                    "wordlevel",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aim to preserve the strong zero-shot capability of the original TTS model while enabling word-level control of emotional expression under the self-training framework. To achieve this, we formulate the input structure as <math alttext=\"\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{C}^{\\text{prompt}\\ \\text{I}},\\bm{C}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{C}^{\\text{tgt}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{prompt}\\ \\text{I}},\\bm{S}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{S}^{\\text{tgt}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.09\" overflow=\"visible\" style=\"vertical-align:-2.18px\" version=\"1.1\" viewbox=\"0 0 11.09 11.09\" width=\"11.09\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.09) matrix(1 0 0 -1 0 0) translate(5.55,0) translate(0,5.55)\"><path d=\"M 5.27 0 C 5.27 2.91 2.91 5.27 0 5.27 C -2.91 5.27 -5.27 2.91 -5.27 0 C -5.27 -2.91 -2.91 -5.27 0 -5.27 C 2.91 -5.27 5.27 -2.91 5.27 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -2.69 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.56em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"5.38\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">S</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><msup><mi>&#119914;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>I</mtext></mrow></msup><mo>,</mo><msup><mi>&#119914;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>II</mtext></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>&#119914;</mi><mtext>tgt</mtext></msup><mo>,</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.84\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><msup><mi>&#119930;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>I</mtext></mrow></msup><mo>,</mo><msup><mi>&#119930;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>II</mtext></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>&#119930;</mi><mtext>tgt</mtext></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{C}^{\\text{prompt}\\ \\text{I}},\\bm{C}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{C}^{\\text{tgt}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{prompt}\\ \\text{I}},\\bm{S}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{S}^{\\text{tgt}}\\}</annotation></semantics></math>, where <math alttext=\"\\bm{C}^{\\text{prompt}\\ i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m2\" intent=\":literal\"><semantics><msup><mi>&#119914;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\bm{C}^{\\text{prompt}\\ i}</annotation></semantics></math> and <math alttext=\"\\bm{S}^{\\text{prompt}\\ i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m3\" intent=\":literal\"><semantics><msup><mi>&#119930;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\bm{S}^{\\text{prompt}\\ i}</annotation></semantics></math> denote the text and speech tokens of the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m4\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th emotional prompt, respectively. <math alttext=\"\\bm{C}^{\\text{tgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m5\" intent=\":literal\"><semantics><msup><mi>&#119914;</mi><mtext>tgt</mtext></msup><annotation encoding=\"application/x-tex\">\\bm{C}^{\\text{tgt}}</annotation></semantics></math> is the target text token sequence, and <math alttext=\"\\bm{S}^{\\text{tgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m6\" intent=\":literal\"><semantics><msup><mi>&#119930;</mi><mtext>tgt</mtext></msup><annotation encoding=\"application/x-tex\">\\bm{S}^{\\text{tgt}}</annotation></semantics></math> is the corresponding speech token sequence used as supervision. The symbols <span class=\"ltx_inline-block\"><svg class=\"ltx_picture ltx_markedasmath\" height=\"11.09\" id=\"S3.SS3.SSS2.p1.m7.pic1\" overflow=\"visible\" style=\"vertical-align:-2.18px\" version=\"1.1\" viewbox=\"0 0 11.09 11.09\" width=\"11.09\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.09) matrix(1 0 0 -1 0 0) translate(5.55,0) translate(0,5.55)\"><path d=\"M 5.27 0 C 5.27 2.91 2.91 5.27 0 5.27 C -2.91 5.27 -5.27 2.91 -5.27 0 C -5.27 -2.91 -2.91 -5.27 0 -5.27 C 2.91 -5.27 5.27 -2.91 5.27 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -2.69 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.56em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"5.38\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">S</span></span></span></foreignobject></g></g></svg></span> and <span class=\"ltx_inline-block\"><svg class=\"ltx_picture ltx_markedasmath\" height=\"11.84\" id=\"S3.SS3.SSS2.p1.m8.pic1\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span> indicate the beginning of text and speech. This design remains fully compatible with the original input format <math alttext=\"\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{C},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.09\" overflow=\"visible\" style=\"vertical-align:-2.18px\" version=\"1.1\" viewbox=\"0 0 11.09 11.09\" width=\"11.09\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.09) matrix(1 0 0 -1 0 0) translate(5.55,0) translate(0,5.55)\"><path d=\"M 5.27 0 C 5.27 2.91 2.91 5.27 0 5.27 C -2.91 5.27 -5.27 2.91 -5.27 0 C -5.27 -2.91 -2.91 -5.27 0 -5.27 C 2.91 -5.27 5.27 -2.91 5.27 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -2.69 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.56em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"5.38\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">S</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><mi>&#119914;</mi><mo>,</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.84\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><mi>&#119930;</mi><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{C},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}\\}</annotation></semantics></math> of CosyVoice2, preserving the autoregressive pattern of the pretrained model.\nTo further encode word-level emotional variation within this unified format, we extend the text-side input by inserting explicit emotion indicator tokens that mark the boundaries between emotional segments. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F4\" title=\"Figure 4 &#8227; 3.3 Self-Training &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the final input sequence preceding <span class=\"ltx_inline-block\"><svg class=\"ltx_picture ltx_markedasmath\" height=\"11.84\" id=\"S3.SS3.SSS2.p1.m10.pic1\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span> becomes <math alttext=\"\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},E^{\\ \\text{I}},\\bm{C}^{\\text{prompt\\ \\text{I}}},E^{\\ \\text{II}},\\bm{C}^{\\text{prompt\\ \\text{II}}},\\ldots\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m11\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.09\" overflow=\"visible\" style=\"vertical-align:-2.18px\" version=\"1.1\" viewbox=\"0 0 11.09 11.09\" width=\"11.09\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.09) matrix(1 0 0 -1 0 0) translate(5.55,0) translate(0,5.55)\"><path d=\"M 5.27 0 C 5.27 2.91 2.91 5.27 0 5.27 C -2.91 5.27 -5.27 2.91 -5.27 0 C -5.27 -2.91 -2.91 -5.27 0 -5.27 C 2.91 -5.27 5.27 -2.91 5.27 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -2.69 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.56em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"5.38\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">S</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><msup><mi>E</mi><mtext>I</mtext></msup><mo>,</mo><msup><mi>&#119914;</mi><mrow><mtext>prompt&#160;</mtext><mtext>I</mtext></mrow></msup><mo>,</mo><msup><mi>E</mi><mtext>II</mtext></msup><mo>,</mo><msup><mi>&#119914;</mi><mrow><mtext>prompt&#160;</mtext><mtext>II</mtext></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},E^{\\ \\text{I}},\\bm{C}^{\\text{prompt\\ \\text{I}}},E^{\\ \\text{II}},\\bm{C}^{\\text{prompt\\ \\text{II}}},\\ldots\\}</annotation></semantics></math>, where each <math alttext=\"E^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m12\" intent=\":literal\"><semantics><msup><mi>E</mi><mi>i</mi></msup><annotation encoding=\"application/x-tex\">E^{i}</annotation></semantics></math> acts as a soft anchor guiding the model to modulate emotion transitions during generation.</p>\n\n",
                "matched_terms": [
                    "each",
                    "emotion",
                    "tts",
                    "control",
                    "cosyvoice2",
                    "wordlevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the above data formatting preserves CosyVoice2&#8217;s generalization by avoiding interference with learned knowledge, it introduces a new challenge: during synthesis, the model may incorrectly attend to emotion-inconsistent prompts. For instance, when generating speech aligned with Emotion&#160;I, attention may drift toward prompts labeled with Emotion&#160;II, leading to emotional inconsistency and degraded synthesis quality.\nTo address this, we propose a dynamic attention bias mechanism that constrains the model&#8217;s focus to emotion-relevant prompt regions based on the predicted emotional trajectory. Concretely, we introduce a causal lightweight Transformer to predict token-level emotion labels <math alttext=\"E_{t}^{\\text{tgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>E</mi><mi>t</mi><mtext>tgt</mtext></msubsup><annotation encoding=\"application/x-tex\">E_{t}^{\\text{tgt}}</annotation></semantics></math> for each speech token <math alttext=\"\\bm{S}_{t}^{\\text{tgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119930;</mi><mi>t</mi><mtext>tgt</mtext></msubsup><annotation encoding=\"application/x-tex\">\\bm{S}_{t}^{\\text{tgt}}</annotation></semantics></math> from historical context.\nUsing the predicted emotion sequence, we introduce a dynamic attention bias mechanism at each Transformer layer. We first concatenate the current text-speech representation with the predicted emotion features and project it through a linear layer. The output is processed in two ways: one path adds a residual and feeds into the next layer, while the other is passed to an MLP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib42\" title=\"\">popescu2009multilayer </a></cite> and softmax to produce a weight vector <math alttext=\"\\bm{\\omega}\\in\\mathbb{R}^{1\\times 7}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#120654;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>7</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\bm{\\omega}\\in\\mathbb{R}^{1\\times 7}</annotation></semantics></math>.\nThe <math alttext=\"\\bm{\\omega}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m4\" intent=\":literal\"><semantics><mi>&#120654;</mi><annotation encoding=\"application/x-tex\">\\bm{\\omega}</annotation></semantics></math> is then used to compute a dynamic attention bias by linearly combining seven predefined attention bias templates <math alttext=\"\\bm{B}^{\\text{temp}}\\in\\mathbb{R}^{7\\times T\\times T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m5\" intent=\":literal\"><semantics><mrow><msup><mi>&#119913;</mi><mtext>temp</mtext></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>7</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\bm{B}^{\\text{temp}}\\in\\mathbb{R}^{7\\times T\\times T}</annotation></semantics></math> (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A5\" title=\"Appendix E Predefined Emotion Attention Bias &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">E</span></a> for details). The resulting bias is computed as:</p>\n\n",
                "matched_terms": [
                    "each",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">WeSCon is trained in two stages. The first stage trains a content aligner to ensure smooth transitions during multi-round inference. In the second stage, a self-training strategy is adopted to transfer the teacher model&#8217;s ability to control word-level emotional expression to the original TTS model.</p>\n\n",
                "matched_terms": [
                    "control",
                    "tts",
                    "wordlevel",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use forced alignment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib43\" title=\"\">mcauliffe2017montreal </a></cite> to generate token-level alignments between transcripts and speech, which serve as supervision for the content aligner. The TTS model remains frozen throughout this stage. Training of the content aligner is conducted without multi-round forwards.\nLet <math alttext=\"\\bm{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>&#119914;</mi><annotation encoding=\"application/x-tex\">\\bm{C}</annotation></semantics></math> and <math alttext=\"\\bm{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi>&#119930;</mi><annotation encoding=\"application/x-tex\">\\bm{S}</annotation></semantics></math> denote the input text and speech token sequences, <math alttext=\"\\bm{Y}^{\\text{token}}\\in\\mathbb{N}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119936;</mi><mtext>token</mtext></msup><mo>&#8712;</mo><msup><mi>&#8469;</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\bm{Y}^{\\text{token}}\\in\\mathbb{N}^{T}</annotation></semantics></math> denote the aligned target token sequence, where each label corresponds to one of <math alttext=\"V_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><msub><mi>V</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">V_{1}</annotation></semantics></math> token classes. Let <math alttext=\"\\bm{Y}^{\\text{bd}}\\in\\mathbb{R}^{T\\times 1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><msup><mi>&#119936;</mi><mtext>bd</mtext></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\bm{Y}^{\\text{bd}}\\in\\mathbb{R}^{T\\times 1}</annotation></semantics></math> be the binary label sequence for content boundary detection. The content aligner is jointly trained with a token-level content classification loss and a binary boundary detection loss:</p>\n\n",
                "matched_terms": [
                    "each",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\bm{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>&#119914;</mi><annotation encoding=\"application/x-tex\">\\bm{C}</annotation></semantics></math> and <math alttext=\"\\bm{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi>&#119930;</mi><annotation encoding=\"application/x-tex\">\\bm{S}</annotation></semantics></math> are text and speech tokens for prompt and target, <math alttext=\"\\bm{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mi>&#119916;</mi><annotation encoding=\"application/x-tex\">\\bm{E}</annotation></semantics></math> are text-level and token-level emotion labels, and trainable <math alttext=\"\\theta^{\\text{tts}},\\theta^{\\text{ea}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><mrow><msup><mi>&#952;</mi><mtext>tts</mtext></msup><mo>,</mo><msup><mi>&#952;</mi><mtext>ea</mtext></msup></mrow><annotation encoding=\"application/x-tex\">\\theta^{\\text{tts}},\\theta^{\\text{ea}}</annotation></semantics></math> denote TTS model and emotion aligner parameters.\nThe second is a token-level cross-entropy loss for emotion prediction:</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage, the content aligner is trained on 200 hours of non-emotional English-Chinese speech from LibriSpeech-100-Clean&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib45\" title=\"\">panayotov2015librispeech </a></cite> and AISHELL-1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib46\" title=\"\">bu2017aishell </a></cite>. In the second stage, the teacher model uses non-transition emotional train-set from ESD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib21\" title=\"\">zhou2021seen </a></cite> as prompts to synthesize training samples based on emotion-transition texts generated by GPT-4o (see <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A3\" title=\"Appendix C Generation of Emotionally Varying Texts &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">C</span></a></span> for generation details and examples).\nWe adopt CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">du2024cosyvoice2 </a></cite> as the backbone TTS model. The content aligner is composed of five non-causal Transformer layers and two 5&#215;5 convolutional layers with stride 1 and batch normalization&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib47\" title=\"\">ioffe2015batch </a></cite>, following CosyVoice2&#8217;s configuration for architectural consistency.\nIn the second stage, the emotion aligner is a lightweight two-layer causal Transformer. The emotional attention bias module includes a linear layer with a hidden dimension of 14 and an MLP output dimension of 7.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "tts",
                    "cosyvoice2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage, the content aligner is trained for 400k steps on 2 NVIDIA 3090 GPUs using Adam <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib48\" title=\"\">kingma2014adam </a></cite> with a learning rate linearly warmed up to 2.5e-4 over the first 10% of steps, then linearly decayed to 0. Each batch contains 90 seconds of speech. In the second stage, the student model is trained for 600k steps on 4 NVIDIA 3090 GPUs. The TTS model is frozen for the first 20k steps to focus on training the emotion aligner. Each batch contains 40 seconds of speech, and Adam is used with a fixed learning rate of 5e-7. Repetition-aware top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> sampling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib49\" title=\"\">chen2024valle2neuralcodec </a></cite> is applied during inference, with <math alttext=\"k=50\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>50</mn></mrow><annotation encoding=\"application/x-tex\">k=50</annotation></semantics></math> and temperature <math alttext=\"=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">=0.9</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "emotion",
                    "tts",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate word-level control over emotion and speaking rate, we construct test sets based on test set of ESD and use outstanding zero-shot TTS models <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib50\" title=\"\">deng2025indextts </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib51\" title=\"\">chen2024f5 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib52\" title=\"\">wang2025spark </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">du2024cosyvoice2 </a></cite> with multi-round concatenative inference as baselines (see <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6\" title=\"Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">F</span></a></span> for details).\nWe use objective and subjective metrics to assess system performance (see <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6.SS3\" title=\"F.3 Evaluation Metrics &#8227; Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">F.3</span></a></span> for details).\nFor intelligibility, we report WER using Whisper-Large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib53\" title=\"\">radford2023robust </a></cite> for English and CER using Paraformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib54\" title=\"\">gao2022paraformer </a></cite> for Chinese.\nSpeaker similarity (S-SIM) is computed via cosine similarity of WavLM-Large embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib55\" title=\"\">chen2022wavlm </a></cite>.\nTo evaluate prosody alignment, we use AutoPCP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib56\" title=\"\">barrault2023seamless </a></cite>. Emotion similarity metrics (Emo2v. and Aro.) are computed using emotion2vec-Large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib57\" title=\"\">ma2023emotion2vec </a></cite> and a wav2vec-based model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib58\" title=\"\">baevski2020wav2vec </a></cite>, respectively.\nWe use the variance of DNSMOS-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib59\" title=\"\">cumlin2024dnsmos </a></cite> (DNSV) to assess the naturalness of emotion transition.\nSubjective evaluation includes four kinds of Mean Opinion Score (MOS): SMOS (speaker similarity), NMOS (naturalness of emotion transition), EMOS (emotion match), and SPMOS (speed match), each rated on a 5-point scale. Both the mean and 95% confidence intervals of MOS are reported.</p>\n\n",
                "matched_terms": [
                    "each",
                    "emo2v",
                    "sets",
                    "test",
                    "emotion",
                    "english",
                    "rate",
                    "tts",
                    "chinese",
                    "autopcp",
                    "control",
                    "aro",
                    "wordlevel",
                    "objective",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct subjective evaluations covering emotional expressiveness (EMOS), speaking rate control (SPMOS), speaker similarity (SMOS), and naturalness of emotion transition (NMOS), with details provided in <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6.SS3\" title=\"F.3 Evaluation Metrics &#8227; Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">F.3</span></a></span>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T2\" title=\"Table 2 &#8227; Subjective Evaluation &#8227; 4.2.1 Comparison with Reference Models &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, our method, WeSCon, consistently outperforms all baselines. It achieves more expressive and controllable speech while maintaining speaker identity, demonstrating effective word-level control in emotional expression. Additionally, WeSCon delivers more natural-sounding speech with smoother and more accurate speaking rate modulation.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "rate",
                    "method",
                    "control",
                    "wordlevel",
                    "wescon",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the impact of the transition-smoothing mechanism by removing the tail-to-head alignment during multi-round inference in the 1st-stage model. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T4\" title=\"Table 4 &#8227; 4.2.2 Ablation Study &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, removing this mechanism (\"w/o smoothing\") leads to a substantial increase in DNSV (from 4.980 to 7.568), indicating degraded smoothness between expressive transitions.\nAdditionally, speaker (S-SIM) and emotion similarity (Emo2V. and Aro.) drop notably, suggesting that the discontinuity negatively affects both emotional expression and speaker consistency. These results confirm that our smoothing strategy plays a crucial role in ensuring coherent segment transitions during generation.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "aro",
                    "emo2v",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To examine the effectiveness of our dynamic speaking rate control, we remove this component from the 1st-stage model (\"w/o speed control\"). As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T4\" title=\"Table 4 &#8227; 4.2.2 Ablation Study &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, DNSV slightly increases from 4.980 to 5.067, and performance drops are observed across most expressive metrics, such as AutoPCP (2.650 to 2.499) and Emo2v. (0.866 to 0.844). This suggests that speaking rate variation provides important prosodic cues for emotional expression in TTS.</p>\n\n",
                "matched_terms": [
                    "emo2v",
                    "tts",
                    "rate",
                    "autopcp",
                    "control",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the 2nd-stage model, we evaluate the effect of removing the dynamic emotional attention bias (\"w/o attention bias\"). As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T4\" title=\"Table 4 &#8227; 4.2.2 Ablation Study &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, this results in a clear performance drop across all metrics, especially emotion similarity. DNSV also increases, indicating reduced smoothness. The results confirm the importance of the attention bias module in enabling the 2nd-stage model to focus on the correct emotional prompt during inference.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further investigate the importance of data formatting in self-training. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T4\" title=\"Table 4 &#8227; 4.2.2 Ablation Study &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, removing the emotion flags (\"w/o emotion flag\") results in performance drops across all metrics, indicating that these flags play a crucial role in signaling the locations of emotional shifts to the model. Furthermore, replacing our input data format with a naive one that simply concatenates prompts and targets (\"w/o data format\"), as <math alttext=\"\\{\\bm{C}^{\\text{prompt}\\ \\text{I}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{C}^{\\text{tgt}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{tgt}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msup><mi>&#119914;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>I</mtext></mrow></msup><mo>,</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.84\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><msup><mi>&#119930;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>II</mtext></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>&#119914;</mi><mtext>tgt</mtext></msup><mo>,</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.84\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><msup><mi>&#119930;</mi><mtext>tgt</mtext></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\bm{C}^{\\text{prompt}\\ \\text{I}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{C}^{\\text{tgt}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{tgt}}\\}</annotation></semantics></math> leads to the most significant degradation in expressive metrics, including a sharp increase in CER from 2.166 to 4.141. These results suggest that aligning the data organization with the structure used during pretraining allows the model to better leverage its pre-trained knowledge.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose WeSCon, the first method to overcome expressive data scarcity and enable word-level emotional expression control through end-to-end inference, under a self-training framework with a dynamic emotional attention bias mechanism. Experimental results show that WeSCon achieves state-of-the-art performance using only limited data without emotion or speed transitions, while maintaining strong zero-shot TTS capabilities.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "tts",
                    "method",
                    "control",
                    "wordlevel",
                    "wescon",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">1) Gradual emotion transitions. While WeSCon achieves smooth signal-level transitions, it lacks semantic modeling of emotional evolution. In human speech, emotional changes often involve intermediate states.\n2) Emotion diversity and composition. The model is limited to a fixed set of discrete emotions and does not support compositional or blended expressions, such as combining anger and sadness to convey despair.\n3) Conditioned control. Emotional transitions are currently predefined by GPT-4o-based plans, which restricts flexibility. Future work will explore more dynamic, context-aware control strategies to enable natural, interactive emotional expression.</p>\n\n",
                "matched_terms": [
                    "control",
                    "emotion",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CosyVoice2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/spaces/FunAudioLLM/CosyVoice2-0.5B\" title=\"\">https://huggingface.co/spaces/FunAudioLLM/CosyVoice2-0.5B</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">37</a>]</cite> is a zero-shot TTS model based on a language model (LM) and flow matching. It first converts speech into discrete tokens through a supervised speech tokenizer module. Its core architecture is identical to the base structure illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F3\" title=\"Figure 3 &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, excluding the additional modules introduced in this work.\nThe supervised speech tokenizer is jointly trained with an ASR task, which encourages the LM component to focus more on semantic modeling, particularly in terms of content, emotional expression, and duration. The flow matching component incorporates speaker embeddings<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/alibaba-damo-academy/3D-Speaker/tree/main/egs/3dspeaker/sv-cam++\" title=\"\">https://github.com/alibaba-damo-academy/3D-Speaker/tree/main/egs/3dspeaker/sv-cam++</a></span></span></span> and target speech to provide speaker characteristics. It transforms the speech tokens produced by the language model into mel-spectrograms, primarily controlling global aspects of speech, especially speaker identity. Finally, the vocoder converts the mel-spectrograms into waveform signals. CosyVoice2&#8217;s disentangled modeling of semantic content and speaker identity provides an important foundation for our method. In addition, since its training data is primarily in Chinese, it demonstrates significantly better performance in Chinese than in English.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "english",
                    "method",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F3\" title=\"Figure 3 &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we control the speaking rate of synthesized speech by applying simple interpolation and downsampling to the prompt speech tokens. To assess whether this dynamic mechanism supports time-varying modulation, we visualize six types of control patterns in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A2.F6\" title=\"Figure 6 &#8227; Appendix B Speed Control &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Numeric labels indicate the ratio between the transformed and original token lengths, where a ratio of 1 indicates no change, 0.5 indicates downsampling to half the length, and 2 represents interpolation that doubles it.\nThe left panel illustrates three downsampling patterns: a gradually increasing interval, a decreasing interval, and a uniform interval. The right panel shows corresponding interpolation patterns. These results demonstrate that global interpolation/downsampling can produce effects comparable to time-varying interpolation/downsampling, particularly when accounting for the inherent randomness introduced by LM sampling.\nBecause both methods provide only utterance-level control over speaking rate, word-level modulation requires integration with our multi-round inference framework.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "control",
                    "wordlevel",
                    "results",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further investigate how different resampling ratios influence the speaking rate of the generated speech. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A2.F7\" title=\"Figure 7 &#8227; Appendix B Speed Control &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, the results reveal a clear correlation between the resampling factor and the output speed. When the token length is reduced to less than 40% of the original through downsampling, the model fails to produce intelligible speech, as indicated by the red circles. Conversely, interpolation beyond three times the original length has minimal additional effect on speaking rate. Notably, the most stable and effective control is achieved when the token length lies between 50% and 200% of the original, suggesting this range as a practical bound for reliable modulation.</p>\n\n",
                "matched_terms": [
                    "control",
                    "rate",
                    "results",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the self-training process, we introduce a data filtering mechanism to ensure the reliability of the teacher model&#8217;s guidance. Specifically, we adopt three metrics for evaluating the quality of generated speech: CER for Chinese and WER for English, speaker similarity, and emotion similarity. The first-stage teacher model has explicit access to the alignment among content, speech, emotion prompts, and speaker prompts, allowing us to directly compute these metrics with the prompt.\nTo avoid introducing bias from the final objective evaluation metrics prematurely, we deliberately use models that differ from those employed during evaluation. For speech recognition, we adopt the SenseVoice model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/FunAudioLLM/SenseVoiceSmall\" title=\"\">https://huggingface.co/FunAudioLLM/SenseVoiceSmall</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib61\" title=\"\">61</a>]</cite>. For emotion representation, we use a Whisper model fine-tuned for speech emotion recognition<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3\" title=\"\">https://huggingface.co/firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3</a></span></span></span>. For speaker embedding, we use Resemblyzer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/resemble-ai/Resemblyzer\" title=\"\">https://github.com/resemble-ai/Resemblyzer</a></span></span></span>.\nWe normalize all three metrics for each data point and compute a combined score by summing them. Only the top 50% of data, ranked by this composite score, are selected for self-training. In other words, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.F5\" title=\"Figure 5 &#8227; Self-Training Data Size &#8227; 4.2.2 Ablation Study &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, for a 500-hour training set, we actually generate approximately 1000 hours of data. Similarly, for a 2000-hour training set, we generate around 4000 hours of data.</p>\n\n",
                "matched_terms": [
                    "each",
                    "emotion",
                    "english",
                    "chinese",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Strict Emotion-Aligned Attention.</span>\nThis corresponds to the original training strategy of CosyVoice2. For instance, when decoding the second emotion segment (green S2), the model is only allowed to attend to the corresponding emotional prompt and its associated text, specifically red and green T2, and red S2.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "cosyvoice2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the train-set, dev-set, and test-set of ESD<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/HLTSingapore/Emotional-Speech-Data\" title=\"\">https://github.com/HLTSingapore/Emotional-Speech-Data</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib21\" title=\"\">21</a>]</cite> for training and evaluation. This dataset contains 350 parallel utterances, averaging 2.9 seconds in duration, spoken by 20 speakers: 10 native English and 10 native Mandarin (5 male and 5 female for each language). Each speaker expresses five emotions: happy, sad, neutral, angry, and surprised. All audio is sampled at 16 kHz and stored as 16-bit files.</p>\n\n",
                "matched_terms": [
                    "each",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, we generate 1,000 emotion-speed-varying text samples (500 in Chinese and 500 in English) using the script provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A3\" title=\"Appendix C Generation of Emotionally Varying Texts &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>. For each text sample, we randomly select emotional prompts from the ESD test set to match the emotion transitions required by the sentence. All emotion prompts within a single sentence are drawn from the same speaker to ensure consistency. The reference audio for the target speaker is also randomly selected from the same language-speaker subset.\nAs a result, approximately 1 out of every 5 samples features emotional prompts and a target speaker from the same speaker-emotion setting, given that the ESD dataset contains 5 emotions.</p>\n\n",
                "matched_terms": [
                    "each",
                    "test",
                    "emotion",
                    "english",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Index-TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">7</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/index-tts/index-tts\" title=\"\">https://github.com/index-tts/index-tts</a></span></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib50\" title=\"\">50</a>]</cite> is a GPT-style TTS model enhanced with pinyin-based pronunciation correction for Chinese characters and punctuation-based pause control. It integrates improved speaker condition modeling and BigVGAN2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib62\" title=\"\">62</a>]</cite> for high-quality audio synthesis. Trained on tens of thousands of hours of data, it supports multilingual zero-shot generation.</p>\n\n",
                "matched_terms": [
                    "control",
                    "tts",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Spark-TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">8</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/SparkAudio/Spark-TTS\" title=\"\">https://github.com/SparkAudio/Spark-TTS</a></span></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib52\" title=\"\">52</a>]</cite> is a large language model-based TTS system built upon Qwen2.5&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib63\" title=\"\">63</a>]</cite>. It directly reconstructs waveforms from LLM-predicted codes, eliminating the need for separate acoustic models. This design simplifies the pipeline and improves inference efficiency. It supports zero-shot voice cloning, cross-lingual/code-switching synthesis, and virtual speaker customization via controllable parameters such as gender, pitch, and speaking rate.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "rate",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">F5-TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">9</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/SWivid/F5-TTS\" title=\"\">https://github.com/SWivid/F5-TTS</a></span></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib51\" title=\"\">51</a>]</cite> is a non-autoregressive TTS system based on Diffusion Transformer (DiT)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib64\" title=\"\">64</a>]</cite> and flow matching&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib65\" title=\"\">65</a>]</cite>. It forgoes duration models and alignment by padding text to match speech length, using ConvNeXt V2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib66\" title=\"\">66</a>]</cite> to refine text features. An inference-time Sway Sampling strategy improves decoding efficiency without retraining. Trained on a 100K-hour multilingual dataset, F5-TTS supports zero-shot synthesis, expressive speech generation, speed control, and seamless code-switching.</p>\n\n",
                "matched_terms": [
                    "control",
                    "f5tts",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CosyVoice2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">37</a>]</cite> is a language model-based TTS system designed for zero-shot control of both emotion and speaker identity. Further architectural and training details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A1\" title=\"Appendix A Details of CosyVoice2 &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "control",
                    "emotion",
                    "tts",
                    "cosyvoice2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All baseline systems share the same inference procedure: each sentence is divided into multiple word-level segments with specified emotional states and speaking rates. These segments are synthesized separately using emotion cloning combined with their respective speaking rate control strategies, and then concatenated to form the final speech.</p>\n\n",
                "matched_terms": [
                    "each",
                    "emotion",
                    "rate",
                    "control",
                    "wordlevel",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The objective evaluation is conducted in two groups:\n<span class=\"ltx_text ltx_font_bold\">Group 1.</span> Given the generated speech, the target speaker&#8217;s prompt, and the reference transcript, we compute three utterance-level metrics: character accuracy, speaker similarity, and DNSV (the variance of DNSMOS-PRO<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/fcumlin/DNSMOSPro\" title=\"\">https://github.com/fcumlin/DNSMOSPro</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib59\" title=\"\">59</a>]</cite> scores).\nCharacter accuracy is computed by comparing the output of an automatic speech recognition (ASR) model against the target transcript. Specifically, we use Paraformer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/modelscope/FunASR\" title=\"\">https://github.com/modelscope/FunASR</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib54\" title=\"\">54</a>]</cite> to calculate character error rate (CER) for Chinese and Whisper Large V3<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\">12</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/openai/whisper\" title=\"\">https://github.com/openai/whisper</a></span></span></span> to compute word error rate (WER) for English.\nSpeaker similarity is measured by extracting utterance-level embeddings from the generated speech and the target prompt using WavLM-Large<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification\" title=\"\">https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib55\" title=\"\">55</a>]</cite>, followed by computing the cosine similarity between them.\nDNSV is used to assess transition smoothness. DNSMOS-PRO scores are calculated over the generated speech using a 2-second window and a 1-second stride. The variance of these scores is used to quantify transition smoothness, with higher variance indicating lower smoothness. Since the value of the variance is often relatively small, we multiply it by 100 for display purposes.\n<span class=\"ltx_text ltx_font_bold\">Group 2.</span> Based on the ASR transcription obtained in Group 1, we perform forced alignment to determine word-level timestamps. A string-matching strategy is then used to align each generated word-level segment with its corresponding emotional prompt, according to the original text-emotion-speed mapping.\nFor each aligned pair,\nto evaluate expressive similarity, the emotional prompt is first adjusted to the target speaking rate using a phase vocoder algorithm<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://librosa.org/doc/latest/generated/librosa.effects.time_stretch.html#librosa-effects-time-stretch\" title=\"\">https://librosa.org/doc/latest/generated/librosa.effects.time_stretch.html#librosa-effects-time-stretch</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib67\" title=\"\">67</a>]</cite>. The generated segment is then compared to the rate-adjusted prompt using AutoPCP<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/facebookresearch/stopes/blob/main/stopes/eval/auto_pcp\" title=\"\">https://github.com/facebookresearch/stopes/blob/main/stopes/eval/auto_pcp</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib56\" title=\"\">56</a>]</cite> to compute prosodic similarity.\nEmotion embeddings are extracted using emotion2vec-large<span class=\"ltx_note ltx_role_footnote\" id=\"footnote16\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_tag ltx_tag_note\">16</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/ddlBoJack/emotion2vec\" title=\"\">https://github.com/ddlBoJack/emotion2vec</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib57\" title=\"\">57</a>]</cite> and a wav2vec-based model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote17\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_tag ltx_tag_note\">17</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/audeering/w2v2-how-to\" title=\"\">https://github.com/audeering/w2v2-how-to</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib58\" title=\"\">58</a>]</cite>, and cosine similarity is calculated to quantify emotion similarity.</p>\n\n",
                "matched_terms": [
                    "each",
                    "emotion",
                    "english",
                    "rate",
                    "chinese",
                    "wordlevel",
                    "objective",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct Mean Opinion Score (MOS) evaluations from four perspectives: emotional consistency, speaking rate consistency, speaker similarity, and smoothness of emotional transitions. For each aspect, we provide participants with detailed evaluation criteria and report both the mean scores and 95% confidence intervals. A total of 15 graduate students with research backgrounds in speech emotion recognition or emotional speech synthesis participated in the evaluation. Prior to the test, all participants were provided with a detailed explanation of the interface and task. They were also informed that the data would be used for scientific research purposes. Each participant rated 20 sets of results (10 in Chinese and 10 in English) generated by five different systems. The complete evaluation took an average of approximately 49 minutes per participant. Scores were assigned on a 1 to 5 scale with 0.5-point intervals. The evaluation interface is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6.F9\" title=\"Figure 9 &#8227; Objective Metrics &#8227; F.3 Evaluation Metrics &#8227; Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "sets",
                    "test",
                    "emotion",
                    "english",
                    "rate",
                    "chinese",
                    "results",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the evolution of key validation metrics throughout the two-stage training process, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A8.F10\" title=\"Figure 10 &#8227; Appendix H Training Progress &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A8.F11\" title=\"Figure 11 &#8227; Appendix H Training Progress &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A8.F10\" title=\"Figure 10 &#8227; Appendix H Training Progress &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> displays the frame-level accuracy of the aligner model in the first stage, covering both text token prediction and boundary detection.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A8.F11\" title=\"Figure 11 &#8227; Appendix H Training Progress &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> reports the accuracy of speech token prediction and the frame-level emotion prediction by the emotion aligner in the second stage.\nAs shown, the aligner consistently achieves high frame-level accuracy in both stages. This is expected, as the target classes for both text and emotion are provided as input, and the aligner&#8217;s primary objective is to learn accurate alignments, which is a relatively straightforward task given the model&#8217;s underlying text-to-speech capabilities.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.</p>\n\n",
                "matched_terms": [
                    "method",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?</p>\n\n",
                "matched_terms": [
                    "results",
                    "test"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis",
        "caption": "Table 2: Subjective results evaluated by 15\nlisteners, with 95% confidence intervals computed from the t-test.",
        "body": "Method\nEMOS ↑\\uparrow\nSPMOS ↑\\uparrow\nSMOS ↑\\uparrow\nNMOS ↑\\uparrow\n\n\nIndex-TTS\n\n3.51±\\pm0.19\n\n\n3.50±\\pm0.21\n\n\n3.06±\\pm0.23\n\n\n2.97±\\pm0.25\n\n\n\nF5-TTS\n\n3.63±\\pm0.15\n\n\n3.51±\\pm0.21\n\n\n3.11±\\pm0.25\n\n\n2.84±\\pm0.26\n\n\n\nSpark-TTS\n\n3.55±\\pm0.19\n\n\n3.63±\\pm0.18\n\n\n2.96±\\pm0.24\n\n\n2.99±\\pm0.26\n\n\n\nCosyVoice2\n\n3.61±\\pm0.17\n\n\n3.56±\\pm0.20\n\n\n3.54±\\pm0.25\n\n\n3.29±\\pm0.23\n\n\n\nWeSCon\n3.70±\\pm0.17\n3.89±\\pm0.18\n3.96±\\pm0.19\n3.93±\\pm0.20",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">EMOS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SPMOS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SMOS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">NMOS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Index-TTS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.51</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">0.19</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.50</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">0.21</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.06</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">0.23</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">2.97</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">0.25</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">F5-TTS</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.63</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">0.15</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.51</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">0.21</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.11</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">0.25</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">2.84</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">0.26</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Spark-TTS</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.55</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m13\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">0.19</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.63</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m14\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">0.18</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">2.96</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m15\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">0.24</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">2.99</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m16\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">0.26</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.61</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m17\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">0.17</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.56</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m18\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">0.20</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.54</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m19\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">0.25</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.29</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m20\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">0.23</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">WeSCon</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.70<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m21\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.89<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m22\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.96<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m23\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.19</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.93<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m24\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.20</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "confidence",
            "intervals",
            "computed",
            "indextts",
            "subjective",
            "351±pm021",
            "356±pm020",
            "f5tts",
            "wescon",
            "listeners",
            "nmos",
            "evaluated",
            "spmos",
            "361±pm017",
            "363±pm018",
            "ttest",
            "354±pm025",
            "297±pm025",
            "350±pm021",
            "from",
            "299±pm026",
            "311±pm025",
            "306±pm023",
            "results",
            "363±pm015",
            "393±pm020",
            "396±pm019",
            "329±pm023",
            "370±pm017",
            "↑uparrow",
            "cosyvoice2",
            "389±pm018",
            "emos",
            "sparktts",
            "296±pm024",
            "355±pm019",
            "method",
            "351±pm019",
            "284±pm026",
            "smos"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We conduct subjective evaluations covering emotional expressiveness (EMOS), speaking rate control (SPMOS), speaker similarity (SMOS), and naturalness of emotion transition (NMOS), with details provided in <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6.SS3\" title=\"F.3 Evaluation Metrics &#8227; Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">F.3</span></a></span>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T2\" title=\"Table 2 &#8227; Subjective Evaluation &#8227; 4.2.1 Comparison with Reference Models &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, our method, WeSCon, consistently outperforms all baselines. It achieves more expressive and controllable speech while maintaining speaker identity, demonstrating effective word-level control in emotional expression. Additionally, WeSCon delivers more natural-sounding speech with smoother and more accurate speaking rate modulation.</p>\n\n",
            "<p class=\"ltx_p\">Justification: We report the mean opinion scores (MOS) along with 95% confidence intervals (CI95), computed using the t-distribution over ratings from 15 independent human listeners in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T2\" title=\"Table 2 &#8227; Subjective Evaluation &#8227; 4.2.1 Comparison with Reference Models &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">While emotional text-to-speech (TTS) has made significant progress, most existing research remains limited to utterance-level emotional expression and fails to support word-level control. Achieving word-level expressive control poses fundamental challenges, primarily due to the complexity of modeling multi-emotion transitions and the scarcity of annotated datasets that capture intra-sentence emotional and prosodic variation.\nIn this paper, we propose WeSCon, the first self-training framework that enables word-level control of both emotion and speaking rate in a pretrained zero-shot TTS model, without relying on datasets containing intra-sentence emotion or speed transitions.\nOur method introduces a transition-smoothing strategy and a dynamic speed control mechanism to guide the pretrained TTS model in performing word-level expressive synthesis through a multi-round inference process. To further simplify the inference, we incorporate a dynamic emotional attention bias mechanism and fine-tune the model via self-training, thereby activating its ability for word-level expressive control in an end-to-end manner.\nExperimental results show that WeSCon effectively overcomes data scarcity, achieving state-of-the-art performance in word-level emotional expression control while preserving the strong zero-shot synthesis capabilities of the original TTS model.\n</p>\n\n",
                "matched_terms": [
                    "method",
                    "results",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, motivated by the zero-shot potential of pretrained TTS models, we propose WeSCon, a two-stage self-training framework that achieves <span class=\"ltx_text ltx_font_bold\">W</span>ord-level <span class=\"ltx_text ltx_font_bold\">E</span>motion and <span class=\"ltx_text ltx_font_bold\">S</span>peed <span class=\"ltx_text ltx_font_bold\">Con</span>trol for TTS using only a small amount of public speech data without emotion or speed transitions.\nIn the first stage, we design a multi-round inference framework that incorporates a transition-smoothing module and a dynamic speed control mechanism. Without relying on any emotional training data, this approach enables a pretrained zero-shot TTS model to perform high-quality word-level emotional expression control in TTS. In the second stage, the original TTS model is repurposed as a student and trained under the supervision of the 1st-stage teacher. A dynamic emotional attention bias is introduced, enabling the student to acquire word-level control of emotion and speed through a simplified end-to-end inference process, without the need for complex iterative generation or smoothing.\nExperimental results show that WeSCon achieves state-of-the-art performance on the task of word-level emotional expression control in TTS, while preserving the zero-shot generalization and generation capabilities of the pretrained TTS model.\nOur contributions are summarized as follows:</p>\n\n",
                "matched_terms": [
                    "results",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct comprehensive experiments to validate the effectiveness of our proposed framework. Results show that our method enables a pretrained zero-shot TTS model to achieve SOTA performance in word-level emotional expression control, while preserving its original zero-shot capabilities. Ablation studies further confirm the contribution of each key design component. Our samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymousdemo999.github.io/\" title=\"\">https://anonymousdemo999.github.io/</a>.</p>\n\n",
                "matched_terms": [
                    "method",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S2\" title=\"2 Related Work &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, current TTS models can perform utterance-level emotion and speaker cloning. Building on this, we adopt the high-performance CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">du2024cosyvoice2 </a></cite> as our backbone (details of the backbone architecture are provided in <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A1\" title=\"Appendix A Details of CosyVoice2 &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">A</span></a></span>) and propose a multi-round inference strategy, where the model synthesizes multiple segments using different emotional prompts to achieve word-level emotion control.\nWhile this approach enables flexible emotional modulation, it often causes unnatural acoustic discontinuities at segment boundaries. To address this, we introduce a transition-smoothing mechanism that improves coherence across inference rounds, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F3\" title=\"Figure 3 &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nWithout modifying CosyVoice2, we append a lightweight content aligner, composed of non-causal Transformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib38\" title=\"\">vaswani2017attention </a></cite> and convolutional layers. Trained on ASR data, this module predicts the corresponding text token for each speech token and requires no emotional supervision.\nDuring inference, the input text is segmented based on a user-defined emotion plan. At each inference round, the final text and speech tokens from the previous round are appended to the current prompt, forming an explicit tail-to-head linkage. This aligns naturally with CosyVoice2&#8217;s continuation-style generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib39\" title=\"\">borsos2023audiolm </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib40\" title=\"\">valle </a></cite>, enabling smooth and coherent emotional transitions.</p>\n\n",
                "matched_terms": [
                    "from",
                    "cosyvoice2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In CosyVoice2, utterance-level temporal prosody, including speaking rate and duration, is entirely determined by the reference speech prompt. To support more flexible and word-level control of speaking rate within a single utterance, we introduce a dynamic speed control mechanism as part of our multi-round inference framework, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F3\" title=\"Figure 3 &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The core idea is to adjust the prompt speech tokens using either nearest-neighbor interpolation or downsampling. Interpolation extends the prompt length, which slows down the generated speech, while downsampling shortens the prompt, resulting in a faster speaking rate. As demonstrated in <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A2\" title=\"Appendix B Speed Control &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">B</span></a></span>, this resampling method provides effective global prosody control. By integrating it into the multi-round inference process, the speaking rate can be dynamically controlled at the word level as needed.</p>\n\n",
                "matched_terms": [
                    "cosyvoice2",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the above data formatting preserves CosyVoice2&#8217;s generalization by avoiding interference with learned knowledge, it introduces a new challenge: during synthesis, the model may incorrectly attend to emotion-inconsistent prompts. For instance, when generating speech aligned with Emotion&#160;I, attention may drift toward prompts labeled with Emotion&#160;II, leading to emotional inconsistency and degraded synthesis quality.\nTo address this, we propose a dynamic attention bias mechanism that constrains the model&#8217;s focus to emotion-relevant prompt regions based on the predicted emotional trajectory. Concretely, we introduce a causal lightweight Transformer to predict token-level emotion labels <math alttext=\"E_{t}^{\\text{tgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>E</mi><mi>t</mi><mtext>tgt</mtext></msubsup><annotation encoding=\"application/x-tex\">E_{t}^{\\text{tgt}}</annotation></semantics></math> for each speech token <math alttext=\"\\bm{S}_{t}^{\\text{tgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119930;</mi><mi>t</mi><mtext>tgt</mtext></msubsup><annotation encoding=\"application/x-tex\">\\bm{S}_{t}^{\\text{tgt}}</annotation></semantics></math> from historical context.\nUsing the predicted emotion sequence, we introduce a dynamic attention bias mechanism at each Transformer layer. We first concatenate the current text-speech representation with the predicted emotion features and project it through a linear layer. The output is processed in two ways: one path adds a residual and feeds into the next layer, while the other is passed to an MLP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib42\" title=\"\">popescu2009multilayer </a></cite> and softmax to produce a weight vector <math alttext=\"\\bm{\\omega}\\in\\mathbb{R}^{1\\times 7}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#120654;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>7</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\bm{\\omega}\\in\\mathbb{R}^{1\\times 7}</annotation></semantics></math>.\nThe <math alttext=\"\\bm{\\omega}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m4\" intent=\":literal\"><semantics><mi>&#120654;</mi><annotation encoding=\"application/x-tex\">\\bm{\\omega}</annotation></semantics></math> is then used to compute a dynamic attention bias by linearly combining seven predefined attention bias templates <math alttext=\"\\bm{B}^{\\text{temp}}\\in\\mathbb{R}^{7\\times T\\times T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m5\" intent=\":literal\"><semantics><mrow><msup><mi>&#119913;</mi><mtext>temp</mtext></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>7</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\bm{B}^{\\text{temp}}\\in\\mathbb{R}^{7\\times T\\times T}</annotation></semantics></math> (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A5\" title=\"Appendix E Predefined Emotion Attention Bias &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">E</span></a> for details). The resulting bias is computed as:</p>\n\n",
                "matched_terms": [
                    "from",
                    "computed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage, the content aligner is trained on 200 hours of non-emotional English-Chinese speech from LibriSpeech-100-Clean&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib45\" title=\"\">panayotov2015librispeech </a></cite> and AISHELL-1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib46\" title=\"\">bu2017aishell </a></cite>. In the second stage, the teacher model uses non-transition emotional train-set from ESD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib21\" title=\"\">zhou2021seen </a></cite> as prompts to synthesize training samples based on emotion-transition texts generated by GPT-4o (see <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A3\" title=\"Appendix C Generation of Emotionally Varying Texts &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">C</span></a></span> for generation details and examples).\nWe adopt CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">du2024cosyvoice2 </a></cite> as the backbone TTS model. The content aligner is composed of five non-causal Transformer layers and two 5&#215;5 convolutional layers with stride 1 and batch normalization&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib47\" title=\"\">ioffe2015batch </a></cite>, following CosyVoice2&#8217;s configuration for architectural consistency.\nIn the second stage, the emotion aligner is a lightweight two-layer causal Transformer. The emotional attention bias module includes a linear layer with a hidden dimension of 14 and an MLP output dimension of 7.</p>\n\n",
                "matched_terms": [
                    "from",
                    "cosyvoice2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate word-level control over emotion and speaking rate, we construct test sets based on test set of ESD and use outstanding zero-shot TTS models <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib50\" title=\"\">deng2025indextts </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib51\" title=\"\">chen2024f5 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib52\" title=\"\">wang2025spark </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">du2024cosyvoice2 </a></cite> with multi-round concatenative inference as baselines (see <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6\" title=\"Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">F</span></a></span> for details).\nWe use objective and subjective metrics to assess system performance (see <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6.SS3\" title=\"F.3 Evaluation Metrics &#8227; Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">F.3</span></a></span> for details).\nFor intelligibility, we report WER using Whisper-Large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib53\" title=\"\">radford2023robust </a></cite> for English and CER using Paraformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib54\" title=\"\">gao2022paraformer </a></cite> for Chinese.\nSpeaker similarity (S-SIM) is computed via cosine similarity of WavLM-Large embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib55\" title=\"\">chen2022wavlm </a></cite>.\nTo evaluate prosody alignment, we use AutoPCP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib56\" title=\"\">barrault2023seamless </a></cite>. Emotion similarity metrics (Emo2v. and Aro.) are computed using emotion2vec-Large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib57\" title=\"\">ma2023emotion2vec </a></cite> and a wav2vec-based model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib58\" title=\"\">baevski2020wav2vec </a></cite>, respectively.\nWe use the variance of DNSMOS-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib59\" title=\"\">cumlin2024dnsmos </a></cite> (DNSV) to assess the naturalness of emotion transition.\nSubjective evaluation includes four kinds of Mean Opinion Score (MOS): SMOS (speaker similarity), NMOS (naturalness of emotion transition), EMOS (emotion match), and SPMOS (speed match), each rated on a 5-point scale. Both the mean and 95% confidence intervals of MOS are reported.</p>\n\n",
                "matched_terms": [
                    "confidence",
                    "emos",
                    "nmos",
                    "intervals",
                    "computed",
                    "spmos",
                    "subjective",
                    "smos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our method on word-level emotion and speaking rate control in both English and Chinese TTS. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, WeSCon (1st-stage) and WeSCon (2nd-stage) consistently outperform baselines on expressive metrics. Notably, the 2nd-stage model achieves the highest Emo2V. and Aro. scores in both languages, demonstrating strong word-level emotional expressiveness enabled by our self-training framework.\nRegarding transition smoothness, our models significantly reduce DNSV compared to CosyVoice2, with values dropping from 7.894 to 4.361 in English and from 7.612 to 4.210 in Chinese. This highlights the effectiveness of our smoothing mechanism and the end-to-end continuous inference in the 2nd-stage model in mitigating acoustic discontinuities across transitions.\nWhile the character error rate is slightly higher than baselines, it remains comparable to CosyVoice2, our backbone model.\nFinally, the 2nd-stage model slightly surpasses the 1st-stage model, benefiting from self-training with selective filtering that retains high-quality supervision from the teacher. Overall, our approach consistently improves upon CosyVoice2 and achieves SOTA performance in key aspects of word-level expressive controllable TTS.</p>\n\n",
                "matched_terms": [
                    "from",
                    "cosyvoice2",
                    "method",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to introducing word-level controllability, we evaluate the performance of our method on the standard zero-shot TTS task using the SEED test set (test-zh)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib60\" title=\"\">anastassiou2024seedttsfamilyhighqualityversatile </a></cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T3\" title=\"Table 3 &#8227; Capability on Zero-shot TTS &#8227; 4.2.1 Comparison with Reference Models &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the WeSCon (1st) model yields results identical to CosyVoice2, as the backbone TTS is frozen during this stage. The 2nd-stage model also achieves comparable results. Together with the findings in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, these results demonstrate that our method enables word-level emotion and speaking rate control without significantly degrading the original zero-shot TTS performance of the pretrained model.</p>\n\n",
                "matched_terms": [
                    "method",
                    "cosyvoice2",
                    "results",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the impact of the transition-smoothing mechanism by removing the tail-to-head alignment during multi-round inference in the 1st-stage model. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T4\" title=\"Table 4 &#8227; 4.2.2 Ablation Study &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, removing this mechanism (\"w/o smoothing\") leads to a substantial increase in DNSV (from 4.980 to 7.568), indicating degraded smoothness between expressive transitions.\nAdditionally, speaker (S-SIM) and emotion similarity (Emo2V. and Aro.) drop notably, suggesting that the discontinuity negatively affects both emotional expression and speaker consistency. These results confirm that our smoothing strategy plays a crucial role in ensuring coherent segment transitions during generation.</p>\n\n",
                "matched_terms": [
                    "from",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further investigate the importance of data formatting in self-training. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T4\" title=\"Table 4 &#8227; 4.2.2 Ablation Study &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, removing the emotion flags (\"w/o emotion flag\") results in performance drops across all metrics, indicating that these flags play a crucial role in signaling the locations of emotional shifts to the model. Furthermore, replacing our input data format with a naive one that simply concatenates prompts and targets (\"w/o data format\"), as <math alttext=\"\\{\\bm{C}^{\\text{prompt}\\ \\text{I}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{C}^{\\text{tgt}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{tgt}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msup><mi>&#119914;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>I</mtext></mrow></msup><mo>,</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.84\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><msup><mi>&#119930;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>II</mtext></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>&#119914;</mi><mtext>tgt</mtext></msup><mo>,</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.84\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><msup><mi>&#119930;</mi><mtext>tgt</mtext></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\bm{C}^{\\text{prompt}\\ \\text{I}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{C}^{\\text{tgt}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{tgt}}\\}</annotation></semantics></math> leads to the most significant degradation in expressive metrics, including a sharp increase in CER from 2.166 to 4.141. These results suggest that aligning the data organization with the structure used during pretraining allows the model to better leverage its pre-trained knowledge.</p>\n\n",
                "matched_terms": [
                    "from",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose WeSCon, the first method to overcome expressive data scarcity and enable word-level emotional expression control through end-to-end inference, under a self-training framework with a dynamic emotional attention bias mechanism. Experimental results show that WeSCon achieves state-of-the-art performance using only limited data without emotion or speed transitions, while maintaining strong zero-shot TTS capabilities.</p>\n\n",
                "matched_terms": [
                    "method",
                    "results",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The objective evaluation is conducted in two groups:\n<span class=\"ltx_text ltx_font_bold\">Group 1.</span> Given the generated speech, the target speaker&#8217;s prompt, and the reference transcript, we compute three utterance-level metrics: character accuracy, speaker similarity, and DNSV (the variance of DNSMOS-PRO<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/fcumlin/DNSMOSPro\" title=\"\">https://github.com/fcumlin/DNSMOSPro</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib59\" title=\"\">59</a>]</cite> scores).\nCharacter accuracy is computed by comparing the output of an automatic speech recognition (ASR) model against the target transcript. Specifically, we use Paraformer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/modelscope/FunASR\" title=\"\">https://github.com/modelscope/FunASR</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib54\" title=\"\">54</a>]</cite> to calculate character error rate (CER) for Chinese and Whisper Large V3<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\">12</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/openai/whisper\" title=\"\">https://github.com/openai/whisper</a></span></span></span> to compute word error rate (WER) for English.\nSpeaker similarity is measured by extracting utterance-level embeddings from the generated speech and the target prompt using WavLM-Large<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification\" title=\"\">https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib55\" title=\"\">55</a>]</cite>, followed by computing the cosine similarity between them.\nDNSV is used to assess transition smoothness. DNSMOS-PRO scores are calculated over the generated speech using a 2-second window and a 1-second stride. The variance of these scores is used to quantify transition smoothness, with higher variance indicating lower smoothness. Since the value of the variance is often relatively small, we multiply it by 100 for display purposes.\n<span class=\"ltx_text ltx_font_bold\">Group 2.</span> Based on the ASR transcription obtained in Group 1, we perform forced alignment to determine word-level timestamps. A string-matching strategy is then used to align each generated word-level segment with its corresponding emotional prompt, according to the original text-emotion-speed mapping.\nFor each aligned pair,\nto evaluate expressive similarity, the emotional prompt is first adjusted to the target speaking rate using a phase vocoder algorithm<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://librosa.org/doc/latest/generated/librosa.effects.time_stretch.html#librosa-effects-time-stretch\" title=\"\">https://librosa.org/doc/latest/generated/librosa.effects.time_stretch.html#librosa-effects-time-stretch</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib67\" title=\"\">67</a>]</cite>. The generated segment is then compared to the rate-adjusted prompt using AutoPCP<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/facebookresearch/stopes/blob/main/stopes/eval/auto_pcp\" title=\"\">https://github.com/facebookresearch/stopes/blob/main/stopes/eval/auto_pcp</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib56\" title=\"\">56</a>]</cite> to compute prosodic similarity.\nEmotion embeddings are extracted using emotion2vec-large<span class=\"ltx_note ltx_role_footnote\" id=\"footnote16\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_tag ltx_tag_note\">16</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/ddlBoJack/emotion2vec\" title=\"\">https://github.com/ddlBoJack/emotion2vec</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib57\" title=\"\">57</a>]</cite> and a wav2vec-based model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote17\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_tag ltx_tag_note\">17</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/audeering/w2v2-how-to\" title=\"\">https://github.com/audeering/w2v2-how-to</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib58\" title=\"\">58</a>]</cite>, and cosine similarity is calculated to quantify emotion similarity.</p>\n\n",
                "matched_terms": [
                    "from",
                    "computed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct Mean Opinion Score (MOS) evaluations from four perspectives: emotional consistency, speaking rate consistency, speaker similarity, and smoothness of emotional transitions. For each aspect, we provide participants with detailed evaluation criteria and report both the mean scores and 95% confidence intervals. A total of 15 graduate students with research backgrounds in speech emotion recognition or emotional speech synthesis participated in the evaluation. Prior to the test, all participants were provided with a detailed explanation of the interface and task. They were also informed that the data would be used for scientific research purposes. Each participant rated 20 sets of results (10 in Chinese and 10 in English) generated by five different systems. The complete evaluation took an average of approximately 49 minutes per participant. Scores were assigned on a 1 to 5 scale with 0.5-point intervals. The evaluation interface is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6.F9\" title=\"Figure 9 &#8227; Objective Metrics &#8227; F.3 Evaluation Metrics &#8227; Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "confidence",
                    "from",
                    "intervals",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the generalization ability of our approach under an out-of-domain dataset, we conduct word-level emotion and speaking rate control experiments on the CASIA dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib68\" title=\"\">68</a>]</cite>, as organized according to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6\" title=\"Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. The CASIA corpus is a Mandarin emotional speech dataset recorded by four native speakers and covers six emotion categories: neutral, angry, fear, happy, sad, and surprise. Some of these emotions are not seen during training, which makes CASIA suitable for testing the cross-domain robustness of controllable speech synthesis.\nThe results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A7.T5\" title=\"Table 5 &#8227; Appendix G Out-of-Domain Evaluation &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nOur method, WeSCon, demonstrates strong performance across nearly all evaluation metrics, achieving lower DNSV and higher speaker similarity (S-SIM), emotional similarity (Emo2vec), and arousal scores compared to other baselines. The overall results are consistent with those in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, further confirming that our method generalizes well to unseen speakers and novel emotional patterns.</p>\n\n",
                "matched_terms": [
                    "method",
                    "results",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.</p>\n\n",
                "matched_terms": [
                    "method",
                    "from",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.</p>\n\n",
                "matched_terms": [
                    "confidence",
                    "intervals",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.</p>\n\n",
                "matched_terms": [
                    "from",
                    "results"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis",
        "caption": "Table 3: Objective evaluation on standard zero-shot TTS performance using character error rate (CER) and speaker similarity (S-SIM).",
        "body": "Method\n\nCER ↓\\downarrow\n\n\nS-SIM ↑\\uparrow\n\n\n\n\nCosyVoice2 du2024cosyvoice2 \n\n1.45\n0.748\n\n\nWeSCon (1st)\nsame with CosyVoice2\n\n\nWeSCon (2nd)\n1.47\n0.744",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CER </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">S-SIM </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice2&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">du2024cosyvoice2 </a></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.748</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">WeSCon (1st)</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">same with CosyVoice2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">WeSCon (2nd)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.744</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "du2024cosyvoice2",
            "rate",
            "↓downarrow",
            "2nd",
            "same",
            "ssim",
            "wescon",
            "objective",
            "standard",
            "tts",
            "zeroshot",
            "speaker",
            "evaluation",
            "similarity",
            "cer",
            "↑uparrow",
            "performance",
            "cosyvoice2",
            "character",
            "1st",
            "method",
            "error"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In addition to introducing word-level controllability, we evaluate the performance of our method on the standard zero-shot TTS task using the SEED test set (test-zh)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib60\" title=\"\">anastassiou2024seedttsfamilyhighqualityversatile </a></cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T3\" title=\"Table 3 &#8227; Capability on Zero-shot TTS &#8227; 4.2.1 Comparison with Reference Models &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the WeSCon (1st) model yields results identical to CosyVoice2, as the backbone TTS is frozen during this stage. The 2nd-stage model also achieves comparable results. Together with the findings in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, these results demonstrate that our method enables word-level emotion and speaking rate control without significantly degrading the original zero-shot TTS performance of the pretrained model.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">While emotional text-to-speech (TTS) has made significant progress, most existing research remains limited to utterance-level emotional expression and fails to support word-level control. Achieving word-level expressive control poses fundamental challenges, primarily due to the complexity of modeling multi-emotion transitions and the scarcity of annotated datasets that capture intra-sentence emotional and prosodic variation.\nIn this paper, we propose WeSCon, the first self-training framework that enables word-level control of both emotion and speaking rate in a pretrained zero-shot TTS model, without relying on datasets containing intra-sentence emotion or speed transitions.\nOur method introduces a transition-smoothing strategy and a dynamic speed control mechanism to guide the pretrained TTS model in performing word-level expressive synthesis through a multi-round inference process. To further simplify the inference, we incorporate a dynamic emotional attention bias mechanism and fine-tune the model via self-training, thereby activating its ability for word-level expressive control in an end-to-end manner.\nExperimental results show that WeSCon effectively overcomes data scarcity, achieving state-of-the-art performance in word-level emotional expression control while preserving the strong zero-shot synthesis capabilities of the original TTS model.\n</p>\n\n",
                "matched_terms": [
                    "tts",
                    "rate",
                    "zeroshot",
                    "method",
                    "performance",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans possess the ability to regulate emotional expression during speech flexibly&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib1\" title=\"\">scherer1995expression </a></cite>. To simulate this expressive capability, recent advances in text-to-speech synthesis (TTS) have increasingly focused on controllable generation of various aspects of speech, such as timbre, emotion, and speaking rate&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib2\" title=\"\">xie2024towards </a></cite>. Such control is a key objective in the development of human-like and expressive TTS.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "rate",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most current TTS models exhibit zero-shot capabilities, enabling them to synthesize speech from text while cloning attributes such as timbre, emotion, and speaking rate from a reference speech sample&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib3\" title=\"\">kharitonov2023speak </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib4\" title=\"\">wang2023neural </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib5\" title=\"\">ju2024naturalspeech </a></cite>.\nDespite these advances, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, emotional and speaking rate control in current models is typically limited to the utterance level.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "rate",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This differs significantly from how humans naturally express emotion in speech. <span class=\"ltx_text ltx_font_bold\">Unlike global speaker identity, emotional expression and speaking rate are dynamic and often vary within a single sentence</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib6\" title=\"\">mozziconacci2002prosody </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib7\" title=\"\">guan2018speech </a></cite>. Therefore, word-level control of these factors is essential for achieving more natural and expressive speech synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib8\" title=\"\">9746323 </a></cite>.\nTo address this limitation, some approaches have proposed phoneme-level emotion prediction from target text to guide expressive synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib9\" title=\"\">tang2024ed </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib10\" title=\"\">9383524 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib11\" title=\"\">du21b_interspeech </a></cite>. While these methods show potential for word-level emotion control, relying solely on text makes it difficult to capture essential acoustic cues such as prosody and intensity, which are vital to emotional expression control&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib12\" title=\"\">chen2022fine </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib13\" title=\"\">chandra2024exploring </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib14\" title=\"\">zhao2023emotion </a></cite>.\nTo address this limitation, recent studies such as ELaTE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib15\" title=\"\">kanda2024making </a></cite> and EmoCtrl-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib16\" title=\"\">wu2024laugh </a></cite> have demonstrated that reference speech with emotional content can support intra-utterance control of time-varying expressive patterns, such as transitions from laughter to crying.\nThese works reflect a growing interest in TTS with word-level control over both emotion and speaking rate, but they also underscore several fundamental challenges.\nFirst, word-level expression control requires multiple emotional speech prompts, which introduces the challenge of guiding the model to attend to the appropriate emotion at each word.\nIn addition, current methods for fine-grained expression control rely on large-scale emotional speech datasets with time-aligned emotion transitions. However, such datasets are limited in both scale and accessibility&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib17\" title=\"\">rathi2024analyzing </a></cite>, making fine-grained control even more difficult to realize in practice.\nThese challenges lead us to ask:\n<span class=\"ltx_text ltx_font_bold\">Is it possible to achieve effective word-level control of both emotion and speaking rate without relying on speech datasets containing emotion or speed transitions?</span></p>\n\n",
                "matched_terms": [
                    "speaker",
                    "tts",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, motivated by the zero-shot potential of pretrained TTS models, we propose WeSCon, a two-stage self-training framework that achieves <span class=\"ltx_text ltx_font_bold\">W</span>ord-level <span class=\"ltx_text ltx_font_bold\">E</span>motion and <span class=\"ltx_text ltx_font_bold\">S</span>peed <span class=\"ltx_text ltx_font_bold\">Con</span>trol for TTS using only a small amount of public speech data without emotion or speed transitions.\nIn the first stage, we design a multi-round inference framework that incorporates a transition-smoothing module and a dynamic speed control mechanism. Without relying on any emotional training data, this approach enables a pretrained zero-shot TTS model to perform high-quality word-level emotional expression control in TTS. In the second stage, the original TTS model is repurposed as a student and trained under the supervision of the 1st-stage teacher. A dynamic emotional attention bias is introduced, enabling the student to acquire word-level control of emotion and speed through a simplified end-to-end inference process, without the need for complex iterative generation or smoothing.\nExperimental results show that WeSCon achieves state-of-the-art performance on the task of word-level emotional expression control in TTS, while preserving the zero-shot generalization and generation capabilities of the pretrained TTS model.\nOur contributions are summarized as follows:</p>\n\n",
                "matched_terms": [
                    "performance",
                    "tts",
                    "wescon",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a multi-round inference mechanism equipped with transition smoothing and dynamic speaking rate control, which is the first to achieve word-level control of both emotion and speaking rate in TTS without relying on any emotional training data.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further introduce a novel self-training framework with a dynamic emotional attention bias mechanism that empowers a pretrained TTS model with end-to-end word-level emotion and speaking rate control, using limited data without intra-sentence emotion or speed transitions.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct comprehensive experiments to validate the effectiveness of our proposed framework. Results show that our method enables a pretrained zero-shot TTS model to achieve SOTA performance in word-level emotional expression control, while preserving its original zero-shot capabilities. Ablation studies further confirm the contribution of each key design component. Our samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymousdemo999.github.io/\" title=\"\">https://anonymousdemo999.github.io/</a>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "tts",
                    "method",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most controllable TTS systems support only utterance-level control, where a single label or reference speech governs the entire sentence&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib26\" title=\"\">anastassiou2024seed </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib27\" title=\"\">du2024cosyvoice </a></cite>. To achieve word-level control, some methods attempt to predict frame- or phoneme-level emotional indicators from text alone&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib28\" title=\"\">phone_level_tts0 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib29\" title=\"\">phone_level_tts1 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib30\" title=\"\">phone_level_tts2 </a></cite>, but they often fail to capture expressive variability due to the lack of acoustic cues such as intensity and prosody&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib9\" title=\"\">tang2024ed </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib10\" title=\"\">9383524 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib11\" title=\"\">du21b_interspeech </a></cite>.\nOther approaches, such as ELaTE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib15\" title=\"\">kanda2024making </a></cite> and EmoCtrl-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib16\" title=\"\">wu2024laugh </a></cite>, introduce emotional reference speech to enable intra-utterance control of specific expressive patterns like laughter or crying. While these represent progress, they are typically limited in expressiveness or rely on large-scale emotional datasets that are rarely publicly available. Consequently, achieving general and flexible word-level control over both emotion and speaking rate remains a major challenge.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Self-training has become a promising approach for low-resource speech signal processing, enabling knowledge transfer without fine-grained datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib31\" title=\"\">zoph2020rethinking </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib32\" title=\"\">amini2025self </a></cite>. While it has been applied to tasks like speaker adaptation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib33\" title=\"\">khurana2021unsupervised </a></cite>, paralinguistic modeling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib34\" title=\"\">yang2024frame </a></cite>, and speech translation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib35\" title=\"\">pino20_interspeech </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib36\" title=\"\">fang-etal-2022-stemm </a></cite>, its use for fine-grained emotional control in TTS remains unexplored, especially without detailed expressive labels.\nTo address the scarcity of fine-grained datasets for word-level expressive control, we propose a self-training framework where a teacher model with multi-round inference, transition smoothing, and dynamic speed control generates expressive pseudo-labels. A student model, sharing the teacher&#8217;s backbone, is then fine-tuned under its supervision to perform word-level emotion and speaking rate control through a simplified end-to-end inference process, using only a small public dataset without intra-sentence emotion or speed transitions.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "tts",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">WeSCon is a two-stage self-training framework that enables word-level control of emotion and speaking rate in a pretrained zero-shot TTS model, using only a small amount of emotional speech data without intra-sentence emotion transitions as prompts. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S2.F2\" title=\"Figure 2 &#8227; Self-Training under Data Scarcity &#8227; 2 Related Work &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, in the first stage, we introduce a multi-round inference process with transition smoothing and dynamic speaking rate control to generate speech with word-level expression variations. In the second stage, the 1st-stage model acts as a teacher to guide the original TTS model, equipped with a dynamic emotional attention bias (DEAB), toward word-level control through a simplified end-to-end inference. Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.SS2\" title=\"3.2 Teacher Model &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.SS3\" title=\"3.3 Self-Training &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a> describe the two stages, and Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.SS4\" title=\"3.4 Detail Training Setup &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a> provides the training details.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "rate",
                    "wescon",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S2\" title=\"2 Related Work &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, current TTS models can perform utterance-level emotion and speaker cloning. Building on this, we adopt the high-performance CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">du2024cosyvoice2 </a></cite> as our backbone (details of the backbone architecture are provided in <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A1\" title=\"Appendix A Details of CosyVoice2 &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">A</span></a></span>) and propose a multi-round inference strategy, where the model synthesizes multiple segments using different emotional prompts to achieve word-level emotion control.\nWhile this approach enables flexible emotional modulation, it often causes unnatural acoustic discontinuities at segment boundaries. To address this, we introduce a transition-smoothing mechanism that improves coherence across inference rounds, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F3\" title=\"Figure 3 &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nWithout modifying CosyVoice2, we append a lightweight content aligner, composed of non-causal Transformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib38\" title=\"\">vaswani2017attention </a></cite> and convolutional layers. Trained on ASR data, this module predicts the corresponding text token for each speech token and requires no emotional supervision.\nDuring inference, the input text is segmented based on a user-defined emotion plan. At each inference round, the final text and speech tokens from the previous round are appended to the current prompt, forming an explicit tail-to-head linkage. This aligns naturally with CosyVoice2&#8217;s continuation-style generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib39\" title=\"\">borsos2023audiolm </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib40\" title=\"\">valle </a></cite>, enabling smooth and coherent emotional transitions.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "du2024cosyvoice2",
                    "tts",
                    "cosyvoice2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In CosyVoice2, utterance-level temporal prosody, including speaking rate and duration, is entirely determined by the reference speech prompt. To support more flexible and word-level control of speaking rate within a single utterance, we introduce a dynamic speed control mechanism as part of our multi-round inference framework, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F3\" title=\"Figure 3 &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The core idea is to adjust the prompt speech tokens using either nearest-neighbor interpolation or downsampling. Interpolation extends the prompt length, which slows down the generated speech, while downsampling shortens the prompt, resulting in a faster speaking rate. As demonstrated in <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A2\" title=\"Appendix B Speed Control &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">B</span></a></span>, this resampling method provides effective global prosody control. By integrating it into the multi-round inference process, the speaking rate can be dynamically controlled at the word level as needed.</p>\n\n",
                "matched_terms": [
                    "method",
                    "cosyvoice2",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the speech tokens in CosyVoice2&#8217;s language model (LM) are primarily designed to encode semantic information (<span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">as introduced in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A1\" title=\"Appendix A Details of CosyVoice2 &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">A</span></a></span>), these speech tokens may still inadvertently leak a small amount of speaker-related information. In contrast, the flow matching serves as a voice conversion-based reconstructor that transforms the generated speech tokens into the voice of a specified target speaker. This design implies that as long as speaker inconsistency is avoided during the multi-round inference process in the LM part, the flow matching can effectively enforce speaker consistency in the final output. To ensure this consistency, we adopt a speaker-aware prompt selection strategy. Specifically, during multi-round inference, we prioritize selecting emotional prompts from different emotions of the same speaker. Then, a reference sample from the target speaker is randomly selected to provide the speaker identity to flow matching for generating the target speaker&#8217;s speech.</p>\n\n",
                "matched_terms": [
                    "same",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the previous section, we enabled word-level control of emotion and speaking rate by introducing a multi-round inference framework for CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">du2024cosyvoice2 </a></cite>. However, components such as the non-causal content aligner, multi-round inference, and tail-to-head linkage introduce significant inference complexity. To reduce this overhead while preserving controllability, we adopt a self-training strategy. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F4\" title=\"Figure 4 &#8227; 3.3 Self-Training &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the enhanced first-stage model serves as a teacher to supervise the original TTS model. The student model, equipped with a dynamic emotional attention bias, learns to achieve word-level emotion and speaking rate control through a simplified end-to-end inference.</p>\n\n",
                "matched_terms": [
                    "du2024cosyvoice2",
                    "tts",
                    "rate",
                    "cosyvoice2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our teacher model achieves word-level control of emotion and speaking rate without modifying the original TTS parameters, relying instead on a complex inference pipeline with dynamic speed control and multi-round generation. To transfer this fine-grained control ability to a simplified end-to-end model, we propose a self-training strategy. Specifically, the 1st-stage teacher model guides the student model to learn word-level controllability. We first use GPT-4o&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib41\" title=\"\">hurst2024gpt </a></cite> to generate emotion-transition text sequences (details are shown in <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A3\" title=\"Appendix C Generation of Emotionally Varying Texts &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">C</span></a></span>), which are paired with public emotional speech samples (without emotion transitions) as prompts. The teacher then synthesizes speech with word-level variation in emotion and speaking rate. These outputs are filtered based on character accuracy and expressive similarity (<span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">details are introduced in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A4\" title=\"Appendix D Data Filtering in Self-Training &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">D</span></a></span>), and the student model is fine-tuned on the filtered supervisions with a small learning rate. This enables word-level emotional expression control during inference without requiring multi-round generation or dynamic concatenation.</p>\n\n",
                "matched_terms": [
                    "character",
                    "similarity",
                    "tts",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aim to preserve the strong zero-shot capability of the original TTS model while enabling word-level control of emotional expression under the self-training framework. To achieve this, we formulate the input structure as <math alttext=\"\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{C}^{\\text{prompt}\\ \\text{I}},\\bm{C}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{C}^{\\text{tgt}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{prompt}\\ \\text{I}},\\bm{S}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{S}^{\\text{tgt}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.09\" overflow=\"visible\" style=\"vertical-align:-2.18px\" version=\"1.1\" viewbox=\"0 0 11.09 11.09\" width=\"11.09\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.09) matrix(1 0 0 -1 0 0) translate(5.55,0) translate(0,5.55)\"><path d=\"M 5.27 0 C 5.27 2.91 2.91 5.27 0 5.27 C -2.91 5.27 -5.27 2.91 -5.27 0 C -5.27 -2.91 -2.91 -5.27 0 -5.27 C 2.91 -5.27 5.27 -2.91 5.27 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -2.69 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.56em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"5.38\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">S</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><msup><mi>&#119914;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>I</mtext></mrow></msup><mo>,</mo><msup><mi>&#119914;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>II</mtext></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>&#119914;</mi><mtext>tgt</mtext></msup><mo>,</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.84\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><msup><mi>&#119930;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>I</mtext></mrow></msup><mo>,</mo><msup><mi>&#119930;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>II</mtext></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>&#119930;</mi><mtext>tgt</mtext></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{C}^{\\text{prompt}\\ \\text{I}},\\bm{C}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{C}^{\\text{tgt}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{prompt}\\ \\text{I}},\\bm{S}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{S}^{\\text{tgt}}\\}</annotation></semantics></math>, where <math alttext=\"\\bm{C}^{\\text{prompt}\\ i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m2\" intent=\":literal\"><semantics><msup><mi>&#119914;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\bm{C}^{\\text{prompt}\\ i}</annotation></semantics></math> and <math alttext=\"\\bm{S}^{\\text{prompt}\\ i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m3\" intent=\":literal\"><semantics><msup><mi>&#119930;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\bm{S}^{\\text{prompt}\\ i}</annotation></semantics></math> denote the text and speech tokens of the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m4\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th emotional prompt, respectively. <math alttext=\"\\bm{C}^{\\text{tgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m5\" intent=\":literal\"><semantics><msup><mi>&#119914;</mi><mtext>tgt</mtext></msup><annotation encoding=\"application/x-tex\">\\bm{C}^{\\text{tgt}}</annotation></semantics></math> is the target text token sequence, and <math alttext=\"\\bm{S}^{\\text{tgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m6\" intent=\":literal\"><semantics><msup><mi>&#119930;</mi><mtext>tgt</mtext></msup><annotation encoding=\"application/x-tex\">\\bm{S}^{\\text{tgt}}</annotation></semantics></math> is the corresponding speech token sequence used as supervision. The symbols <span class=\"ltx_inline-block\"><svg class=\"ltx_picture ltx_markedasmath\" height=\"11.09\" id=\"S3.SS3.SSS2.p1.m7.pic1\" overflow=\"visible\" style=\"vertical-align:-2.18px\" version=\"1.1\" viewbox=\"0 0 11.09 11.09\" width=\"11.09\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.09) matrix(1 0 0 -1 0 0) translate(5.55,0) translate(0,5.55)\"><path d=\"M 5.27 0 C 5.27 2.91 2.91 5.27 0 5.27 C -2.91 5.27 -5.27 2.91 -5.27 0 C -5.27 -2.91 -2.91 -5.27 0 -5.27 C 2.91 -5.27 5.27 -2.91 5.27 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -2.69 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.56em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"5.38\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">S</span></span></span></foreignobject></g></g></svg></span> and <span class=\"ltx_inline-block\"><svg class=\"ltx_picture ltx_markedasmath\" height=\"11.84\" id=\"S3.SS3.SSS2.p1.m8.pic1\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span> indicate the beginning of text and speech. This design remains fully compatible with the original input format <math alttext=\"\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{C},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.09\" overflow=\"visible\" style=\"vertical-align:-2.18px\" version=\"1.1\" viewbox=\"0 0 11.09 11.09\" width=\"11.09\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.09) matrix(1 0 0 -1 0 0) translate(5.55,0) translate(0,5.55)\"><path d=\"M 5.27 0 C 5.27 2.91 2.91 5.27 0 5.27 C -2.91 5.27 -5.27 2.91 -5.27 0 C -5.27 -2.91 -2.91 -5.27 0 -5.27 C 2.91 -5.27 5.27 -2.91 5.27 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -2.69 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.56em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"5.38\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">S</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><mi>&#119914;</mi><mo>,</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.84\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><mi>&#119930;</mi><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{C},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}\\}</annotation></semantics></math> of CosyVoice2, preserving the autoregressive pattern of the pretrained model.\nTo further encode word-level emotional variation within this unified format, we extend the text-side input by inserting explicit emotion indicator tokens that mark the boundaries between emotional segments. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F4\" title=\"Figure 4 &#8227; 3.3 Self-Training &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the final input sequence preceding <span class=\"ltx_inline-block\"><svg class=\"ltx_picture ltx_markedasmath\" height=\"11.84\" id=\"S3.SS3.SSS2.p1.m10.pic1\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span> becomes <math alttext=\"\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},E^{\\ \\text{I}},\\bm{C}^{\\text{prompt\\ \\text{I}}},E^{\\ \\text{II}},\\bm{C}^{\\text{prompt\\ \\text{II}}},\\ldots\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m11\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.09\" overflow=\"visible\" style=\"vertical-align:-2.18px\" version=\"1.1\" viewbox=\"0 0 11.09 11.09\" width=\"11.09\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.09) matrix(1 0 0 -1 0 0) translate(5.55,0) translate(0,5.55)\"><path d=\"M 5.27 0 C 5.27 2.91 2.91 5.27 0 5.27 C -2.91 5.27 -5.27 2.91 -5.27 0 C -5.27 -2.91 -2.91 -5.27 0 -5.27 C 2.91 -5.27 5.27 -2.91 5.27 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -2.69 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.56em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"5.38\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">S</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><msup><mi>E</mi><mtext>I</mtext></msup><mo>,</mo><msup><mi>&#119914;</mi><mrow><mtext>prompt&#160;</mtext><mtext>I</mtext></mrow></msup><mo>,</mo><msup><mi>E</mi><mtext>II</mtext></msup><mo>,</mo><msup><mi>&#119914;</mi><mrow><mtext>prompt&#160;</mtext><mtext>II</mtext></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},E^{\\ \\text{I}},\\bm{C}^{\\text{prompt\\ \\text{I}}},E^{\\ \\text{II}},\\bm{C}^{\\text{prompt\\ \\text{II}}},\\ldots\\}</annotation></semantics></math>, where each <math alttext=\"E^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m12\" intent=\":literal\"><semantics><msup><mi>E</mi><mi>i</mi></msup><annotation encoding=\"application/x-tex\">E^{i}</annotation></semantics></math> acts as a soft anchor guiding the model to modulate emotion transitions during generation.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "cosyvoice2",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">WeSCon is trained in two stages. The first stage trains a content aligner to ensure smooth transitions during multi-round inference. In the second stage, a self-training strategy is adopted to transfer the teacher model&#8217;s ability to control word-level emotional expression to the original TTS model.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage, the content aligner is trained on 200 hours of non-emotional English-Chinese speech from LibriSpeech-100-Clean&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib45\" title=\"\">panayotov2015librispeech </a></cite> and AISHELL-1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib46\" title=\"\">bu2017aishell </a></cite>. In the second stage, the teacher model uses non-transition emotional train-set from ESD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib21\" title=\"\">zhou2021seen </a></cite> as prompts to synthesize training samples based on emotion-transition texts generated by GPT-4o (see <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A3\" title=\"Appendix C Generation of Emotionally Varying Texts &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">C</span></a></span> for generation details and examples).\nWe adopt CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">du2024cosyvoice2 </a></cite> as the backbone TTS model. The content aligner is composed of five non-causal Transformer layers and two 5&#215;5 convolutional layers with stride 1 and batch normalization&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib47\" title=\"\">ioffe2015batch </a></cite>, following CosyVoice2&#8217;s configuration for architectural consistency.\nIn the second stage, the emotion aligner is a lightweight two-layer causal Transformer. The emotional attention bias module includes a linear layer with a hidden dimension of 14 and an MLP output dimension of 7.</p>\n\n",
                "matched_terms": [
                    "du2024cosyvoice2",
                    "tts",
                    "cosyvoice2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage, the content aligner is trained for 400k steps on 2 NVIDIA 3090 GPUs using Adam <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib48\" title=\"\">kingma2014adam </a></cite> with a learning rate linearly warmed up to 2.5e-4 over the first 10% of steps, then linearly decayed to 0. Each batch contains 90 seconds of speech. In the second stage, the student model is trained for 600k steps on 4 NVIDIA 3090 GPUs. The TTS model is frozen for the first 20k steps to focus on training the emotion aligner. Each batch contains 40 seconds of speech, and Adam is used with a fixed learning rate of 5e-7. Repetition-aware top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> sampling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib49\" title=\"\">chen2024valle2neuralcodec </a></cite> is applied during inference, with <math alttext=\"k=50\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>50</mn></mrow><annotation encoding=\"application/x-tex\">k=50</annotation></semantics></math> and temperature <math alttext=\"=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">=0.9</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate word-level control over emotion and speaking rate, we construct test sets based on test set of ESD and use outstanding zero-shot TTS models <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib50\" title=\"\">deng2025indextts </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib51\" title=\"\">chen2024f5 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib52\" title=\"\">wang2025spark </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">du2024cosyvoice2 </a></cite> with multi-round concatenative inference as baselines (see <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6\" title=\"Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">F</span></a></span> for details).\nWe use objective and subjective metrics to assess system performance (see <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6.SS3\" title=\"F.3 Evaluation Metrics &#8227; Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">F.3</span></a></span> for details).\nFor intelligibility, we report WER using Whisper-Large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib53\" title=\"\">radford2023robust </a></cite> for English and CER using Paraformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib54\" title=\"\">gao2022paraformer </a></cite> for Chinese.\nSpeaker similarity (S-SIM) is computed via cosine similarity of WavLM-Large embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib55\" title=\"\">chen2022wavlm </a></cite>.\nTo evaluate prosody alignment, we use AutoPCP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib56\" title=\"\">barrault2023seamless </a></cite>. Emotion similarity metrics (Emo2v. and Aro.) are computed using emotion2vec-Large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib57\" title=\"\">ma2023emotion2vec </a></cite> and a wav2vec-based model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib58\" title=\"\">baevski2020wav2vec </a></cite>, respectively.\nWe use the variance of DNSMOS-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib59\" title=\"\">cumlin2024dnsmos </a></cite> (DNSV) to assess the naturalness of emotion transition.\nSubjective evaluation includes four kinds of Mean Opinion Score (MOS): SMOS (speaker similarity), NMOS (naturalness of emotion transition), EMOS (emotion match), and SPMOS (speed match), each rated on a 5-point scale. Both the mean and 95% confidence intervals of MOS are reported.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "du2024cosyvoice2",
                    "tts",
                    "rate",
                    "evaluation",
                    "zeroshot",
                    "similarity",
                    "cer",
                    "speaker",
                    "ssim",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our method on word-level emotion and speaking rate control in both English and Chinese TTS. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, WeSCon (1st-stage) and WeSCon (2nd-stage) consistently outperform baselines on expressive metrics. Notably, the 2nd-stage model achieves the highest Emo2V. and Aro. scores in both languages, demonstrating strong word-level emotional expressiveness enabled by our self-training framework.\nRegarding transition smoothness, our models significantly reduce DNSV compared to CosyVoice2, with values dropping from 7.894 to 4.361 in English and from 7.612 to 4.210 in Chinese. This highlights the effectiveness of our smoothing mechanism and the end-to-end continuous inference in the 2nd-stage model in mitigating acoustic discontinuities across transitions.\nWhile the character error rate is slightly higher than baselines, it remains comparable to CosyVoice2, our backbone model.\nFinally, the 2nd-stage model slightly surpasses the 1st-stage model, benefiting from self-training with selective filtering that retains high-quality supervision from the teacher. Overall, our approach consistently improves upon CosyVoice2 and achieves SOTA performance in key aspects of word-level expressive controllable TTS.</p>\n\n",
                "matched_terms": [
                    "character",
                    "tts",
                    "rate",
                    "method",
                    "performance",
                    "cosyvoice2",
                    "error",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct subjective evaluations covering emotional expressiveness (EMOS), speaking rate control (SPMOS), speaker similarity (SMOS), and naturalness of emotion transition (NMOS), with details provided in <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6.SS3\" title=\"F.3 Evaluation Metrics &#8227; Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">F.3</span></a></span>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T2\" title=\"Table 2 &#8227; Subjective Evaluation &#8227; 4.2.1 Comparison with Reference Models &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, our method, WeSCon, consistently outperforms all baselines. It achieves more expressive and controllable speech while maintaining speaker identity, demonstrating effective word-level control in emotional expression. Additionally, WeSCon delivers more natural-sounding speech with smoother and more accurate speaking rate modulation.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "similarity",
                    "method",
                    "speaker",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the impact of the transition-smoothing mechanism by removing the tail-to-head alignment during multi-round inference in the 1st-stage model. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T4\" title=\"Table 4 &#8227; 4.2.2 Ablation Study &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, removing this mechanism (\"w/o smoothing\") leads to a substantial increase in DNSV (from 4.980 to 7.568), indicating degraded smoothness between expressive transitions.\nAdditionally, speaker (S-SIM) and emotion similarity (Emo2V. and Aro.) drop notably, suggesting that the discontinuity negatively affects both emotional expression and speaker consistency. These results confirm that our smoothing strategy plays a crucial role in ensuring coherent segment transitions during generation.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "speaker",
                    "ssim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To examine the effectiveness of our dynamic speaking rate control, we remove this component from the 1st-stage model (\"w/o speed control\"). As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T4\" title=\"Table 4 &#8227; 4.2.2 Ablation Study &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, DNSV slightly increases from 4.980 to 5.067, and performance drops are observed across most expressive metrics, such as AutoPCP (2.650 to 2.499) and Emo2v. (0.866 to 0.844). This suggests that speaking rate variation provides important prosodic cues for emotional expression in TTS.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "tts",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the 2nd-stage model, we evaluate the effect of removing the dynamic emotional attention bias (\"w/o attention bias\"). As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T4\" title=\"Table 4 &#8227; 4.2.2 Ablation Study &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, this results in a clear performance drop across all metrics, especially emotion similarity. DNSV also increases, indicating reduced smoothness. The results confirm the importance of the attention bias module in enabling the 2nd-stage model to focus on the correct emotional prompt during inference.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "similarity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further investigate the importance of data formatting in self-training. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T4\" title=\"Table 4 &#8227; 4.2.2 Ablation Study &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, removing the emotion flags (\"w/o emotion flag\") results in performance drops across all metrics, indicating that these flags play a crucial role in signaling the locations of emotional shifts to the model. Furthermore, replacing our input data format with a naive one that simply concatenates prompts and targets (\"w/o data format\"), as <math alttext=\"\\{\\bm{C}^{\\text{prompt}\\ \\text{I}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{C}^{\\text{tgt}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{tgt}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msup><mi>&#119914;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>I</mtext></mrow></msup><mo>,</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.84\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><msup><mi>&#119930;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>II</mtext></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>&#119914;</mi><mtext>tgt</mtext></msup><mo>,</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.84\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><msup><mi>&#119930;</mi><mtext>tgt</mtext></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\bm{C}^{\\text{prompt}\\ \\text{I}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{C}^{\\text{tgt}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{tgt}}\\}</annotation></semantics></math> leads to the most significant degradation in expressive metrics, including a sharp increase in CER from 2.166 to 4.141. These results suggest that aligning the data organization with the structure used during pretraining allows the model to better leverage its pre-trained knowledge.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the impact of training data size in the self-training process by varying the amount of synthetic speech used to fine-tune the 2nd-stage model. Metrics are normalized between 0 and 1. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.F5\" title=\"Figure 5 &#8227; Self-Training Data Size &#8227; 4.2.2 Ablation Study &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, performance improves with more data and peaks at 500 hours. Beyond this point, metrics begin to decline. This trend is attributed to the limited variety of emotional categories and speaker identities in the ESD, which restricts expressive diversity and leads to overfitting when the data scale becomes overly redundant.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose WeSCon, the first method to overcome expressive data scarcity and enable word-level emotional expression control through end-to-end inference, under a self-training framework with a dynamic emotional attention bias mechanism. Experimental results show that WeSCon achieves state-of-the-art performance using only limited data without emotion or speed transitions, while maintaining strong zero-shot TTS capabilities.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "zeroshot",
                    "method",
                    "performance",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">WeSCon can be applied to expressive speech synthesis, virtual agents, and emotional storytelling. However, it may also pose risks related to speaker impersonation, especially when specific content and speaker prompts are combined. Like other generative models, it may produce biased or inappropriate outputs, although no such cases were observed during testing.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CosyVoice2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/spaces/FunAudioLLM/CosyVoice2-0.5B\" title=\"\">https://huggingface.co/spaces/FunAudioLLM/CosyVoice2-0.5B</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">37</a>]</cite> is a zero-shot TTS model based on a language model (LM) and flow matching. It first converts speech into discrete tokens through a supervised speech tokenizer module. Its core architecture is identical to the base structure illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F3\" title=\"Figure 3 &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, excluding the additional modules introduced in this work.\nThe supervised speech tokenizer is jointly trained with an ASR task, which encourages the LM component to focus more on semantic modeling, particularly in terms of content, emotional expression, and duration. The flow matching component incorporates speaker embeddings<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/alibaba-damo-academy/3D-Speaker/tree/main/egs/3dspeaker/sv-cam++\" title=\"\">https://github.com/alibaba-damo-academy/3D-Speaker/tree/main/egs/3dspeaker/sv-cam++</a></span></span></span> and target speech to provide speaker characteristics. It transforms the speech tokens produced by the language model into mel-spectrograms, primarily controlling global aspects of speech, especially speaker identity. Finally, the vocoder converts the mel-spectrograms into waveform signals. CosyVoice2&#8217;s disentangled modeling of semantic content and speaker identity provides an important foundation for our method. In addition, since its training data is primarily in Chinese, it demonstrates significantly better performance in Chinese than in English.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "zeroshot",
                    "speaker",
                    "method",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the self-training process, we introduce a data filtering mechanism to ensure the reliability of the teacher model&#8217;s guidance. Specifically, we adopt three metrics for evaluating the quality of generated speech: CER for Chinese and WER for English, speaker similarity, and emotion similarity. The first-stage teacher model has explicit access to the alignment among content, speech, emotion prompts, and speaker prompts, allowing us to directly compute these metrics with the prompt.\nTo avoid introducing bias from the final objective evaluation metrics prematurely, we deliberately use models that differ from those employed during evaluation. For speech recognition, we adopt the SenseVoice model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/FunAudioLLM/SenseVoiceSmall\" title=\"\">https://huggingface.co/FunAudioLLM/SenseVoiceSmall</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib61\" title=\"\">61</a>]</cite>. For emotion representation, we use a Whisper model fine-tuned for speech emotion recognition<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3\" title=\"\">https://huggingface.co/firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3</a></span></span></span>. For speaker embedding, we use Resemblyzer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/resemble-ai/Resemblyzer\" title=\"\">https://github.com/resemble-ai/Resemblyzer</a></span></span></span>.\nWe normalize all three metrics for each data point and compute a combined score by summing them. Only the top 50% of data, ranked by this composite score, are selected for self-training. In other words, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.F5\" title=\"Figure 5 &#8227; Self-Training Data Size &#8227; 4.2.2 Ablation Study &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, for a 500-hour training set, we actually generate approximately 1000 hours of data. Similarly, for a 2000-hour training set, we generate around 4000 hours of data.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "similarity",
                    "cer",
                    "speaker",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the train-set, dev-set, and test-set of ESD<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/HLTSingapore/Emotional-Speech-Data\" title=\"\">https://github.com/HLTSingapore/Emotional-Speech-Data</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib21\" title=\"\">21</a>]</cite> for training and evaluation. This dataset contains 350 parallel utterances, averaging 2.9 seconds in duration, spoken by 20 speakers: 10 native English and 10 native Mandarin (5 male and 5 female for each language). Each speaker expresses five emotions: happy, sad, neutral, angry, and surprised. All audio is sampled at 16 kHz and stored as 16-bit files.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, we generate 1,000 emotion-speed-varying text samples (500 in Chinese and 500 in English) using the script provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A3\" title=\"Appendix C Generation of Emotionally Varying Texts &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>. For each text sample, we randomly select emotional prompts from the ESD test set to match the emotion transitions required by the sentence. All emotion prompts within a single sentence are drawn from the same speaker to ensure consistency. The reference audio for the target speaker is also randomly selected from the same language-speaker subset.\nAs a result, approximately 1 out of every 5 samples features emotional prompts and a target speaker from the same speaker-emotion setting, given that the ESD dataset contains 5 emotions.</p>\n\n",
                "matched_terms": [
                    "same",
                    "speaker",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt four strong zero-shot TTS systems as baselines:</p>\n\n",
                "matched_terms": [
                    "tts",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Index-TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">7</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/index-tts/index-tts\" title=\"\">https://github.com/index-tts/index-tts</a></span></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib50\" title=\"\">50</a>]</cite> is a GPT-style TTS model enhanced with pinyin-based pronunciation correction for Chinese characters and punctuation-based pause control. It integrates improved speaker condition modeling and BigVGAN2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib62\" title=\"\">62</a>]</cite> for high-quality audio synthesis. Trained on tens of thousands of hours of data, it supports multilingual zero-shot generation.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "tts",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Spark-TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">8</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/SparkAudio/Spark-TTS\" title=\"\">https://github.com/SparkAudio/Spark-TTS</a></span></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib52\" title=\"\">52</a>]</cite> is a large language model-based TTS system built upon Qwen2.5&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib63\" title=\"\">63</a>]</cite>. It directly reconstructs waveforms from LLM-predicted codes, eliminating the need for separate acoustic models. This design simplifies the pipeline and improves inference efficiency. It supports zero-shot voice cloning, cross-lingual/code-switching synthesis, and virtual speaker customization via controllable parameters such as gender, pitch, and speaking rate.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "tts",
                    "rate",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">F5-TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">9</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/SWivid/F5-TTS\" title=\"\">https://github.com/SWivid/F5-TTS</a></span></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib51\" title=\"\">51</a>]</cite> is a non-autoregressive TTS system based on Diffusion Transformer (DiT)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib64\" title=\"\">64</a>]</cite> and flow matching&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib65\" title=\"\">65</a>]</cite>. It forgoes duration models and alignment by padding text to match speech length, using ConvNeXt V2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib66\" title=\"\">66</a>]</cite> to refine text features. An inference-time Sway Sampling strategy improves decoding efficiency without retraining. Trained on a 100K-hour multilingual dataset, F5-TTS supports zero-shot synthesis, expressive speech generation, speed control, and seamless code-switching.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CosyVoice2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">37</a>]</cite> is a language model-based TTS system designed for zero-shot control of both emotion and speaker identity. Further architectural and training details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A1\" title=\"Appendix A Details of CosyVoice2 &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "tts",
                    "cosyvoice2",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All baseline systems share the same inference procedure: each sentence is divided into multiple word-level segments with specified emotional states and speaking rates. These segments are synthesized separately using emotion cloning combined with their respective speaking rate control strategies, and then concatenated to form the final speech.</p>\n\n",
                "matched_terms": [
                    "same",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The objective evaluation is conducted in two groups:\n<span class=\"ltx_text ltx_font_bold\">Group 1.</span> Given the generated speech, the target speaker&#8217;s prompt, and the reference transcript, we compute three utterance-level metrics: character accuracy, speaker similarity, and DNSV (the variance of DNSMOS-PRO<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/fcumlin/DNSMOSPro\" title=\"\">https://github.com/fcumlin/DNSMOSPro</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib59\" title=\"\">59</a>]</cite> scores).\nCharacter accuracy is computed by comparing the output of an automatic speech recognition (ASR) model against the target transcript. Specifically, we use Paraformer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/modelscope/FunASR\" title=\"\">https://github.com/modelscope/FunASR</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib54\" title=\"\">54</a>]</cite> to calculate character error rate (CER) for Chinese and Whisper Large V3<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\">12</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/openai/whisper\" title=\"\">https://github.com/openai/whisper</a></span></span></span> to compute word error rate (WER) for English.\nSpeaker similarity is measured by extracting utterance-level embeddings from the generated speech and the target prompt using WavLM-Large<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification\" title=\"\">https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib55\" title=\"\">55</a>]</cite>, followed by computing the cosine similarity between them.\nDNSV is used to assess transition smoothness. DNSMOS-PRO scores are calculated over the generated speech using a 2-second window and a 1-second stride. The variance of these scores is used to quantify transition smoothness, with higher variance indicating lower smoothness. Since the value of the variance is often relatively small, we multiply it by 100 for display purposes.\n<span class=\"ltx_text ltx_font_bold\">Group 2.</span> Based on the ASR transcription obtained in Group 1, we perform forced alignment to determine word-level timestamps. A string-matching strategy is then used to align each generated word-level segment with its corresponding emotional prompt, according to the original text-emotion-speed mapping.\nFor each aligned pair,\nto evaluate expressive similarity, the emotional prompt is first adjusted to the target speaking rate using a phase vocoder algorithm<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://librosa.org/doc/latest/generated/librosa.effects.time_stretch.html#librosa-effects-time-stretch\" title=\"\">https://librosa.org/doc/latest/generated/librosa.effects.time_stretch.html#librosa-effects-time-stretch</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib67\" title=\"\">67</a>]</cite>. The generated segment is then compared to the rate-adjusted prompt using AutoPCP<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/facebookresearch/stopes/blob/main/stopes/eval/auto_pcp\" title=\"\">https://github.com/facebookresearch/stopes/blob/main/stopes/eval/auto_pcp</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib56\" title=\"\">56</a>]</cite> to compute prosodic similarity.\nEmotion embeddings are extracted using emotion2vec-large<span class=\"ltx_note ltx_role_footnote\" id=\"footnote16\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_tag ltx_tag_note\">16</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/ddlBoJack/emotion2vec\" title=\"\">https://github.com/ddlBoJack/emotion2vec</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib57\" title=\"\">57</a>]</cite> and a wav2vec-based model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote17\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_tag ltx_tag_note\">17</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/audeering/w2v2-how-to\" title=\"\">https://github.com/audeering/w2v2-how-to</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib58\" title=\"\">58</a>]</cite>, and cosine similarity is calculated to quantify emotion similarity.</p>\n\n",
                "matched_terms": [
                    "character",
                    "rate",
                    "evaluation",
                    "similarity",
                    "cer",
                    "speaker",
                    "error",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct Mean Opinion Score (MOS) evaluations from four perspectives: emotional consistency, speaking rate consistency, speaker similarity, and smoothness of emotional transitions. For each aspect, we provide participants with detailed evaluation criteria and report both the mean scores and 95% confidence intervals. A total of 15 graduate students with research backgrounds in speech emotion recognition or emotional speech synthesis participated in the evaluation. Prior to the test, all participants were provided with a detailed explanation of the interface and task. They were also informed that the data would be used for scientific research purposes. Each participant rated 20 sets of results (10 in Chinese and 10 in English) generated by five different systems. The complete evaluation took an average of approximately 49 minutes per participant. Scores were assigned on a 1 to 5 scale with 0.5-point intervals. The evaluation interface is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6.F9\" title=\"Figure 9 &#8227; Objective Metrics &#8227; F.3 Evaluation Metrics &#8227; Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "speaker",
                    "rate",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the generalization ability of our approach under an out-of-domain dataset, we conduct word-level emotion and speaking rate control experiments on the CASIA dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib68\" title=\"\">68</a>]</cite>, as organized according to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6\" title=\"Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. The CASIA corpus is a Mandarin emotional speech dataset recorded by four native speakers and covers six emotion categories: neutral, angry, fear, happy, sad, and surprise. Some of these emotions are not seen during training, which makes CASIA suitable for testing the cross-domain robustness of controllable speech synthesis.\nThe results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A7.T5\" title=\"Table 5 &#8227; Appendix G Out-of-Domain Evaluation &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nOur method, WeSCon, demonstrates strong performance across nearly all evaluation metrics, achieving lower DNSV and higher speaker similarity (S-SIM), emotional similarity (Emo2vec), and arousal scores compared to other baselines. The overall results are consistent with those in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, further confirming that our method generalizes well to unseen speakers and novel emotional patterns.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "rate",
                    "evaluation",
                    "similarity",
                    "method",
                    "speaker",
                    "ssim",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the student model (WeSCon 2nd) shows slightly weaker performance than the teacher model on the out-of-domain test set, in some metrics. This degradation is primarily caused by the data filtering strategy adopted during self-training, which improves performance on in-domain speakers and emotions but may introduce subtle biases, resulting in mild overfitting. Nevertheless, such performance fluctuations are acceptable given that the second-stage model significantly simplifies the inference process.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "2nd",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.</p>\n\n",
                "matched_terms": [
                    "same",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)</p>\n\n",
                "matched_terms": [
                    "method",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It should be clear whether the error bar is the standard deviation or the standard error of the mean.</p>\n\n",
                "matched_terms": [
                    "standard",
                    "error"
                ]
            }
        ]
    },
    "S4.T4": {
        "source_file": "Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis",
        "caption": "Table 4: Ablation study on two stages for word-level controllability on Chinese testset.",
        "body": "Method\nCER↓\\downarrow\nDNSV↓\\downarrow\nS-SIM↑\\uparrow\n\n\n\nAuto\n\nPCP\n ↑\\uparrow\n\nEmotion↑\\uparrow\n\n\n\nEmo2v.\nAro.\n\n\nWeSCon (1st)\n2.129\n4.980\n0.595\n2.650\n0.866\n0.551\n\n\nw/o smoothing\n2.209\n7.568\n0.576\n2.596\n0.851\n0.531\n\n\nw/o speed control\n2.126\n5.067\n0.582\n2.499\n0.844\n0.526\n\n\nWeSCon (2nd)\n2.122\n4.210\n0.599\n2.663\n0.872\n0.556\n\n\nw/o attention bias\n2.398\n5.534\n0.575\n2.511\n0.837\n0.519\n\n\nw/o emotion flag\n2.455\n5.880\n0.573\n2.492\n0.831\n0.515\n\n\nw/o datafilter\n2.237\n4.494\n0.592\n2.627\n0.859\n0.542\n\n\nw/o dataformat\n4.141\n5.697\n0.579\n2.504\n0.819\n0.509",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt\" rowspan=\"2\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Method</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">DNSV<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">S-SIM<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\">Auto</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\">PCP</span></span>\n</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:1.9pt;padding-right:1.9pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Emotion</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Emo2v.</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Aro.</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">WeSCon (1st)</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.129</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.980</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.595</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.650</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.866</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.551</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">w/o smoothing</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.209</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.568</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.576</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.596</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.851</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.531</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">w/o speed control</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.126</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.067</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.582</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.499</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.844</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.526</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">WeSCon (2nd)</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.122</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.210</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.599</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.663</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.872</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.556</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">w/o attention bias</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.398</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.534</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.575</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.511</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.837</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.519</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">w/o emotion flag</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.455</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.880</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.573</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.492</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.831</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.515</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">w/o datafilter</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.237</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.494</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.592</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.627</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.859</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.542</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">w/o dataformat</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.141</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.697</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.579</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.504</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.819</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.9pt;padding-right:1.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.509</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "emo2v",
            "ablation",
            "2nd",
            "dnsv↓downarrow",
            "wescon",
            "datafilter",
            "cer↓downarrow",
            "smoothing",
            "ssim↑uparrow",
            "flag",
            "wordlevel",
            "dataformat",
            "speed",
            "pcp",
            "chinese",
            "testset",
            "bias",
            "↑uparrow",
            "aro",
            "auto",
            "1st",
            "study",
            "emotion↑uparrow",
            "emotion",
            "two",
            "stages",
            "controllability",
            "attention",
            "method",
            "control"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluate the impact of the transition-smoothing mechanism by removing the tail-to-head alignment during multi-round inference in the 1st-stage model. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T4\" title=\"Table 4 &#8227; 4.2.2 Ablation Study &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, removing this mechanism (\"w/o smoothing\") leads to a substantial increase in DNSV (from 4.980 to 7.568), indicating degraded smoothness between expressive transitions.\nAdditionally, speaker (S-SIM) and emotion similarity (Emo2V. and Aro.) drop notably, suggesting that the discontinuity negatively affects both emotional expression and speaker consistency. These results confirm that our smoothing strategy plays a crucial role in ensuring coherent segment transitions during generation.</p>\n\n",
            "<p class=\"ltx_p\">To examine the effectiveness of our dynamic speaking rate control, we remove this component from the 1st-stage model (\"w/o speed control\"). As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T4\" title=\"Table 4 &#8227; 4.2.2 Ablation Study &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, DNSV slightly increases from 4.980 to 5.067, and performance drops are observed across most expressive metrics, such as AutoPCP (2.650 to 2.499) and Emo2v. (0.866 to 0.844). This suggests that speaking rate variation provides important prosodic cues for emotional expression in TTS.</p>\n\n",
            "<p class=\"ltx_p\">In the 2nd-stage model, we evaluate the effect of removing the dynamic emotional attention bias (\"w/o attention bias\"). As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T4\" title=\"Table 4 &#8227; 4.2.2 Ablation Study &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, this results in a clear performance drop across all metrics, especially emotion similarity. DNSV also increases, indicating reduced smoothness. The results confirm the importance of the attention bias module in enabling the 2nd-stage model to focus on the correct emotional prompt during inference.</p>\n\n",
            "<p class=\"ltx_p\">We further investigate the importance of data formatting in self-training. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T4\" title=\"Table 4 &#8227; 4.2.2 Ablation Study &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, removing the emotion flags (\"w/o emotion flag\") results in performance drops across all metrics, indicating that these flags play a crucial role in signaling the locations of emotional shifts to the model. Furthermore, replacing our input data format with a naive one that simply concatenates prompts and targets (\"w/o data format\"), as <math alttext=\"\\{\\bm{C}^{\\text{prompt}\\ \\text{I}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{C}^{\\text{tgt}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{tgt}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msup><mi>&#119914;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>I</mtext></mrow></msup><mo>,</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.84\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><msup><mi>&#119930;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>II</mtext></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>&#119914;</mi><mtext>tgt</mtext></msup><mo>,</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.84\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><msup><mi>&#119930;</mi><mtext>tgt</mtext></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\bm{C}^{\\text{prompt}\\ \\text{I}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{C}^{\\text{tgt}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{tgt}}\\}</annotation></semantics></math> leads to the most significant degradation in expressive metrics, including a sharp increase in CER from 2.166 to 4.141. These results suggest that aligning the data organization with the structure used during pretraining allows the model to better leverage its pre-trained knowledge.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">While emotional text-to-speech (TTS) has made significant progress, most existing research remains limited to utterance-level emotional expression and fails to support word-level control. Achieving word-level expressive control poses fundamental challenges, primarily due to the complexity of modeling multi-emotion transitions and the scarcity of annotated datasets that capture intra-sentence emotional and prosodic variation.\nIn this paper, we propose WeSCon, the first self-training framework that enables word-level control of both emotion and speaking rate in a pretrained zero-shot TTS model, without relying on datasets containing intra-sentence emotion or speed transitions.\nOur method introduces a transition-smoothing strategy and a dynamic speed control mechanism to guide the pretrained TTS model in performing word-level expressive synthesis through a multi-round inference process. To further simplify the inference, we incorporate a dynamic emotional attention bias mechanism and fine-tune the model via self-training, thereby activating its ability for word-level expressive control in an end-to-end manner.\nExperimental results show that WeSCon effectively overcomes data scarcity, achieving state-of-the-art performance in word-level emotional expression control while preserving the strong zero-shot synthesis capabilities of the original TTS model.\n</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "speed",
                    "attention",
                    "bias",
                    "method",
                    "control",
                    "wordlevel",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans possess the ability to regulate emotional expression during speech flexibly&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib1\" title=\"\">scherer1995expression </a></cite>. To simulate this expressive capability, recent advances in text-to-speech synthesis (TTS) have increasingly focused on controllable generation of various aspects of speech, such as timbre, emotion, and speaking rate&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib2\" title=\"\">xie2024towards </a></cite>. Such control is a key objective in the development of human-like and expressive TTS.</p>\n\n",
                "matched_terms": [
                    "control",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most current TTS models exhibit zero-shot capabilities, enabling them to synthesize speech from text while cloning attributes such as timbre, emotion, and speaking rate from a reference speech sample&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib3\" title=\"\">kharitonov2023speak </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib4\" title=\"\">wang2023neural </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib5\" title=\"\">ju2024naturalspeech </a></cite>.\nDespite these advances, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, emotional and speaking rate control in current models is typically limited to the utterance level.</p>\n\n",
                "matched_terms": [
                    "control",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This differs significantly from how humans naturally express emotion in speech. <span class=\"ltx_text ltx_font_bold\">Unlike global speaker identity, emotional expression and speaking rate are dynamic and often vary within a single sentence</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib6\" title=\"\">mozziconacci2002prosody </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib7\" title=\"\">guan2018speech </a></cite>. Therefore, word-level control of these factors is essential for achieving more natural and expressive speech synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib8\" title=\"\">9746323 </a></cite>.\nTo address this limitation, some approaches have proposed phoneme-level emotion prediction from target text to guide expressive synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib9\" title=\"\">tang2024ed </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib10\" title=\"\">9383524 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib11\" title=\"\">du21b_interspeech </a></cite>. While these methods show potential for word-level emotion control, relying solely on text makes it difficult to capture essential acoustic cues such as prosody and intensity, which are vital to emotional expression control&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib12\" title=\"\">chen2022fine </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib13\" title=\"\">chandra2024exploring </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib14\" title=\"\">zhao2023emotion </a></cite>.\nTo address this limitation, recent studies such as ELaTE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib15\" title=\"\">kanda2024making </a></cite> and EmoCtrl-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib16\" title=\"\">wu2024laugh </a></cite> have demonstrated that reference speech with emotional content can support intra-utterance control of time-varying expressive patterns, such as transitions from laughter to crying.\nThese works reflect a growing interest in TTS with word-level control over both emotion and speaking rate, but they also underscore several fundamental challenges.\nFirst, word-level expression control requires multiple emotional speech prompts, which introduces the challenge of guiding the model to attend to the appropriate emotion at each word.\nIn addition, current methods for fine-grained expression control rely on large-scale emotional speech datasets with time-aligned emotion transitions. However, such datasets are limited in both scale and accessibility&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib17\" title=\"\">rathi2024analyzing </a></cite>, making fine-grained control even more difficult to realize in practice.\nThese challenges lead us to ask:\n<span class=\"ltx_text ltx_font_bold\">Is it possible to achieve effective word-level control of both emotion and speaking rate without relying on speech datasets containing emotion or speed transitions?</span></p>\n\n",
                "matched_terms": [
                    "control",
                    "emotion",
                    "wordlevel",
                    "speed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, motivated by the zero-shot potential of pretrained TTS models, we propose WeSCon, a two-stage self-training framework that achieves <span class=\"ltx_text ltx_font_bold\">W</span>ord-level <span class=\"ltx_text ltx_font_bold\">E</span>motion and <span class=\"ltx_text ltx_font_bold\">S</span>peed <span class=\"ltx_text ltx_font_bold\">Con</span>trol for TTS using only a small amount of public speech data without emotion or speed transitions.\nIn the first stage, we design a multi-round inference framework that incorporates a transition-smoothing module and a dynamic speed control mechanism. Without relying on any emotional training data, this approach enables a pretrained zero-shot TTS model to perform high-quality word-level emotional expression control in TTS. In the second stage, the original TTS model is repurposed as a student and trained under the supervision of the 1st-stage teacher. A dynamic emotional attention bias is introduced, enabling the student to acquire word-level control of emotion and speed through a simplified end-to-end inference process, without the need for complex iterative generation or smoothing.\nExperimental results show that WeSCon achieves state-of-the-art performance on the task of word-level emotional expression control in TTS, while preserving the zero-shot generalization and generation capabilities of the pretrained TTS model.\nOur contributions are summarized as follows:</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "smoothing",
                    "speed",
                    "attention",
                    "bias",
                    "control",
                    "wordlevel",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a multi-round inference mechanism equipped with transition smoothing and dynamic speaking rate control, which is the first to achieve word-level control of both emotion and speaking rate in TTS without relying on any emotional training data.</p>\n\n",
                "matched_terms": [
                    "control",
                    "emotion",
                    "wordlevel",
                    "smoothing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further introduce a novel self-training framework with a dynamic emotional attention bias mechanism that empowers a pretrained TTS model with end-to-end word-level emotion and speaking rate control, using limited data without intra-sentence emotion or speed transitions.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "attention",
                    "bias",
                    "control",
                    "wordlevel",
                    "speed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct comprehensive experiments to validate the effectiveness of our proposed framework. Results show that our method enables a pretrained zero-shot TTS model to achieve SOTA performance in word-level emotional expression control, while preserving its original zero-shot capabilities. Ablation studies further confirm the contribution of each key design component. Our samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymousdemo999.github.io/\" title=\"\">https://anonymousdemo999.github.io/</a>.</p>\n\n",
                "matched_terms": [
                    "method",
                    "wordlevel",
                    "control",
                    "ablation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development of controllable TTS, particularly for emotional expression control, depends heavily on high-quality emotional speech datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib18\" title=\"\">zhu2024metts_emodata </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib19\" title=\"\">guo2023emodiff_emodata </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib20\" title=\"\">yang2025emovoicellmbasedemotionaltexttospeech_emodata </a></cite>. While public corpora such as ESD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib21\" title=\"\">zhou2021seen </a></cite>, IEMOCAP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib22\" title=\"\">busso2008iemocap </a></cite>, and CREMA-D&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib23\" title=\"\">cao2014crema </a></cite> are available, they primarily provide utterance-level annotations and lack word-level or time-aligned emotional labels. These datasets are also limited in size and diversity, often consisting of scripted speech and covering a narrow range of emotions and speakers&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib24\" title=\"\">ma24b_interspeech </a></cite>.\nMore importantly, emotional datasets with intra-sentence variation, which are essential for learning word-level control, remain extremely scarce and are typically restricted to private use&#160; <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib15\" title=\"\">kanda2024making </a></cite>. Creating such datasets is expensive, requiring detailed word- or frame-level annotation and subjective emotional labeling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib25\" title=\"\">liu2024emphasisrenderingconversationaltexttospeech </a></cite>. This lack of fine-grained emotional data poses a major challenge for training models capable of word-level expressive TTS.</p>\n\n",
                "matched_terms": [
                    "control",
                    "wordlevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most controllable TTS systems support only utterance-level control, where a single label or reference speech governs the entire sentence&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib26\" title=\"\">anastassiou2024seed </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib27\" title=\"\">du2024cosyvoice </a></cite>. To achieve word-level control, some methods attempt to predict frame- or phoneme-level emotional indicators from text alone&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib28\" title=\"\">phone_level_tts0 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib29\" title=\"\">phone_level_tts1 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib30\" title=\"\">phone_level_tts2 </a></cite>, but they often fail to capture expressive variability due to the lack of acoustic cues such as intensity and prosody&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib9\" title=\"\">tang2024ed </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib10\" title=\"\">9383524 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib11\" title=\"\">du21b_interspeech </a></cite>.\nOther approaches, such as ELaTE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib15\" title=\"\">kanda2024making </a></cite> and EmoCtrl-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib16\" title=\"\">wu2024laugh </a></cite>, introduce emotional reference speech to enable intra-utterance control of specific expressive patterns like laughter or crying. While these represent progress, they are typically limited in expressiveness or rely on large-scale emotional datasets that are rarely publicly available. Consequently, achieving general and flexible word-level control over both emotion and speaking rate remains a major challenge.</p>\n\n",
                "matched_terms": [
                    "control",
                    "emotion",
                    "wordlevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Self-training has become a promising approach for low-resource speech signal processing, enabling knowledge transfer without fine-grained datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib31\" title=\"\">zoph2020rethinking </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib32\" title=\"\">amini2025self </a></cite>. While it has been applied to tasks like speaker adaptation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib33\" title=\"\">khurana2021unsupervised </a></cite>, paralinguistic modeling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib34\" title=\"\">yang2024frame </a></cite>, and speech translation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib35\" title=\"\">pino20_interspeech </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib36\" title=\"\">fang-etal-2022-stemm </a></cite>, its use for fine-grained emotional control in TTS remains unexplored, especially without detailed expressive labels.\nTo address the scarcity of fine-grained datasets for word-level expressive control, we propose a self-training framework where a teacher model with multi-round inference, transition smoothing, and dynamic speed control generates expressive pseudo-labels. A student model, sharing the teacher&#8217;s backbone, is then fine-tuned under its supervision to perform word-level emotion and speaking rate control through a simplified end-to-end inference process, using only a small public dataset without intra-sentence emotion or speed transitions.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "smoothing",
                    "control",
                    "wordlevel",
                    "speed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">WeSCon is a two-stage self-training framework that enables word-level control of emotion and speaking rate in a pretrained zero-shot TTS model, using only a small amount of emotional speech data without intra-sentence emotion transitions as prompts. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S2.F2\" title=\"Figure 2 &#8227; Self-Training under Data Scarcity &#8227; 2 Related Work &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, in the first stage, we introduce a multi-round inference process with transition smoothing and dynamic speaking rate control to generate speech with word-level expression variations. In the second stage, the 1st-stage model acts as a teacher to guide the original TTS model, equipped with a dynamic emotional attention bias (DEAB), toward word-level control through a simplified end-to-end inference. Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.SS2\" title=\"3.2 Teacher Model &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.SS3\" title=\"3.3 Self-Training &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a> describe the two stages, and Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.SS4\" title=\"3.4 Detail Training Setup &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a> provides the training details.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "smoothing",
                    "two",
                    "stages",
                    "attention",
                    "bias",
                    "control",
                    "wordlevel",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S2\" title=\"2 Related Work &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, current TTS models can perform utterance-level emotion and speaker cloning. Building on this, we adopt the high-performance CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">du2024cosyvoice2 </a></cite> as our backbone (details of the backbone architecture are provided in <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A1\" title=\"Appendix A Details of CosyVoice2 &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">A</span></a></span>) and propose a multi-round inference strategy, where the model synthesizes multiple segments using different emotional prompts to achieve word-level emotion control.\nWhile this approach enables flexible emotional modulation, it often causes unnatural acoustic discontinuities at segment boundaries. To address this, we introduce a transition-smoothing mechanism that improves coherence across inference rounds, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F3\" title=\"Figure 3 &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nWithout modifying CosyVoice2, we append a lightweight content aligner, composed of non-causal Transformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib38\" title=\"\">vaswani2017attention </a></cite> and convolutional layers. Trained on ASR data, this module predicts the corresponding text token for each speech token and requires no emotional supervision.\nDuring inference, the input text is segmented based on a user-defined emotion plan. At each inference round, the final text and speech tokens from the previous round are appended to the current prompt, forming an explicit tail-to-head linkage. This aligns naturally with CosyVoice2&#8217;s continuation-style generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib39\" title=\"\">borsos2023audiolm </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib40\" title=\"\">valle </a></cite>, enabling smooth and coherent emotional transitions.</p>\n\n",
                "matched_terms": [
                    "control",
                    "emotion",
                    "wordlevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In CosyVoice2, utterance-level temporal prosody, including speaking rate and duration, is entirely determined by the reference speech prompt. To support more flexible and word-level control of speaking rate within a single utterance, we introduce a dynamic speed control mechanism as part of our multi-round inference framework, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F3\" title=\"Figure 3 &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The core idea is to adjust the prompt speech tokens using either nearest-neighbor interpolation or downsampling. Interpolation extends the prompt length, which slows down the generated speech, while downsampling shortens the prompt, resulting in a faster speaking rate. As demonstrated in <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A2\" title=\"Appendix B Speed Control &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">B</span></a></span>, this resampling method provides effective global prosody control. By integrating it into the multi-round inference process, the speaking rate can be dynamically controlled at the word level as needed.</p>\n\n",
                "matched_terms": [
                    "method",
                    "control",
                    "wordlevel",
                    "speed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the previous section, we enabled word-level control of emotion and speaking rate by introducing a multi-round inference framework for CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">du2024cosyvoice2 </a></cite>. However, components such as the non-causal content aligner, multi-round inference, and tail-to-head linkage introduce significant inference complexity. To reduce this overhead while preserving controllability, we adopt a self-training strategy. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F4\" title=\"Figure 4 &#8227; 3.3 Self-Training &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the enhanced first-stage model serves as a teacher to supervise the original TTS model. The student model, equipped with a dynamic emotional attention bias, learns to achieve word-level emotion and speaking rate control through a simplified end-to-end inference.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "attention",
                    "controllability",
                    "bias",
                    "control",
                    "wordlevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our teacher model achieves word-level control of emotion and speaking rate without modifying the original TTS parameters, relying instead on a complex inference pipeline with dynamic speed control and multi-round generation. To transfer this fine-grained control ability to a simplified end-to-end model, we propose a self-training strategy. Specifically, the 1st-stage teacher model guides the student model to learn word-level controllability. We first use GPT-4o&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib41\" title=\"\">hurst2024gpt </a></cite> to generate emotion-transition text sequences (details are shown in <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A3\" title=\"Appendix C Generation of Emotionally Varying Texts &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">C</span></a></span>), which are paired with public emotional speech samples (without emotion transitions) as prompts. The teacher then synthesizes speech with word-level variation in emotion and speaking rate. These outputs are filtered based on character accuracy and expressive similarity (<span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">details are introduced in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A4\" title=\"Appendix D Data Filtering in Self-Training &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">D</span></a></span>), and the student model is fine-tuned on the filtered supervisions with a small learning rate. This enables word-level emotional expression control during inference without requiring multi-round generation or dynamic concatenation.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "controllability",
                    "control",
                    "wordlevel",
                    "speed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aim to preserve the strong zero-shot capability of the original TTS model while enabling word-level control of emotional expression under the self-training framework. To achieve this, we formulate the input structure as <math alttext=\"\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{C}^{\\text{prompt}\\ \\text{I}},\\bm{C}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{C}^{\\text{tgt}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{prompt}\\ \\text{I}},\\bm{S}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{S}^{\\text{tgt}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.09\" overflow=\"visible\" style=\"vertical-align:-2.18px\" version=\"1.1\" viewbox=\"0 0 11.09 11.09\" width=\"11.09\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.09) matrix(1 0 0 -1 0 0) translate(5.55,0) translate(0,5.55)\"><path d=\"M 5.27 0 C 5.27 2.91 2.91 5.27 0 5.27 C -2.91 5.27 -5.27 2.91 -5.27 0 C -5.27 -2.91 -2.91 -5.27 0 -5.27 C 2.91 -5.27 5.27 -2.91 5.27 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -2.69 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.56em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"5.38\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">S</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><msup><mi>&#119914;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>I</mtext></mrow></msup><mo>,</mo><msup><mi>&#119914;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>II</mtext></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>&#119914;</mi><mtext>tgt</mtext></msup><mo>,</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.84\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><msup><mi>&#119930;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>I</mtext></mrow></msup><mo>,</mo><msup><mi>&#119930;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>II</mtext></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>&#119930;</mi><mtext>tgt</mtext></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{C}^{\\text{prompt}\\ \\text{I}},\\bm{C}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{C}^{\\text{tgt}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{prompt}\\ \\text{I}},\\bm{S}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{S}^{\\text{tgt}}\\}</annotation></semantics></math>, where <math alttext=\"\\bm{C}^{\\text{prompt}\\ i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m2\" intent=\":literal\"><semantics><msup><mi>&#119914;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\bm{C}^{\\text{prompt}\\ i}</annotation></semantics></math> and <math alttext=\"\\bm{S}^{\\text{prompt}\\ i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m3\" intent=\":literal\"><semantics><msup><mi>&#119930;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\bm{S}^{\\text{prompt}\\ i}</annotation></semantics></math> denote the text and speech tokens of the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m4\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th emotional prompt, respectively. <math alttext=\"\\bm{C}^{\\text{tgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m5\" intent=\":literal\"><semantics><msup><mi>&#119914;</mi><mtext>tgt</mtext></msup><annotation encoding=\"application/x-tex\">\\bm{C}^{\\text{tgt}}</annotation></semantics></math> is the target text token sequence, and <math alttext=\"\\bm{S}^{\\text{tgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m6\" intent=\":literal\"><semantics><msup><mi>&#119930;</mi><mtext>tgt</mtext></msup><annotation encoding=\"application/x-tex\">\\bm{S}^{\\text{tgt}}</annotation></semantics></math> is the corresponding speech token sequence used as supervision. The symbols <span class=\"ltx_inline-block\"><svg class=\"ltx_picture ltx_markedasmath\" height=\"11.09\" id=\"S3.SS3.SSS2.p1.m7.pic1\" overflow=\"visible\" style=\"vertical-align:-2.18px\" version=\"1.1\" viewbox=\"0 0 11.09 11.09\" width=\"11.09\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.09) matrix(1 0 0 -1 0 0) translate(5.55,0) translate(0,5.55)\"><path d=\"M 5.27 0 C 5.27 2.91 2.91 5.27 0 5.27 C -2.91 5.27 -5.27 2.91 -5.27 0 C -5.27 -2.91 -2.91 -5.27 0 -5.27 C 2.91 -5.27 5.27 -2.91 5.27 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -2.69 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.56em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"5.38\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">S</span></span></span></foreignobject></g></g></svg></span> and <span class=\"ltx_inline-block\"><svg class=\"ltx_picture ltx_markedasmath\" height=\"11.84\" id=\"S3.SS3.SSS2.p1.m8.pic1\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span> indicate the beginning of text and speech. This design remains fully compatible with the original input format <math alttext=\"\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{C},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.09\" overflow=\"visible\" style=\"vertical-align:-2.18px\" version=\"1.1\" viewbox=\"0 0 11.09 11.09\" width=\"11.09\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.09) matrix(1 0 0 -1 0 0) translate(5.55,0) translate(0,5.55)\"><path d=\"M 5.27 0 C 5.27 2.91 2.91 5.27 0 5.27 C -2.91 5.27 -5.27 2.91 -5.27 0 C -5.27 -2.91 -2.91 -5.27 0 -5.27 C 2.91 -5.27 5.27 -2.91 5.27 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -2.69 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.56em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"5.38\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">S</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><mi>&#119914;</mi><mo>,</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.84\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><mi>&#119930;</mi><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{C},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}\\}</annotation></semantics></math> of CosyVoice2, preserving the autoregressive pattern of the pretrained model.\nTo further encode word-level emotional variation within this unified format, we extend the text-side input by inserting explicit emotion indicator tokens that mark the boundaries between emotional segments. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F4\" title=\"Figure 4 &#8227; 3.3 Self-Training &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the final input sequence preceding <span class=\"ltx_inline-block\"><svg class=\"ltx_picture ltx_markedasmath\" height=\"11.84\" id=\"S3.SS3.SSS2.p1.m10.pic1\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span> becomes <math alttext=\"\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},E^{\\ \\text{I}},\\bm{C}^{\\text{prompt\\ \\text{I}}},E^{\\ \\text{II}},\\bm{C}^{\\text{prompt\\ \\text{II}}},\\ldots\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m11\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.09\" overflow=\"visible\" style=\"vertical-align:-2.18px\" version=\"1.1\" viewbox=\"0 0 11.09 11.09\" width=\"11.09\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.09) matrix(1 0 0 -1 0 0) translate(5.55,0) translate(0,5.55)\"><path d=\"M 5.27 0 C 5.27 2.91 2.91 5.27 0 5.27 C -2.91 5.27 -5.27 2.91 -5.27 0 C -5.27 -2.91 -2.91 -5.27 0 -5.27 C 2.91 -5.27 5.27 -2.91 5.27 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -2.69 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.56em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"5.38\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">S</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><msup><mi>E</mi><mtext>I</mtext></msup><mo>,</mo><msup><mi>&#119914;</mi><mrow><mtext>prompt&#160;</mtext><mtext>I</mtext></mrow></msup><mo>,</mo><msup><mi>E</mi><mtext>II</mtext></msup><mo>,</mo><msup><mi>&#119914;</mi><mrow><mtext>prompt&#160;</mtext><mtext>II</mtext></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},E^{\\ \\text{I}},\\bm{C}^{\\text{prompt\\ \\text{I}}},E^{\\ \\text{II}},\\bm{C}^{\\text{prompt\\ \\text{II}}},\\ldots\\}</annotation></semantics></math>, where each <math alttext=\"E^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m12\" intent=\":literal\"><semantics><msup><mi>E</mi><mi>i</mi></msup><annotation encoding=\"application/x-tex\">E^{i}</annotation></semantics></math> acts as a soft anchor guiding the model to modulate emotion transitions during generation.</p>\n\n",
                "matched_terms": [
                    "control",
                    "emotion",
                    "wordlevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the above data formatting preserves CosyVoice2&#8217;s generalization by avoiding interference with learned knowledge, it introduces a new challenge: during synthesis, the model may incorrectly attend to emotion-inconsistent prompts. For instance, when generating speech aligned with Emotion&#160;I, attention may drift toward prompts labeled with Emotion&#160;II, leading to emotional inconsistency and degraded synthesis quality.\nTo address this, we propose a dynamic attention bias mechanism that constrains the model&#8217;s focus to emotion-relevant prompt regions based on the predicted emotional trajectory. Concretely, we introduce a causal lightweight Transformer to predict token-level emotion labels <math alttext=\"E_{t}^{\\text{tgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>E</mi><mi>t</mi><mtext>tgt</mtext></msubsup><annotation encoding=\"application/x-tex\">E_{t}^{\\text{tgt}}</annotation></semantics></math> for each speech token <math alttext=\"\\bm{S}_{t}^{\\text{tgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119930;</mi><mi>t</mi><mtext>tgt</mtext></msubsup><annotation encoding=\"application/x-tex\">\\bm{S}_{t}^{\\text{tgt}}</annotation></semantics></math> from historical context.\nUsing the predicted emotion sequence, we introduce a dynamic attention bias mechanism at each Transformer layer. We first concatenate the current text-speech representation with the predicted emotion features and project it through a linear layer. The output is processed in two ways: one path adds a residual and feeds into the next layer, while the other is passed to an MLP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib42\" title=\"\">popescu2009multilayer </a></cite> and softmax to produce a weight vector <math alttext=\"\\bm{\\omega}\\in\\mathbb{R}^{1\\times 7}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#120654;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>7</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\bm{\\omega}\\in\\mathbb{R}^{1\\times 7}</annotation></semantics></math>.\nThe <math alttext=\"\\bm{\\omega}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m4\" intent=\":literal\"><semantics><mi>&#120654;</mi><annotation encoding=\"application/x-tex\">\\bm{\\omega}</annotation></semantics></math> is then used to compute a dynamic attention bias by linearly combining seven predefined attention bias templates <math alttext=\"\\bm{B}^{\\text{temp}}\\in\\mathbb{R}^{7\\times T\\times T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m5\" intent=\":literal\"><semantics><mrow><msup><mi>&#119913;</mi><mtext>temp</mtext></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>7</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\bm{B}^{\\text{temp}}\\in\\mathbb{R}^{7\\times T\\times T}</annotation></semantics></math> (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A5\" title=\"Appendix E Predefined Emotion Attention Bias &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">E</span></a> for details). The resulting bias is computed as:</p>\n\n",
                "matched_terms": [
                    "bias",
                    "emotion",
                    "two",
                    "attention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Then we multiply the bias with the softmax-normalized attention to selectively emphasize regions aligned with the current emotional context. The final self-attention output is computed as:</p>\n\n",
                "matched_terms": [
                    "bias",
                    "attention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">WeSCon is trained in two stages. The first stage trains a content aligner to ensure smooth transitions during multi-round inference. In the second stage, a self-training strategy is adopted to transfer the teacher model&#8217;s ability to control word-level emotional expression to the original TTS model.</p>\n\n",
                "matched_terms": [
                    "two",
                    "stages",
                    "control",
                    "wordlevel",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The teacher model generates supervision via multi-round inference using GPT-4o-generated texts with emotion labels. Token-level emotion labels are aligned based on emotion-text correspondence.\nThe student model is optimized by two objectives. The first is a negative log-likelihood for speech token prediction:</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage, the content aligner is trained on 200 hours of non-emotional English-Chinese speech from LibriSpeech-100-Clean&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib45\" title=\"\">panayotov2015librispeech </a></cite> and AISHELL-1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib46\" title=\"\">bu2017aishell </a></cite>. In the second stage, the teacher model uses non-transition emotional train-set from ESD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib21\" title=\"\">zhou2021seen </a></cite> as prompts to synthesize training samples based on emotion-transition texts generated by GPT-4o (see <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A3\" title=\"Appendix C Generation of Emotionally Varying Texts &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">C</span></a></span> for generation details and examples).\nWe adopt CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">du2024cosyvoice2 </a></cite> as the backbone TTS model. The content aligner is composed of five non-causal Transformer layers and two 5&#215;5 convolutional layers with stride 1 and batch normalization&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib47\" title=\"\">ioffe2015batch </a></cite>, following CosyVoice2&#8217;s configuration for architectural consistency.\nIn the second stage, the emotion aligner is a lightweight two-layer causal Transformer. The emotional attention bias module includes a linear layer with a hidden dimension of 14 and an MLP output dimension of 7.</p>\n\n",
                "matched_terms": [
                    "bias",
                    "emotion",
                    "two",
                    "attention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate word-level control over emotion and speaking rate, we construct test sets based on test set of ESD and use outstanding zero-shot TTS models <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib50\" title=\"\">deng2025indextts </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib51\" title=\"\">chen2024f5 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib52\" title=\"\">wang2025spark </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">du2024cosyvoice2 </a></cite> with multi-round concatenative inference as baselines (see <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6\" title=\"Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">F</span></a></span> for details).\nWe use objective and subjective metrics to assess system performance (see <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6.SS3\" title=\"F.3 Evaluation Metrics &#8227; Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">F.3</span></a></span> for details).\nFor intelligibility, we report WER using Whisper-Large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib53\" title=\"\">radford2023robust </a></cite> for English and CER using Paraformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib54\" title=\"\">gao2022paraformer </a></cite> for Chinese.\nSpeaker similarity (S-SIM) is computed via cosine similarity of WavLM-Large embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib55\" title=\"\">chen2022wavlm </a></cite>.\nTo evaluate prosody alignment, we use AutoPCP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib56\" title=\"\">barrault2023seamless </a></cite>. Emotion similarity metrics (Emo2v. and Aro.) are computed using emotion2vec-Large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib57\" title=\"\">ma2023emotion2vec </a></cite> and a wav2vec-based model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib58\" title=\"\">baevski2020wav2vec </a></cite>, respectively.\nWe use the variance of DNSMOS-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib59\" title=\"\">cumlin2024dnsmos </a></cite> (DNSV) to assess the naturalness of emotion transition.\nSubjective evaluation includes four kinds of Mean Opinion Score (MOS): SMOS (speaker similarity), NMOS (naturalness of emotion transition), EMOS (emotion match), and SPMOS (speed match), each rated on a 5-point scale. Both the mean and 95% confidence intervals of MOS are reported.</p>\n\n",
                "matched_terms": [
                    "emo2v",
                    "emotion",
                    "chinese",
                    "control",
                    "aro",
                    "wordlevel",
                    "speed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our method on word-level emotion and speaking rate control in both English and Chinese TTS. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, WeSCon (1st-stage) and WeSCon (2nd-stage) consistently outperform baselines on expressive metrics. Notably, the 2nd-stage model achieves the highest Emo2V. and Aro. scores in both languages, demonstrating strong word-level emotional expressiveness enabled by our self-training framework.\nRegarding transition smoothness, our models significantly reduce DNSV compared to CosyVoice2, with values dropping from 7.894 to 4.361 in English and from 7.612 to 4.210 in Chinese. This highlights the effectiveness of our smoothing mechanism and the end-to-end continuous inference in the 2nd-stage model in mitigating acoustic discontinuities across transitions.\nWhile the character error rate is slightly higher than baselines, it remains comparable to CosyVoice2, our backbone model.\nFinally, the 2nd-stage model slightly surpasses the 1st-stage model, benefiting from self-training with selective filtering that retains high-quality supervision from the teacher. Overall, our approach consistently improves upon CosyVoice2 and achieves SOTA performance in key aspects of word-level expressive controllable TTS.</p>\n\n",
                "matched_terms": [
                    "emo2v",
                    "emotion",
                    "smoothing",
                    "chinese",
                    "method",
                    "control",
                    "aro",
                    "wordlevel",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct subjective evaluations covering emotional expressiveness (EMOS), speaking rate control (SPMOS), speaker similarity (SMOS), and naturalness of emotion transition (NMOS), with details provided in <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6.SS3\" title=\"F.3 Evaluation Metrics &#8227; Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">F.3</span></a></span>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T2\" title=\"Table 2 &#8227; Subjective Evaluation &#8227; 4.2.1 Comparison with Reference Models &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, our method, WeSCon, consistently outperforms all baselines. It achieves more expressive and controllable speech while maintaining speaker identity, demonstrating effective word-level control in emotional expression. Additionally, WeSCon delivers more natural-sounding speech with smoother and more accurate speaking rate modulation.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "method",
                    "control",
                    "wordlevel",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to introducing word-level controllability, we evaluate the performance of our method on the standard zero-shot TTS task using the SEED test set (test-zh)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib60\" title=\"\">anastassiou2024seedttsfamilyhighqualityversatile </a></cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T3\" title=\"Table 3 &#8227; Capability on Zero-shot TTS &#8227; 4.2.1 Comparison with Reference Models &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the WeSCon (1st) model yields results identical to CosyVoice2, as the backbone TTS is frozen during this stage. The 2nd-stage model also achieves comparable results. Together with the findings in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, these results demonstrate that our method enables word-level emotion and speaking rate control without significantly degrading the original zero-shot TTS performance of the pretrained model.</p>\n\n",
                "matched_terms": [
                    "1st",
                    "emotion",
                    "controllability",
                    "method",
                    "control",
                    "wordlevel",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose WeSCon, the first method to overcome expressive data scarcity and enable word-level emotional expression control through end-to-end inference, under a self-training framework with a dynamic emotional attention bias mechanism. Experimental results show that WeSCon achieves state-of-the-art performance using only limited data without emotion or speed transitions, while maintaining strong zero-shot TTS capabilities.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "speed",
                    "attention",
                    "bias",
                    "method",
                    "control",
                    "wordlevel",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">1) Gradual emotion transitions. While WeSCon achieves smooth signal-level transitions, it lacks semantic modeling of emotional evolution. In human speech, emotional changes often involve intermediate states.\n2) Emotion diversity and composition. The model is limited to a fixed set of discrete emotions and does not support compositional or blended expressions, such as combining anger and sadness to convey despair.\n3) Conditioned control. Emotional transitions are currently predefined by GPT-4o-based plans, which restricts flexibility. Future work will explore more dynamic, context-aware control strategies to enable natural, interactive emotional expression.</p>\n\n",
                "matched_terms": [
                    "control",
                    "emotion",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CosyVoice2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/spaces/FunAudioLLM/CosyVoice2-0.5B\" title=\"\">https://huggingface.co/spaces/FunAudioLLM/CosyVoice2-0.5B</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">37</a>]</cite> is a zero-shot TTS model based on a language model (LM) and flow matching. It first converts speech into discrete tokens through a supervised speech tokenizer module. Its core architecture is identical to the base structure illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F3\" title=\"Figure 3 &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, excluding the additional modules introduced in this work.\nThe supervised speech tokenizer is jointly trained with an ASR task, which encourages the LM component to focus more on semantic modeling, particularly in terms of content, emotional expression, and duration. The flow matching component incorporates speaker embeddings<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/alibaba-damo-academy/3D-Speaker/tree/main/egs/3dspeaker/sv-cam++\" title=\"\">https://github.com/alibaba-damo-academy/3D-Speaker/tree/main/egs/3dspeaker/sv-cam++</a></span></span></span> and target speech to provide speaker characteristics. It transforms the speech tokens produced by the language model into mel-spectrograms, primarily controlling global aspects of speech, especially speaker identity. Finally, the vocoder converts the mel-spectrograms into waveform signals. CosyVoice2&#8217;s disentangled modeling of semantic content and speaker identity provides an important foundation for our method. In addition, since its training data is primarily in Chinese, it demonstrates significantly better performance in Chinese than in English.</p>\n\n",
                "matched_terms": [
                    "method",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F3\" title=\"Figure 3 &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we control the speaking rate of synthesized speech by applying simple interpolation and downsampling to the prompt speech tokens. To assess whether this dynamic mechanism supports time-varying modulation, we visualize six types of control patterns in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A2.F6\" title=\"Figure 6 &#8227; Appendix B Speed Control &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Numeric labels indicate the ratio between the transformed and original token lengths, where a ratio of 1 indicates no change, 0.5 indicates downsampling to half the length, and 2 represents interpolation that doubles it.\nThe left panel illustrates three downsampling patterns: a gradually increasing interval, a decreasing interval, and a uniform interval. The right panel shows corresponding interpolation patterns. These results demonstrate that global interpolation/downsampling can produce effects comparable to time-varying interpolation/downsampling, particularly when accounting for the inherent randomness introduced by LM sampling.\nBecause both methods provide only utterance-level control over speaking rate, word-level modulation requires integration with our multi-round inference framework.</p>\n\n",
                "matched_terms": [
                    "control",
                    "wordlevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further investigate how different resampling ratios influence the speaking rate of the generated speech. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A2.F7\" title=\"Figure 7 &#8227; Appendix B Speed Control &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, the results reveal a clear correlation between the resampling factor and the output speed. When the token length is reduced to less than 40% of the original through downsampling, the model fails to produce intelligible speech, as indicated by the red circles. Conversely, interpolation beyond three times the original length has minimal additional effect on speaking rate. Notably, the most stable and effective control is achieved when the token length lies between 50% and 200% of the original, suggesting this range as a practical bound for reliable modulation.</p>\n\n",
                "matched_terms": [
                    "control",
                    "speed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the self-training process, we introduce a data filtering mechanism to ensure the reliability of the teacher model&#8217;s guidance. Specifically, we adopt three metrics for evaluating the quality of generated speech: CER for Chinese and WER for English, speaker similarity, and emotion similarity. The first-stage teacher model has explicit access to the alignment among content, speech, emotion prompts, and speaker prompts, allowing us to directly compute these metrics with the prompt.\nTo avoid introducing bias from the final objective evaluation metrics prematurely, we deliberately use models that differ from those employed during evaluation. For speech recognition, we adopt the SenseVoice model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/FunAudioLLM/SenseVoiceSmall\" title=\"\">https://huggingface.co/FunAudioLLM/SenseVoiceSmall</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib61\" title=\"\">61</a>]</cite>. For emotion representation, we use a Whisper model fine-tuned for speech emotion recognition<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3\" title=\"\">https://huggingface.co/firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3</a></span></span></span>. For speaker embedding, we use Resemblyzer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/resemble-ai/Resemblyzer\" title=\"\">https://github.com/resemble-ai/Resemblyzer</a></span></span></span>.\nWe normalize all three metrics for each data point and compute a combined score by summing them. Only the top 50% of data, ranked by this composite score, are selected for self-training. In other words, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.F5\" title=\"Figure 5 &#8227; Self-Training Data Size &#8227; 4.2.2 Ablation Study &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, for a 500-hour training set, we actually generate approximately 1000 hours of data. Similarly, for a 2000-hour training set, we generate around 4000 hours of data.</p>\n\n",
                "matched_terms": [
                    "bias",
                    "emotion",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the emotional alignment of the student sequence input can be obtained from the output of the emotion aligner, we introduce seven predefined attention bias patterns to reduce the modeling burden of the emotional attention shift module. These typical patterns are illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A5.F8\" title=\"Figure 8 &#8227; Appendix E Predefined Emotion Attention Bias &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, and described below.</p>\n\n",
                "matched_terms": [
                    "bias",
                    "emotion",
                    "attention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Strict Emotion-Aligned Attention.</span>\nThis corresponds to the original training strategy of CosyVoice2. For instance, when decoding the second emotion segment (green S2), the model is only allowed to attend to the corresponding emotional prompt and its associated text, specifically red and green T2, and red S2.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "attention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although some attention bias configurations, such as (5), (6), and (7), are relatively uncommon in standard architectures, our predefined template-based computation allows the Emotional Attention Bias module to focus solely on selecting and composing from these candidate biases. This design significantly reduces computational overhead and prevents the generation of implausible or inconsistent attention patterns.</p>\n\n",
                "matched_terms": [
                    "bias",
                    "attention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, we generate 1,000 emotion-speed-varying text samples (500 in Chinese and 500 in English) using the script provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A3\" title=\"Appendix C Generation of Emotionally Varying Texts &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>. For each text sample, we randomly select emotional prompts from the ESD test set to match the emotion transitions required by the sentence. All emotion prompts within a single sentence are drawn from the same speaker to ensure consistency. The reference audio for the target speaker is also randomly selected from the same language-speaker subset.\nAs a result, approximately 1 out of every 5 samples features emotional prompts and a target speaker from the same speaker-emotion setting, given that the ESD dataset contains 5 emotions.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Index-TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">7</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/index-tts/index-tts\" title=\"\">https://github.com/index-tts/index-tts</a></span></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib50\" title=\"\">50</a>]</cite> is a GPT-style TTS model enhanced with pinyin-based pronunciation correction for Chinese characters and punctuation-based pause control. It integrates improved speaker condition modeling and BigVGAN2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib62\" title=\"\">62</a>]</cite> for high-quality audio synthesis. Trained on tens of thousands of hours of data, it supports multilingual zero-shot generation.</p>\n\n",
                "matched_terms": [
                    "control",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">F5-TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">9</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/SWivid/F5-TTS\" title=\"\">https://github.com/SWivid/F5-TTS</a></span></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib51\" title=\"\">51</a>]</cite> is a non-autoregressive TTS system based on Diffusion Transformer (DiT)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib64\" title=\"\">64</a>]</cite> and flow matching&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib65\" title=\"\">65</a>]</cite>. It forgoes duration models and alignment by padding text to match speech length, using ConvNeXt V2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib66\" title=\"\">66</a>]</cite> to refine text features. An inference-time Sway Sampling strategy improves decoding efficiency without retraining. Trained on a 100K-hour multilingual dataset, F5-TTS supports zero-shot synthesis, expressive speech generation, speed control, and seamless code-switching.</p>\n\n",
                "matched_terms": [
                    "control",
                    "speed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CosyVoice2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">37</a>]</cite> is a language model-based TTS system designed for zero-shot control of both emotion and speaker identity. Further architectural and training details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A1\" title=\"Appendix A Details of CosyVoice2 &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "control",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All baseline systems share the same inference procedure: each sentence is divided into multiple word-level segments with specified emotional states and speaking rates. These segments are synthesized separately using emotion cloning combined with their respective speaking rate control strategies, and then concatenated to form the final speech.</p>\n\n",
                "matched_terms": [
                    "control",
                    "emotion",
                    "wordlevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The objective evaluation is conducted in two groups:\n<span class=\"ltx_text ltx_font_bold\">Group 1.</span> Given the generated speech, the target speaker&#8217;s prompt, and the reference transcript, we compute three utterance-level metrics: character accuracy, speaker similarity, and DNSV (the variance of DNSMOS-PRO<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/fcumlin/DNSMOSPro\" title=\"\">https://github.com/fcumlin/DNSMOSPro</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib59\" title=\"\">59</a>]</cite> scores).\nCharacter accuracy is computed by comparing the output of an automatic speech recognition (ASR) model against the target transcript. Specifically, we use Paraformer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/modelscope/FunASR\" title=\"\">https://github.com/modelscope/FunASR</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib54\" title=\"\">54</a>]</cite> to calculate character error rate (CER) for Chinese and Whisper Large V3<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\">12</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/openai/whisper\" title=\"\">https://github.com/openai/whisper</a></span></span></span> to compute word error rate (WER) for English.\nSpeaker similarity is measured by extracting utterance-level embeddings from the generated speech and the target prompt using WavLM-Large<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification\" title=\"\">https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib55\" title=\"\">55</a>]</cite>, followed by computing the cosine similarity between them.\nDNSV is used to assess transition smoothness. DNSMOS-PRO scores are calculated over the generated speech using a 2-second window and a 1-second stride. The variance of these scores is used to quantify transition smoothness, with higher variance indicating lower smoothness. Since the value of the variance is often relatively small, we multiply it by 100 for display purposes.\n<span class=\"ltx_text ltx_font_bold\">Group 2.</span> Based on the ASR transcription obtained in Group 1, we perform forced alignment to determine word-level timestamps. A string-matching strategy is then used to align each generated word-level segment with its corresponding emotional prompt, according to the original text-emotion-speed mapping.\nFor each aligned pair,\nto evaluate expressive similarity, the emotional prompt is first adjusted to the target speaking rate using a phase vocoder algorithm<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://librosa.org/doc/latest/generated/librosa.effects.time_stretch.html#librosa-effects-time-stretch\" title=\"\">https://librosa.org/doc/latest/generated/librosa.effects.time_stretch.html#librosa-effects-time-stretch</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib67\" title=\"\">67</a>]</cite>. The generated segment is then compared to the rate-adjusted prompt using AutoPCP<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/facebookresearch/stopes/blob/main/stopes/eval/auto_pcp\" title=\"\">https://github.com/facebookresearch/stopes/blob/main/stopes/eval/auto_pcp</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib56\" title=\"\">56</a>]</cite> to compute prosodic similarity.\nEmotion embeddings are extracted using emotion2vec-large<span class=\"ltx_note ltx_role_footnote\" id=\"footnote16\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_tag ltx_tag_note\">16</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/ddlBoJack/emotion2vec\" title=\"\">https://github.com/ddlBoJack/emotion2vec</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib57\" title=\"\">57</a>]</cite> and a wav2vec-based model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote17\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_tag ltx_tag_note\">17</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/audeering/w2v2-how-to\" title=\"\">https://github.com/audeering/w2v2-how-to</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib58\" title=\"\">58</a>]</cite>, and cosine similarity is calculated to quantify emotion similarity.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "wordlevel",
                    "two",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct Mean Opinion Score (MOS) evaluations from four perspectives: emotional consistency, speaking rate consistency, speaker similarity, and smoothness of emotional transitions. For each aspect, we provide participants with detailed evaluation criteria and report both the mean scores and 95% confidence intervals. A total of 15 graduate students with research backgrounds in speech emotion recognition or emotional speech synthesis participated in the evaluation. Prior to the test, all participants were provided with a detailed explanation of the interface and task. They were also informed that the data would be used for scientific research purposes. Each participant rated 20 sets of results (10 in Chinese and 10 in English) generated by five different systems. The complete evaluation took an average of approximately 49 minutes per participant. Scores were assigned on a 1 to 5 scale with 0.5-point intervals. The evaluation interface is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6.F9\" title=\"Figure 9 &#8227; Objective Metrics &#8227; F.3 Evaluation Metrics &#8227; Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the generalization ability of our approach under an out-of-domain dataset, we conduct word-level emotion and speaking rate control experiments on the CASIA dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib68\" title=\"\">68</a>]</cite>, as organized according to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6\" title=\"Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. The CASIA corpus is a Mandarin emotional speech dataset recorded by four native speakers and covers six emotion categories: neutral, angry, fear, happy, sad, and surprise. Some of these emotions are not seen during training, which makes CASIA suitable for testing the cross-domain robustness of controllable speech synthesis.\nThe results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A7.T5\" title=\"Table 5 &#8227; Appendix G Out-of-Domain Evaluation &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nOur method, WeSCon, demonstrates strong performance across nearly all evaluation metrics, achieving lower DNSV and higher speaker similarity (S-SIM), emotional similarity (Emo2vec), and arousal scores compared to other baselines. The overall results are consistent with those in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, further confirming that our method generalizes well to unseen speakers and novel emotional patterns.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "method",
                    "control",
                    "wordlevel",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the student model (WeSCon 2nd) shows slightly weaker performance than the teacher model on the out-of-domain test set, in some metrics. This degradation is primarily caused by the data filtering strategy adopted during self-training, which improves performance on in-domain speakers and emotions but may introduce subtle biases, resulting in mild overfitting. Nevertheless, such performance fluctuations are acceptable given that the second-stage model significantly simplifies the inference process.</p>\n\n",
                "matched_terms": [
                    "2nd",
                    "wescon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the evolution of key validation metrics throughout the two-stage training process, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A8.F10\" title=\"Figure 10 &#8227; Appendix H Training Progress &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A8.F11\" title=\"Figure 11 &#8227; Appendix H Training Progress &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A8.F10\" title=\"Figure 10 &#8227; Appendix H Training Progress &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> displays the frame-level accuracy of the aligner model in the first stage, covering both text token prediction and boundary detection.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A8.F11\" title=\"Figure 11 &#8227; Appendix H Training Progress &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> reports the accuracy of speech token prediction and the frame-level emotion prediction by the emotion aligner in the second stage.\nAs shown, the aligner consistently achieves high frame-level accuracy in both stages. This is expected, as the target classes for both text and emotion are provided as input, and the aligner&#8217;s primary objective is to learn accurate alignments, which is a relatively straightforward task given the model&#8217;s underlying text-to-speech capabilities.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "stages"
                ]
            }
        ]
    },
    "A7.T5": {
        "source_file": "Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis",
        "caption": "Table 5: Objective evaluation results on the CASIA-based evaluation dataset for word-level emotion and speaking rate control.",
        "body": "AutoPCP",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">AutoPCP</td>\n</tr>\n</table> \n",
        "informative_terms_identified": [
            "wordlevel",
            "emotion",
            "speaking",
            "evaluation",
            "rate",
            "dataset",
            "autopcp",
            "control",
            "casiabased",
            "objective",
            "results"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To evaluate the generalization ability of our approach under an out-of-domain dataset, we conduct word-level emotion and speaking rate control experiments on the CASIA dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib68\" title=\"\">68</a>]</cite>, as organized according to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6\" title=\"Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. The CASIA corpus is a Mandarin emotional speech dataset recorded by four native speakers and covers six emotion categories: neutral, angry, fear, happy, sad, and surprise. Some of these emotions are not seen during training, which makes CASIA suitable for testing the cross-domain robustness of controllable speech synthesis.\nThe results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A7.T5\" title=\"Table 5 &#8227; Appendix G Out-of-Domain Evaluation &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nOur method, WeSCon, demonstrates strong performance across nearly all evaluation metrics, achieving lower DNSV and higher speaker similarity (S-SIM), emotional similarity (Emo2vec), and arousal scores compared to other baselines. The overall results are consistent with those in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, further confirming that our method generalizes well to unseen speakers and novel emotional patterns.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">While emotional text-to-speech (TTS) has made significant progress, most existing research remains limited to utterance-level emotional expression and fails to support word-level control. Achieving word-level expressive control poses fundamental challenges, primarily due to the complexity of modeling multi-emotion transitions and the scarcity of annotated datasets that capture intra-sentence emotional and prosodic variation.\nIn this paper, we propose WeSCon, the first self-training framework that enables word-level control of both emotion and speaking rate in a pretrained zero-shot TTS model, without relying on datasets containing intra-sentence emotion or speed transitions.\nOur method introduces a transition-smoothing strategy and a dynamic speed control mechanism to guide the pretrained TTS model in performing word-level expressive synthesis through a multi-round inference process. To further simplify the inference, we incorporate a dynamic emotional attention bias mechanism and fine-tune the model via self-training, thereby activating its ability for word-level expressive control in an end-to-end manner.\nExperimental results show that WeSCon effectively overcomes data scarcity, achieving state-of-the-art performance in word-level emotional expression control while preserving the strong zero-shot synthesis capabilities of the original TTS model.\n</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "rate",
                    "control",
                    "wordlevel",
                    "results",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans possess the ability to regulate emotional expression during speech flexibly&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib1\" title=\"\">scherer1995expression </a></cite>. To simulate this expressive capability, recent advances in text-to-speech synthesis (TTS) have increasingly focused on controllable generation of various aspects of speech, such as timbre, emotion, and speaking rate&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib2\" title=\"\">xie2024towards </a></cite>. Such control is a key objective in the development of human-like and expressive TTS.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "rate",
                    "control",
                    "objective",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most current TTS models exhibit zero-shot capabilities, enabling them to synthesize speech from text while cloning attributes such as timbre, emotion, and speaking rate from a reference speech sample&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib3\" title=\"\">kharitonov2023speak </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib4\" title=\"\">wang2023neural </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib5\" title=\"\">ju2024naturalspeech </a></cite>.\nDespite these advances, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, emotional and speaking rate control in current models is typically limited to the utterance level.</p>\n\n",
                "matched_terms": [
                    "control",
                    "emotion",
                    "rate",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This differs significantly from how humans naturally express emotion in speech. <span class=\"ltx_text ltx_font_bold\">Unlike global speaker identity, emotional expression and speaking rate are dynamic and often vary within a single sentence</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib6\" title=\"\">mozziconacci2002prosody </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib7\" title=\"\">guan2018speech </a></cite>. Therefore, word-level control of these factors is essential for achieving more natural and expressive speech synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib8\" title=\"\">9746323 </a></cite>.\nTo address this limitation, some approaches have proposed phoneme-level emotion prediction from target text to guide expressive synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib9\" title=\"\">tang2024ed </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib10\" title=\"\">9383524 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib11\" title=\"\">du21b_interspeech </a></cite>. While these methods show potential for word-level emotion control, relying solely on text makes it difficult to capture essential acoustic cues such as prosody and intensity, which are vital to emotional expression control&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib12\" title=\"\">chen2022fine </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib13\" title=\"\">chandra2024exploring </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib14\" title=\"\">zhao2023emotion </a></cite>.\nTo address this limitation, recent studies such as ELaTE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib15\" title=\"\">kanda2024making </a></cite> and EmoCtrl-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib16\" title=\"\">wu2024laugh </a></cite> have demonstrated that reference speech with emotional content can support intra-utterance control of time-varying expressive patterns, such as transitions from laughter to crying.\nThese works reflect a growing interest in TTS with word-level control over both emotion and speaking rate, but they also underscore several fundamental challenges.\nFirst, word-level expression control requires multiple emotional speech prompts, which introduces the challenge of guiding the model to attend to the appropriate emotion at each word.\nIn addition, current methods for fine-grained expression control rely on large-scale emotional speech datasets with time-aligned emotion transitions. However, such datasets are limited in both scale and accessibility&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib17\" title=\"\">rathi2024analyzing </a></cite>, making fine-grained control even more difficult to realize in practice.\nThese challenges lead us to ask:\n<span class=\"ltx_text ltx_font_bold\">Is it possible to achieve effective word-level control of both emotion and speaking rate without relying on speech datasets containing emotion or speed transitions?</span></p>\n\n",
                "matched_terms": [
                    "emotion",
                    "rate",
                    "control",
                    "wordlevel",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, motivated by the zero-shot potential of pretrained TTS models, we propose WeSCon, a two-stage self-training framework that achieves <span class=\"ltx_text ltx_font_bold\">W</span>ord-level <span class=\"ltx_text ltx_font_bold\">E</span>motion and <span class=\"ltx_text ltx_font_bold\">S</span>peed <span class=\"ltx_text ltx_font_bold\">Con</span>trol for TTS using only a small amount of public speech data without emotion or speed transitions.\nIn the first stage, we design a multi-round inference framework that incorporates a transition-smoothing module and a dynamic speed control mechanism. Without relying on any emotional training data, this approach enables a pretrained zero-shot TTS model to perform high-quality word-level emotional expression control in TTS. In the second stage, the original TTS model is repurposed as a student and trained under the supervision of the 1st-stage teacher. A dynamic emotional attention bias is introduced, enabling the student to acquire word-level control of emotion and speed through a simplified end-to-end inference process, without the need for complex iterative generation or smoothing.\nExperimental results show that WeSCon achieves state-of-the-art performance on the task of word-level emotional expression control in TTS, while preserving the zero-shot generalization and generation capabilities of the pretrained TTS model.\nOur contributions are summarized as follows:</p>\n\n",
                "matched_terms": [
                    "control",
                    "emotion",
                    "wordlevel",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a multi-round inference mechanism equipped with transition smoothing and dynamic speaking rate control, which is the first to achieve word-level control of both emotion and speaking rate in TTS without relying on any emotional training data.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "rate",
                    "control",
                    "wordlevel",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further introduce a novel self-training framework with a dynamic emotional attention bias mechanism that empowers a pretrained TTS model with end-to-end word-level emotion and speaking rate control, using limited data without intra-sentence emotion or speed transitions.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "rate",
                    "control",
                    "wordlevel",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct comprehensive experiments to validate the effectiveness of our proposed framework. Results show that our method enables a pretrained zero-shot TTS model to achieve SOTA performance in word-level emotional expression control, while preserving its original zero-shot capabilities. Ablation studies further confirm the contribution of each key design component. Our samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymousdemo999.github.io/\" title=\"\">https://anonymousdemo999.github.io/</a>.</p>\n\n",
                "matched_terms": [
                    "control",
                    "wordlevel",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development of controllable TTS, particularly for emotional expression control, depends heavily on high-quality emotional speech datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib18\" title=\"\">zhu2024metts_emodata </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib19\" title=\"\">guo2023emodiff_emodata </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib20\" title=\"\">yang2025emovoicellmbasedemotionaltexttospeech_emodata </a></cite>. While public corpora such as ESD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib21\" title=\"\">zhou2021seen </a></cite>, IEMOCAP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib22\" title=\"\">busso2008iemocap </a></cite>, and CREMA-D&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib23\" title=\"\">cao2014crema </a></cite> are available, they primarily provide utterance-level annotations and lack word-level or time-aligned emotional labels. These datasets are also limited in size and diversity, often consisting of scripted speech and covering a narrow range of emotions and speakers&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib24\" title=\"\">ma24b_interspeech </a></cite>.\nMore importantly, emotional datasets with intra-sentence variation, which are essential for learning word-level control, remain extremely scarce and are typically restricted to private use&#160; <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib15\" title=\"\">kanda2024making </a></cite>. Creating such datasets is expensive, requiring detailed word- or frame-level annotation and subjective emotional labeling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib25\" title=\"\">liu2024emphasisrenderingconversationaltexttospeech </a></cite>. This lack of fine-grained emotional data poses a major challenge for training models capable of word-level expressive TTS.</p>\n\n",
                "matched_terms": [
                    "control",
                    "wordlevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most controllable TTS systems support only utterance-level control, where a single label or reference speech governs the entire sentence&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib26\" title=\"\">anastassiou2024seed </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib27\" title=\"\">du2024cosyvoice </a></cite>. To achieve word-level control, some methods attempt to predict frame- or phoneme-level emotional indicators from text alone&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib28\" title=\"\">phone_level_tts0 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib29\" title=\"\">phone_level_tts1 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib30\" title=\"\">phone_level_tts2 </a></cite>, but they often fail to capture expressive variability due to the lack of acoustic cues such as intensity and prosody&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib9\" title=\"\">tang2024ed </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib10\" title=\"\">9383524 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib11\" title=\"\">du21b_interspeech </a></cite>.\nOther approaches, such as ELaTE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib15\" title=\"\">kanda2024making </a></cite> and EmoCtrl-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib16\" title=\"\">wu2024laugh </a></cite>, introduce emotional reference speech to enable intra-utterance control of specific expressive patterns like laughter or crying. While these represent progress, they are typically limited in expressiveness or rely on large-scale emotional datasets that are rarely publicly available. Consequently, achieving general and flexible word-level control over both emotion and speaking rate remains a major challenge.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "rate",
                    "control",
                    "wordlevel",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Self-training has become a promising approach for low-resource speech signal processing, enabling knowledge transfer without fine-grained datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib31\" title=\"\">zoph2020rethinking </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib32\" title=\"\">amini2025self </a></cite>. While it has been applied to tasks like speaker adaptation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib33\" title=\"\">khurana2021unsupervised </a></cite>, paralinguistic modeling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib34\" title=\"\">yang2024frame </a></cite>, and speech translation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib35\" title=\"\">pino20_interspeech </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib36\" title=\"\">fang-etal-2022-stemm </a></cite>, its use for fine-grained emotional control in TTS remains unexplored, especially without detailed expressive labels.\nTo address the scarcity of fine-grained datasets for word-level expressive control, we propose a self-training framework where a teacher model with multi-round inference, transition smoothing, and dynamic speed control generates expressive pseudo-labels. A student model, sharing the teacher&#8217;s backbone, is then fine-tuned under its supervision to perform word-level emotion and speaking rate control through a simplified end-to-end inference process, using only a small public dataset without intra-sentence emotion or speed transitions.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "rate",
                    "dataset",
                    "control",
                    "wordlevel",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">WeSCon is a two-stage self-training framework that enables word-level control of emotion and speaking rate in a pretrained zero-shot TTS model, using only a small amount of emotional speech data without intra-sentence emotion transitions as prompts. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S2.F2\" title=\"Figure 2 &#8227; Self-Training under Data Scarcity &#8227; 2 Related Work &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, in the first stage, we introduce a multi-round inference process with transition smoothing and dynamic speaking rate control to generate speech with word-level expression variations. In the second stage, the 1st-stage model acts as a teacher to guide the original TTS model, equipped with a dynamic emotional attention bias (DEAB), toward word-level control through a simplified end-to-end inference. Sections&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.SS2\" title=\"3.2 Teacher Model &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.SS3\" title=\"3.3 Self-Training &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a> describe the two stages, and Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.SS4\" title=\"3.4 Detail Training Setup &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a> provides the training details.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "rate",
                    "control",
                    "wordlevel",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S2\" title=\"2 Related Work &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, current TTS models can perform utterance-level emotion and speaker cloning. Building on this, we adopt the high-performance CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">du2024cosyvoice2 </a></cite> as our backbone (details of the backbone architecture are provided in <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A1\" title=\"Appendix A Details of CosyVoice2 &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">A</span></a></span>) and propose a multi-round inference strategy, where the model synthesizes multiple segments using different emotional prompts to achieve word-level emotion control.\nWhile this approach enables flexible emotional modulation, it often causes unnatural acoustic discontinuities at segment boundaries. To address this, we introduce a transition-smoothing mechanism that improves coherence across inference rounds, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F3\" title=\"Figure 3 &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nWithout modifying CosyVoice2, we append a lightweight content aligner, composed of non-causal Transformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib38\" title=\"\">vaswani2017attention </a></cite> and convolutional layers. Trained on ASR data, this module predicts the corresponding text token for each speech token and requires no emotional supervision.\nDuring inference, the input text is segmented based on a user-defined emotion plan. At each inference round, the final text and speech tokens from the previous round are appended to the current prompt, forming an explicit tail-to-head linkage. This aligns naturally with CosyVoice2&#8217;s continuation-style generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib39\" title=\"\">borsos2023audiolm </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib40\" title=\"\">valle </a></cite>, enabling smooth and coherent emotional transitions.</p>\n\n",
                "matched_terms": [
                    "control",
                    "emotion",
                    "wordlevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In CosyVoice2, utterance-level temporal prosody, including speaking rate and duration, is entirely determined by the reference speech prompt. To support more flexible and word-level control of speaking rate within a single utterance, we introduce a dynamic speed control mechanism as part of our multi-round inference framework, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F3\" title=\"Figure 3 &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The core idea is to adjust the prompt speech tokens using either nearest-neighbor interpolation or downsampling. Interpolation extends the prompt length, which slows down the generated speech, while downsampling shortens the prompt, resulting in a faster speaking rate. As demonstrated in <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A2\" title=\"Appendix B Speed Control &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">B</span></a></span>, this resampling method provides effective global prosody control. By integrating it into the multi-round inference process, the speaking rate can be dynamically controlled at the word level as needed.</p>\n\n",
                "matched_terms": [
                    "control",
                    "wordlevel",
                    "rate",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the previous section, we enabled word-level control of emotion and speaking rate by introducing a multi-round inference framework for CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">du2024cosyvoice2 </a></cite>. However, components such as the non-causal content aligner, multi-round inference, and tail-to-head linkage introduce significant inference complexity. To reduce this overhead while preserving controllability, we adopt a self-training strategy. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F4\" title=\"Figure 4 &#8227; 3.3 Self-Training &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the enhanced first-stage model serves as a teacher to supervise the original TTS model. The student model, equipped with a dynamic emotional attention bias, learns to achieve word-level emotion and speaking rate control through a simplified end-to-end inference.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "rate",
                    "control",
                    "wordlevel",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our teacher model achieves word-level control of emotion and speaking rate without modifying the original TTS parameters, relying instead on a complex inference pipeline with dynamic speed control and multi-round generation. To transfer this fine-grained control ability to a simplified end-to-end model, we propose a self-training strategy. Specifically, the 1st-stage teacher model guides the student model to learn word-level controllability. We first use GPT-4o&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib41\" title=\"\">hurst2024gpt </a></cite> to generate emotion-transition text sequences (details are shown in <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A3\" title=\"Appendix C Generation of Emotionally Varying Texts &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">C</span></a></span>), which are paired with public emotional speech samples (without emotion transitions) as prompts. The teacher then synthesizes speech with word-level variation in emotion and speaking rate. These outputs are filtered based on character accuracy and expressive similarity (<span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">details are introduced in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A4\" title=\"Appendix D Data Filtering in Self-Training &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">D</span></a></span>), and the student model is fine-tuned on the filtered supervisions with a small learning rate. This enables word-level emotional expression control during inference without requiring multi-round generation or dynamic concatenation.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "rate",
                    "control",
                    "wordlevel",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aim to preserve the strong zero-shot capability of the original TTS model while enabling word-level control of emotional expression under the self-training framework. To achieve this, we formulate the input structure as <math alttext=\"\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{C}^{\\text{prompt}\\ \\text{I}},\\bm{C}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{C}^{\\text{tgt}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{prompt}\\ \\text{I}},\\bm{S}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{S}^{\\text{tgt}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.09\" overflow=\"visible\" style=\"vertical-align:-2.18px\" version=\"1.1\" viewbox=\"0 0 11.09 11.09\" width=\"11.09\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.09) matrix(1 0 0 -1 0 0) translate(5.55,0) translate(0,5.55)\"><path d=\"M 5.27 0 C 5.27 2.91 2.91 5.27 0 5.27 C -2.91 5.27 -5.27 2.91 -5.27 0 C -5.27 -2.91 -2.91 -5.27 0 -5.27 C 2.91 -5.27 5.27 -2.91 5.27 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -2.69 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.56em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"5.38\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">S</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><msup><mi>&#119914;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>I</mtext></mrow></msup><mo>,</mo><msup><mi>&#119914;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>II</mtext></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>&#119914;</mi><mtext>tgt</mtext></msup><mo>,</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.84\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><msup><mi>&#119930;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>I</mtext></mrow></msup><mo>,</mo><msup><mi>&#119930;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>II</mtext></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>&#119930;</mi><mtext>tgt</mtext></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{C}^{\\text{prompt}\\ \\text{I}},\\bm{C}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{C}^{\\text{tgt}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{prompt}\\ \\text{I}},\\bm{S}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{S}^{\\text{tgt}}\\}</annotation></semantics></math>, where <math alttext=\"\\bm{C}^{\\text{prompt}\\ i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m2\" intent=\":literal\"><semantics><msup><mi>&#119914;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\bm{C}^{\\text{prompt}\\ i}</annotation></semantics></math> and <math alttext=\"\\bm{S}^{\\text{prompt}\\ i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m3\" intent=\":literal\"><semantics><msup><mi>&#119930;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\bm{S}^{\\text{prompt}\\ i}</annotation></semantics></math> denote the text and speech tokens of the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m4\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th emotional prompt, respectively. <math alttext=\"\\bm{C}^{\\text{tgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m5\" intent=\":literal\"><semantics><msup><mi>&#119914;</mi><mtext>tgt</mtext></msup><annotation encoding=\"application/x-tex\">\\bm{C}^{\\text{tgt}}</annotation></semantics></math> is the target text token sequence, and <math alttext=\"\\bm{S}^{\\text{tgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m6\" intent=\":literal\"><semantics><msup><mi>&#119930;</mi><mtext>tgt</mtext></msup><annotation encoding=\"application/x-tex\">\\bm{S}^{\\text{tgt}}</annotation></semantics></math> is the corresponding speech token sequence used as supervision. The symbols <span class=\"ltx_inline-block\"><svg class=\"ltx_picture ltx_markedasmath\" height=\"11.09\" id=\"S3.SS3.SSS2.p1.m7.pic1\" overflow=\"visible\" style=\"vertical-align:-2.18px\" version=\"1.1\" viewbox=\"0 0 11.09 11.09\" width=\"11.09\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.09) matrix(1 0 0 -1 0 0) translate(5.55,0) translate(0,5.55)\"><path d=\"M 5.27 0 C 5.27 2.91 2.91 5.27 0 5.27 C -2.91 5.27 -5.27 2.91 -5.27 0 C -5.27 -2.91 -2.91 -5.27 0 -5.27 C 2.91 -5.27 5.27 -2.91 5.27 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -2.69 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.56em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"5.38\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">S</span></span></span></foreignobject></g></g></svg></span> and <span class=\"ltx_inline-block\"><svg class=\"ltx_picture ltx_markedasmath\" height=\"11.84\" id=\"S3.SS3.SSS2.p1.m8.pic1\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span> indicate the beginning of text and speech. This design remains fully compatible with the original input format <math alttext=\"\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{C},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.09\" overflow=\"visible\" style=\"vertical-align:-2.18px\" version=\"1.1\" viewbox=\"0 0 11.09 11.09\" width=\"11.09\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.09) matrix(1 0 0 -1 0 0) translate(5.55,0) translate(0,5.55)\"><path d=\"M 5.27 0 C 5.27 2.91 2.91 5.27 0 5.27 C -2.91 5.27 -5.27 2.91 -5.27 0 C -5.27 -2.91 -2.91 -5.27 0 -5.27 C 2.91 -5.27 5.27 -2.91 5.27 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -2.69 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.56em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"5.38\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">S</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><mi>&#119914;</mi><mo>,</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.84\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><mi>&#119930;</mi><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{C},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}\\}</annotation></semantics></math> of CosyVoice2, preserving the autoregressive pattern of the pretrained model.\nTo further encode word-level emotional variation within this unified format, we extend the text-side input by inserting explicit emotion indicator tokens that mark the boundaries between emotional segments. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F4\" title=\"Figure 4 &#8227; 3.3 Self-Training &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the final input sequence preceding <span class=\"ltx_inline-block\"><svg class=\"ltx_picture ltx_markedasmath\" height=\"11.84\" id=\"S3.SS3.SSS2.p1.m10.pic1\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span> becomes <math alttext=\"\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},E^{\\ \\text{I}},\\bm{C}^{\\text{prompt\\ \\text{I}}},E^{\\ \\text{II}},\\bm{C}^{\\text{prompt\\ \\text{II}}},\\ldots\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m11\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.09\" overflow=\"visible\" style=\"vertical-align:-2.18px\" version=\"1.1\" viewbox=\"0 0 11.09 11.09\" width=\"11.09\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.09) matrix(1 0 0 -1 0 0) translate(5.55,0) translate(0,5.55)\"><path d=\"M 5.27 0 C 5.27 2.91 2.91 5.27 0 5.27 C -2.91 5.27 -5.27 2.91 -5.27 0 C -5.27 -2.91 -2.91 -5.27 0 -5.27 C 2.91 -5.27 5.27 -2.91 5.27 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -2.69 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.56em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"5.38\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">S</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><msup><mi>E</mi><mtext>I</mtext></msup><mo>,</mo><msup><mi>&#119914;</mi><mrow><mtext>prompt&#160;</mtext><mtext>I</mtext></mrow></msup><mo>,</mo><msup><mi>E</mi><mtext>II</mtext></msup><mo>,</mo><msup><mi>&#119914;</mi><mrow><mtext>prompt&#160;</mtext><mtext>II</mtext></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\hbox to8.02pt{\\vbox to8.02pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.0081pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{3.8081pt}{0.0pt}\\pgfsys@curveto{3.8081pt}{2.10318pt}{2.10318pt}{3.8081pt}{0.0pt}{3.8081pt}\\pgfsys@curveto{-2.10318pt}{3.8081pt}{-3.8081pt}{2.10318pt}{-3.8081pt}{0.0pt}\\pgfsys@curveto{-3.8081pt}{-2.10318pt}{-2.10318pt}{-3.8081pt}{0.0pt}{-3.8081pt}\\pgfsys@curveto{2.10318pt}{-3.8081pt}{3.8081pt}{-2.10318pt}{3.8081pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-1.94444pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{S}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},E^{\\ \\text{I}},\\bm{C}^{\\text{prompt\\ \\text{I}}},E^{\\ \\text{II}},\\bm{C}^{\\text{prompt\\ \\text{II}}},\\ldots\\}</annotation></semantics></math>, where each <math alttext=\"E^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m12\" intent=\":literal\"><semantics><msup><mi>E</mi><mi>i</mi></msup><annotation encoding=\"application/x-tex\">E^{i}</annotation></semantics></math> acts as a soft anchor guiding the model to modulate emotion transitions during generation.</p>\n\n",
                "matched_terms": [
                    "control",
                    "emotion",
                    "wordlevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">WeSCon is trained in two stages. The first stage trains a content aligner to ensure smooth transitions during multi-round inference. In the second stage, a self-training strategy is adopted to transfer the teacher model&#8217;s ability to control word-level emotional expression to the original TTS model.</p>\n\n",
                "matched_terms": [
                    "control",
                    "wordlevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage, the content aligner is trained for 400k steps on 2 NVIDIA 3090 GPUs using Adam <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib48\" title=\"\">kingma2014adam </a></cite> with a learning rate linearly warmed up to 2.5e-4 over the first 10% of steps, then linearly decayed to 0. Each batch contains 90 seconds of speech. In the second stage, the student model is trained for 600k steps on 4 NVIDIA 3090 GPUs. The TTS model is frozen for the first 20k steps to focus on training the emotion aligner. Each batch contains 40 seconds of speech, and Adam is used with a fixed learning rate of 5e-7. Repetition-aware top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> sampling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib49\" title=\"\">chen2024valle2neuralcodec </a></cite> is applied during inference, with <math alttext=\"k=50\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>50</mn></mrow><annotation encoding=\"application/x-tex\">k=50</annotation></semantics></math> and temperature <math alttext=\"=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">=0.9</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate word-level control over emotion and speaking rate, we construct test sets based on test set of ESD and use outstanding zero-shot TTS models <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib50\" title=\"\">deng2025indextts </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib51\" title=\"\">chen2024f5 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib52\" title=\"\">wang2025spark </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">du2024cosyvoice2 </a></cite> with multi-round concatenative inference as baselines (see <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6\" title=\"Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">F</span></a></span> for details).\nWe use objective and subjective metrics to assess system performance (see <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6.SS3\" title=\"F.3 Evaluation Metrics &#8227; Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">F.3</span></a></span> for details).\nFor intelligibility, we report WER using Whisper-Large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib53\" title=\"\">radford2023robust </a></cite> for English and CER using Paraformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib54\" title=\"\">gao2022paraformer </a></cite> for Chinese.\nSpeaker similarity (S-SIM) is computed via cosine similarity of WavLM-Large embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib55\" title=\"\">chen2022wavlm </a></cite>.\nTo evaluate prosody alignment, we use AutoPCP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib56\" title=\"\">barrault2023seamless </a></cite>. Emotion similarity metrics (Emo2v. and Aro.) are computed using emotion2vec-Large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib57\" title=\"\">ma2023emotion2vec </a></cite> and a wav2vec-based model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib58\" title=\"\">baevski2020wav2vec </a></cite>, respectively.\nWe use the variance of DNSMOS-Pro&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib59\" title=\"\">cumlin2024dnsmos </a></cite> (DNSV) to assess the naturalness of emotion transition.\nSubjective evaluation includes four kinds of Mean Opinion Score (MOS): SMOS (speaker similarity), NMOS (naturalness of emotion transition), EMOS (emotion match), and SPMOS (speed match), each rated on a 5-point scale. Both the mean and 95% confidence intervals of MOS are reported.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "evaluation",
                    "rate",
                    "autopcp",
                    "control",
                    "wordlevel",
                    "objective",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our method on word-level emotion and speaking rate control in both English and Chinese TTS. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, WeSCon (1st-stage) and WeSCon (2nd-stage) consistently outperform baselines on expressive metrics. Notably, the 2nd-stage model achieves the highest Emo2V. and Aro. scores in both languages, demonstrating strong word-level emotional expressiveness enabled by our self-training framework.\nRegarding transition smoothness, our models significantly reduce DNSV compared to CosyVoice2, with values dropping from 7.894 to 4.361 in English and from 7.612 to 4.210 in Chinese. This highlights the effectiveness of our smoothing mechanism and the end-to-end continuous inference in the 2nd-stage model in mitigating acoustic discontinuities across transitions.\nWhile the character error rate is slightly higher than baselines, it remains comparable to CosyVoice2, our backbone model.\nFinally, the 2nd-stage model slightly surpasses the 1st-stage model, benefiting from self-training with selective filtering that retains high-quality supervision from the teacher. Overall, our approach consistently improves upon CosyVoice2 and achieves SOTA performance in key aspects of word-level expressive controllable TTS.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "rate",
                    "control",
                    "wordlevel",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct subjective evaluations covering emotional expressiveness (EMOS), speaking rate control (SPMOS), speaker similarity (SMOS), and naturalness of emotion transition (NMOS), with details provided in <span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6.SS3\" title=\"F.3 Evaluation Metrics &#8227; Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">F.3</span></a></span>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T2\" title=\"Table 2 &#8227; Subjective Evaluation &#8227; 4.2.1 Comparison with Reference Models &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, our method, WeSCon, consistently outperforms all baselines. It achieves more expressive and controllable speech while maintaining speaker identity, demonstrating effective word-level control in emotional expression. Additionally, WeSCon delivers more natural-sounding speech with smoother and more accurate speaking rate modulation.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "rate",
                    "control",
                    "wordlevel",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to introducing word-level controllability, we evaluate the performance of our method on the standard zero-shot TTS task using the SEED test set (test-zh)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib60\" title=\"\">anastassiou2024seedttsfamilyhighqualityversatile </a></cite>. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T3\" title=\"Table 3 &#8227; Capability on Zero-shot TTS &#8227; 4.2.1 Comparison with Reference Models &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the WeSCon (1st) model yields results identical to CosyVoice2, as the backbone TTS is frozen during this stage. The 2nd-stage model also achieves comparable results. Together with the findings in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, these results demonstrate that our method enables word-level emotion and speaking rate control without significantly degrading the original zero-shot TTS performance of the pretrained model.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "rate",
                    "control",
                    "wordlevel",
                    "results",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the impact of the transition-smoothing mechanism by removing the tail-to-head alignment during multi-round inference in the 1st-stage model. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T4\" title=\"Table 4 &#8227; 4.2.2 Ablation Study &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, removing this mechanism (\"w/o smoothing\") leads to a substantial increase in DNSV (from 4.980 to 7.568), indicating degraded smoothness between expressive transitions.\nAdditionally, speaker (S-SIM) and emotion similarity (Emo2V. and Aro.) drop notably, suggesting that the discontinuity negatively affects both emotional expression and speaker consistency. These results confirm that our smoothing strategy plays a crucial role in ensuring coherent segment transitions during generation.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To examine the effectiveness of our dynamic speaking rate control, we remove this component from the 1st-stage model (\"w/o speed control\"). As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T4\" title=\"Table 4 &#8227; 4.2.2 Ablation Study &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, DNSV slightly increases from 4.980 to 5.067, and performance drops are observed across most expressive metrics, such as AutoPCP (2.650 to 2.499) and Emo2v. (0.866 to 0.844). This suggests that speaking rate variation provides important prosodic cues for emotional expression in TTS.</p>\n\n",
                "matched_terms": [
                    "control",
                    "autopcp",
                    "rate",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the 2nd-stage model, we evaluate the effect of removing the dynamic emotional attention bias (\"w/o attention bias\"). As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T4\" title=\"Table 4 &#8227; 4.2.2 Ablation Study &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, this results in a clear performance drop across all metrics, especially emotion similarity. DNSV also increases, indicating reduced smoothness. The results confirm the importance of the attention bias module in enabling the 2nd-stage model to focus on the correct emotional prompt during inference.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further investigate the importance of data formatting in self-training. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.T4\" title=\"Table 4 &#8227; 4.2.2 Ablation Study &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, removing the emotion flags (\"w/o emotion flag\") results in performance drops across all metrics, indicating that these flags play a crucial role in signaling the locations of emotional shifts to the model. Furthermore, replacing our input data format with a naive one that simply concatenates prompts and targets (\"w/o data format\"), as <math alttext=\"\\{\\bm{C}^{\\text{prompt}\\ \\text{I}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{C}^{\\text{tgt}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{&#10;{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }&#10;}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}&#10;}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope}}}&#10;}&#10;\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{tgt}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msup><mi>&#119914;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>I</mtext></mrow></msup><mo>,</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.84\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><msup><mi>&#119930;</mi><mrow><mtext>prompt</mtext><mo lspace=\"0.410em\" rspace=\"0em\">&#8203;</mo><mtext>II</mtext></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>&#119914;</mi><mtext>tgt</mtext></msup><mo>,</mo><mtext><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"11.84\" overflow=\"visible\" style=\"vertical-align:-2.56px\" version=\"1.1\" viewbox=\"0 0 11.84 11.84\" width=\"11.84\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,11.84) matrix(1 0 0 -1 0 0) translate(5.92,0) translate(0,5.92)\"><path d=\"M 5.64 0 C 5.64 3.12 3.12 5.64 0 5.64 C -3.12 5.64 -5.64 3.12 -5.64 0 C -5.64 -3.12 -3.12 -5.64 0 -5.64 C 3.12 -5.64 5.64 -3.12 5.64 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.23 -3.36)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :0.67em;--fo_height:0.69em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"6.46\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">B</span></span></span></foreignobject></g></g></svg></span></mtext><mo>,</mo><msup><mi>&#119930;</mi><mtext>tgt</mtext></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\bm{C}^{\\text{prompt}\\ \\text{I}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{prompt}\\ \\text{II}},\\ldots,\\bm{C}^{\\text{tgt}},\\hbox to8.55pt{\\vbox to8.55pt{\\pgfpicture\\makeatletter\\hbox{\\enskip\\lower-4.27715pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\\pgfsys@moveto{4.07715pt}{0.0pt}\\pgfsys@curveto{4.07715pt}{2.25177pt}{2.25177pt}{4.07715pt}{0.0pt}{4.07715pt}\\pgfsys@curveto{-2.25177pt}{4.07715pt}{-4.07715pt}{2.25177pt}{-4.07715pt}{0.0pt}\\pgfsys@curveto{-4.07715pt}{-2.25177pt}{-2.25177pt}{-4.07715pt}{0.0pt}{-4.07715pt}\\pgfsys@curveto{2.25177pt}{-4.07715pt}{4.07715pt}{-2.25177pt}{4.07715pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.33334pt}{-2.43054pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{\\scriptsize{B}}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope{{{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}},\\bm{S}^{\\text{tgt}}\\}</annotation></semantics></math> leads to the most significant degradation in expressive metrics, including a sharp increase in CER from 2.166 to 4.141. These results suggest that aligning the data organization with the structure used during pretraining allows the model to better leverage its pre-trained knowledge.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose WeSCon, the first method to overcome expressive data scarcity and enable word-level emotional expression control through end-to-end inference, under a self-training framework with a dynamic emotional attention bias mechanism. Experimental results show that WeSCon achieves state-of-the-art performance using only limited data without emotion or speed transitions, while maintaining strong zero-shot TTS capabilities.</p>\n\n",
                "matched_terms": [
                    "control",
                    "emotion",
                    "wordlevel",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">1) Gradual emotion transitions. While WeSCon achieves smooth signal-level transitions, it lacks semantic modeling of emotional evolution. In human speech, emotional changes often involve intermediate states.\n2) Emotion diversity and composition. The model is limited to a fixed set of discrete emotions and does not support compositional or blended expressions, such as combining anger and sadness to convey despair.\n3) Conditioned control. Emotional transitions are currently predefined by GPT-4o-based plans, which restricts flexibility. Future work will explore more dynamic, context-aware control strategies to enable natural, interactive emotional expression.</p>\n\n",
                "matched_terms": [
                    "control",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S3.F3\" title=\"Figure 3 &#8227; 3 WeSCon &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we control the speaking rate of synthesized speech by applying simple interpolation and downsampling to the prompt speech tokens. To assess whether this dynamic mechanism supports time-varying modulation, we visualize six types of control patterns in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A2.F6\" title=\"Figure 6 &#8227; Appendix B Speed Control &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Numeric labels indicate the ratio between the transformed and original token lengths, where a ratio of 1 indicates no change, 0.5 indicates downsampling to half the length, and 2 represents interpolation that doubles it.\nThe left panel illustrates three downsampling patterns: a gradually increasing interval, a decreasing interval, and a uniform interval. The right panel shows corresponding interpolation patterns. These results demonstrate that global interpolation/downsampling can produce effects comparable to time-varying interpolation/downsampling, particularly when accounting for the inherent randomness introduced by LM sampling.\nBecause both methods provide only utterance-level control over speaking rate, word-level modulation requires integration with our multi-round inference framework.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "control",
                    "wordlevel",
                    "results",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further investigate how different resampling ratios influence the speaking rate of the generated speech. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A2.F7\" title=\"Figure 7 &#8227; Appendix B Speed Control &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, the results reveal a clear correlation between the resampling factor and the output speed. When the token length is reduced to less than 40% of the original through downsampling, the model fails to produce intelligible speech, as indicated by the red circles. Conversely, interpolation beyond three times the original length has minimal additional effect on speaking rate. Notably, the most stable and effective control is achieved when the token length lies between 50% and 200% of the original, suggesting this range as a practical bound for reliable modulation.</p>\n\n",
                "matched_terms": [
                    "control",
                    "rate",
                    "results",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the self-training process, we introduce a data filtering mechanism to ensure the reliability of the teacher model&#8217;s guidance. Specifically, we adopt three metrics for evaluating the quality of generated speech: CER for Chinese and WER for English, speaker similarity, and emotion similarity. The first-stage teacher model has explicit access to the alignment among content, speech, emotion prompts, and speaker prompts, allowing us to directly compute these metrics with the prompt.\nTo avoid introducing bias from the final objective evaluation metrics prematurely, we deliberately use models that differ from those employed during evaluation. For speech recognition, we adopt the SenseVoice model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/FunAudioLLM/SenseVoiceSmall\" title=\"\">https://huggingface.co/FunAudioLLM/SenseVoiceSmall</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib61\" title=\"\">61</a>]</cite>. For emotion representation, we use a Whisper model fine-tuned for speech emotion recognition<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3\" title=\"\">https://huggingface.co/firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3</a></span></span></span>. For speaker embedding, we use Resemblyzer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/resemble-ai/Resemblyzer\" title=\"\">https://github.com/resemble-ai/Resemblyzer</a></span></span></span>.\nWe normalize all three metrics for each data point and compute a combined score by summing them. Only the top 50% of data, ranked by this composite score, are selected for self-training. In other words, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#S4.F5\" title=\"Figure 5 &#8227; Self-Training Data Size &#8227; 4.2.2 Ablation Study &#8227; 4.2 Experimental Results &#8227; 4 Experiments &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, for a 500-hour training set, we actually generate approximately 1000 hours of data. Similarly, for a 2000-hour training set, we generate around 4000 hours of data.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "evaluation",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the train-set, dev-set, and test-set of ESD<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/HLTSingapore/Emotional-Speech-Data\" title=\"\">https://github.com/HLTSingapore/Emotional-Speech-Data</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib21\" title=\"\">21</a>]</cite> for training and evaluation. This dataset contains 350 parallel utterances, averaging 2.9 seconds in duration, spoken by 20 speakers: 10 native English and 10 native Mandarin (5 male and 5 female for each language). Each speaker expresses five emotions: happy, sad, neutral, angry, and surprised. All audio is sampled at 16 kHz and stored as 16-bit files.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, we generate 1,000 emotion-speed-varying text samples (500 in Chinese and 500 in English) using the script provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A3\" title=\"Appendix C Generation of Emotionally Varying Texts &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>. For each text sample, we randomly select emotional prompts from the ESD test set to match the emotion transitions required by the sentence. All emotion prompts within a single sentence are drawn from the same speaker to ensure consistency. The reference audio for the target speaker is also randomly selected from the same language-speaker subset.\nAs a result, approximately 1 out of every 5 samples features emotional prompts and a target speaker from the same speaker-emotion setting, given that the ESD dataset contains 5 emotions.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "evaluation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Spark-TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">8</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/SparkAudio/Spark-TTS\" title=\"\">https://github.com/SparkAudio/Spark-TTS</a></span></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib52\" title=\"\">52</a>]</cite> is a large language model-based TTS system built upon Qwen2.5&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib63\" title=\"\">63</a>]</cite>. It directly reconstructs waveforms from LLM-predicted codes, eliminating the need for separate acoustic models. This design simplifies the pipeline and improves inference efficiency. It supports zero-shot voice cloning, cross-lingual/code-switching synthesis, and virtual speaker customization via controllable parameters such as gender, pitch, and speaking rate.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">F5-TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">9</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/SWivid/F5-TTS\" title=\"\">https://github.com/SWivid/F5-TTS</a></span></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib51\" title=\"\">51</a>]</cite> is a non-autoregressive TTS system based on Diffusion Transformer (DiT)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib64\" title=\"\">64</a>]</cite> and flow matching&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib65\" title=\"\">65</a>]</cite>. It forgoes duration models and alignment by padding text to match speech length, using ConvNeXt V2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib66\" title=\"\">66</a>]</cite> to refine text features. An inference-time Sway Sampling strategy improves decoding efficiency without retraining. Trained on a 100K-hour multilingual dataset, F5-TTS supports zero-shot synthesis, expressive speech generation, speed control, and seamless code-switching.</p>\n\n",
                "matched_terms": [
                    "control",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CosyVoice2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib37\" title=\"\">37</a>]</cite> is a language model-based TTS system designed for zero-shot control of both emotion and speaker identity. Further architectural and training details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A1\" title=\"Appendix A Details of CosyVoice2 &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "control",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All baseline systems share the same inference procedure: each sentence is divided into multiple word-level segments with specified emotional states and speaking rates. These segments are synthesized separately using emotion cloning combined with their respective speaking rate control strategies, and then concatenated to form the final speech.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "rate",
                    "control",
                    "wordlevel",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The objective evaluation is conducted in two groups:\n<span class=\"ltx_text ltx_font_bold\">Group 1.</span> Given the generated speech, the target speaker&#8217;s prompt, and the reference transcript, we compute three utterance-level metrics: character accuracy, speaker similarity, and DNSV (the variance of DNSMOS-PRO<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/fcumlin/DNSMOSPro\" title=\"\">https://github.com/fcumlin/DNSMOSPro</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib59\" title=\"\">59</a>]</cite> scores).\nCharacter accuracy is computed by comparing the output of an automatic speech recognition (ASR) model against the target transcript. Specifically, we use Paraformer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/modelscope/FunASR\" title=\"\">https://github.com/modelscope/FunASR</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib54\" title=\"\">54</a>]</cite> to calculate character error rate (CER) for Chinese and Whisper Large V3<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\">12</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/openai/whisper\" title=\"\">https://github.com/openai/whisper</a></span></span></span> to compute word error rate (WER) for English.\nSpeaker similarity is measured by extracting utterance-level embeddings from the generated speech and the target prompt using WavLM-Large<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification\" title=\"\">https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib55\" title=\"\">55</a>]</cite>, followed by computing the cosine similarity between them.\nDNSV is used to assess transition smoothness. DNSMOS-PRO scores are calculated over the generated speech using a 2-second window and a 1-second stride. The variance of these scores is used to quantify transition smoothness, with higher variance indicating lower smoothness. Since the value of the variance is often relatively small, we multiply it by 100 for display purposes.\n<span class=\"ltx_text ltx_font_bold\">Group 2.</span> Based on the ASR transcription obtained in Group 1, we perform forced alignment to determine word-level timestamps. A string-matching strategy is then used to align each generated word-level segment with its corresponding emotional prompt, according to the original text-emotion-speed mapping.\nFor each aligned pair,\nto evaluate expressive similarity, the emotional prompt is first adjusted to the target speaking rate using a phase vocoder algorithm<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://librosa.org/doc/latest/generated/librosa.effects.time_stretch.html#librosa-effects-time-stretch\" title=\"\">https://librosa.org/doc/latest/generated/librosa.effects.time_stretch.html#librosa-effects-time-stretch</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib67\" title=\"\">67</a>]</cite>. The generated segment is then compared to the rate-adjusted prompt using AutoPCP<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/facebookresearch/stopes/blob/main/stopes/eval/auto_pcp\" title=\"\">https://github.com/facebookresearch/stopes/blob/main/stopes/eval/auto_pcp</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib56\" title=\"\">56</a>]</cite> to compute prosodic similarity.\nEmotion embeddings are extracted using emotion2vec-large<span class=\"ltx_note ltx_role_footnote\" id=\"footnote16\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_tag ltx_tag_note\">16</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/ddlBoJack/emotion2vec\" title=\"\">https://github.com/ddlBoJack/emotion2vec</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib57\" title=\"\">57</a>]</cite> and a wav2vec-based model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote17\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">17</sup><span class=\"ltx_tag ltx_tag_note\">17</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/audeering/w2v2-how-to\" title=\"\">https://github.com/audeering/w2v2-how-to</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#bib.bib58\" title=\"\">58</a>]</cite>, and cosine similarity is calculated to quantify emotion similarity.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "evaluation",
                    "rate",
                    "wordlevel",
                    "objective",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct Mean Opinion Score (MOS) evaluations from four perspectives: emotional consistency, speaking rate consistency, speaker similarity, and smoothness of emotional transitions. For each aspect, we provide participants with detailed evaluation criteria and report both the mean scores and 95% confidence intervals. A total of 15 graduate students with research backgrounds in speech emotion recognition or emotional speech synthesis participated in the evaluation. Prior to the test, all participants were provided with a detailed explanation of the interface and task. They were also informed that the data would be used for scientific research purposes. Each participant rated 20 sets of results (10 in Chinese and 10 in English) generated by five different systems. The complete evaluation took an average of approximately 49 minutes per participant. Scores were assigned on a 1 to 5 scale with 0.5-point intervals. The evaluation interface is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A6.F9\" title=\"Figure 9 &#8227; Objective Metrics &#8227; F.3 Evaluation Metrics &#8227; Appendix F Evaluation Setup &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "evaluation",
                    "rate",
                    "results",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the evolution of key validation metrics throughout the two-stage training process, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A8.F10\" title=\"Figure 10 &#8227; Appendix H Training Progress &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A8.F11\" title=\"Figure 11 &#8227; Appendix H Training Progress &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A8.F10\" title=\"Figure 10 &#8227; Appendix H Training Progress &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> displays the frame-level accuracy of the aligner model in the first stage, covering both text token prediction and boundary detection.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24629v1#A8.F11\" title=\"Figure 11 &#8227; Appendix H Training Progress &#8227; Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> reports the accuracy of speech token prediction and the frame-level emotion prediction by the emotion aligner in the second stage.\nAs shown, the aligner consistently achieves high frame-level accuracy in both stages. This is expected, as the target classes for both text and emotion are provided as input, and the aligner&#8217;s primary objective is to learn accurate alignments, which is a relatively straightforward task given the model&#8217;s underlying text-to-speech capabilities.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.</p>\n\n",
                "matched_terms": [
                    "results",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "dataset",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).</p>\n\n",
                "matched_terms": [
                    "results",
                    "dataset"
                ]
            }
        ]
    }
}