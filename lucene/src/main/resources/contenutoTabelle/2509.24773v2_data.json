{
    "S4.T1": {
        "caption": "Table 1: \nV2S evaluation results on the VGGSound benchmark. “Param.Count” denotes the number of parameters of the generation backbone. “Visual Rep.” indicates the type of visual representation for condition – O.F. refers to optical flow, S.AV. refers to Segment AVCLIP, I.B. refers to ImageBind features, and S.F. refers to SynchFormer-extracted features. “Extra Data” lists additional training datasets beyond VGGSound. A.S refers to another commonly used V2S dataset, AudioSet.\nExcept for this, MMAudio includes additional higher-quality text-to-sound data, AudioCaps and WavCaps (A.&W.C.), for training. V2A-Mapper (V2A-M.) and Foleycrafter (FoleyC.) utilize a frozen pretrained text-to-sound module. Therefore, we label them in gray and do not directly compare with them.\nFor each metric, the highest score is in bold and the second-highest score is underlined.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"4\" style=\"padding:0.5pt 2.0pt;\">Model Information</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"5\" style=\"padding:0.5pt 2.0pt;\">Sound Quality</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\" style=\"padding:0.5pt 2.0pt;\">Synchronization</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 2.0pt;\">Seman.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"2\" style=\"padding:0.5pt 2.0pt;\">Method</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">Param.</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">Visual</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">Extra</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\"><math alttext=\"\\text{FAD}\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m1\" intent=\":literal\"><semantics><mrow><mtext>FAD</mtext><mo stretchy=\"false\">&#8595;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">\\text{FAD}\\downarrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\"><math alttext=\"\\text{FAD}\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m2\" intent=\":literal\"><semantics><mrow><mtext>FAD</mtext><mo stretchy=\"false\">&#8595;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">\\text{FAD}\\downarrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\"><math alttext=\"\\text{FAD}\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m3\" intent=\":literal\"><semantics><mrow><mtext>FAD</mtext><mo stretchy=\"false\">&#8595;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">\\text{FAD}\\downarrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\"><math alttext=\"\\text{IS}\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m4\" intent=\":literal\"><semantics><mrow><mtext>IS</mtext><mo stretchy=\"false\">&#8593;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">\\text{IS}\\uparrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 2.0pt;\"><math alttext=\"\\text{KL}\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m5\" intent=\":literal\"><semantics><mrow><mtext>KL</mtext><mo stretchy=\"false\">&#8595;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">\\text{KL}\\downarrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">Onset</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">Onset</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">DeSync</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">IB-VA <span class=\"ltx_rule\" style=\"width:0.0pt;height:13.0pt;--ltx-bg-color:black;display:inline-block;\"/>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding:0.5pt 2.0pt;\">Count</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding:0.5pt 2.0pt;\">Rep.</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.0pt;\">Data</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">(vgg.)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">(pann.)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">(pas.)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">(pas.)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.0pt;\">(pas.)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><math alttext=\"\\text{Acc.}\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m6\" intent=\":literal\"><semantics><mrow><mtext>Acc.</mtext><mo stretchy=\"false\">&#8593;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">\\text{Acc.}\\uparrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><math alttext=\"\\text{Ap.}\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m7\" intent=\":literal\"><semantics><mrow><mtext>Ap.</mtext><mo stretchy=\"false\">&#8593;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">\\text{Ap.}\\uparrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.0pt;\"><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">SpecVQ.</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">377M</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">RGB, O.F.</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">4.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">30.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">288.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">4.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">3.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">5.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">20.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">1.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">14.26</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 2.0pt;\">Im2Wav</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding:0.5pt 2.0pt;\">360M</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding:0.5pt 2.0pt;\">CLIP</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">4.95</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">20.19</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">258.43</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">5.89</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">2.24</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">6.19</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">19.81</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">1.22</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">19.59</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 2.0pt;\">V-AURA</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding:0.5pt 2.0pt;\">893M</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding:0.5pt 2.0pt;\">S.AV.</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">4.01</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">31.76</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">328.36</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">8.50</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">2.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">6.50</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">22.59</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">1.14</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">24.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">VAB</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">403M</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">CLIP</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">A.S.</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">3.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">19.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">238.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">7.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">2.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">6.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">18.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">1.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">25.67</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">Difffoley</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">859M</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">CAVP</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">A.S.</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">4.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">25.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">414.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">9.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">2.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">7.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">13.94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.99</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">10.35</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 2.0pt;\">Seeing.</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding:0.5pt 2.0pt;\">416M</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding:0.5pt 2.0pt;\">I.B.</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">3.93</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">22.95</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">245.01</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">5.68</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">2.71</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">6.56</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">16.22</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">1.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">V2A-M.</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">CLIP</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">1.45</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">8.33</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">166.55</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">10.33</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">2.52</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">7.82</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">16.75</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">1.23</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">21.91</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">FoleyC.</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">1126M</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">CLIP</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">2.17</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">17.94</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">150.58</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">9.82</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">2.30</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">5.93</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">21.42</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">1.21</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">27.55</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 2.0pt;\">TiVA</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding:0.5pt 2.0pt;\">346M</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding:0.5pt 2.0pt;\">CLIP</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.0pt;\">A.S.</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">3.90</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">25.24</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">323.77</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">4.61</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">2.77</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">4.84</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">22.32</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">1.15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">16.46</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 2.0pt;\">LoVA</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding:0.5pt 2.0pt;\">1057M</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding:0.5pt 2.0pt;\">CLIP</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.0pt;\">A.S.</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">2.03</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">17.67</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text ltx_font_bold\">120.32</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">9.91</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">2.15</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">7.02</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">19.95</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">1.22</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\">26.01</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">Frieren</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">421M</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">CAVP</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">2.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">29.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">214.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text ltx_font_bold\">12.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">2.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text ltx_font_bold\">7.61</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">18.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">1.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 2.0pt;\">22.82</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">MMAudio</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">1054M</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">CLIP, S.F.</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">A.&amp;W.C.</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">2.45</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">7.56</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">131.09</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">13.52</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">2.00</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">7.12</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">24.86</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">0.81</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#CCCCCC;\">32.99 <span class=\"ltx_rule\" style=\"width:0.0pt;height:8.0pt;--ltx-bg-color:black;display:inline-block;\"/></span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">VSSFlow</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_bb\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">443M</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_bb\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">CLIP</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">1.34</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">11.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#E6E6E6;\">187.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">7.09</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">2.13</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#E6E6E6;\">7.16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">22.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">1.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">26.01<span class=\"ltx_text ltx_font_medium\" style=\"--ltx-bg-color:#E6E6E6;\"> <span class=\"ltx_rule\" style=\"width:0.0pt;height:12.0pt;--ltx-bg-color:black;display:inline-block;\"/></span></span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "compare",
            "421m",
            "evaluation",
            "each",
            "secondhighest",
            "frieren",
            "information",
            "onset",
            "gray",
            "extra",
            "416m",
            "kl↓textkldownarrow",
            "859m",
            "bold",
            "pann",
            "results",
            "vgg",
            "parameters",
            "datasets",
            "1126m",
            "commonly",
            "wavcaps",
            "representation",
            "synchformerextracted",
            "method",
            "mmaudio",
            "module",
            "type",
            "audiocaps",
            "therefore",
            "label",
            "893m",
            "frozen",
            "denotes",
            "cavp",
            "not",
            "seman",
            "higherquality",
            "sav",
            "avclip",
            "synchronization",
            "generation",
            "v2am",
            "data”",
            "used",
            "segment",
            "443m",
            "“paramcount”",
            "desync",
            "count",
            "condition",
            "im2wav",
            "indicates",
            "audioset",
            "underlined",
            "346m",
            "pas",
            "benchmark",
            "“visual",
            "imagebind",
            "acc↑textaccuparrow",
            "additional",
            "ap↑textapuparrow",
            "v2amapper",
            "rep”",
            "1057m",
            "tiva",
            "403m",
            "lists",
            "↓downarrow",
            "v2s",
            "another",
            "360m",
            "directly",
            "rep",
            "beyond",
            "them",
            "utilize",
            "foleycrafter",
            "vssflow",
            "number",
            "pretrained",
            "score",
            "features",
            "↑uparrow",
            "ibva",
            "model",
            "highest",
            "data",
            "foleyc",
            "“extra",
            "refers",
            "vaura",
            "awc",
            "visual",
            "param",
            "optical",
            "clip",
            "texttosound",
            "vggsound",
            "fad↓textfaddownarrow",
            "except",
            "training",
            "sound",
            "rgb",
            "difffoley",
            "backbone",
            "flow",
            "includes",
            "1054m",
            "metric",
            "377m",
            "vab",
            "seeing",
            "is↑textisuparrow",
            "lova",
            "specvq",
            "dataset",
            "quality"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">For V2S evaluation, we compare VSSFlow against baselines from autoregressive, mask-based, diffusion, and flow-based paradigms as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.T1\" title=\"Table 1 &#8227; Findings. &#8227; 4.3 Evaluations on Joint Learning of Sound and Speech Generation &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Our primary baselines are <span class=\"ltx_text ltx_font_bold\">Frieren</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib59\" title=\"\">2024c</a>)</cite>, which has comparable parameters, flow matching paradigm, and uses the same V2S dataset. And <span class=\"ltx_text ltx_font_bold\">LoVA</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib5\" title=\"\">2024</a>)</cite>, which uses more parameters, diffusion paradigm, and is fully initialized from Stable Audio Open.\nFor sound quality, we use Fr&#233;chet Audio Distance (FAD)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kilgour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib28\" title=\"\">2018</a>)</cite>, Inception Score (IS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Salimans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib46\" title=\"\">2016</a>)</cite>, and Mean KL-Divergence (KL). For temporal alignment, we report wildly-adopted onset accuracy (Onset Acc.) and onset average precision (Onset AP) following prior works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib63\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib14\" title=\"\">2023</a>)</cite>, plus DeSync Score following&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib4\" title=\"\">2025</a>)</cite>. See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A4\" title=\"Appendix D V2S benchmark &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">D</span></a> for details.</p>\n\n",
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.T1\" title=\"Table 1 &#8227; Findings. &#8227; 4.3 Evaluations on Joint Learning of Sound and Speech Generation &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents sound generation results on VGGSound benchmark, where VSSFlow matches SOTA performance. Versus Frieren, VSSFlow excels in sound quality and sound-visual semantic alignment, though slightly weaker in temporal alignment. Against LoVA, VSSFlow &#8212; with fewer parameters &#8212; matches sound quality and outperforms in temporal and semantic alignment.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.T2\" title=\"Table 2 &#8227; Findings. &#8227; 4.3 Evaluations on Joint Learning of Sound and Speech Generation &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the VisualTTS results on Chem and GRID benchmarks. VSSFlow achieves strong results in Spk.Sim., MCD, and LSE, demonstrating its effective capture of speaker traits and generation of lip-synced, high-fidelity speech.</p>\n\n",
            "<p class=\"ltx_p\">We evaluate the video-to-sound performance on the standard VGGSound benchmark. We compare VSSFlow against baselines representing different paradigms, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.T1\" title=\"Table 1 &#8227; Findings. &#8227; 4.3 Evaluations on Joint Learning of Sound and Speech Generation &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Autoregressive baselines include SpecVQGan&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Iashin &amp; Rahtu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib23\" title=\"\">2021</a>)</cite>, Im2Wav&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sheffer &amp; Adi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib47\" title=\"\">2022</a>)</cite> and V-AURA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Viertola et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib53\" title=\"\">2025</a>)</cite>. Mask-based baselines include VAB&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pascual et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib40\" title=\"\">2024</a>)</cite>. Diffusion baselines include Difffoley&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib37\" title=\"\">2023</a>)</cite>, Seeing&amp;Hearing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xing et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib60\" title=\"\">2024</a>)</cite>, V2A-Mapper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib56\" title=\"\">2024a</a>)</cite>, FoleyCrafter&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib63\" title=\"\">2024</a>)</cite>, TiVA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib58\" title=\"\">2024b</a>)</cite> and LoVA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib5\" title=\"\">2024</a>)</cite>. Flow-based approaches include Frieren&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib59\" title=\"\">2024c</a>)</cite> and MMAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib4\" title=\"\">2025</a>)</cite>. The baseline results are obtained either through official code execution or from released generated sounds. All sound samples are padded to 10 seconds for consistency. Since V-AURA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Viertola et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib53\" title=\"\">2025</a>)</cite> generates 2.56-second clips, we repeat each generated clip three times before padding it to 10 seconds.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Video-conditioned sound and speech generation, encompassing video-to-sound\n(V2S)\nand visual text-to-speech (VisualTTS) tasks, are conventionally addressed as separate tasks, with limited exploration to unify them within a signle framework.\nRecent attempts to unify V2S and VisualTTS face challenges in handling distinct condition types (e.g., heterogeneous video and transcript conditions) and require complex training stages. Unifying these two tasks remains an open problem.\nTo bridge this gap, we present <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">VSSFlow</span>, which seamlessly integrates both <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">V2S</span> and <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">VisualTTS</span> tasks into a unified <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">flow</span>-matching framework.\nVSSFlow uses a novel condition aggregation mechanism to handle distinct input signals.\nWe find that cross-attention and self-attention layer exhibit different inductive biases in the process of introducing condition. Therefore, VSSFlow leverages these inductive biases to effectively handle different representations:\ncross-attention for ambiguous video conditions and self-attention for more deterministic speech transcripts.\nFurthermore, contrary to the prevailing belief that joint training on the two tasks requires complex training strategies and may degrade performance, we find that VSSFlow benefits from the end-to-end joint learning process for sound and speech generation without extra designs on training stages. Detailed analysis attributes it to the learned general audio prior shared between tasks, which accelerates convergence, enhances conditional generation, and stabilizes the classifier-free guidance process.\nExtensive experiments demonstrate that VSSFlow surpasses the state-of-the-art domain-specific baselines on both V2S and VisualTTS benchmarks, underscoring the critical potential of unified generative models.</p>\n\n",
                "matched_terms": [
                    "condition",
                    "therefore",
                    "visual",
                    "generation",
                    "them",
                    "training",
                    "vssflow",
                    "sound",
                    "v2s",
                    "extra"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal generative models have achieved remarkable progress recently, with sound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Iashin &amp; Rahtu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib23\" title=\"\">2021</a>)</cite>, speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib3\" title=\"\">2022</a>)</cite>, and video&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib55\" title=\"\">2025</a>; Guan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib19\" title=\"\">2025</a>)</cite> generation emerging as central components of multimedia content creation.\nIn industry, multimodal systems are now capable of generating sound and speech jointly from the input video&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DeepMind, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib13\" title=\"\">2024</a>)</cite>. Veo3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Google, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib18\" title=\"\">2025</a>)</cite> even shows remarkable performance of sound, speech, and video joint generation capability.\nIn contrast, academic research mainly focuses on the single task: video-to-sound (V2S&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>In prior works, the term &#8220;Video-to-Audio (V2A)&#8221; has commonly been used. To avoid ambiguity in this paper, we define &#8220;sound&#8221; as non-linguistic audio, such as environmental or natural audio, distinct from speech. &#8220;Speech&#8221; refers to audio containing linguistic information, such as spoken language. &#8220;Audio&#8221; is used to encompass sound, speech, and all other types of auditory content.</span></span></span>), which generates synchronized and semantically-aligned sound but struggles to produce intelligible speech, and visual text-to-speech (VisualTTS), which generates high-quality lip-sync speech but is constrained in portrait-style videos and fail to generate non-speech sound.\nThis work takes a step further to address the missing link between video-conditioned sound and speech generation under one unified framework,\nsupporting both V2S and VisualTTS tasks, as well as their combination, i.e, generating speech alongside environmental sound simultaneously.</p>\n\n",
                "matched_terms": [
                    "commonly",
                    "visual",
                    "generation",
                    "sound",
                    "refers",
                    "used",
                    "v2s",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent efforts have aimed to unify both V2S and VisualTTS tasks. AudioGen-Omni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib57\" title=\"\">2025</a>)</cite> employs a single flow-based model to process all video and text conditions through in-context conditioning (concatenating or adding conditions to noisy audio tokens).\nHowever, we find this approach suboptimal. Using the same in-context condition mechanism for features from different modalities fails to capture the distinct interactions between video, text, and audio.\nDualDub&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib52\" title=\"\">2025</a>)</cite>, an autoregressive model, treats text as a prefix and combines video conditions with audio tokens via an additional fusion module. While this design effectively handles multiple conditions, it introduces a complex training procedure that relies on curriculum learning to progressively acquire speech and sound knowledge.\nConsequently, unifying V2S and VisualTTS remains an open challenge, particularly in managing diverse condition types and efficiently learning both sound and speech knowledge within a single model.</p>\n\n",
                "matched_terms": [
                    "features",
                    "condition",
                    "additional",
                    "model",
                    "training",
                    "sound",
                    "v2s",
                    "module"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges, we introduce VSSFlow, a flow-based framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib32\" title=\"\">2022</a>)</cite> that unifies V2S and VisualTTS generation.\nFirstly, to handle both video and transcript conditions, we systematically explore the optimal conditioning mechanism within the Diffusion Transformer (DiT)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peebles &amp; Xie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib41\" title=\"\">2022</a>)</cite> block. The two types of conditions exhibit inherently different characteristics: speech transcripts provide more deterministic guidance for speech generation, whereas video features are more ambiguous\nfor sound generation.\nAlso, two typical types of conditioning mechanisms lead to distinct inductive biases, i.e., cross-attention conditioning\nprocesses ambiguous video features well,\nwhile in-context/self-attention conditioning better exploits\ntranscript features. Accordingly, VSSFlow integrates transcript embeddings by concatenating with audio latents and processing them through self-attention, while incorporating video representations via cross-attention layers.\nSecondly, under our VSSFlow framework, we observe that no complex design for training stages is necessary. The joint learning of sound and speech does not lead to interference or performance degradation, a common issue in multitask learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kendall et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib27\" title=\"\">2018</a>; Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib52\" title=\"\">2025</a>)</cite>. Instead, it results in mutual enhancement on both tasks. We conduct analysis and reveal that this improvement stems from the model&#8217;s ability to learn general audio knowledge, which leads to faster convergence, better conditioned generation results, and more stable classifier-free guidance process.</p>\n\n",
                "matched_terms": [
                    "features",
                    "generation",
                    "them",
                    "training",
                    "results",
                    "vssflow",
                    "sound",
                    "v2s",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond unifying V2S and VisualTTS tasks, real-world scenarios often require generating sound and speech jointly. We further apply continual training on the VSSFlow model with synthetic sound-speech mixed data (i.e., speech with environmental sounds).\nAs shown in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S0.F1\" title=\"Figure 1 &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(c), the model rapidly adapts to this setting, successfully generating mixed outputs for out-of-domain videos &#8212;e.g., generating car engine sounds alongside a police man speaking clearly.</p>\n\n",
                "matched_terms": [
                    "model",
                    "beyond",
                    "data",
                    "training",
                    "vssflow",
                    "sound",
                    "v2s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evaluations on V2S and VisualTTS benchmarks show that VSSFlow achieves exceptional sound fidelity and speech quality. VSSFlow lays a powerful foundation for video-conditioned sound and speech generation. In summary, our contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "generation",
                    "vssflow",
                    "sound",
                    "v2s",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(1) We introduce VSSFlow, a unified framework for video-conditioned sound and speech generation, with an effective condition aggregation mechanism for integrating video and speech features into DiT blocks and handling both V2S and VisualTTS tasks.\n(2) Through systematic analysis, we demonstrate that under the flow model&#8217;s end-to-end training setting, the joint learning of sound and speech generation produces a mutual promotion effect, highlighting the critical role of unified models in the sound and speech generation field.\n(3) Extensive evaluations confirm VSSFlow&#8217;s superior performance, surpassing the SOTA domain-specific baselines on V2S and VisualTTS benchmarks.</p>\n\n",
                "matched_terms": [
                    "features",
                    "condition",
                    "flow",
                    "generation",
                    "training",
                    "vssflow",
                    "sound",
                    "v2s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Video-to-Sound (V2S) generation aims to produce environmental or natural sound, which is semantically- and temporally-aligned with the given silent video. Recent advances in generative models have driven significant progress in the V2S field, with approaches categorized into three main paradigms: Autoregressive models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sheffer &amp; Adi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib47\" title=\"\">2022</a>; Iashin &amp; Rahtu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib23\" title=\"\">2021</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib39\" title=\"\">2024</a>; Viertola et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib53\" title=\"\">2025</a>)</cite> convert sound into discrete tokens and predict the sound token sequence conditioned on video features; Mask-based methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib35\" title=\"\">2024b</a>; Pascual et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib40\" title=\"\">2024</a>; Su et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib51\" title=\"\">2024b</a>)</cite> also treat sound as tokens and reconstruct the sequences by predicting masked tokens; Flow- or diffusion-based methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib37\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib63\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib58\" title=\"\">2024b</a>; Cheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib5\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib59\" title=\"\">2024c</a>; Xing et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib60\" title=\"\">2024</a>; Cheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib4\" title=\"\">2025</a>)</cite> iteratively transform noise into a waveform guided by video conditions, achieving high generation fidelity. Another related task, visual text-to-speech (VisualTTS) generation, aims to generate speech that is consistent with the given video in aspects like speaker&#8217;s style, lip movements, emotions, and so on. These models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib3\" title=\"\">2022</a>; Cong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib8\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib11\" title=\"\">2025</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib36\" title=\"\">2023</a>; Cong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib10\" title=\"\">2024b</a>; Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib22\" title=\"\">2021</a>)</cite> are\noften compact in size, and built on mature TTS frameworks like FastSpeech2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chien et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib6\" title=\"\">2021</a>)</cite> and MatchaTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mehta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib38\" title=\"\">2024</a>)</cite>.\nBy incorporating multiple prosodic features like pitch, energy, emotional cues and so on, these models demonstrate powerful speech generation capability.</p>\n\n",
                "matched_terms": [
                    "features",
                    "flow",
                    "visual",
                    "v2s",
                    "generation",
                    "sound",
                    "another"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech generation focuses on capturing speaker characteristics and ensuring linguistic accuracy, while sound generation aims to accurately reproduce various non-speech audio types. Consequently, these two related fields are typically treated as distinct domains. Most V2S models fail to produce intelligible speech, and VisualTTS models are unable to generate non-speech sounds. In this paper, we propose a unified framework to address both tasks, aiming to develop a model that efficiently introduces different conditional signals and performs comparably to domain-specific baselines.</p>\n\n",
                "matched_terms": [
                    "sound",
                    "generation",
                    "v2s",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Some advancements have been made to integrate video-conditioned sound and speech generation into unified models. For example, Audiobox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vyas et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib54\" title=\"\">2023</a>)</cite> from Meta and V2S model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DeepMind, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib13\" title=\"\">2024</a>)</cite> from Google leverage diffusion models to generate high-fidelity sound and speech from multimodal prompts.\nMore recently, Veo3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Google, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib18\" title=\"\">2025</a>)</cite> has garnered significant attention for its ability to generate videos with synchronized background sound and human speech, sparking renewed interest in unified visual-sound-speech generation models. In the academic community, AudioGen-Omni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib57\" title=\"\">2025</a>)</cite> integrates multiple video and text prompts all through in-context conditioning into a unified flow model. DeepAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib62\" title=\"\">2025</a>)</cite> and DualDub&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib52\" title=\"\">2025</a>)</cite> have explored unified video-conditioned audio generation by employing fusion modules, such as large language models (LLMs), to integrate speech and sound generation heads. They rely on multi-stage training strategies, where the model is progressively trained to acquire distinct generation capabilities for speech and sound.</p>\n\n",
                "matched_terms": [
                    "flow",
                    "model",
                    "generation",
                    "training",
                    "sound",
                    "v2s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The interplay between sound and speech generation poses a critical question for unified audio generation, yet it remains underexplored in prior work. Are sound and speech, as distinct audio modalities, entirely separate tasks, or can they be effectively learned jointly? A common belief is that end-to-end joint training degrades generation performance compared to curriculum learning approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib52\" title=\"\">2025</a>)</cite>. In contrast, our proposed VSSFlow framework offers a novel perspective: end-to-end joint learning of sound and speech within a flow-matching framework does not lead to mutual suppression. Instead, it fosters a synergistic effect, driven by the shared knowledge between the sound and speech modalities. By leveraging a tailored condition aggregation mechanism and end-to-end joint learning, VSSFlow effectively balances the distinct requirements of V2S and VisualTTS tasks, achieving superior performance across diverse video contexts.</p>\n\n",
                "matched_terms": [
                    "condition",
                    "generation",
                    "training",
                    "vssflow",
                    "sound",
                    "v2s",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide an overview of the flow-matching model and classifier-free guidance, which serve as the foundation for our proposed VSSFlow framework.</p>\n\n",
                "matched_terms": [
                    "model",
                    "vssflow"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Flow-matching&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib32\" title=\"\">2022</a>)</cite> transforms a source distribution <math alttext=\"\\mathcal{P}(x_{0})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}(x_{0})</annotation></semantics></math>, typically Gaussian noise <math alttext=\"x_{0}\\sim\\mathcal{N}(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">x_{0}\\sim\\mathcal{N}(0,1)</annotation></semantics></math>, into a target audio distribution <math alttext=\"\\mathcal{P}(x_{1})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}(x_{1})</annotation></semantics></math>. This process is governed by a continuous-time ODE with learned velocity field <math alttext=\"v_{\\theta}(x_{t},c,t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>v</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><mi>c</mi><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">v_{\\theta}(x_{t},c,t)</annotation></semantics></math>, where <math alttext=\"t\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">t\\in[0,1]</annotation></semantics></math>, <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math> is the sample state at timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> is an optional condition, and <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> denotes parameters:</p>\n\n",
                "matched_terms": [
                    "denotes",
                    "condition",
                    "parameters"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our unified video-conditioned sound and speech generation setting, condition <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> mainly includes video representations <math alttext=\"c_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><msub><mi>c</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">c_{v}</annotation></semantics></math> and speech transcript representations <math alttext=\"c_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><msub><mi>c</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">c_{p}</annotation></semantics></math>. The training objective minimizes the difference between predicted and ground-truth velocities along the optimal transport path:</p>\n\n",
                "matched_terms": [
                    "condition",
                    "includes",
                    "generation",
                    "training",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In inference, Gaussian noise <math alttext=\"x_{0}\\sim\\mathcal{N}(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">x_{0}\\sim\\mathcal{N}(0,1)</annotation></semantics></math> is sampled, and the model uses condition <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> to progressively refine <math alttext=\"x_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><msub><mi>x</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">x_{0}</annotation></semantics></math> into a clean latent representation <math alttext=\"x_{1}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mn>1</mn><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">x_{1}^{\\prime}</annotation></semantics></math>. Classifier-free guidance (CFG)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho &amp; Salimans, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib21\" title=\"\">2022</a>)</cite> enhances generation quality in conditional flow-matching models. During training, the model learns both conditional and unconditional modeling ability by randomly setting <math alttext=\"c=\\emptyset\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>=</mo><mi mathvariant=\"normal\">&#8709;</mi></mrow><annotation encoding=\"application/x-tex\">c=\\emptyset</annotation></semantics></math>. In inference, predictions at each timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p1.m6\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> are interpolated as:</p>\n\n",
                "matched_terms": [
                    "condition",
                    "model",
                    "representation",
                    "generation",
                    "each",
                    "training",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S3.F2\" title=\"Figure 2 &#8227; Classifier-Free Guidance. &#8227; 3.1 Preliminary &#8227; 3 method &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, VSSFlow adopts 10-layer cross-attention-based DiT for unified video-conditioned sound and speech generation. Each DiT block follows the implementation in Stable Audio Open&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Evans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib15\" title=\"\">2025</a>)</cite>. Besides, to better capture temporal relationships between the sound sequence and video frame sequence in the V2S process, we incorporate 1D Rotary Position Embedding (RoPE)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Su et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib50\" title=\"\">2024a</a>)</cite> into the query and key matrix in both self- and cross-attention blocks. The waveform is converted to a melspectrogram and processed by the VAE encoder to get <math alttext=\"x_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><msub><mi>x</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">x_{0}</annotation></semantics></math>. The denoised latent <math alttext=\"x^{\\prime}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mn>1</mn><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">x^{\\prime}_{1}</annotation></semantics></math> is decoded to a melspectrogram by the VAE decoder and then converted into a waveform via a vocoder. More details can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A1\" title=\"Appendix A Condition Representations &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "each",
                    "vssflow",
                    "sound",
                    "v2s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Video-conditioned unified audio generation relies on two primary condition signals: video frame representations and speech transcripts. These signals differ fundamentally in their relationship to the generated audio. Speech transcripts exhibit high determinism, as they directly encode linguistic content, resulting in speech outputs that are relatively consistent, with variations mainly in prosody, timbre, and style. Conversely, video frames introduce greater uncertainty, as identical visual inputs can map to a diverse set of plausible sound outputs, reflecting the more variable nature of non-linguistic audio. Consequently, V2S generation poses greater modeling complexity compared to the more deterministic VisualTTS process, while VisualTTS generation has higher requirements for the accuracy of the generated speech. These distinct characteristics present challenges for the design of the model&#8217;s condition mechanism.</p>\n\n",
                "matched_terms": [
                    "condition",
                    "directly",
                    "visual",
                    "generation",
                    "sound",
                    "v2s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our work, video is encoded by CLIP model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib43\" title=\"\">2021</a>)</cite> at 10 FPS, resulting in visual representations <math alttext=\"c_{v}\\in\\mathbb{R}^{T_{v}\\times D_{v}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>c</mi><mi>v</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>v</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>D</mi><mi>v</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">c_{v}\\in\\mathbb{R}^{T_{v}\\times D_{v}}</annotation></semantics></math>, where <math alttext=\"T_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><msub><mi>T</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">T_{v}</annotation></semantics></math> is the frame number and <math alttext=\"D_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><msub><mi>D</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">D_{v}</annotation></semantics></math> is CLIP embedding dimension. Speech transcripts are converted to phoneme sequences representations <math alttext=\"c_{p}\\in\\mathbb{R}^{T_{p}\\times D_{p}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>c</mi><mi>p</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>p</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>D</mi><mi>p</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">c_{p}\\in\\mathbb{R}^{T_{p}\\times D_{p}}</annotation></semantics></math>, where <math alttext=\"T_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><msub><mi>T</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">T_{p}</annotation></semantics></math> is phoneme token sequence length and <math alttext=\"D_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p2.m6\" intent=\":literal\"><semantics><msub><mi>D</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">D_{p}</annotation></semantics></math> denotes the phoneme embedding dimension. More details on condition representation can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A1\" title=\"Appendix A Condition Representations &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "clip",
                    "condition",
                    "model",
                    "representation",
                    "visual",
                    "denotes",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present the design choices of the condition aggregation mechanism for effectively integrating video and speech representations. Within the cross-attention-based DiT architecture, complex sequence conditions are incorporated through two primary mechanisms: 1) via cross-attention layers, where conditions are used as key and value matrices to guide the generation process, and 2) via concatenation with the initial latent representation, which is processed primarily through self-attention blocks. Cross- and self-attention block has no structural difference, but the self-attention block additionally models the relationships among latent token sequences. As discussed before, to determine the optimal way of introducing different signals, we propose and evaluate four variants of the condition mechanism, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S3.F2\" title=\"Figure 2 &#8227; Classifier-Free Guidance. &#8227; 3.1 Preliminary &#8227; 3 method &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "generation",
                    "representation",
                    "condition"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the experiment section, we respond to the unaddressed challenges mentioned above: (1) How to build a unified end-to-end framework for video-conditioned sound and speech generation, and find the optimal method to integrate different types of conditions? (2) Does joint training of speech and sound affect each other&#8217;s generation performance, and is this impact positive or negative?\nVSSFlow is trained end-to-end on multiple sound and speech datasets using the flow loss function defined in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S3.E2\" title=\"In Flow-Matching Framework. &#8227; 3.1 Preliminary &#8227; 3 method &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. The data covers three tasks: (1) Video-to-sound (V2S), with phoneme conditions <math alttext=\"c_{p}=0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>c</mi><mi>p</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">c_{p}=0</annotation></semantics></math>; (2) Visual test-to-speech (VisualTTS), with both <math alttext=\"c_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>c</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">c_{p}</annotation></semantics></math> and <math alttext=\"c_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>c</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">c_{v}</annotation></semantics></math> are active; And (3) text-to-speech (TTS), with <math alttext=\"c_{v}=0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>c</mi><mi>v</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">c_{v}=0</annotation></semantics></math>. See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A3\" title=\"Appendix C Dataset and Experiments Details &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for more details about the dataset and training settings.</p>\n\n",
                "matched_terms": [
                    "flow",
                    "visual",
                    "generation",
                    "data",
                    "each",
                    "training",
                    "method",
                    "vssflow",
                    "sound",
                    "dataset",
                    "v2s",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first explore optimal mechanisms for integrating video and speech conditions, training four variants (illustrated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S3.SS2\" title=\"3.2 VSSFlow Framework &#8227; 3 method &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>) for 100 epochs and evaluating them on both V2S and VisualTTS tasks. The main evaluation results are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.F3\" title=\"Figure 3 &#8227; 4.1 Experiments Setup &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, and additional metrics can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A2.SS1\" title=\"B.1 Ablations on Condition Mechanism &#8227; Appendix B Experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>.\nBased on the results, we draw the following key findings:</p>\n\n",
                "matched_terms": [
                    "additional",
                    "evaluation",
                    "them",
                    "training",
                    "results",
                    "v2s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Aligned phoneme embeddings&#8212;which are deterministic and content-tied&#8212;suit concatenation-based integration. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.F3\" title=\"Figure 3 &#8227; 4.1 Experiments Setup &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(b), phoneme-concatenation variants (CrossV, ConcatVS) converge faster. Notably, Variant CrossV reaches very low WER in only 10 epochs. As the strong alignment between the speech features and the speech waveform, the concatenation method leverages self-attention&#8217;s inductive bias to speed convergence and boost speech generation quality.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "features",
                    "method",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Video CLIP features&#8212;which are weakly linked to mel-spectrograms&#8212;favor flexible cross-attention integration under the setting of sound-video joint training. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.F3\" title=\"Figure 3 &#8227; 4.1 Experiments Setup &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a) shows that cross-attention of video features (CrossV, CrossVS) outperforms the concatenation methods (ConcatV, ConcatVS). The video features are less determinant for the sound waveform. Therefore, using concatenation to introduce the video features harms the learning process and sound quality due to the aforementioned inductive bias of the self-attention block. Through cross-attention, the model can learn the relationship in a flexible way and enhance sound generation performance.</p>\n\n",
                "matched_terms": [
                    "clip",
                    "features",
                    "therefore",
                    "model",
                    "generation",
                    "training",
                    "sound",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CrossV variant works as final condition mechanism as better performance on both generation tasks.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "condition"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further investigate the impact of combining sound and speech data during training. We train VSSFlow under various data configurations, with primary results shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.F4\" title=\"Figure 4 &#8227; 4.2 Comparison of Condition Aggregation Mechanisms &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. Additional metrics are shown in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A2.SS2\" title=\"B.2 Ablations on Joint Training &#8227; Appendix B Experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "additional",
                    "data",
                    "training",
                    "results",
                    "vssflow",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For V2S, we compare three settings: (1) V2S data only, (2) V2S + VisualTTS data, and (3) V2S + TTS data. Results in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.F4\" title=\"Figure 4 &#8227; 4.2 Comparison of Condition Aggregation Mechanisms &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(a.1) show that incorporating speech data &#8212; whether visual modality is included or not (VisualTTS or TTS) &#8212; enhances sound generation performance. Models trained with both speech and sound data significantly outperform the V2S-only baseline at equivalent training steps.</p>\n\n",
                "matched_terms": [
                    "compare",
                    "visual",
                    "generation",
                    "data",
                    "training",
                    "results",
                    "sound",
                    "v2s",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For VisualTTS, we train VSSFlow with two data settings: (1) VisualTTS data only and (2) V2S + VisualTTS data. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.F4\" title=\"Figure 4 &#8227; 4.2 Comparison of Condition Aggregation Mechanisms &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(b) shows that the two variants have comparable speech generation performance at the same training step, indicating that joint training with sound does not impair speech generation performance or convergence speed. Moreover, the model with extra sound training data achieves better convergence performance after the same number of epochs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "generation",
                    "data",
                    "training",
                    "vssflow",
                    "sound",
                    "v2s",
                    "not",
                    "number",
                    "extra"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Contrary to prior work&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib52\" title=\"\">2025</a>)</cite>, our experiments show that joint learning of sound and speech generation is mutually beneficial, not suppressive, within the end-to-end VSSFlow framework. Including multiple data types boosts performance on both tasks, highlighting the potential of unified generative models for multimodal sound and speech synthesis tasks.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "data",
                    "vssflow",
                    "sound",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training on diverse audio types (including both sound and speech) enables VSSFlow to fit the audio distribution more comprehensively, therefore enhancing the efficacy of CFG process. Capturing a more general audio distribution through joint sound-speech learning, the difference between conditional and unconditional prediction produces a more informative direction, improving the guidance quality of CFG.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.F4\" title=\"Figure 4 &#8227; 4.2 Comparison of Condition Aggregation Mechanisms &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(a.2), we compare models trained for 10,000 steps across three data settings, evaluating their sound generation performance under varying cfg scales. According to Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S3.E3\" title=\"In Classifier-Free Guidance. &#8227; 3.1 Preliminary &#8227; 3 method &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, at cfg = 1, when the unconditional generation result has not yet been utilized, all models exhibit similar conditional generation capabilities, with joint-training settings perform a little better (3.5, 3.6 vs. 4.1). However, at higher cfg (cfg = 3), when unconditional result is also taken into account, models trained with additional speech data show substantial gains, while the V2S-only model improves minimally or even exhibits degradation. Prior work &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib16\" title=\"\">2025</a>)</cite> shows that implementing CFG on underfitted models can degrade the generation performance. The narrow distribution of just sound data may contribute to this. However, joint sound-speech training mitigates this phenomenon, highlighting the importance of the shared knowledge across domains.</p>\n\n",
                "matched_terms": [
                    "compare",
                    "therefore",
                    "additional",
                    "model",
                    "generation",
                    "data",
                    "training",
                    "vssflow",
                    "sound",
                    "not",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the findings in section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.SS2\" title=\"4.2 Comparison of Condition Aggregation Mechanisms &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a> and &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.SS3\" title=\"4.3 Evaluations on Joint Learning of Sound and Speech Generation &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>, the reported VSSFlow model is based on architecture CrossV, trained for 150 epochs using a total of 503k data from V2S, TTS, and VisualTTS datasets. For the V2S generation, we evaluate the model&#8217;s performance on the standard VGGSound benchmark. For VisualTTS generation, we use widely adopted Chem and GRID benchmarks.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "vggsound",
                    "model",
                    "generation",
                    "data",
                    "vssflow",
                    "v2s",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For VisualTTS evaluation, we compare VSSFlow against four SOTA baselines: DSU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib36\" title=\"\">2023</a>)</cite>, HPMDubbing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib8\" title=\"\">2023</a>)</cite>, StyleDubber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib10\" title=\"\">2024b</a>)</cite>, and EmoDubber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib9\" title=\"\">2024a</a>)</cite>. For speech quality, we use Word Error Rate (WER) and UTokyo-SaruLab Mean Opinion Score (UTMOS). For speech alignment, we compute MCD, MCD-DTW, and MCD-DTW-SL. For speech-visual alignment, we report Lip Sync Error Distance (LSE-D) and Lip Sync Error Confidence (LSE-C). See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A5\" title=\"Appendix E VisualTTS benchmark &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">E</span></a> for details.</p>\n\n",
                "matched_terms": [
                    "score",
                    "compare",
                    "evaluation",
                    "vssflow",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For V2S generation, the experiments highlight three key factors that affect the performance: video representations, training data quality, and model paradigm. First, CLIP features yield robust semantic representations for sound quality and semantic alignment but lack temporality, addressable via features like CAVP or Synchformer. Second, high-quality text-to-sound data or frozen text-to-sound modules further enhance results. Third, flow/diffusion and mask models outperform autoregressive ones.\nFor VisualTTS, the metrics validate that a simple unified model suffices for VisualTTS generation without a complex data preprocessing pipeline. However, the use of VAE-vocoder incurs detail loss, reducing UTMOS versus ground truth (as seen in GT-vocoder baselines).</p>\n\n",
                "matched_terms": [
                    "clip",
                    "texttosound",
                    "features",
                    "model",
                    "frozen",
                    "generation",
                    "data",
                    "cavp",
                    "training",
                    "results",
                    "sound",
                    "v2s",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VSSFlow shows excellent performance in V2S and VisualTTS tasks, but lacks the inherent ability of sound-speech joint generation. We stimulate this capability by fine-tuning pretrained VSSFlow model with &#160;170k synthetic sound-speech mixtures, created via randomized concatenation or overlay of data from VGGSound and LRS2. Fine-tuning the base model for only 15 epochs yields a model capable of jointly generating sound and speech aligned with visual content. We evaluate its out-of-domain performance using videos from Veo3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Google, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib18\" title=\"\">2025</a>)</cite>. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S0.F1\" title=\"Figure 1 &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(c), the model accurately generates a car driving sound and a police officer&#8217;s speech, along with the braking sound in the background. The generated speech is precisely aligned with the character&#8217;s lip movements, timbre (adult male), and emotional tone (slightly excited). We politely invite you to visit our project page<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Project Page: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://vasflow1.github.io/vasflow/\" title=\"\">https://vasflow1.github.io/vasflow/</a></span></span></span> for more playable demos. These results demonstrate VSSFlow&#8217;s capability to process video signals and transcripts simultaneously, and the capability to generate the environmental sound alongside clear and accurate speech jointly.</p>\n\n",
                "matched_terms": [
                    "pretrained",
                    "vggsound",
                    "model",
                    "visual",
                    "generation",
                    "data",
                    "results",
                    "vssflow",
                    "sound",
                    "v2s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work presents a unified flow model integrating video-to-sound (V2S) and visual text-to-speech (VisualTTS) tasks, establishing a new paradigm for video-conditioned sound and speech generation. Our framework demonstrates an effective condition aggregation mechanism for incorporating speech and video conditions into the DiT architecture. Besides, we reveal a mutual boosting effect of sound-speech joint learning through analysis, highlighting the value of a unified generation model.\nFor future research, there are several directions that merit further exploration. First, the scarcity of high-quality video-speech-sound data limits the development of unified generative models. Additionally, developing better representation methods for sound and speech, which can preserve speech details while maintaining compact dimensions, is a critical future challenge.</p>\n\n",
                "matched_terms": [
                    "condition",
                    "flow",
                    "model",
                    "representation",
                    "visual",
                    "generation",
                    "data",
                    "sound",
                    "v2s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work adheres to the Code of Ethics, with all authors having read and acknowledged it during submission. Our research on VSSFlow does not involve human subjects directly. It relies totally on publicly available datasets for model training. However, we recognize potential ethical concerns related to the model&#8217;s applications, including the risk of generating misleading or harmful content, such as deepfake audio that could exacerbate misinformation or privacy violations if misused. Besides, there may be limitations for the groups that appear relatively infrequently in the training data. To mitigate discrimination, bias, and fairness issues, we evaluated the model on diverse datasets to ensure better robustness across different settings and demographics.</p>\n\n",
                "matched_terms": [
                    "model",
                    "directly",
                    "data",
                    "training",
                    "vssflow",
                    "not",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate reproducibility, this paper provides detailed descriptions of the VSSFlow architecture (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S3.SS2\" title=\"3.2 VSSFlow Framework &#8227; 3 method &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>). data preprocessing steps (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A1\" title=\"Appendix A Condition Representations &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>) and splitting method (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A3\" title=\"Appendix C Dataset and Experiments Details &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>), evaluation metrics (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A4\" title=\"Appendix D V2S benchmark &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">D</span></a> and &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A5\" title=\"Appendix E VisualTTS benchmark &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>), training procedures with additional implementation details (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.SS1\" title=\"4.1 Experiments Setup &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A3\" title=\"Appendix C Dataset and Experiments Details &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>), extensive ablation results (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A2\" title=\"Appendix B Experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>), etc.\nIn addition, to facilitate the replication and further research by the academic community, we promise to release the training code and checkpoint weights in our <a class=\"ltx_ref ltx_href\" href=\"https://vasflow1.github.io/vasflow/\" title=\"\">project page</a> as soon as possible.</p>\n\n",
                "matched_terms": [
                    "additional",
                    "evaluation",
                    "data",
                    "training",
                    "results",
                    "vssflow",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each video is truncated or padded to 10 seconds. For video representations, we employ CLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib43\" title=\"\">2021</a>)</cite> model to extract video features at 10 FPS, resulting in representations <math alttext=\"c_{v}\\in\\mathbb{R}^{T_{v}\\times D_{v}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>c</mi><mi>v</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>v</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>D</mi><mi>v</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">c_{v}\\in\\mathbb{R}^{T_{v}\\times D_{v}}</annotation></semantics></math>, where <math alttext=\"T_{v}=100\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mi>v</mi></msub><mo>=</mo><mn>100</mn></mrow><annotation encoding=\"application/x-tex\">T_{v}=100</annotation></semantics></math> is the total number of frames and <math alttext=\"D_{v}=768\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>D</mi><mi>v</mi></msub><mo>=</mo><mn>768</mn></mrow><annotation encoding=\"application/x-tex\">D_{v}=768</annotation></semantics></math> is the CLIP feature dimension. For speech representations, transcripts are first converted to phoneme sequences, which are then encoded by a custom-trained embedding network into representations <math alttext=\"c_{p}\\in\\mathbb{R}^{T_{p}\\times D_{p}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>c</mi><mi>p</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>p</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>D</mi><mi>p</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">c_{p}\\in\\mathbb{R}^{T_{p}\\times D_{p}}</annotation></semantics></math>. <math alttext=\"T_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m5\" intent=\":literal\"><semantics><msub><mi>T</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">T_{p}</annotation></semantics></math> varies with sentence length and <math alttext=\"D_{p}=32\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>D</mi><mi>p</mi></msub><mo>=</mo><mn>32</mn></mrow><annotation encoding=\"application/x-tex\">D_{p}=32</annotation></semantics></math> denotes the phoneme embedding dimension.</p>\n\n",
                "matched_terms": [
                    "clip",
                    "features",
                    "model",
                    "denotes",
                    "each",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio waveforms are resampled at 16kHz, truncated, or padded to 10 seconds. Waveforms are first converted into melspectrum and then encoded into a latent representation <math alttext=\"x_{1}\\in\\mathbb{R}^{T_{a}\\times D_{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>a</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>D</mi><mi>a</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">x_{1}\\in\\mathbb{R}^{T_{a}\\times D_{a}}</annotation></semantics></math> using the Variational Autoencoder (VAE) from AudioLDM 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib34\" title=\"\">2024a</a>)</cite>, where <math alttext=\"T_{a}=250\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mi>a</mi></msub><mo>=</mo><mn>250</mn></mrow><annotation encoding=\"application/x-tex\">T_{a}=250</annotation></semantics></math> is the length of the latent sequence and <math alttext=\"D_{a}=64\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>D</mi><mi>a</mi></msub><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">D_{a}=64</annotation></semantics></math> is the latent dimension. Timestep condition <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> is encoded and padded before the latent representation sequence to form the final input latent <math alttext=\"x_{t}\\in\\mathbb{R}^{(T_{a}+1)\\times D_{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>T</mi><mi>a</mi></msub><mo>+</mo><mn>1</mn></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><msub><mi>D</mi><mi>a</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">x_{t}\\in\\mathbb{R}^{(T_{a}+1)\\times D_{a}}</annotation></semantics></math>\nDuring inference, the predicted latent is converted into a mel-spectrogram through the VAE decoder and then reconstructed to a waveform by HiFiGAN vocoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib29\" title=\"\">2020a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "representation",
                    "condition"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure temporal alignment across modalities, video features <math alttext=\"c_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m1\" intent=\":literal\"><semantics><msub><mi>c</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">c_{v}</annotation></semantics></math> are linearly interpolated to match the latent audio length <math alttext=\"T_{v}=T_{a}=250\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m2\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mi>v</mi></msub><mo>=</mo><msub><mi>T</mi><mi>a</mi></msub><mo>=</mo><mn>250</mn></mrow><annotation encoding=\"application/x-tex\">T_{v}=T_{a}=250</annotation></semantics></math>, yielding the final <math alttext=\"c_{v}\\in\\mathbb{R}^{250\\times 768}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mi>c</mi><mi>v</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>250</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>768</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">c_{v}\\in\\mathbb{R}^{250\\times 768}</annotation></semantics></math>. For the phoneme-based speech condition <math alttext=\"c_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m4\" intent=\":literal\"><semantics><msub><mi>c</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">c_{p}</annotation></semantics></math>, inspired by prior works in text-to-speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chien et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib6\" title=\"\">2021</a>; Cong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib9\" title=\"\">2024a</a>)</cite>, we train a duration predictor that takes the phoneme sequence and AV-HuBERT features&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib48\" title=\"\">2022</a>)</cite> from video, using an attention mechanism to produce a phoneme-to-frame alignment matrix from which durations are derived. The predictor enables us to repeat and pad phoneme embeddings to obtain <math alttext=\"T_{p}=T_{a}=250\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mi>p</mi></msub><mo>=</mo><msub><mi>T</mi><mi>a</mi></msub><mo>=</mo><mn>250</mn></mrow><annotation encoding=\"application/x-tex\">T_{p}=T_{a}=250</annotation></semantics></math>. This results in temporal-aligned phoneme features <math alttext=\"c_{p}\\in\\mathbb{R}^{250\\times 32}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m6\" intent=\":literal\"><semantics><mrow><msub><mi>c</mi><mi>p</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>250</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>32</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">c_{p}\\in\\mathbb{R}^{250\\times 32}</annotation></semantics></math>.\nTo align with prior VisualTTS baselines, we extract speaker embeddings from reference speech using RawNet3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib26\" title=\"\">2022</a>)</cite>, optionally prefixing them to <math alttext=\"c_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m7\" intent=\":literal\"><semantics><msub><mi>c</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">c_{v}</annotation></semantics></math> for speaker consistency.</p>\n\n",
                "matched_terms": [
                    "features",
                    "condition",
                    "them",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">V2S Dataset.</span> We use <span class=\"ltx_text ltx_font_italic\">VGGSound</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib2\" title=\"\">2020</a>)</cite>, a widely adopted benchmark for the V2S task, which contains approximately <span class=\"ltx_text ltx_font_bold\">182k</span> training samples and 15k test samples. VGGSound provides a diverse collection of sound-visual pairs, making it well-suited for training models in video-conditioned sound generation.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "vggsound",
                    "generation",
                    "training",
                    "sound",
                    "dataset",
                    "v2s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">VisualTTS Datasets.</span> We employ three standard VisualTTS datasets: <span class=\"ltx_text ltx_font_italic\">Chem</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Prajwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib42\" title=\"\">2020</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">GRID</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cooke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib12\" title=\"\">2006</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">LRS2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Afouras et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib1\" title=\"\">2018</a>)</cite>, comprising a total of <span class=\"ltx_text ltx_font_bold\">162k</span> training samples. Chem is a single-speaker English dataset of chemistry lecture recordings. Following prior work&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib8\" title=\"\">2023</a>)</cite>, we use 6,240 samples for training and 200 for testing. GRID is a multi-speaker English dataset consisting of 33 speakers, each contributing 1,000 utterances. In line with&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib9\" title=\"\">2024a</a>)</cite>, we select 100 segments per speaker for testing, resulting in 29,600 training samples and 3,291 test samples. LRS2 is a diverse sentence-level dataset collected from BBC television programs, providing real-world variability. It contains 126k training samples and 700 test samples.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "each",
                    "training",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">TTS Datasets.</span> For the TTS task, we use <span class=\"ltx_text ltx_font_italic\">LJSpeech</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ito &amp; Johnson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib25\" title=\"\">2017</a>)</cite> and <span class=\"ltx_text ltx_font_italic\">LibriTTS</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib61\" title=\"\">2019</a>)</cite>, totaling approximately <span class=\"ltx_text ltx_font_bold\">160k</span> training samples. LJSpeech is a single-speaker dataset with 13,100 narrated passages from nonfiction books. Following&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chien et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib6\" title=\"\">2021</a>)</cite>, we use 12,577 samples for training and 523 for testing. LibriTTS is a large-scale multi-speaker corpus of English read speech, comprising 147k training samples with diverse speaker identities.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "datasets",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training, phoneme durations follow ground-truth alignments from speech data. We set the unconditional probability of video representation <math alttext=\"c_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m1\" intent=\":literal\"><semantics><msub><mi>c</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">c_{v}</annotation></semantics></math> and speech representation <math alttext=\"c_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m2\" intent=\":literal\"><semantics><msub><mi>c</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">c_{p}</annotation></semantics></math> to 0.1, use a batch size of 36 per GPU, a global learning rate of 4e-7, and apply 2,000 warm-up steps. The model is trained for 200 epochs on 4 H100 GPUs. At the inference stage, we apply a classifier-free guidance scale of 3.0 for the V2S task and a scale of 1.5 for the VisualTTS task. The Dopri5 is adopted to ensure high-quality sampling.</p>\n\n",
                "matched_terms": [
                    "model",
                    "representation",
                    "data",
                    "training",
                    "v2s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our primary comparisons focus on two baselines: <span class=\"ltx_text ltx_font_bold\">Frieren<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text ltx_font_medium\">(</span>Wang et&#160;al.<span class=\"ltx_text ltx_font_medium\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib59\" title=\"\">2024c</a><span class=\"ltx_text ltx_font_medium\">)</span></cite></span>, which has a comparable parameter count, uses flow matching, and is trained on the same V2S dataset as VSSFlow. It introduces video conditions by concatenating video representations with latent. And <span class=\"ltx_text ltx_font_bold\">LoVA<cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text ltx_font_medium\">(</span>Cheng et&#160;al.<span class=\"ltx_text ltx_font_medium\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib5\" title=\"\">2024</a><span class=\"ltx_text ltx_font_medium\">)</span></cite></span>, which employs a 24-layer DiT backbone and adopts a diffusion paradigm. Compared to VSSFlow, LoVA has significantly more parameters and is fully initialized with Stable Audio Open weights.</p>\n\n",
                "matched_terms": [
                    "count",
                    "backbone",
                    "flow",
                    "lova",
                    "vssflow",
                    "dataset",
                    "v2s",
                    "parameters"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Consistent with prior work&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Iashin &amp; Rahtu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib23\" title=\"\">2021</a>; Sheffer &amp; Adi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib47\" title=\"\">2022</a>; Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib37\" title=\"\">2023</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib33\" title=\"\">2023</a>)</cite>, we adopt widely used metrics, including Fr&#233;chet Audio Distance (FAD)<cite class=\"ltx_cite ltx_citemacro_citep\">(Kilgour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib28\" title=\"\">2018</a>)</cite>, Inception Score (IS)<cite class=\"ltx_cite ltx_citemacro_citep\">(Salimans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib46\" title=\"\">2016</a>)</cite>, and Mean KL-Divergence (KL), to assess the quality of generated sound. To mitigate the impact of varying sampling rates, we first downsample the generated sound to 16 kHz and then resample it to the required rates for the respective audio classifiers (16 kHz for VGGish&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib20\" title=\"\">2017</a>)</cite>, 32 kHz for PaSST&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Koutini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib31\" title=\"\">2022</a>)</cite>, and 32 kHz for PANN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib30\" title=\"\">2020b</a>)</cite>).\nFor temporal alignment evaluation, we use Onset Accuracy (Onset Acc.) and Onset Average Precision (Onset AP), following&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib63\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib14\" title=\"\">2023</a>)</cite>. Additionally, inspired by&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib4\" title=\"\">2025</a>)</cite>, we extract features using the SynchFormer model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Iashin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib24\" title=\"\">2024</a>)</cite> to compute offsets, producing the DeSync Score. To quantify soundvisual relevance, we calculate the cosine similarity between the embeddings of the input video and the generated sound using the ImageBind&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Girdhar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib17\" title=\"\">2023</a>)</cite> model (VA-IB).</p>\n\n",
                "matched_terms": [
                    "score",
                    "features",
                    "imagebind",
                    "onset",
                    "model",
                    "evaluation",
                    "pann",
                    "sound",
                    "used",
                    "quality",
                    "desync"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate VSSFlow&#8217;s performance against other VisualTTS baselines on the widely adopted Chem and GRID datasets. During generation, we randomly select a speech sample from the same speaker as the reference. We compare VSSFlow against four SOTA baselines: DSU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib36\" title=\"\">2023</a>)</cite>, HPMDubbing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib8\" title=\"\">2023</a>)</cite>, StyleDubber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib10\" title=\"\">2024b</a>)</cite> and EmoDubber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib9\" title=\"\">2024a</a>)</cite>.\nThe baseline data is either generated from the official implementation or from author-released results. We also obtain the results of the GT-vocoder by compressing the ground truth mel-spectrogram into the latent space via a VAE encoder, reconstructing it through a VAE decoder, and then using a vocoder to recover the waveform. Theoretically, the indicators in this row represent the upper limit for VSSFlow.\n</p>\n\n",
                "matched_terms": [
                    "compare",
                    "generation",
                    "data",
                    "results",
                    "vssflow",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate visual text-to-speech (VisualTTS) performance using several metrics. For overall speech quality, Word Error Rate (WER) measures intelligibility using the Whisper-V3 model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib44\" title=\"\">2023</a>)</cite>, while the UTokyo-SaruLab Mean Opinion Score (UTMOS)<cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib45\" title=\"\">2022</a>)</cite>, computed via Versa<cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib49\" title=\"\">2024</a>)</cite>, assesses clarity, naturalness, and fluency. Speaker similarity measures the similarity of two speech samples, also computed via Versa.\nFor speech alignment between the ground truth speech and the predicted speech, Mel-Cepstral Distortion (MCD) quantifies the distance between two MFCC vectors. MCD-DTW applies Dynamic Time Warping (DTW) to compute the minimum MCD. MCD-DTW-SL, weighted by speech length, evaluates duration synchronization by incorporating a penalty term to capture local and global temporal similarities.\nFor speech-visual alignment, Lip Sync Error Distance (LSE-D) and Lip Sync Error Confidence (LSE-C), computed using the pre-trained SyncNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chung &amp; Zisserman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib7\" title=\"\">2016</a>)</cite> model, assess lip synchronization accuracy and confidence, respectively.</p>\n\n",
                "matched_terms": [
                    "pretrained",
                    "score",
                    "model",
                    "visual",
                    "synchronization",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work presents a unified flow model integrating video-to-sound (V2S) and visual text-to-speech (VisualTTS) tasks, establishing a new paradigm for video-conditioned sound and speech generation. Our framework demonstrates an effective condition aggregation mechanism for incorporating speech and video conditions into the DiT architecture. Besides, we reveal a mutual boosting effect of sound-speech joint learning through analysis, highlighting the value of a unified generation model.\nThere are several directions that merit further exploration. First, the scarcity of high-quality video-speech-sound data limits the development of unified generative models. Additionally, developing better representation methods for sound and speech, which can preserve speech details while maintaining compact dimensions, is a critical future challenge.</p>\n\n",
                "matched_terms": [
                    "condition",
                    "flow",
                    "model",
                    "representation",
                    "visual",
                    "generation",
                    "data",
                    "sound",
                    "v2s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The use of synthesized sound-speech mixed data offers a viable approach to joint generation. However, its effectiveness is likely inferior to that of native sound-speech joint data. In future, when evaluation and data are sufficient, these synthesized data can be replaced to further enhance performance.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "evaluation",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, large language models (LLMs) were employed only as a writing assistant to refine drafts, improve clarity, and suggest structural improvements to the manuscript.\nHowever, all core research contributions&#8212;including model design, mathematical formulations, and experimental analyses&#8212;were developed independently by the authors. The authors take full responsibility for the final content, ensuring no plagiarism or fabrication occurred. This usage did not warrant authorship attribution to the LLM, as it remained auxiliary to human-led innovation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "not"
                ]
            }
        ]
    },
    "S4.T2": {
        "caption": "Table 2: \nVisualTTS evaluation results on Chem and GRID benchmark. For each metric, the highest score is in bold and the second-highest score is underlined.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding:-0.75pt 1.0pt;\">Bench</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding:-0.75pt 1.0pt;\">Method</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-0.75pt 1.0pt;\"><math alttext=\"\\text{WER}\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mrow><mtext>WER</mtext><mo stretchy=\"false\">&#8595;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">\\text{WER}\\downarrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-0.75pt 1.0pt;\"><math alttext=\"\\text{Spk. Sim.}\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mrow><mtext>Spk. Sim.</mtext><mo stretchy=\"false\">&#8593;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">\\text{Spk. Sim.}\\uparrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-0.75pt 1.0pt;\"><math alttext=\"\\text{UTMOS}\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mrow><mtext>UTMOS</mtext><mo stretchy=\"false\">&#8593;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">\\text{UTMOS}\\uparrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-0.75pt 1.0pt;\"><math alttext=\"\\text{MCD}\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mrow><mtext>MCD</mtext><mo stretchy=\"false\">&#8595;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">\\text{MCD}\\downarrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-0.75pt 1.0pt;\"><math alttext=\"\\text{MCD-D.}\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mrow><mtext>MCD-D.</mtext><mo stretchy=\"false\">&#8595;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">\\text{MCD-D.}\\downarrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-0.75pt 1.0pt;\"><math alttext=\"\\text{MCD-DS.}\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mrow><mtext>MCD-DS.</mtext><mo stretchy=\"false\">&#8595;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">\\text{MCD-DS.}\\downarrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-0.75pt 1.0pt;\"><math alttext=\"\\text{LSE-C}\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mrow><mtext>LSE-C</mtext><mo stretchy=\"false\">&#8593;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">\\text{LSE-C}\\uparrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-0.75pt 1.0pt;\"><math alttext=\"\\text{LSE-D}\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mrow><mtext>LSE-D</mtext><mo stretchy=\"false\">&#8595;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">\\text{LSE-D}\\downarrow</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"7\" style=\"padding:-0.75pt 1.0pt;\">Chem</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\">GT</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\">3.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\">100</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\">4.19</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\">0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\">0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\">0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\">7.66</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\">6.88</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding:-0.75pt 1.0pt;\">GT-vocoder</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">3.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">93.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">3.19</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">3.33</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">2.67</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">2.67</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">7.57</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">6.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\">DSU</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\">37.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\">72.9</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.33</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">10.52</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\">6.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\">6.73</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\">5.88</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">7.86</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding:-0.75pt 1.0pt;\">HPMDubbing</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">22.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">44.6</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">3.11</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">11.54</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">5.19</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">6.30</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_font_bold\">7.30</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_font_bold\">7.58</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding:-0.75pt 1.0pt;\">StyleDubber</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">16.3</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">73.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">3.14</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">14.46</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">6.01</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">6.36</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">3.58</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">11.08</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding:-0.75pt 1.0pt;\">EmoDubber</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">16.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">78.1</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.87</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">14.87</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">5.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">7.16</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">5.48</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">8.27</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;padding:-0.75pt 1.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">VSSFlow</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">15.1</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">79.7</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;padding:-0.75pt 1.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">3.17</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">9.55</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">5.18</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">5.19</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#E6E6E6;\">6.1</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;padding:-0.75pt 1.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">8.37</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_tt\" rowspan=\"7\" style=\"padding:-0.75pt 1.0pt;\">GRID</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding:-0.75pt 1.0pt;\">GT</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-0.75pt 1.0pt;\">12.9</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-0.75pt 1.0pt;\">100</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-0.75pt 1.0pt;\">4.04</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-0.75pt 1.0pt;\">0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-0.75pt 1.0pt;\">0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-0.75pt 1.0pt;\">0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-0.75pt 1.0pt;\">5.49</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-0.75pt 1.0pt;\">8.56 <span class=\"ltx_rule\" style=\"width:0.0pt;height:10.0pt;--ltx-bg-color:black;display:inline-block;\"/>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding:-0.75pt 1.0pt;\">GT-vocoder</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">13.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">64.1</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">3.37</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">5.02</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">3.88</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">3.89</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">6.83</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">8.16</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\">DSU</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\">34.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\">5.9</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\">3.55</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\">13.82</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\">10.55</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\">10.57</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\">5.63</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">8.73</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding:-0.75pt 1.0pt;\">HPMDubbing</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">27.6</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">31.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">2.11</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">12.31</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">8.05</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">8.23</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">6.02</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">8.85</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding:-0.75pt 1.0pt;\">StyleDubber</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_font_bold\">10.9</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">51.4</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.74</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">12.85</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">7.81</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">7.91</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">6.33</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">8.77</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding:-0.75pt 1.0pt;\">EmoDubber</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">15.9</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">50.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.98</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">15.52</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">5.89</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">9.83</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">3.48</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.75pt 1.0pt;\">10.35</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;padding:-0.75pt 1.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">VSSFlow</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;padding:-0.75pt 1.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">18.2</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">51.5</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;padding:-0.75pt 1.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">3.31</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">8.66</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">5.23</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">5.23</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">6.37</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;padding:-0.75pt 1.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">8.6</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "hpmdubbing",
            "gtvocoder",
            "chem",
            "evaluation",
            "each",
            "secondhighest",
            "styledubber",
            "utmos↑textutmosuparrow",
            "lsec↑textlsecuparrow",
            "spk",
            "wer↓textwerdownarrow",
            "bench",
            "simuparrow",
            "mcd↓textmcddownarrow",
            "dsu",
            "visualtts",
            "mcdd↓textmcdddownarrow",
            "bold",
            "emodubber",
            "grid",
            "results",
            "underlined",
            "vssflow",
            "metric",
            "benchmark",
            "score",
            "lsed↓textlseddownarrow",
            "highest",
            "sim↑textspk",
            "method",
            "mcdds↓textmcddsdownarrow"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.T1\" title=\"Table 1 &#8227; Findings. &#8227; 4.3 Evaluations on Joint Learning of Sound and Speech Generation &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents sound generation results on VGGSound benchmark, where VSSFlow matches SOTA performance. Versus Frieren, VSSFlow excels in sound quality and sound-visual semantic alignment, though slightly weaker in temporal alignment. Against LoVA, VSSFlow &#8212; with fewer parameters &#8212; matches sound quality and outperforms in temporal and semantic alignment.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.T2\" title=\"Table 2 &#8227; Findings. &#8227; 4.3 Evaluations on Joint Learning of Sound and Speech Generation &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the VisualTTS results on Chem and GRID benchmarks. VSSFlow achieves strong results in Spk.Sim., MCD, and LSE, demonstrating its effective capture of speaker traits and generation of lip-synced, high-fidelity speech.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Video-conditioned sound and speech generation, encompassing video-to-sound\n(V2S)\nand visual text-to-speech (VisualTTS) tasks, are conventionally addressed as separate tasks, with limited exploration to unify them within a signle framework.\nRecent attempts to unify V2S and VisualTTS face challenges in handling distinct condition types (e.g., heterogeneous video and transcript conditions) and require complex training stages. Unifying these two tasks remains an open problem.\nTo bridge this gap, we present <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">VSSFlow</span>, which seamlessly integrates both <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">V2S</span> and <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">VisualTTS</span> tasks into a unified <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">flow</span>-matching framework.\nVSSFlow uses a novel condition aggregation mechanism to handle distinct input signals.\nWe find that cross-attention and self-attention layer exhibit different inductive biases in the process of introducing condition. Therefore, VSSFlow leverages these inductive biases to effectively handle different representations:\ncross-attention for ambiguous video conditions and self-attention for more deterministic speech transcripts.\nFurthermore, contrary to the prevailing belief that joint training on the two tasks requires complex training strategies and may degrade performance, we find that VSSFlow benefits from the end-to-end joint learning process for sound and speech generation without extra designs on training stages. Detailed analysis attributes it to the learned general audio prior shared between tasks, which accelerates convergence, enhances conditional generation, and stabilizes the classifier-free guidance process.\nExtensive experiments demonstrate that VSSFlow surpasses the state-of-the-art domain-specific baselines on both V2S and VisualTTS benchmarks, underscoring the critical potential of unified generative models.</p>\n\n",
                "matched_terms": [
                    "vssflow",
                    "visualtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges, we introduce VSSFlow, a flow-based framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib32\" title=\"\">2022</a>)</cite> that unifies V2S and VisualTTS generation.\nFirstly, to handle both video and transcript conditions, we systematically explore the optimal conditioning mechanism within the Diffusion Transformer (DiT)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peebles &amp; Xie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib41\" title=\"\">2022</a>)</cite> block. The two types of conditions exhibit inherently different characteristics: speech transcripts provide more deterministic guidance for speech generation, whereas video features are more ambiguous\nfor sound generation.\nAlso, two typical types of conditioning mechanisms lead to distinct inductive biases, i.e., cross-attention conditioning\nprocesses ambiguous video features well,\nwhile in-context/self-attention conditioning better exploits\ntranscript features. Accordingly, VSSFlow integrates transcript embeddings by concatenating with audio latents and processing them through self-attention, while incorporating video representations via cross-attention layers.\nSecondly, under our VSSFlow framework, we observe that no complex design for training stages is necessary. The joint learning of sound and speech does not lead to interference or performance degradation, a common issue in multitask learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kendall et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib27\" title=\"\">2018</a>; Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib52\" title=\"\">2025</a>)</cite>. Instead, it results in mutual enhancement on both tasks. We conduct analysis and reveal that this improvement stems from the model&#8217;s ability to learn general audio knowledge, which leads to faster convergence, better conditioned generation results, and more stable classifier-free guidance process.</p>\n\n",
                "matched_terms": [
                    "vssflow",
                    "results",
                    "visualtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond unifying V2S and VisualTTS tasks, real-world scenarios often require generating sound and speech jointly. We further apply continual training on the VSSFlow model with synthetic sound-speech mixed data (i.e., speech with environmental sounds).\nAs shown in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S0.F1\" title=\"Figure 1 &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(c), the model rapidly adapts to this setting, successfully generating mixed outputs for out-of-domain videos &#8212;e.g., generating car engine sounds alongside a police man speaking clearly.</p>\n\n",
                "matched_terms": [
                    "vssflow",
                    "visualtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evaluations on V2S and VisualTTS benchmarks show that VSSFlow achieves exceptional sound fidelity and speech quality. VSSFlow lays a powerful foundation for video-conditioned sound and speech generation. In summary, our contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "vssflow",
                    "visualtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(1) We introduce VSSFlow, a unified framework for video-conditioned sound and speech generation, with an effective condition aggregation mechanism for integrating video and speech features into DiT blocks and handling both V2S and VisualTTS tasks.\n(2) Through systematic analysis, we demonstrate that under the flow model&#8217;s end-to-end training setting, the joint learning of sound and speech generation produces a mutual promotion effect, highlighting the critical role of unified models in the sound and speech generation field.\n(3) Extensive evaluations confirm VSSFlow&#8217;s superior performance, surpassing the SOTA domain-specific baselines on V2S and VisualTTS benchmarks.</p>\n\n",
                "matched_terms": [
                    "vssflow",
                    "visualtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The interplay between sound and speech generation poses a critical question for unified audio generation, yet it remains underexplored in prior work. Are sound and speech, as distinct audio modalities, entirely separate tasks, or can they be effectively learned jointly? A common belief is that end-to-end joint training degrades generation performance compared to curriculum learning approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib52\" title=\"\">2025</a>)</cite>. In contrast, our proposed VSSFlow framework offers a novel perspective: end-to-end joint learning of sound and speech within a flow-matching framework does not lead to mutual suppression. Instead, it fosters a synergistic effect, driven by the shared knowledge between the sound and speech modalities. By leveraging a tailored condition aggregation mechanism and end-to-end joint learning, VSSFlow effectively balances the distinct requirements of V2S and VisualTTS tasks, achieving superior performance across diverse video contexts.</p>\n\n",
                "matched_terms": [
                    "vssflow",
                    "visualtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S3.F2\" title=\"Figure 2 &#8227; Classifier-Free Guidance. &#8227; 3.1 Preliminary &#8227; 3 method &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, VSSFlow adopts 10-layer cross-attention-based DiT for unified video-conditioned sound and speech generation. Each DiT block follows the implementation in Stable Audio Open&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Evans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib15\" title=\"\">2025</a>)</cite>. Besides, to better capture temporal relationships between the sound sequence and video frame sequence in the V2S process, we incorporate 1D Rotary Position Embedding (RoPE)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Su et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib50\" title=\"\">2024a</a>)</cite> into the query and key matrix in both self- and cross-attention blocks. The waveform is converted to a melspectrogram and processed by the VAE encoder to get <math alttext=\"x_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><msub><mi>x</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">x_{0}</annotation></semantics></math>. The denoised latent <math alttext=\"x^{\\prime}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mn>1</mn><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">x^{\\prime}_{1}</annotation></semantics></math> is decoded to a melspectrogram by the VAE decoder and then converted into a waveform via a vocoder. More details can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A1\" title=\"Appendix A Condition Representations &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "vssflow"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the experiment section, we respond to the unaddressed challenges mentioned above: (1) How to build a unified end-to-end framework for video-conditioned sound and speech generation, and find the optimal method to integrate different types of conditions? (2) Does joint training of speech and sound affect each other&#8217;s generation performance, and is this impact positive or negative?\nVSSFlow is trained end-to-end on multiple sound and speech datasets using the flow loss function defined in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S3.E2\" title=\"In Flow-Matching Framework. &#8227; 3.1 Preliminary &#8227; 3 method &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. The data covers three tasks: (1) Video-to-sound (V2S), with phoneme conditions <math alttext=\"c_{p}=0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>c</mi><mi>p</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">c_{p}=0</annotation></semantics></math>; (2) Visual test-to-speech (VisualTTS), with both <math alttext=\"c_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>c</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">c_{p}</annotation></semantics></math> and <math alttext=\"c_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>c</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">c_{v}</annotation></semantics></math> are active; And (3) text-to-speech (TTS), with <math alttext=\"c_{v}=0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>c</mi><mi>v</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">c_{v}=0</annotation></semantics></math>. See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A3\" title=\"Appendix C Dataset and Experiments Details &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for more details about the dataset and training settings.</p>\n\n",
                "matched_terms": [
                    "method",
                    "each",
                    "vssflow",
                    "visualtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first explore optimal mechanisms for integrating video and speech conditions, training four variants (illustrated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S3.SS2\" title=\"3.2 VSSFlow Framework &#8227; 3 method &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>) for 100 epochs and evaluating them on both V2S and VisualTTS tasks. The main evaluation results are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.F3\" title=\"Figure 3 &#8227; 4.1 Experiments Setup &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, and additional metrics can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A2.SS1\" title=\"B.1 Ablations on Condition Mechanism &#8227; Appendix B Experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>.\nBased on the results, we draw the following key findings:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "visualtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further investigate the impact of combining sound and speech data during training. We train VSSFlow under various data configurations, with primary results shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.F4\" title=\"Figure 4 &#8227; 4.2 Comparison of Condition Aggregation Mechanisms &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. Additional metrics are shown in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A2.SS2\" title=\"B.2 Ablations on Joint Training &#8227; Appendix B Experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "vssflow"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For V2S, we compare three settings: (1) V2S data only, (2) V2S + VisualTTS data, and (3) V2S + TTS data. Results in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.F4\" title=\"Figure 4 &#8227; 4.2 Comparison of Condition Aggregation Mechanisms &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(a.1) show that incorporating speech data &#8212; whether visual modality is included or not (VisualTTS or TTS) &#8212; enhances sound generation performance. Models trained with both speech and sound data significantly outperform the V2S-only baseline at equivalent training steps.</p>\n\n",
                "matched_terms": [
                    "results",
                    "visualtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For VisualTTS, we train VSSFlow with two data settings: (1) VisualTTS data only and (2) V2S + VisualTTS data. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.F4\" title=\"Figure 4 &#8227; 4.2 Comparison of Condition Aggregation Mechanisms &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(b) shows that the two variants have comparable speech generation performance at the same training step, indicating that joint training with sound does not impair speech generation performance or convergence speed. Moreover, the model with extra sound training data achieves better convergence performance after the same number of epochs.</p>\n\n",
                "matched_terms": [
                    "vssflow",
                    "visualtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the findings in section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.SS2\" title=\"4.2 Comparison of Condition Aggregation Mechanisms &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a> and &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.SS3\" title=\"4.3 Evaluations on Joint Learning of Sound and Speech Generation &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>, the reported VSSFlow model is based on architecture CrossV, trained for 150 epochs using a total of 503k data from V2S, TTS, and VisualTTS datasets. For the V2S generation, we evaluate the model&#8217;s performance on the standard VGGSound benchmark. For VisualTTS generation, we use widely adopted Chem and GRID benchmarks.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "chem",
                    "grid",
                    "vssflow",
                    "visualtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For V2S evaluation, we compare VSSFlow against baselines from autoregressive, mask-based, diffusion, and flow-based paradigms as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.T1\" title=\"Table 1 &#8227; Findings. &#8227; 4.3 Evaluations on Joint Learning of Sound and Speech Generation &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Our primary baselines are <span class=\"ltx_text ltx_font_bold\">Frieren</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib59\" title=\"\">2024c</a>)</cite>, which has comparable parameters, flow matching paradigm, and uses the same V2S dataset. And <span class=\"ltx_text ltx_font_bold\">LoVA</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib5\" title=\"\">2024</a>)</cite>, which uses more parameters, diffusion paradigm, and is fully initialized from Stable Audio Open.\nFor sound quality, we use Fr&#233;chet Audio Distance (FAD)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kilgour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib28\" title=\"\">2018</a>)</cite>, Inception Score (IS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Salimans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib46\" title=\"\">2016</a>)</cite>, and Mean KL-Divergence (KL). For temporal alignment, we report wildly-adopted onset accuracy (Onset Acc.) and onset average precision (Onset AP) following prior works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib63\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib14\" title=\"\">2023</a>)</cite>, plus DeSync Score following&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib4\" title=\"\">2025</a>)</cite>. See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A4\" title=\"Appendix D V2S benchmark &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">D</span></a> for details.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "score",
                    "vssflow"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For VisualTTS evaluation, we compare VSSFlow against four SOTA baselines: DSU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib36\" title=\"\">2023</a>)</cite>, HPMDubbing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib8\" title=\"\">2023</a>)</cite>, StyleDubber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib10\" title=\"\">2024b</a>)</cite>, and EmoDubber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib9\" title=\"\">2024a</a>)</cite>. For speech quality, we use Word Error Rate (WER) and UTokyo-SaruLab Mean Opinion Score (UTMOS). For speech alignment, we compute MCD, MCD-DTW, and MCD-DTW-SL. For speech-visual alignment, we report Lip Sync Error Distance (LSE-D) and Lip Sync Error Confidence (LSE-C). See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A5\" title=\"Appendix E VisualTTS benchmark &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">E</span></a> for details.</p>\n\n",
                "matched_terms": [
                    "hpmdubbing",
                    "score",
                    "dsu",
                    "emodubber",
                    "evaluation",
                    "vssflow",
                    "styledubber",
                    "visualtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For V2S generation, the experiments highlight three key factors that affect the performance: video representations, training data quality, and model paradigm. First, CLIP features yield robust semantic representations for sound quality and semantic alignment but lack temporality, addressable via features like CAVP or Synchformer. Second, high-quality text-to-sound data or frozen text-to-sound modules further enhance results. Third, flow/diffusion and mask models outperform autoregressive ones.\nFor VisualTTS, the metrics validate that a simple unified model suffices for VisualTTS generation without a complex data preprocessing pipeline. However, the use of VAE-vocoder incurs detail loss, reducing UTMOS versus ground truth (as seen in GT-vocoder baselines).</p>\n\n",
                "matched_terms": [
                    "gtvocoder",
                    "results",
                    "visualtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VSSFlow shows excellent performance in V2S and VisualTTS tasks, but lacks the inherent ability of sound-speech joint generation. We stimulate this capability by fine-tuning pretrained VSSFlow model with &#160;170k synthetic sound-speech mixtures, created via randomized concatenation or overlay of data from VGGSound and LRS2. Fine-tuning the base model for only 15 epochs yields a model capable of jointly generating sound and speech aligned with visual content. We evaluate its out-of-domain performance using videos from Veo3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Google, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib18\" title=\"\">2025</a>)</cite>. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S0.F1\" title=\"Figure 1 &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(c), the model accurately generates a car driving sound and a police officer&#8217;s speech, along with the braking sound in the background. The generated speech is precisely aligned with the character&#8217;s lip movements, timbre (adult male), and emotional tone (slightly excited). We politely invite you to visit our project page<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Project Page: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://vasflow1.github.io/vasflow/\" title=\"\">https://vasflow1.github.io/vasflow/</a></span></span></span> for more playable demos. These results demonstrate VSSFlow&#8217;s capability to process video signals and transcripts simultaneously, and the capability to generate the environmental sound alongside clear and accurate speech jointly.</p>\n\n",
                "matched_terms": [
                    "vssflow",
                    "results",
                    "visualtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate reproducibility, this paper provides detailed descriptions of the VSSFlow architecture (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S3.SS2\" title=\"3.2 VSSFlow Framework &#8227; 3 method &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>). data preprocessing steps (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A1\" title=\"Appendix A Condition Representations &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>) and splitting method (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A3\" title=\"Appendix C Dataset and Experiments Details &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>), evaluation metrics (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A4\" title=\"Appendix D V2S benchmark &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">D</span></a> and &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A5\" title=\"Appendix E VisualTTS benchmark &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>), training procedures with additional implementation details (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.SS1\" title=\"4.1 Experiments Setup &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A3\" title=\"Appendix C Dataset and Experiments Details &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>), extensive ablation results (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#A2\" title=\"Appendix B Experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>), etc.\nIn addition, to facilitate the replication and further research by the academic community, we promise to release the training code and checkpoint weights in our <a class=\"ltx_ref ltx_href\" href=\"https://vasflow1.github.io/vasflow/\" title=\"\">project page</a> as soon as possible.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "method",
                    "results",
                    "vssflow"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure temporal alignment across modalities, video features <math alttext=\"c_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m1\" intent=\":literal\"><semantics><msub><mi>c</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">c_{v}</annotation></semantics></math> are linearly interpolated to match the latent audio length <math alttext=\"T_{v}=T_{a}=250\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m2\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mi>v</mi></msub><mo>=</mo><msub><mi>T</mi><mi>a</mi></msub><mo>=</mo><mn>250</mn></mrow><annotation encoding=\"application/x-tex\">T_{v}=T_{a}=250</annotation></semantics></math>, yielding the final <math alttext=\"c_{v}\\in\\mathbb{R}^{250\\times 768}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mi>c</mi><mi>v</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>250</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>768</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">c_{v}\\in\\mathbb{R}^{250\\times 768}</annotation></semantics></math>. For the phoneme-based speech condition <math alttext=\"c_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m4\" intent=\":literal\"><semantics><msub><mi>c</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">c_{p}</annotation></semantics></math>, inspired by prior works in text-to-speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chien et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib6\" title=\"\">2021</a>; Cong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib9\" title=\"\">2024a</a>)</cite>, we train a duration predictor that takes the phoneme sequence and AV-HuBERT features&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib48\" title=\"\">2022</a>)</cite> from video, using an attention mechanism to produce a phoneme-to-frame alignment matrix from which durations are derived. The predictor enables us to repeat and pad phoneme embeddings to obtain <math alttext=\"T_{p}=T_{a}=250\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mi>p</mi></msub><mo>=</mo><msub><mi>T</mi><mi>a</mi></msub><mo>=</mo><mn>250</mn></mrow><annotation encoding=\"application/x-tex\">T_{p}=T_{a}=250</annotation></semantics></math>. This results in temporal-aligned phoneme features <math alttext=\"c_{p}\\in\\mathbb{R}^{250\\times 32}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m6\" intent=\":literal\"><semantics><mrow><msub><mi>c</mi><mi>p</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>250</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>32</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">c_{p}\\in\\mathbb{R}^{250\\times 32}</annotation></semantics></math>.\nTo align with prior VisualTTS baselines, we extract speaker embeddings from reference speech using RawNet3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib26\" title=\"\">2022</a>)</cite>, optionally prefixing them to <math alttext=\"c_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m7\" intent=\":literal\"><semantics><msub><mi>c</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">c_{v}</annotation></semantics></math> for speaker consistency.</p>\n\n",
                "matched_terms": [
                    "results",
                    "visualtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">VisualTTS Datasets.</span> We employ three standard VisualTTS datasets: <span class=\"ltx_text ltx_font_italic\">Chem</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Prajwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib42\" title=\"\">2020</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">GRID</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cooke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib12\" title=\"\">2006</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">LRS2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Afouras et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib1\" title=\"\">2018</a>)</cite>, comprising a total of <span class=\"ltx_text ltx_font_bold\">162k</span> training samples. Chem is a single-speaker English dataset of chemistry lecture recordings. Following prior work&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib8\" title=\"\">2023</a>)</cite>, we use 6,240 samples for training and 200 for testing. GRID is a multi-speaker English dataset consisting of 33 speakers, each contributing 1,000 utterances. In line with&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib9\" title=\"\">2024a</a>)</cite>, we select 100 segments per speaker for testing, resulting in 29,600 training samples and 3,291 test samples. LRS2 is a diverse sentence-level dataset collected from BBC television programs, providing real-world variability. It contains 126k training samples and 700 test samples.</p>\n\n",
                "matched_terms": [
                    "chem",
                    "each",
                    "grid",
                    "visualtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the video-to-sound performance on the standard VGGSound benchmark. We compare VSSFlow against baselines representing different paradigms, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#S4.T1\" title=\"Table 1 &#8227; Findings. &#8227; 4.3 Evaluations on Joint Learning of Sound and Speech Generation &#8227; 4 experiments &#8227; VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Autoregressive baselines include SpecVQGan&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Iashin &amp; Rahtu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib23\" title=\"\">2021</a>)</cite>, Im2Wav&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sheffer &amp; Adi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib47\" title=\"\">2022</a>)</cite> and V-AURA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Viertola et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib53\" title=\"\">2025</a>)</cite>. Mask-based baselines include VAB&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pascual et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib40\" title=\"\">2024</a>)</cite>. Diffusion baselines include Difffoley&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib37\" title=\"\">2023</a>)</cite>, Seeing&amp;Hearing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xing et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib60\" title=\"\">2024</a>)</cite>, V2A-Mapper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib56\" title=\"\">2024a</a>)</cite>, FoleyCrafter&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib63\" title=\"\">2024</a>)</cite>, TiVA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib58\" title=\"\">2024b</a>)</cite> and LoVA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib5\" title=\"\">2024</a>)</cite>. Flow-based approaches include Frieren&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib59\" title=\"\">2024c</a>)</cite> and MMAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib4\" title=\"\">2025</a>)</cite>. The baseline results are obtained either through official code execution or from released generated sounds. All sound samples are padded to 10 seconds for consistency. Since V-AURA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Viertola et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib53\" title=\"\">2025</a>)</cite> generates 2.56-second clips, we repeat each generated clip three times before padding it to 10 seconds.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "each",
                    "results",
                    "vssflow"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Consistent with prior work&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Iashin &amp; Rahtu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib23\" title=\"\">2021</a>; Sheffer &amp; Adi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib47\" title=\"\">2022</a>; Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib37\" title=\"\">2023</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib33\" title=\"\">2023</a>)</cite>, we adopt widely used metrics, including Fr&#233;chet Audio Distance (FAD)<cite class=\"ltx_cite ltx_citemacro_citep\">(Kilgour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib28\" title=\"\">2018</a>)</cite>, Inception Score (IS)<cite class=\"ltx_cite ltx_citemacro_citep\">(Salimans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib46\" title=\"\">2016</a>)</cite>, and Mean KL-Divergence (KL), to assess the quality of generated sound. To mitigate the impact of varying sampling rates, we first downsample the generated sound to 16 kHz and then resample it to the required rates for the respective audio classifiers (16 kHz for VGGish&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib20\" title=\"\">2017</a>)</cite>, 32 kHz for PaSST&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Koutini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib31\" title=\"\">2022</a>)</cite>, and 32 kHz for PANN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib30\" title=\"\">2020b</a>)</cite>).\nFor temporal alignment evaluation, we use Onset Accuracy (Onset Acc.) and Onset Average Precision (Onset AP), following&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib63\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib14\" title=\"\">2023</a>)</cite>. Additionally, inspired by&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib4\" title=\"\">2025</a>)</cite>, we extract features using the SynchFormer model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Iashin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib24\" title=\"\">2024</a>)</cite> to compute offsets, producing the DeSync Score. To quantify soundvisual relevance, we calculate the cosine similarity between the embeddings of the input video and the generated sound using the ImageBind&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Girdhar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib17\" title=\"\">2023</a>)</cite> model (VA-IB).</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate VSSFlow&#8217;s performance against other VisualTTS baselines on the widely adopted Chem and GRID datasets. During generation, we randomly select a speech sample from the same speaker as the reference. We compare VSSFlow against four SOTA baselines: DSU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib36\" title=\"\">2023</a>)</cite>, HPMDubbing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib8\" title=\"\">2023</a>)</cite>, StyleDubber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib10\" title=\"\">2024b</a>)</cite> and EmoDubber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib9\" title=\"\">2024a</a>)</cite>.\nThe baseline data is either generated from the official implementation or from author-released results. We also obtain the results of the GT-vocoder by compressing the ground truth mel-spectrogram into the latent space via a VAE encoder, reconstructing it through a VAE decoder, and then using a vocoder to recover the waveform. Theoretically, the indicators in this row represent the upper limit for VSSFlow.\n</p>\n\n",
                "matched_terms": [
                    "hpmdubbing",
                    "dsu",
                    "gtvocoder",
                    "chem",
                    "emodubber",
                    "grid",
                    "results",
                    "vssflow",
                    "styledubber",
                    "visualtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate visual text-to-speech (VisualTTS) performance using several metrics. For overall speech quality, Word Error Rate (WER) measures intelligibility using the Whisper-V3 model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib44\" title=\"\">2023</a>)</cite>, while the UTokyo-SaruLab Mean Opinion Score (UTMOS)<cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib45\" title=\"\">2022</a>)</cite>, computed via Versa<cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib49\" title=\"\">2024</a>)</cite>, assesses clarity, naturalness, and fluency. Speaker similarity measures the similarity of two speech samples, also computed via Versa.\nFor speech alignment between the ground truth speech and the predicted speech, Mel-Cepstral Distortion (MCD) quantifies the distance between two MFCC vectors. MCD-DTW applies Dynamic Time Warping (DTW) to compute the minimum MCD. MCD-DTW-SL, weighted by speech length, evaluates duration synchronization by incorporating a penalty term to capture local and global temporal similarities.\nFor speech-visual alignment, Lip Sync Error Distance (LSE-D) and Lip Sync Error Confidence (LSE-C), computed using the pre-trained SyncNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chung &amp; Zisserman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24773v2#bib.bib7\" title=\"\">2016</a>)</cite> model, assess lip synchronization accuracy and confidence, respectively.</p>\n\n",
                "matched_terms": [
                    "score",
                    "visualtts"
                ]
            }
        ]
    }
}