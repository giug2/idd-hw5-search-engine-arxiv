{
    "S1.F2": {
        "source_file": "Seeing What You Say: Expressive Image Generation from Speech",
        "caption": "Figure 2: \n(a) The cascaded system consisting of ASR and T2I, and (b) audio-to-text feature mapping-based methodsÂ [29, 52] limits in cost than (c) VoxStudio (ours).\nThe diffusion process is excluded from GFLOPs and time computations. The parameters of the image generator are also excluded from Params.",
        "body": "(a) Cascaded\n(b) Mapping\n\n(c) VoxStudio\n\n\n\nInference time\n654.3ms\n23.8ms\n22.2ms\n\n\nGFLOPS\n4919G\n154G\n128G\n\n\n# Params.\n2.36B\n1.2B\n0.64B",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">(a) Cascaded</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">(b) Mapping</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">(c) </span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">VoxStudio</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Inference time</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">654.3ms</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.8ms</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">22.2ms</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">GFLOPS</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4919G</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">154G</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">128G</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\"># Params.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.36B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.2B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.64B</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "mapping",
            "4919g",
            "ours",
            "voxstudio",
            "064b",
            "process",
            "audiototext",
            "also",
            "12b",
            "cost",
            "cascaded",
            "6543ms",
            "238ms",
            "from",
            "128g",
            "methods",
            "consisting",
            "system",
            "154g",
            "gflops",
            "params",
            "limits",
            "diffusion",
            "image",
            "than",
            "generator",
            "computations",
            "excluded",
            "asr",
            "feature",
            "236b",
            "time",
            "t2i",
            "mappingbased",
            "inference",
            "222ms",
            "parameters"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Recent advances in text-to-image (T2I) generation have demonstrated remarkable progress, but they struggle to utilize the innate expressiveness and accessibility of speech.\nMost cascaded framework&#8212;where an utterance is first transcribed into text or textual feature and then used as input for T2I models, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S1.F2\" title=\"In 1 Introduction &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> (a, b)&#8212;encounters several significant challenges.\nFirst, speech-to-text (<em class=\"ltx_emph ltx_font_italic\">i.e</em>., ASR) transcription is limited to capturing prosody and speaker intention.\nHowever, transcription errors propagate into the image generative model, then degrade visual quality.\nSecond, this sequential approach inherently decouples speech and image generation, making it difficult to transfer crucial prosodic and temporal cues&#8212;such as speaking rate, pitch variation, and emotional style&#8212;that can influence the mood, color palette, or overall aesthetic of the generated image.\nRelying on intermediate text also excludes languages without written forms&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib50\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">50</span></a>]</cite>.\nEven for languages that do have a writing system, coverage for the cascaded approach remains far from comprehensive: there are over 7,100 languages worldwide&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib13\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">13</span></a>]</cite>, <em class=\"ltx_emph ltx_font_italic\">e.g</em>. Google API covers only 125<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1a\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://cloud.google.com/speech-to-text</span></span></span>.\nFinally, the cascaded system limits the inference speed and requires a higher cost than our unified system, as shown in the table of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S1.F2\" title=\"In 1 Introduction &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nThese limitations underscore the necessity of an end-to-end approach that directly maps raw speech to images, enabling a more seamless and expressive integration of modalities.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This paper proposes <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, the first unified and end-to-end speech-to-image model that generates expressive images directly from spoken descriptions by jointly aligning linguistic and paralinguistic information.\nAt its core is a speech information bottleneck (SIB) module, which compresses raw speech into compact semantic tokens, preserving prosody and emotional nuance.\nBy operating directly on these tokens, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> eliminates the need for an additional speech-to-text system, which often ignores the hidden details beyond text, <em class=\"ltx_emph ltx_font_italic\">e.g</em>., tone or emotion.\nWe also release VoxEmoset, a large&#8208;scale paired emotional speech&#8211;image dataset built via an advanced TTS engine to affordably generate richly expressive utterances.\nComprehensive experiments on the SpokenCOCO, Flickr8kAudio, and VoxEmoset benchmarks demonstrate the feasibility of our method and highlight key challenges, including emotional consistency and linguistic ambiguity, paving the way for future research.\n<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\"/><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\"/><span class=\"ltx_tag ltx_tag_note\"/><sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup> Partly work done in NAVER AI Lab.\n<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> Corresponding author.</span></span></span></p>\n\n",
                "matched_terms": [
                    "system",
                    "from",
                    "also",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans naturally &#8220;imagine&#8221; vivid mental images when listening to speech, which conveys not only semantics but also emotion, tone, and intent.\nSpeech-to-image (S2I) generation taps this rich, multimodal expressiveness to produce visuals that are more nuanced and emotionally resonant than those driven by text alone. By translating spoken descriptions directly into images, S2I can unlock applications in accessibility, creative media, and voice&#8208;driven interfaces&#8212;treating speech as a first&#8208;class modality for content creation rather than a mere precursor to text.</p>\n\n",
                "matched_terms": [
                    "also",
                    "than"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, incorporating speech input directly into a pre-trained T2I model poses distinct obstacles rooted in the nature of the two modalities.\nSpeech is a continuous, high-dimensional signal rich in temporal dynamics and spectral detail, whereas T2I models are designed to process compact sequences of token embeddings.\nBridging this gap requires effective speech representations that can capture both semantic and paralinguistic cues - yet remain mappable to their latent space.\nThis alignment is complicated by differing tokenization schemes, variable sequence lengths, and unique contextual subtleties inherent to spoken language.</p>\n\n",
                "matched_terms": [
                    "t2i",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, a novel speech-to-image model that bridges the rich information in speech with the image modality space, enabling more diverse and expressive visual representations.\nBuilding upon T2I models, our framework is suitable for the unique characteristics of speech, which differ from text:\n(1) Speech generally contains longer and more variable sequences than text, leading to uneven information density across embeddings.\n(2) Speech signals vary significantly depending on speaker identity, recording environment, and emotional state, affecting articulation and duration, even for the same content.\nTo address these challenges, we introduce a speech information bottleneck (SIB) that efficiently aligns cross-modal latent spaces while preserving key speech features.\nOur SIB encodes compressed conditional features that guide the image generation process.\nThrough extensive experiments, we establish an effective speech-based guidance for image generation by identifying the optimal combination of speech encoder, SIB, and image generator.</p>\n\n",
                "matched_terms": [
                    "t2i",
                    "voxstudio",
                    "from",
                    "image",
                    "than",
                    "process",
                    "generator"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is a unified image generation model with expressive utterance, where both linguistic and paralinguistic cues are compactly captured via the SIB module.</p>\n\n",
                "matched_terms": [
                    "image",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conditions for image generation.</span>\nRecently, diffusion-based conditional image generative models have emerged with remarkable performance <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib44\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">44</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib40\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">40</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib41\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">41</span></a>]</cite>. Specifically, stable diffusion (SD)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite> has shown impressive results in both quality and generalizability.\nGiven that these models only take text as a condition, they have struggled to reflect individual thoughts and emotions beyond text into compelling images.\nSome methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib57\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">57</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite> have proposed emotional image generation, pointing out the importance of reacting to the user&#8217;s sentiment.\nHowever, they relied on explicit linguistic expressions (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., <span class=\"ltx_text ltx_font_italic\">&#8216;with a sense of happiness and joy&#8217;</span> in the text prompt) and focused on reflecting emotion in texture and color only&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib2\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></a>]</cite>.\nRecently, EmoGen&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib59\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">59</span></a>]</cite> and EmoEdit&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib60\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">60</span></a>]</cite> argued that emotional contents beyond color and style should be effectively expressed as semantic variations in a generated image.\nThey learned a more flexible generative model using a large-scale EmoSet dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib58\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">58</span></a>]</cite>, but still required users to explicitly specify emotion prompts.\nIn contrast, our approach automatically infers these nuances directly from the speaker&#8217;s voice.</p>\n\n",
                "matched_terms": [
                    "from",
                    "image",
                    "diffusion",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, speech naturally encodes nuanced emotion and tone&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib50\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">50</span></a>]</cite>, offering a more intuitive means for generating emotionally resonant images, yet it remains largely untapped as a conditioning signal.\nRecent audiovisual generation methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib29\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">29</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">7</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib26\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">26</span></a>]</cite> have been limited to relying only on semantic instances expressed in text, where the other expressions are excluded.\nMoreover, existing approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib54\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">54</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib55\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">55</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite> for S2I generation have used highly limited datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite>, restricting their expressive versatility.\nWe aim to design a unified and emotion-driven S2I framework as well as to introduce a large-scale dataset for both training and evaluation.</p>\n\n",
                "matched_terms": [
                    "methods",
                    "excluded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Relationship between speech and image.</span>\nSpeech-image relationships have been widely explored in biometrics&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib31\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">31</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib39\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">39</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>]</cite>, linguistic alignment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>, and phonetic articulation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib56\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">56</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib9\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">9</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib10\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">10</span></a>]</cite>.\nThese studies provide valuable insights into how speech and vision interact in different contexts.\nAlso, image-speech retrieval&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> has explored the alignment between spoken descriptions and images, highlighting the importance of understanding the semantics in both modalities.\nDespite these advances, most of the existing studies focus on isolated characteristics between modalities.\nBy moving beyond traditional mappings, our work aims to bridge the gap by simultaneously leveraging natural semantic correspondences, <em class=\"ltx_emph ltx_font_italic\">i.e</em>., both linguistic and paralinguistic information, between speech and vision.</p>\n\n",
                "matched_terms": [
                    "image",
                    "also"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S2.F3\" title=\"In 2 Related Work &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the overall framework of <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, consisting of (1) a pretrained speech encoder, (2) SIB to reduce the computational cost and effectively connect heterogeneous two modalities, and (3) an image generator to synthesize an image from the compressed speech representations.\nNext, we describe each module in detail.</p>\n\n",
                "matched_terms": [
                    "cost",
                    "voxstudio",
                    "from",
                    "image",
                    "generator",
                    "consisting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider two pre-trained speech encoders, SONAR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> and Whisper large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib43\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">43</span></a>]</cite>, to deploy comprehensive speech features considering both linguistic and paralinguistic information.\nBriefly reviewed, SONAR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> has learned global semantic alignment between speech and text, enabling it to encode meaning beyond phonetic content.\nWe remove its last aggregation layer to ensure that the speech embeddings retain both linguistic and paralinguistic information.\nWe also test Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib43\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">43</span></a>]</cite>, a widely used speech recognition model. Whisper is known to be capable of capturing paralinguistic information, such as emotion and speaker identity&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib64\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">64</span></a>]</cite>.\nFormally, given an input utterance <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math>, we obtain a speech embedding <math alttext=\"s\\in\\mathrm{R}^{N\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo>&#8712;</mo><msup><mi mathvariant=\"normal\">R</mi><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">s\\in\\mathrm{R}^{N\\times D}</annotation></semantics></math>, where <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> depends on the length of the speech and models, and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> is the channel dimension of the final output layer.\nWe note that our work does not employ ASR system or text encoder to explicitly map the speech into text.</p>\n\n",
                "matched_terms": [
                    "system",
                    "also",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although speech embeddings contain rich representations, they are excessively long and lead to a lower information density in each speech token compared to text (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., Whisper encodes a maximum of 1500 tokens for 30-seconds long speech, while CLIP text encoder limited to 77 tokens).\nThis low density makes direct usage challenging to condition the image generator.\nTo solve this problem, we design a Transformer-based speech information bottleneck (SIB) module.\nSIB compacts semantics in speech embeddings, motivated by previous works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib51\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">51</span></a>]</cite> applied to individual image and audio encoders.\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S2.F3\" title=\"In 2 Related Work &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, SIB reduces the number of embeddings with a strided convolution layer after a Transformer block along the time axis.\nBased on our findings, a pooling ratio of 8 provides the optimal balance, allowing us to maximize the information retention of speech features.\nAs a result, the initial embedding <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> is processed into a compressed speech condition <math alttext=\"c=f_{\\psi}(s)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>=</mo><mrow><msub><mi>f</mi><mi>&#968;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">c=f_{\\psi}(s)</annotation></semantics></math>, where <math alttext=\"c\\in\\mathrm{R}^{M\\times D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>&#8712;</mo><msup><mi mathvariant=\"normal\">R</mi><mrow><mi>M</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>D</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">c\\in\\mathrm{R}^{M\\times D^{\\prime}}</annotation></semantics></math>, <math alttext=\"M=N/8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>=</mo><mrow><mi>N</mi><mo>/</mo><mn>8</mn></mrow></mrow><annotation encoding=\"application/x-tex\">M=N/8</annotation></semantics></math> and <math alttext=\"D^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><msup><mi>D</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">D^{\\prime}</annotation></semantics></math> is the input channel of the cross-attention block in the image generator.\nThose compressed representations improve the efficiency of the S2I process while preserving both linguistic and emotional expressiveness.</p>\n\n",
                "matched_terms": [
                    "generator",
                    "image",
                    "process",
                    "time"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The image generator is based on the latent diffusion model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite>.\nThe speech condition <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math>, compressed through SIB, is fed into the generator as a guidance of the synthesis process.\nSpecifically, the speech embeddings are injected into the UNet through cross-attention layers to condition the image synthesis.\nThis conditioning allows the model to incorporate the emotional, semantic content of speech into the generation process.\nThe image generator and SIB are optimized with the diffusion loss&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite>.\nGiven that we do not design a specialized loss function compared to previous works such as contrastive learning in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>]</cite>, AR modeling in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>, our simple training framework ensures versatile connections for various image generators.\nIn inference, the denoised latent is decoded into the image through the decoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib30\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>]</cite>.\nGiven that we do not design a specialized loss function compared to previous works, such as contrastive learning in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>]</cite>, AR modeling in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>,\nour simple framework ensures versatile connections for various image generators.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "image",
                    "process",
                    "inference",
                    "generator"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is to generate an image with a corresponding spoken description, even for emotional expression.\nHowever, prior datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite> overlooked paralinguistic features in speech, and also required significant costs for human recordings.\nOur benchmark uniquely leverages synthesized speech, enabling the natural and cost-effective creation of a large-scale dataset.\nSpecifically, VoxEmoset leverages semantic knowledge and the generative powers of pre-trained multimodal LLMs and diffusion models to generate diverse synthetic data samples.\nFirst, a multimodal LLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite> generates corresponding captions that are factual descriptions of a given emotional image based on explaining environments or objects.\nThen, a TTS model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite> generates emotional speech samples from text captions, using emotional voice samples from other datasets as references.\nConsequently, we efficiently and cheaply generate large-scale emotional utterances along with text captions, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S3.F4\" title=\"In 3.3 Image generator &#8227; 3 VoxStudio &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "voxstudio",
                    "from",
                    "image",
                    "also"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While EmoSet categorized emotion classes, there is no sentence-level description for visual scenes.\nWe generate captions using the instruction prompt in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A1\" title=\"Appendix A VoxEmoset &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">A</span></a>, restricting immediate emotional expressions while focusing on factual descriptions.\nLLaVA-OneVision&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib32\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">32</span></a>]</cite>, using SigLIP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib62\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">62</span></a>]</cite> as an image encoder and Qwen-2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib8\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></a>]</cite> as LLM, generates three different captions for each image to prevent the model from simply generating emotionally biased captions, <em class=\"ltx_emph ltx_font_italic\">e.g</em>., &#8216;<span class=\"ltx_text ltx_font_italic\">a person is happy.</span>&#8217;, &#8216;<span class=\"ltx_text ltx_font_italic\">disgusting rotten egg in the plate.</span>&#8217;.\nThe word count distribution of our 247k generated captions closely matches that of existing benchmarks, indicating that they were carefully crafted to resemble real-world datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F1\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B1</span></a> in Appendix).</p>\n\n",
                "matched_terms": [
                    "from",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Emotional utterances are generated by a text-to-speech (TTS) system that can synthesize the speech with emotional attributes.\nThis strategy eliminates the dependency on skilled voice actors or noisy crowdsourcing.\nThrough empirical comparison of recent TTS models based on diffusion, autoregressive, and non-autoregressive architectures, F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite> demonstrates remarkable quality in both linguistic and emotional expression.</p>\n\n",
                "matched_terms": [
                    "system",
                    "diffusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, to build a diverse range of emotional voice references for TTS, emotional speech data was collected from multiple datasets, including CREMA-D&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib3\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></a>]</cite>, MEAD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib53\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">53</span></a>]</cite>, and RAVDESS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib34\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">34</span></a>]</cite>.\nThese datasets contain English-spoken utterances from a variety of speakers.\nFollowing EmoBox&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>]</cite>, we split the datasets into training and test sets.\nWe validate the emotions in the generated speech using Emotion2Vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib37\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">37</span></a>]</cite> to measure emotional intensity, filtering and re-generating inadequate samples. After this process, 247k speech samples are generated.\nFurther details are provided in Appendix.</p>\n\n",
                "matched_terms": [
                    "from",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets.</span>\nWe use SpokenCOCO&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite> and VoxEmoset to train <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>.\nVoxEmoset includes 208k utterances with paired 69k images for training, while SpokenCOCO contains 118k images with 591k utterances.\nFlickr8kAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> is used to evaluate zero-shot generalizability.\nEach image in SpokenCOCO and Flickr8kAudio has five voice recordings from unskilled annotators, resulting in inherently noisy audio (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., the recording may contain background noise, reading speed or volume can vary, and pronunciation may not be as clear as that of skilled voice actors as in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.SS4\" title=\"B.4 Failure cases and limitation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B.4</span></a>).\nVoxEmoset is automatically generated and less prone to recording noise.\nWe use the Karpathy split&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib27\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">27</span></a>]</cite> for SpokenCOCO and Flickr8kAudio.</p>\n\n",
                "matched_terms": [
                    "from",
                    "image",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation details.</span>\nTraining a high-performance image generator requires a vast amount of resources (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., SD1.5 requires 6,000 A100 GPU days&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib5\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></a>]</cite>).\nWe initialize the image generator with a pre-trained SD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite> for efficient learning.\nWe use SONAR as the speech encoder, freezing during the training, in which its last aggregation layer is removed.\nWe use AdamW <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib35\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">35</span></a>]</cite> with the learning rate of <math alttext=\"1e\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">1e</annotation></semantics></math>-<math alttext=\"6\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>, the batch size of 128 using 8 V100 GPUs.\nFP16 precision is used for all experiments.\nThe code will be released.</p>\n\n",
                "matched_terms": [
                    "generator",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation metrics.</span>\nWe assess the generation quality using FID&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib22\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">22</span></a>]</cite>, while content alignment between speech and generated images is measured with CLIPScore&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib21\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite> using text transcriptions.\nFor SpokenCOCO and VoxEmoset, random samples of 10k condition prompts, either speech or text, are used for evaluation.\nFor Flickr8kAudio, we use 5k test prompts for evaluation.\nWe also report emotion classification accuracy (Emo-A)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib59\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">59</span></a>]</cite> on generated images to examine whether the results reflect emotion from prompts.\nNote that we measure accuracy only with scores for the 5 emotion categories <math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math><span class=\"ltx_text ltx_font_italic\">&#8216;amusement&#8217;</span> and <span class=\"ltx_text ltx_font_italic\">&#8216;excitement&#8217;</span> are classified as the same class<math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math> in the trained emotion classifier.</p>\n\n",
                "matched_terms": [
                    "from",
                    "also"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results on SpokenCOCO and VoxEmoset.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S4.T2\" title=\"In 4.4 Dataset quality &#8227; 4 VoxEmoset Benchmark &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the comparison of <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> and baselines<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>EmoGen was excluded because its pretrained weights are publicly unavailable, and our reimplementation was unable to match its reported performance.</span></span></span> on SpokenCOCO and VoxEmoset.\nSD1.5 with the text inputs (<em class=\"ltx_emph ltx_font_italic\">i.e</em>., without speech) is shown as a baseline.\nEspecially, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S4.F5\" title=\"In 4.4 Dataset quality &#8227; 4 VoxEmoset Benchmark &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> highlights the stark contrast between text- and speech-based generation.\nWhile speech conveys emotions even with the same wording, the text-based model inherently ignores these cues and focuses on fact-based generation.\nEven when trained on VoxEmoset, &#8216;SD (finetuning)&#8217; struggles to express emotions as semantic content, but speech leads to a richer and intense emotional expression.\nFor example, given the prompt &#8216;A black trash can is placed against a white wall,&#8217; our model detects disgust from spoken nuances and visually emphasizes the unpleasantness of trash, whereas the text-based model remains neutral.</p>\n\n",
                "matched_terms": [
                    "from",
                    "excluded",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, despite the inherent noise in speech features and our method needs significantly lower latency compared to text-based approaches, the performance gap remains minimal in image quality and text alignment.\nMoreover, as shown in the last example in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S4.F5\" title=\"In 4.4 Dataset quality &#8227; 4 VoxEmoset Benchmark &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, the CLIP encoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib42\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">42</span></a>]</cite> often overlooks information from the latter part of a sentence&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib63\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">63</span></a>]</cite> (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., &#8216;bright yellow&#8217; in the last example).\nHowever, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> excels in conveying emotions when trained on the same datasets.\nThis advocates that speech, as a richer modality for emotional expression, provides a more effective signal to generate emotionally compelling images.</p>\n\n",
                "matched_terms": [
                    "from",
                    "image",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Remarkably, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> outperforms SpeechCLIP+ and TMT on SpokenCOCO, where <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> does not use Flickr8kAudio for training.\nWhile TMT additionally used huge synthesized speech data from CC3M&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib49\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">49</span></a>]</cite> and CC12M&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>]</cite> for training, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> also show comparable results on VoxEmoset.\nThis result demonstrates that our diffusion model is a powerful learner for speech-to-expressive image alignment than contrastive learning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>]</cite> and auto-regressive training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>.\nThe qualitative comparison on SpokenCOCO shows that SpeechCLIP+ and TMT often ignore keywords in the prompts, while <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> can capture the details, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F6\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "voxstudio",
                    "from",
                    "image",
                    "also",
                    "than"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results on Flickr8kAudio.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T3\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the performance comparison on Flickr8kAudio.\nHere, while TMT and SpeechCLIP+ used Flickr8kAudio for training, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> was evaluated in a zero-shot manner.\nSurprisingly, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> outperforms existing methods by large margins.\nIt shows that end-to-end training in <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is more robust in aligning the speech-language space.\nBy contrast, speech features in <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> are more robust to the order or length of the prompt.\nMoreover, VoxEmoset might improve the robustness on generality as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F6\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "methods",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human evaluation.</span>\nA user study is conducted to assess how well humans perceive the alignment between speech and image atmosphere.\n26 participants evaluated 25 images to rate how well the emotion conveyed in the image matched the given speech.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F7.sf1\" title=\"In Figure 7 &#8227; 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7(a)</span></a> shows that results from <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> are more aligned with the emotion than text-based SD in all categories.\nIn other words, with an average of 57.09% preference, the images generated by our <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> were rated as better at expressing emotions.\nIt highlights the effectiveness of speech prompts for expressive image synthesis.\nWe also carried out another human evaluation on SpokenCOCO across speech-based models.\nThis experiment is performed on 17 participants who evaluate 10 generated images for each model with a 5-point Likert scale.\nAs demonstrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F7.sf2\" title=\"In Figure 7 &#8227; 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7(b)</span></a>, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> outperforms existing approaches by generating high-quality images that accurately reflect the nuances of the input prompts.</p>\n\n",
                "matched_terms": [
                    "voxstudio",
                    "from",
                    "image",
                    "than",
                    "also"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effect of emotion.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F8\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a> demonstrates that the same description, when spoken with different emotions, leads to distinct visual outputs by <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>. This highlights <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>&#8217;s capability to produce emotional nuances beyond linguistic content.\nFor instance, a neutral statement spoken in a disgusted tone results in negative visual details (top-left), while an &#8220;enjoying&#8221; tone generates a more positive scene (top-right).\nThese findings show that our speech-based approach effectively leverages emotional cues, enabling more expressive and context-rich image generation.</p>\n\n",
                "matched_terms": [
                    "image",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generalization on real speech.</span>\nTo evaluate how well our model generalizes, we test on utterances in ESD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib65\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">65</span></a>]</cite> dataset, excluded from our reference samples during speech synthesis.\nWe visualize generated samples from real speakers&#8217; utterances in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F9\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a>.\nText-based generator is limited to expressing the tone in speech prompt, but <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> successfully expresses atmospheres despite ambiguous words.\nIt also proves the superiority of VoxEmoset in that <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> trained on synthesized emotional speech is well generalized in real utterances.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F8\" title=\"In B.3 Details of human evaluation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B8</span></a> also demonstrates that <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> extends naturally to various applications such as image editing by spoken prompt.</p>\n\n",
                "matched_terms": [
                    "voxstudio",
                    "from",
                    "image",
                    "also",
                    "generator",
                    "excluded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training strategies.</span>\nDiffusion training is usually computationally expensive.\nWe test different training strategies in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T5\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>: full finetuning, LoRA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib24\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">24</span></a>]</cite>, and freezing the model.\nWhile finetuning achieves the best performance, LoRA and frozen models show comparable results in CLIPScore and Emo-A.\nAdditionally, although speech is noisier than text, our method outperforms full finetuning for original SD1.5 (&#8216;SD(T2I)-FT&#8217; in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T5\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>) in terms of emotional expression, while maintaining the generation quality.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "than"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scale of the image generator.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T6\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> demonstrates the performance of image generators at different scales.\nDue to the resource limit, we compare UNet of SD1.5&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite> and SDXL&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib41\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">41</span></a>]</cite> as our image generator in a frozen state during the training.\nInterestingly, although the small generator achieves a higher CLIPScore, the larger generator excels at displaying emotional nuances.\nThis finding suggests that larger-scale generators are inherently better at representing content beyond simple text cues.</p>\n\n",
                "matched_terms": [
                    "generator",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech embedding.</span>\nWe compare SONAR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> and Whisper-Large v3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib43\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">43</span></a>]</cite> encoders as a speech input handler of our method.\nWhisper is a widely used ASR model, also known to be capable of capturing paralinguistic information&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib64\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">64</span></a>]</cite>.\nWhile SONAR is sentence-level speech-text aligned features, Whisper is trained at the phoneme-level by predicting which words are spoken in a given audio snippet.\nThis fundamental difference affects how each encoder preserves linguistic content and emotional cues when mapping speech to image descriptions.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T7\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a> demonstrates that text-aligned embeddings (<em class=\"ltx_emph ltx_font_italic\">i.e</em>., SONAR) show more robust performance on our task.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "image",
                    "also",
                    "mapping"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image editing using speech prompt.</span>\n<span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is built upon the SD architecture, allowing seamless integration with various extensions and applications.\nFor instance, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F8\" title=\"In B.3 Details of human evaluation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B8</span></a>, the image editing pipeline can be directly applied with speech prompts to modify input images.\nBeyond basic editing, our framework can be extended to other tasks built on SD, including personalized generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib47\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">47</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib61\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">61</span></a>]</cite> and multimodal content synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib17\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">17</span></a>]</cite>.\nIt provides a versatile foundation for future developments in S2I generation.</p>\n\n",
                "matched_terms": [
                    "image",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is the first end-to-end S2I model that captures both linguistic and emotional nuances from speech.\nUnlike text-based methods, our approach totally leverages speech&#8217;s expressiveness to generate emotionally aligned images. VoxEmoset is built cheaply, but it is complementary with real-world datasets.\nOur experiments demonstrate that <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> not only outperforms prior speech-based methods in conveying sentiment through images, but also matches text-driven approaches in semantic alignment, despite the higher noise and lower latency of the speech modality.\nWe believe our work facilitates future research in voice-driven generative models and their applications.</p>\n\n",
                "matched_terms": [
                    "from",
                    "also",
                    "methods",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results for <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> on SpokenCOCO according to the difference of training data is shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.T2\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B2</span></a>.\nIn the results, we observe that SpokenCOCO does not fully capture the diversity of real-world scenarios.\nMost images convey neutral emotions and primarily depict scenes suitable for objective descriptions.\nHowever, real-world photographs go beyond presenting mere facts, often communicating higher-level meanings such as feelings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib59\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">59</span></a>]</cite>.\nThis demonstrates that our dataset extends beyond the distribution of SpokenCOCO and Flickr8kAudio, offering a more realistic and emotionally expressive representation.\nThis claim is further supported by Table 2 of the main paper, where models trained on the SpokenCOCO and Flickr8kAudio datasets&#8212; even TMT, which was trained on a massive amount of synthesized speech from CC3M and CC12M&#8212;fail to generalize to our dataset.</p>\n\n",
                "matched_terms": [
                    "from",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We show more qualitative comparisons in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F2\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F3\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B3</span></a>.\nIn those experiments, our image generators are initialized by UNet parameters in SD1.5.\nWhen training SD with a CLIP encoder using text prompts from our dataset, the model better follows the content of the text compared to zero-shot generation.\nHowever, in terms of emotional intensity and expressiveness, it performs weaker than our approach using speech prompts.\nFor example, in the last row in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F3\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B3</span></a>, generating an emotionally rich image from a sentence like &#8216;a tomato is cut into sections on a white plate&#8217; is challenging.\nBy using speech input that conveys a feeling of disgust, our model generates an image where the tomato appears distorted, conditioned on the given emotion category.\nMoreover, despite the inherent noise in speech, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> utilizes SIB to refine the information and capture its meaning, effectively following the content of the prompt.</p>\n\n",
                "matched_terms": [
                    "voxstudio",
                    "from",
                    "image",
                    "than",
                    "parameters"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F4\" title=\"In B.2 Qualitative results &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B4</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F5\" title=\"In B.2 Qualitative results &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B5</span></a> provide more results generated using the parameters of SD1.5 and SDXL, respectively.\nEspecially for the example of &#8216;a woman with blonde hair is standing in a room.&#8217;, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> poses the sadness in her facial expression.\nWhile we freeze parameters of the image generator, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F5\" title=\"In B.2 Qualitative results &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B5</span></a> shows that generated images ensure high fidelity and text relevancy.</p>\n\n",
                "matched_terms": [
                    "generator",
                    "image",
                    "parameters",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We recruit 17 independent evaluators to assess image quality and speech prompt fidelity on SpokenCOCO in Fig. 7(b) of the main paper.\nIn this experiment, the instructions in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F7\" title=\"In B.3 Details of human evaluation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B7</span></a> are used for evaluating 10 different images generated by SpeechCLIP+, TMT, and <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, respectively.</p>\n\n",
                "matched_terms": [
                    "image",
                    "voxstudio"
                ]
            }
        ]
    },
    "S3.T1": {
        "source_file": "Seeing What You Say: Expressive Image Generation from Speech",
        "caption": "Table 1: SpokenCOCOÂ [23], Flickr8kAudioÂ [18], and our VoxEmoset contain paired image-utterance data while CREMA-DÂ [3]\ncontains utterance only. Our VoxEmoset shows compatible quality for real-world speech in terms of NMOS and emotional confidence (Emo-C).",
        "body": "Benchmark\n# Images\n# Utterances\nClipScore\nLength (s)\nAvg. Words\nNMOS\nEmotion\nEmo-C\n\n\nSpokenCOCO\n123k\n615k\n30.42\n4.34\n10.45\n2.9616\nâ\n-\n\n\nFlickr8kAudio\n8k\n40k\n31.27\n4.12\n10.87\n2.9689\nâ\n-\n\n\nCREMA-D\nâ\n7k\n-\n2.54\n5.26\n2.0314\nâ\n0.8465\n\n\nVoxEmoset (ours)\n82k\n247k\n30.27\n4.25\n11.19\n2.9683\nâ\n0.8998",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Benchmark</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\"># Images</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\"># Utterances</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">ClipScore</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Length (s)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Avg. Words</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">NMOS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Emotion</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Emo-C</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">SpokenCOCO</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">123k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">615k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.42</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.34</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.9616</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#B3B3B3;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Flickr8kAudio</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">8k</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">40k</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.27</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.12</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.87</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.9689</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#B3B3B3;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">CREMA-D</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#B3B3B3;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">7k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.54</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.26</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.0314</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#004D99;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8465</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">VoxEmoset (ours)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">82k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">247k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.19</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.9683</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#004D99;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8998</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "images",
            "confidence",
            "emotional",
            "quality",
            "ours",
            "82k",
            "avg",
            "length",
            "words",
            "benchmark",
            "our",
            "cremad",
            "speech",
            "shows",
            "contain",
            "nmos",
            "40k",
            "realworld",
            "247k",
            "emoc",
            "voxemoset",
            "utterances",
            "615k",
            "123k",
            "data",
            "clipscore",
            "spokencoco",
            "only",
            "flickr8kaudio",
            "terms",
            "utterance",
            "emotion",
            "paired",
            "while",
            "imageutterance",
            "compatible",
            "contains"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Our benchmark uses images in EmoSet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib58\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">58</span></a>]</cite>, the large-scale visual emotion dataset annotated with Mikels model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib38\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">38</span></a>]</cite>.\nWe use the partial of 118k subset labeled by humans and machines, including six categories: amusement, excitement, anger, disgust, fear, and sadness.\nIn line with <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib14\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">14</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib48\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">48</span></a>]</cite>, we group amusement and excitement into a single emotion category, &#8216;enjoyment&#8217;, because these two categories are difficult to distinguish solely through voice expression.\nOn the other hand, we exclude &#8216;awe&#8217; and &#8216;contentment&#8217; emotion categories which are hard to express in voice.\nThe final number of images in VoxEmoset is shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S3.T1\" title=\"In 3.3 Image generator &#8227; 3 VoxStudio &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">To objectively assess the quality of generated utterances, we randomly sample 10k utterances from each dataset and measure NMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib45\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">45</span></a>]</cite>. For CREMA-D, we use the entire samples.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S3.T1\" title=\"In 3.3 Image generator &#8227; 3 VoxStudio &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> shows that VoxEmoset is compatible with existing speech-image datasets such as SpokenCOCO and Flickr8kAudio in terms of speech quality (NMOS) and description quality (CLIPScore).\nHowever, only our benchmark explicitly expresses emotion in speech.\nThe last two rows in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S3.T1\" title=\"In 3.3 Image generator &#8227; 3 VoxStudio &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> validates that VoxEmoset guarantees high perceptual fidelity with clear affect, where emotion discriminability (Emo-C) is measured as the emotion classifier&#8217;s average confidence score.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This paper proposes <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, the first unified and end-to-end speech-to-image model that generates expressive images directly from spoken descriptions by jointly aligning linguistic and paralinguistic information.\nAt its core is a speech information bottleneck (SIB) module, which compresses raw speech into compact semantic tokens, preserving prosody and emotional nuance.\nBy operating directly on these tokens, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> eliminates the need for an additional speech-to-text system, which often ignores the hidden details beyond text, <em class=\"ltx_emph ltx_font_italic\">e.g</em>., tone or emotion.\nWe also release VoxEmoset, a large&#8208;scale paired emotional speech&#8211;image dataset built via an advanced TTS engine to affordably generate richly expressive utterances.\nComprehensive experiments on the SpokenCOCO, Flickr8kAudio, and VoxEmoset benchmarks demonstrate the feasibility of our method and highlight key challenges, including emotional consistency and linguistic ambiguity, paving the way for future research.\n<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\"/><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\"/><span class=\"ltx_tag ltx_tag_note\"/><sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup> Partly work done in NAVER AI Lab.\n<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> Corresponding author.</span></span></span></p>\n\n",
                "matched_terms": [
                    "images",
                    "speech",
                    "emotional",
                    "voxemoset",
                    "utterances",
                    "emotion",
                    "paired",
                    "spokencoco",
                    "flickr8kaudio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans naturally &#8220;imagine&#8221; vivid mental images when listening to speech, which conveys not only semantics but also emotion, tone, and intent.\nSpeech-to-image (S2I) generation taps this rich, multimodal expressiveness to produce visuals that are more nuanced and emotionally resonant than those driven by text alone. By translating spoken descriptions directly into images, S2I can unlock applications in accessibility, creative media, and voice&#8208;driven interfaces&#8212;treating speech as a first&#8208;class modality for content creation rather than a mere precursor to text.</p>\n\n",
                "matched_terms": [
                    "images",
                    "only",
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in text-to-image (T2I) generation have demonstrated remarkable progress, but they struggle to utilize the innate expressiveness and accessibility of speech.\nMost cascaded framework&#8212;where an utterance is first transcribed into text or textual feature and then used as input for T2I models, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S1.F2\" title=\"In 1 Introduction &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> (a, b)&#8212;encounters several significant challenges.\nFirst, speech-to-text (<em class=\"ltx_emph ltx_font_italic\">i.e</em>., ASR) transcription is limited to capturing prosody and speaker intention.\nHowever, transcription errors propagate into the image generative model, then degrade visual quality.\nSecond, this sequential approach inherently decouples speech and image generation, making it difficult to transfer crucial prosodic and temporal cues&#8212;such as speaking rate, pitch variation, and emotional style&#8212;that can influence the mood, color palette, or overall aesthetic of the generated image.\nRelying on intermediate text also excludes languages without written forms&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib50\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">50</span></a>]</cite>.\nEven for languages that do have a writing system, coverage for the cascaded approach remains far from comprehensive: there are over 7,100 languages worldwide&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib13\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">13</span></a>]</cite>, <em class=\"ltx_emph ltx_font_italic\">e.g</em>. Google API covers only 125<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1a\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://cloud.google.com/speech-to-text</span></span></span>.\nFinally, the cascaded system limits the inference speed and requires a higher cost than our unified system, as shown in the table of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S1.F2\" title=\"In 1 Introduction &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nThese limitations underscore the necessity of an end-to-end approach that directly maps raw speech to images, enabling a more seamless and expressive integration of modalities.</p>\n\n",
                "matched_terms": [
                    "images",
                    "speech",
                    "utterance",
                    "emotional",
                    "quality",
                    "only",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, a novel speech-to-image model that bridges the rich information in speech with the image modality space, enabling more diverse and expressive visual representations.\nBuilding upon T2I models, our framework is suitable for the unique characteristics of speech, which differ from text:\n(1) Speech generally contains longer and more variable sequences than text, leading to uneven information density across embeddings.\n(2) Speech signals vary significantly depending on speaker identity, recording environment, and emotional state, affecting articulation and duration, even for the same content.\nTo address these challenges, we introduce a speech information bottleneck (SIB) that efficiently aligns cross-modal latent spaces while preserving key speech features.\nOur SIB encodes compressed conditional features that guide the image generation process.\nThrough extensive experiments, we establish an effective speech-based guidance for image generation by identifying the optimal combination of speech encoder, SIB, and image generator.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotional",
                    "while",
                    "contains",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">VoxEmoset</span>, an automatically (and efficiently) synthesized dataset of 247k emotional spoken descriptions for sentiment images. VoxEmoset is used for both training and evaluation of S2I.</p>\n\n",
                "matched_terms": [
                    "images",
                    "247k",
                    "emotional",
                    "voxemoset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> in various S2I benchmarks, including SpokenCOCO&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite>, Flickr8kAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> and VoxEmoset, and demonstrate its superiority and high fidelity over baselines.</p>\n\n",
                "matched_terms": [
                    "flickr8kaudio",
                    "voxemoset",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conditions for image generation.</span>\nRecently, diffusion-based conditional image generative models have emerged with remarkable performance <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib44\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">44</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib40\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">40</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib41\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">41</span></a>]</cite>. Specifically, stable diffusion (SD)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite> has shown impressive results in both quality and generalizability.\nGiven that these models only take text as a condition, they have struggled to reflect individual thoughts and emotions beyond text into compelling images.\nSome methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib57\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">57</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite> have proposed emotional image generation, pointing out the importance of reacting to the user&#8217;s sentiment.\nHowever, they relied on explicit linguistic expressions (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., <span class=\"ltx_text ltx_font_italic\">&#8216;with a sense of happiness and joy&#8217;</span> in the text prompt) and focused on reflecting emotion in texture and color only&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib2\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></a>]</cite>.\nRecently, EmoGen&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib59\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">59</span></a>]</cite> and EmoEdit&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib60\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">60</span></a>]</cite> argued that emotional contents beyond color and style should be effectively expressed as semantic variations in a generated image.\nThey learned a more flexible generative model using a large-scale EmoSet dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib58\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">58</span></a>]</cite>, but still required users to explicitly specify emotion prompts.\nIn contrast, our approach automatically infers these nuances directly from the speaker&#8217;s voice.</p>\n\n",
                "matched_terms": [
                    "images",
                    "emotional",
                    "quality",
                    "emotion",
                    "only",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, speech naturally encodes nuanced emotion and tone&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib50\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">50</span></a>]</cite>, offering a more intuitive means for generating emotionally resonant images, yet it remains largely untapped as a conditioning signal.\nRecent audiovisual generation methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib29\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">29</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">7</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib26\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">26</span></a>]</cite> have been limited to relying only on semantic instances expressed in text, where the other expressions are excluded.\nMoreover, existing approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib54\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">54</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib55\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">55</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite> for S2I generation have used highly limited datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite>, restricting their expressive versatility.\nWe aim to design a unified and emotion-driven S2I framework as well as to introduce a large-scale dataset for both training and evaluation.</p>\n\n",
                "matched_terms": [
                    "images",
                    "only",
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Relationship between speech and image.</span>\nSpeech-image relationships have been widely explored in biometrics&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib31\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">31</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib39\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">39</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>]</cite>, linguistic alignment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>, and phonetic articulation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib56\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">56</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib9\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">9</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib10\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">10</span></a>]</cite>.\nThese studies provide valuable insights into how speech and vision interact in different contexts.\nAlso, image-speech retrieval&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> has explored the alignment between spoken descriptions and images, highlighting the importance of understanding the semantics in both modalities.\nDespite these advances, most of the existing studies focus on isolated characteristics between modalities.\nBy moving beyond traditional mappings, our work aims to bridge the gap by simultaneously leveraging natural semantic correspondences, <em class=\"ltx_emph ltx_font_italic\">i.e</em>., both linguistic and paralinguistic information, between speech and vision.</p>\n\n",
                "matched_terms": [
                    "images",
                    "speech",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S2.F3\" title=\"In 2 Related Work &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the overall framework of <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, consisting of (1) a pretrained speech encoder, (2) SIB to reduce the computational cost and effectively connect heterogeneous two modalities, and (3) an image generator to synthesize an image from the compressed speech representations.\nNext, we describe each module in detail.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "shows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider two pre-trained speech encoders, SONAR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> and Whisper large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib43\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">43</span></a>]</cite>, to deploy comprehensive speech features considering both linguistic and paralinguistic information.\nBriefly reviewed, SONAR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> has learned global semantic alignment between speech and text, enabling it to encode meaning beyond phonetic content.\nWe remove its last aggregation layer to ensure that the speech embeddings retain both linguistic and paralinguistic information.\nWe also test Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib43\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">43</span></a>]</cite>, a widely used speech recognition model. Whisper is known to be capable of capturing paralinguistic information, such as emotion and speaker identity&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib64\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">64</span></a>]</cite>.\nFormally, given an input utterance <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math>, we obtain a speech embedding <math alttext=\"s\\in\\mathrm{R}^{N\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo>&#8712;</mo><msup><mi mathvariant=\"normal\">R</mi><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">s\\in\\mathrm{R}^{N\\times D}</annotation></semantics></math>, where <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> depends on the length of the speech and models, and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> is the channel dimension of the final output layer.\nWe note that our work does not employ ASR system or text encoder to explicitly map the speech into text.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "utterance",
                    "emotion",
                    "length",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although speech embeddings contain rich representations, they are excessively long and lead to a lower information density in each speech token compared to text (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., Whisper encodes a maximum of 1500 tokens for 30-seconds long speech, while CLIP text encoder limited to 77 tokens).\nThis low density makes direct usage challenging to condition the image generator.\nTo solve this problem, we design a Transformer-based speech information bottleneck (SIB) module.\nSIB compacts semantics in speech embeddings, motivated by previous works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib51\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">51</span></a>]</cite> applied to individual image and audio encoders.\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S2.F3\" title=\"In 2 Related Work &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, SIB reduces the number of embeddings with a strided convolution layer after a Transformer block along the time axis.\nBased on our findings, a pooling ratio of 8 provides the optimal balance, allowing us to maximize the information retention of speech features.\nAs a result, the initial embedding <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> is processed into a compressed speech condition <math alttext=\"c=f_{\\psi}(s)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>=</mo><mrow><msub><mi>f</mi><mi>&#968;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">c=f_{\\psi}(s)</annotation></semantics></math>, where <math alttext=\"c\\in\\mathrm{R}^{M\\times D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>&#8712;</mo><msup><mi mathvariant=\"normal\">R</mi><mrow><mi>M</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>D</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">c\\in\\mathrm{R}^{M\\times D^{\\prime}}</annotation></semantics></math>, <math alttext=\"M=N/8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>=</mo><mrow><mi>N</mi><mo>/</mo><mn>8</mn></mrow></mrow><annotation encoding=\"application/x-tex\">M=N/8</annotation></semantics></math> and <math alttext=\"D^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><msup><mi>D</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">D^{\\prime}</annotation></semantics></math> is the input channel of the cross-attention block in the image generator.\nThose compressed representations improve the efficiency of the S2I process while preserving both linguistic and emotional expressiveness.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotional",
                    "our",
                    "while",
                    "contain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The image generator is based on the latent diffusion model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite>.\nThe speech condition <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math>, compressed through SIB, is fed into the generator as a guidance of the synthesis process.\nSpecifically, the speech embeddings are injected into the UNet through cross-attention layers to condition the image synthesis.\nThis conditioning allows the model to incorporate the emotional, semantic content of speech into the generation process.\nThe image generator and SIB are optimized with the diffusion loss&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite>.\nGiven that we do not design a specialized loss function compared to previous works such as contrastive learning in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>]</cite>, AR modeling in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>, our simple training framework ensures versatile connections for various image generators.\nIn inference, the denoised latent is decoded into the image through the decoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib30\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>]</cite>.\nGiven that we do not design a specialized loss function compared to previous works, such as contrastive learning in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>]</cite>, AR modeling in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>,\nour simple framework ensures versatile connections for various image generators.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotional",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is to generate an image with a corresponding spoken description, even for emotional expression.\nHowever, prior datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite> overlooked paralinguistic features in speech, and also required significant costs for human recordings.\nOur benchmark uniquely leverages synthesized speech, enabling the natural and cost-effective creation of a large-scale dataset.\nSpecifically, VoxEmoset leverages semantic knowledge and the generative powers of pre-trained multimodal LLMs and diffusion models to generate diverse synthetic data samples.\nFirst, a multimodal LLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite> generates corresponding captions that are factual descriptions of a given emotional image based on explaining environments or objects.\nThen, a TTS model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite> generates emotional speech samples from text captions, using emotional voice samples from other datasets as references.\nConsequently, we efficiently and cheaply generate large-scale emotional utterances along with text captions, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S3.F4\" title=\"In 3.3 Image generator &#8227; 3 VoxStudio &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotional",
                    "voxemoset",
                    "utterances",
                    "data",
                    "benchmark",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While EmoSet categorized emotion classes, there is no sentence-level description for visual scenes.\nWe generate captions using the instruction prompt in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A1\" title=\"Appendix A VoxEmoset &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">A</span></a>, restricting immediate emotional expressions while focusing on factual descriptions.\nLLaVA-OneVision&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib32\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">32</span></a>]</cite>, using SigLIP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib62\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">62</span></a>]</cite> as an image encoder and Qwen-2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib8\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></a>]</cite> as LLM, generates three different captions for each image to prevent the model from simply generating emotionally biased captions, <em class=\"ltx_emph ltx_font_italic\">e.g</em>., &#8216;<span class=\"ltx_text ltx_font_italic\">a person is happy.</span>&#8217;, &#8216;<span class=\"ltx_text ltx_font_italic\">disgusting rotten egg in the plate.</span>&#8217;.\nThe word count distribution of our 247k generated captions closely matches that of existing benchmarks, indicating that they were carefully crafted to resemble real-world datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F1\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B1</span></a> in Appendix).</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "emotion",
                    "realworld",
                    "247k",
                    "while",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Emotional utterances are generated by a text-to-speech (TTS) system that can synthesize the speech with emotional attributes.\nThis strategy eliminates the dependency on skilled voice actors or noisy crowdsourcing.\nThrough empirical comparison of recent TTS models based on diffusion, autoregressive, and non-autoregressive architectures, F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite> demonstrates remarkable quality in both linguistic and emotional expression.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotional",
                    "quality",
                    "utterances"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, to build a diverse range of emotional voice references for TTS, emotional speech data was collected from multiple datasets, including CREMA-D&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib3\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></a>]</cite>, MEAD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib53\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">53</span></a>]</cite>, and RAVDESS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib34\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">34</span></a>]</cite>.\nThese datasets contain English-spoken utterances from a variety of speakers.\nFollowing EmoBox&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>]</cite>, we split the datasets into training and test sets.\nWe validate the emotions in the generated speech using Emotion2Vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib37\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">37</span></a>]</cite> to measure emotional intensity, filtering and re-generating inadequate samples. After this process, 247k speech samples are generated.\nFurther details are provided in Appendix.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotional",
                    "utterances",
                    "cremad",
                    "247k",
                    "data",
                    "contain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets.</span>\nWe use SpokenCOCO&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite> and VoxEmoset to train <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>.\nVoxEmoset includes 208k utterances with paired 69k images for training, while SpokenCOCO contains 118k images with 591k utterances.\nFlickr8kAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> is used to evaluate zero-shot generalizability.\nEach image in SpokenCOCO and Flickr8kAudio has five voice recordings from unskilled annotators, resulting in inherently noisy audio (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., the recording may contain background noise, reading speed or volume can vary, and pronunciation may not be as clear as that of skilled voice actors as in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.SS4\" title=\"B.4 Failure cases and limitation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B.4</span></a>).\nVoxEmoset is automatically generated and less prone to recording noise.\nWe use the Karpathy split&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib27\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">27</span></a>]</cite> for SpokenCOCO and Flickr8kAudio.</p>\n\n",
                "matched_terms": [
                    "images",
                    "utterances",
                    "voxemoset",
                    "paired",
                    "while",
                    "spokencoco",
                    "contains",
                    "flickr8kaudio",
                    "contain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation metrics.</span>\nWe assess the generation quality using FID&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib22\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">22</span></a>]</cite>, while content alignment between speech and generated images is measured with CLIPScore&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib21\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite> using text transcriptions.\nFor SpokenCOCO and VoxEmoset, random samples of 10k condition prompts, either speech or text, are used for evaluation.\nFor Flickr8kAudio, we use 5k test prompts for evaluation.\nWe also report emotion classification accuracy (Emo-A)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib59\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">59</span></a>]</cite> on generated images to examine whether the results reflect emotion from prompts.\nNote that we measure accuracy only with scores for the 5 emotion categories <math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math><span class=\"ltx_text ltx_font_italic\">&#8216;amusement&#8217;</span> and <span class=\"ltx_text ltx_font_italic\">&#8216;excitement&#8217;</span> are classified as the same class<math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math> in the trained emotion classifier.</p>\n\n",
                "matched_terms": [
                    "images",
                    "speech",
                    "quality",
                    "voxemoset",
                    "emotion",
                    "clipscore",
                    "while",
                    "spokencoco",
                    "only",
                    "flickr8kaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results on SpokenCOCO and VoxEmoset.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S4.T2\" title=\"In 4.4 Dataset quality &#8227; 4 VoxEmoset Benchmark &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the comparison of <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> and baselines<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>EmoGen was excluded because its pretrained weights are publicly unavailable, and our reimplementation was unable to match its reported performance.</span></span></span> on SpokenCOCO and VoxEmoset.\nSD1.5 with the text inputs (<em class=\"ltx_emph ltx_font_italic\">i.e</em>., without speech) is shown as a baseline.\nEspecially, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S4.F5\" title=\"In 4.4 Dataset quality &#8227; 4 VoxEmoset Benchmark &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> highlights the stark contrast between text- and speech-based generation.\nWhile speech conveys emotions even with the same wording, the text-based model inherently ignores these cues and focuses on fact-based generation.\nEven when trained on VoxEmoset, &#8216;SD (finetuning)&#8217; struggles to express emotions as semantic content, but speech leads to a richer and intense emotional expression.\nFor example, given the prompt &#8216;A black trash can is placed against a white wall,&#8217; our model detects disgust from spoken nuances and visually emphasizes the unpleasantness of trash, whereas the text-based model remains neutral.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotional",
                    "our",
                    "voxemoset",
                    "while",
                    "spokencoco",
                    "shows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, despite the inherent noise in speech features and our method needs significantly lower latency compared to text-based approaches, the performance gap remains minimal in image quality and text alignment.\nMoreover, as shown in the last example in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S4.F5\" title=\"In 4.4 Dataset quality &#8227; 4 VoxEmoset Benchmark &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, the CLIP encoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib42\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">42</span></a>]</cite> often overlooks information from the latter part of a sentence&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib63\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">63</span></a>]</cite> (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., &#8216;bright yellow&#8217; in the last example).\nHowever, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> excels in conveying emotions when trained on the same datasets.\nThis advocates that speech, as a richer modality for emotional expression, provides a more effective signal to generate emotionally compelling images.</p>\n\n",
                "matched_terms": [
                    "images",
                    "speech",
                    "emotional",
                    "quality",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Remarkably, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> outperforms SpeechCLIP+ and TMT on SpokenCOCO, where <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> does not use Flickr8kAudio for training.\nWhile TMT additionally used huge synthesized speech data from CC3M&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib49\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">49</span></a>]</cite> and CC12M&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>]</cite> for training, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> also show comparable results on VoxEmoset.\nThis result demonstrates that our diffusion model is a powerful learner for speech-to-expressive image alignment than contrastive learning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>]</cite> and auto-regressive training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>.\nThe qualitative comparison on SpokenCOCO shows that SpeechCLIP+ and TMT often ignore keywords in the prompts, while <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> can capture the details, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F6\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "our",
                    "voxemoset",
                    "while",
                    "spokencoco",
                    "data",
                    "flickr8kaudio",
                    "shows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results on Flickr8kAudio.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T3\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the performance comparison on Flickr8kAudio.\nHere, while TMT and SpeechCLIP+ used Flickr8kAudio for training, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> was evaluated in a zero-shot manner.\nSurprisingly, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> outperforms existing methods by large margins.\nIt shows that end-to-end training in <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is more robust in aligning the speech-language space.\nBy contrast, speech features in <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> are more robust to the order or length of the prompt.\nMoreover, VoxEmoset might improve the robustness on generality as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F6\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "voxemoset",
                    "while",
                    "length",
                    "flickr8kaudio",
                    "shows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human evaluation.</span>\nA user study is conducted to assess how well humans perceive the alignment between speech and image atmosphere.\n26 participants evaluated 25 images to rate how well the emotion conveyed in the image matched the given speech.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F7.sf1\" title=\"In Figure 7 &#8227; 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7(a)</span></a> shows that results from <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> are more aligned with the emotion than text-based SD in all categories.\nIn other words, with an average of 57.09% preference, the images generated by our <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> were rated as better at expressing emotions.\nIt highlights the effectiveness of speech prompts for expressive image synthesis.\nWe also carried out another human evaluation on SpokenCOCO across speech-based models.\nThis experiment is performed on 17 participants who evaluate 10 generated images for each model with a 5-point Likert scale.\nAs demonstrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F7.sf2\" title=\"In Figure 7 &#8227; 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7(b)</span></a>, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> outperforms existing approaches by generating high-quality images that accurately reflect the nuances of the input prompts.</p>\n\n",
                "matched_terms": [
                    "images",
                    "speech",
                    "our",
                    "emotion",
                    "spokencoco",
                    "words",
                    "shows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effect of emotion.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F8\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a> demonstrates that the same description, when spoken with different emotions, leads to distinct visual outputs by <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>. This highlights <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>&#8217;s capability to produce emotional nuances beyond linguistic content.\nFor instance, a neutral statement spoken in a disgusted tone results in negative visual details (top-left), while an &#8220;enjoying&#8221; tone generates a more positive scene (top-right).\nThese findings show that our speech-based approach effectively leverages emotional cues, enabling more expressive and context-rich image generation.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "emotional",
                    "while",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generalization on real speech.</span>\nTo evaluate how well our model generalizes, we test on utterances in ESD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib65\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">65</span></a>]</cite> dataset, excluded from our reference samples during speech synthesis.\nWe visualize generated samples from real speakers&#8217; utterances in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F9\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a>.\nText-based generator is limited to expressing the tone in speech prompt, but <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> successfully expresses atmospheres despite ambiguous words.\nIt also proves the superiority of VoxEmoset in that <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> trained on synthesized emotional speech is well generalized in real utterances.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F8\" title=\"In B.3 Details of human evaluation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B8</span></a> also demonstrates that <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> extends naturally to various applications such as image editing by spoken prompt.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotional",
                    "voxemoset",
                    "utterances",
                    "words",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training datasets.</span> <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T4\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that VoxEmoset is complementary with the real-world spoken dataset, SpokenCOCO, improving both visual fidelity and semantic relevance.</p>\n\n",
                "matched_terms": [
                    "realworld",
                    "shows",
                    "voxemoset",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training strategies.</span>\nDiffusion training is usually computationally expensive.\nWe test different training strategies in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T5\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>: full finetuning, LoRA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib24\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">24</span></a>]</cite>, and freezing the model.\nWhile finetuning achieves the best performance, LoRA and frozen models show comparable results in CLIPScore and Emo-A.\nAdditionally, although speech is noisier than text, our method outperforms full finetuning for original SD1.5 (&#8216;SD(T2I)-FT&#8217; in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T5\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>) in terms of emotional expression, while maintaining the generation quality.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "terms",
                    "emotional",
                    "quality",
                    "clipscore",
                    "while",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scale of the image generator.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T6\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> demonstrates the performance of image generators at different scales.\nDue to the resource limit, we compare UNet of SD1.5&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite> and SDXL&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib41\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">41</span></a>]</cite> as our image generator in a frozen state during the training.\nInterestingly, although the small generator achieves a higher CLIPScore, the larger generator excels at displaying emotional nuances.\nThis finding suggests that larger-scale generators are inherently better at representing content beyond simple text cues.</p>\n\n",
                "matched_terms": [
                    "clipscore",
                    "emotional",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech embedding.</span>\nWe compare SONAR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> and Whisper-Large v3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib43\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">43</span></a>]</cite> encoders as a speech input handler of our method.\nWhisper is a widely used ASR model, also known to be capable of capturing paralinguistic information&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib64\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">64</span></a>]</cite>.\nWhile SONAR is sentence-level speech-text aligned features, Whisper is trained at the phoneme-level by predicting which words are spoken in a given audio snippet.\nThis fundamental difference affects how each encoder preserves linguistic content and emotional cues when mapping speech to image descriptions.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T7\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a> demonstrates that text-aligned embeddings (<em class=\"ltx_emph ltx_font_italic\">i.e</em>., SONAR) show more robust performance on our task.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotional",
                    "while",
                    "words",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Architecture choices on SIB.</span>\nWe propose SIB to represent the speech condition compactly to address the issue of low information density of speech tokens.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T8\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a> compares its performance against a standard transformer structure.\nOur design choice achieves better performance with fewer parameters.\nGradually reducing the speech token length while simultaneously increasing their density across multiple layers enhances both the linguistic and paralinguistic expressiveness of the speech signal.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "our",
                    "while",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image editing using speech prompt.</span>\n<span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is built upon the SD architecture, allowing seamless integration with various extensions and applications.\nFor instance, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F8\" title=\"In B.3 Details of human evaluation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B8</span></a>, the image editing pipeline can be directly applied with speech prompts to modify input images.\nBeyond basic editing, our framework can be extended to other tasks built on SD, including personalized generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib47\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">47</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib61\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">61</span></a>]</cite> and multimodal content synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib17\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">17</span></a>]</cite>.\nIt provides a versatile foundation for future developments in S2I generation.</p>\n\n",
                "matched_terms": [
                    "images",
                    "speech",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is the first end-to-end S2I model that captures both linguistic and emotional nuances from speech.\nUnlike text-based methods, our approach totally leverages speech&#8217;s expressiveness to generate emotionally aligned images. VoxEmoset is built cheaply, but it is complementary with real-world datasets.\nOur experiments demonstrate that <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> not only outperforms prior speech-based methods in conveying sentiment through images, but also matches text-driven approaches in semantic alignment, despite the higher noise and lower latency of the speech modality.\nWe believe our work facilitates future research in voice-driven generative models and their applications.</p>\n\n",
                "matched_terms": [
                    "images",
                    "speech",
                    "emotional",
                    "voxemoset",
                    "realworld",
                    "only",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our objective for data construction is primarily on (1) synthesizing large-scale image and speech pairs, (2) the speech will be emotional rich, and (3) closely matches the quality of real recordings while diversifying the range of speakers.\nHere we supplement the details of VoxEmoset.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotional",
                    "quality",
                    "voxemoset",
                    "while",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the characteristics of SpokenCOCO, we limit the length of captions and ensure they accurately describe the context of the image.\nIn <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F1\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B1</span></a>, the word count distribution remains similar to SpokenCOCO and Flickr8kAudio, but with a more structured and consistent pattern.</p>\n\n",
                "matched_terms": [
                    "flickr8kaudio",
                    "length",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech generation, we use state-of-the-art F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite>, where the vocoder is trained on 24kHz.\nThe generated speech is resampled to 16kHz to use SONAR and Whisper encoders.\nTo diversify the speaker characteristics, we collect multiple emotional speech datasets: MEAD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib53\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">53</span></a>]</cite>, CREMA-D&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib3\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></a>]</cite>, and RAVDESS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib34\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">34</span></a>]</cite>, which are widely used in emotional speech synthesis and speech emotion recognition.\nMEAD is an audiovisual dataset annotated in 8 emotional categories.\nCREMA-D is a crowd-sourced actor dataset, using 6 emotion classes.\nRAVDESS contains audio and video, where the professional actors express emotion.\nAll datasets used English.\nThe split cleaned up by EmoBox&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>]</cite> is used, especially the fold 1 split for RAVDESS.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.T1\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B1</span></a> summarizes the number of emotion classes and speakers for each dataset.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotional",
                    "emotion",
                    "contains",
                    "cremad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results for <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> on SpokenCOCO according to the difference of training data is shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.T2\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B2</span></a>.\nIn the results, we observe that SpokenCOCO does not fully capture the diversity of real-world scenarios.\nMost images convey neutral emotions and primarily depict scenes suitable for objective descriptions.\nHowever, real-world photographs go beyond presenting mere facts, often communicating higher-level meanings such as feelings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib59\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">59</span></a>]</cite>.\nThis demonstrates that our dataset extends beyond the distribution of SpokenCOCO and Flickr8kAudio, offering a more realistic and emotionally expressive representation.\nThis claim is further supported by Table 2 of the main paper, where models trained on the SpokenCOCO and Flickr8kAudio datasets&#8212; even TMT, which was trained on a massive amount of synthesized speech from CC3M and CC12M&#8212;fail to generalize to our dataset.</p>\n\n",
                "matched_terms": [
                    "images",
                    "speech",
                    "realworld",
                    "spokencoco",
                    "data",
                    "flickr8kaudio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We show more qualitative comparisons in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F2\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F3\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B3</span></a>.\nIn those experiments, our image generators are initialized by UNet parameters in SD1.5.\nWhen training SD with a CLIP encoder using text prompts from our dataset, the model better follows the content of the text compared to zero-shot generation.\nHowever, in terms of emotional intensity and expressiveness, it performs weaker than our approach using speech prompts.\nFor example, in the last row in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F3\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B3</span></a>, generating an emotionally rich image from a sentence like &#8216;a tomato is cut into sections on a white plate&#8217; is challenging.\nBy using speech input that conveys a feeling of disgust, our model generates an image where the tomato appears distorted, conditioned on the given emotion category.\nMoreover, despite the inherent noise in speech, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> utilizes SIB to refine the information and capture its meaning, effectively following the content of the prompt.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "terms",
                    "emotional",
                    "emotion",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F4\" title=\"In B.2 Qualitative results &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B4</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F5\" title=\"In B.2 Qualitative results &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B5</span></a> provide more results generated using the parameters of SD1.5 and SDXL, respectively.\nEspecially for the example of &#8216;a woman with blonde hair is standing in a room.&#8217;, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> poses the sadness in her facial expression.\nWhile we freeze parameters of the image generator, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F5\" title=\"In B.2 Qualitative results &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B5</span></a> shows that generated images ensure high fidelity and text relevancy.</p>\n\n",
                "matched_terms": [
                    "images",
                    "while",
                    "shows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess how well our model captures paralinguistic information in speech, we conducted a human evaluation to measure the alignment between the emotions perceived in the input speech and those conveyed in the generated images.\nWe had 26 locally recruited participants evaluate 25 images.\nFive images represent each emotion, but we did not provide any information about which emotion each speech sample conveyed.\nParticipants evaluated the images with randomly mixed emotion classes.\nThe instruction in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F6\" title=\"In B.3 Details of human evaluation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B6</span></a> is used in human evaluation.</p>\n\n",
                "matched_terms": [
                    "images",
                    "emotion",
                    "speech",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We recruit 17 independent evaluators to assess image quality and speech prompt fidelity on SpokenCOCO in Fig. 7(b) of the main paper.\nIn this experiment, the instructions in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F7\" title=\"In B.3 Details of human evaluation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B7</span></a> are used for evaluating 10 different images generated by SpeechCLIP+, TMT, and <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, respectively.</p>\n\n",
                "matched_terms": [
                    "images",
                    "speech",
                    "quality",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although it is impossible to manually verify all samples, we found that SpokenCOCO dataset, which was created using human annotators via AMT, often contains misrecorded speech samples.\nFor example, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F9.sf1\" title=\"In Figure B9 &#8227; B.4 Failure cases and limitation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9(a)</span></a>, some recordings mispronounce the text prompt originally associated with the image.\nTherefore, using speech inputs that contain such errors is inevitably prone to performance degradation compared to using text inputs.\nAdditionally, the clarity of word representation in speech depends on the speaker&#8217;s pronunciation, making it challenging to distinguish homophones or similarly pronounced words.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "words",
                    "spokencoco",
                    "contains",
                    "contain"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "Seeing What You Say: Expressive Image Generation from Speech",
        "caption": "Table 2: Performance comparison with baselines; SDÂ [46], SpeechCLIP+Â [52] and TMTÂ [28].\nâInputâ denotes the data type of the input condition for generative models: âTâ is text and âSâ is speech. SpokenCOCO contains 591k training utterances, Flickr has 30k, and VoxEmoset includes 208k. All methods were implemented on frozen image generators. â \\dagger: SpeechCLIP+ is finetuned on VoxEmoset. â¡\\ddagger: TMT used an additional 15M synthesized speech for training.",
        "body": "Method\nSD\n\n\n\n# training\n\nutterances\n \nInput\n(Spoken)COCO\nVoxEmoset\n\n\n\nFIDâ\\downarrow\n\n\nCLIPScoreâ\\uparrow\n\n\nFIDâ\\downarrow\n\n\nCLIPScoreâ\\uparrow\n\n\nEmo-Aâ\\uparrow\n\n\n\nT2I\n1.5\n-\nText\n23.37\n31.14\n20.21\n31.70\n60.81\n\n\nWhisper (ASR)\n1.5\n-\nText\n22.95\n31.08\n20.23\n31.57\n60.41\n\n\nSpeechCLIP+\n1.5\n621k\nSpeech\n28.29\n25.03\n33.75\n21.84\n37.42\n\n\n\nSpeechCLIP+â \\dagger\n\n1.5\n829k\nSpeech\n27.58\n26.29\n28.80\n26.72\n56.39\n\n\nTMT\n2.1\n\n15.6Mâ¡\\ddagger\n\nSpeech\n25.48\n28.26\n29.48\n26.08\n48.54\n\n\nVoxStudio\n1.5\n799k\nSpeech\n27.20\n28.71\n25.01\n28.71\n67.09",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">SD</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\"># training</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">utterances</span></span>\n</span> </span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Input</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">(Spoken)COCO</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text\" style=\"font-size:90%;\">VoxEmoset</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FID</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CLIPScore</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FID</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CLIPScore</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Emo-A</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">T2I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Text</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.37</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">31.14</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">20.21</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">31.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">60.81</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Whisper (ASR)</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">Text</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">22.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.08</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.57</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">60.41</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">SpeechCLIP+</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">621k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Speech</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.29</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">33.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">21.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">37.42</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SpeechCLIP+</span><math alttext=\"\\dagger\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#8224;</mo><annotation encoding=\"application/x-tex\">\\dagger</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">829k</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">Speech</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.58</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.29</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.80</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.72</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.39</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">TMT</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.1</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">15.6M</span><math alttext=\"\\ddagger\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#8225;</mo><annotation encoding=\"application/x-tex\">\\ddagger</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">Speech</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">25.48</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.26</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.48</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.08</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">48.54</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">VoxStudio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">799k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">Speech</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">28.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">25.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">28.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">67.09</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "type",
            "training",
            "text",
            "input",
            "voxstudio",
            "implemented",
            "includes",
            "frozen",
            "âinputâ",
            "used",
            "generative",
            "has",
            "additional",
            "speech",
            "clipscoreâuparrow",
            "829k",
            "15m",
            "all",
            "synthesized",
            "speechclip",
            "methods",
            "fidâdownarrow",
            "621k",
            "whisper",
            "799k",
            "208k",
            "utterances",
            "voxemoset",
            "30k",
            "â¡ddagger",
            "denotes",
            "image",
            "âsâ",
            "âtâ",
            "spokencoco",
            "performance",
            "591k",
            "156mâ¡ddagger",
            "generators",
            "asr",
            "finetuned",
            "models",
            "flickr",
            "t2i",
            "tmt",
            "comparison",
            "method",
            "condition",
            "â dagger",
            "emoaâuparrow",
            "data",
            "baselines",
            "contains",
            "speechclipâ dagger"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results on SpokenCOCO and VoxEmoset.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S4.T2\" title=\"In 4.4 Dataset quality &#8227; 4 VoxEmoset Benchmark &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the comparison of <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> and baselines<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>EmoGen was excluded because its pretrained weights are publicly unavailable, and our reimplementation was unable to match its reported performance.</span></span></span> on SpokenCOCO and VoxEmoset.\nSD1.5 with the text inputs (<em class=\"ltx_emph ltx_font_italic\">i.e</em>., without speech) is shown as a baseline.\nEspecially, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S4.F5\" title=\"In 4.4 Dataset quality &#8227; 4 VoxEmoset Benchmark &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> highlights the stark contrast between text- and speech-based generation.\nWhile speech conveys emotions even with the same wording, the text-based model inherently ignores these cues and focuses on fact-based generation.\nEven when trained on VoxEmoset, &#8216;SD (finetuning)&#8217; struggles to express emotions as semantic content, but speech leads to a richer and intense emotional expression.\nFor example, given the prompt &#8216;A black trash can is placed against a white wall,&#8217; our model detects disgust from spoken nuances and visually emphasizes the unpleasantness of trash, whereas the text-based model remains neutral.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This paper proposes <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, the first unified and end-to-end speech-to-image model that generates expressive images directly from spoken descriptions by jointly aligning linguistic and paralinguistic information.\nAt its core is a speech information bottleneck (SIB) module, which compresses raw speech into compact semantic tokens, preserving prosody and emotional nuance.\nBy operating directly on these tokens, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> eliminates the need for an additional speech-to-text system, which often ignores the hidden details beyond text, <em class=\"ltx_emph ltx_font_italic\">e.g</em>., tone or emotion.\nWe also release VoxEmoset, a large&#8208;scale paired emotional speech&#8211;image dataset built via an advanced TTS engine to affordably generate richly expressive utterances.\nComprehensive experiments on the SpokenCOCO, Flickr8kAudio, and VoxEmoset benchmarks demonstrate the feasibility of our method and highlight key challenges, including emotional consistency and linguistic ambiguity, paving the way for future research.\n<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\"/><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\"/><span class=\"ltx_tag ltx_tag_note\"/><sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup> Partly work done in NAVER AI Lab.\n<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> Corresponding author.</span></span></span></p>\n\n",
                "matched_terms": [
                    "speech",
                    "utterances",
                    "voxemoset",
                    "text",
                    "voxstudio",
                    "method",
                    "spokencoco",
                    "additional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans naturally &#8220;imagine&#8221; vivid mental images when listening to speech, which conveys not only semantics but also emotion, tone, and intent.\nSpeech-to-image (S2I) generation taps this rich, multimodal expressiveness to produce visuals that are more nuanced and emotionally resonant than those driven by text alone. By translating spoken descriptions directly into images, S2I can unlock applications in accessibility, creative media, and voice&#8208;driven interfaces&#8212;treating speech as a first&#8208;class modality for content creation rather than a mere precursor to text.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in text-to-image (T2I) generation have demonstrated remarkable progress, but they struggle to utilize the innate expressiveness and accessibility of speech.\nMost cascaded framework&#8212;where an utterance is first transcribed into text or textual feature and then used as input for T2I models, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S1.F2\" title=\"In 1 Introduction &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> (a, b)&#8212;encounters several significant challenges.\nFirst, speech-to-text (<em class=\"ltx_emph ltx_font_italic\">i.e</em>., ASR) transcription is limited to capturing prosody and speaker intention.\nHowever, transcription errors propagate into the image generative model, then degrade visual quality.\nSecond, this sequential approach inherently decouples speech and image generation, making it difficult to transfer crucial prosodic and temporal cues&#8212;such as speaking rate, pitch variation, and emotional style&#8212;that can influence the mood, color palette, or overall aesthetic of the generated image.\nRelying on intermediate text also excludes languages without written forms&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib50\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">50</span></a>]</cite>.\nEven for languages that do have a writing system, coverage for the cascaded approach remains far from comprehensive: there are over 7,100 languages worldwide&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib13\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">13</span></a>]</cite>, <em class=\"ltx_emph ltx_font_italic\">e.g</em>. Google API covers only 125<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1a\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://cloud.google.com/speech-to-text</span></span></span>.\nFinally, the cascaded system limits the inference speed and requires a higher cost than our unified system, as shown in the table of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S1.F2\" title=\"In 1 Introduction &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nThese limitations underscore the necessity of an end-to-end approach that directly maps raw speech to images, enabling a more seamless and expressive integration of modalities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "models",
                    "t2i",
                    "input",
                    "image",
                    "used",
                    "generative",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, incorporating speech input directly into a pre-trained T2I model poses distinct obstacles rooted in the nature of the two modalities.\nSpeech is a continuous, high-dimensional signal rich in temporal dynamics and spectral detail, whereas T2I models are designed to process compact sequences of token embeddings.\nBridging this gap requires effective speech representations that can capture both semantic and paralinguistic cues - yet remain mappable to their latent space.\nThis alignment is complicated by differing tokenization schemes, variable sequence lengths, and unique contextual subtleties inherent to spoken language.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "t2i",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, a novel speech-to-image model that bridges the rich information in speech with the image modality space, enabling more diverse and expressive visual representations.\nBuilding upon T2I models, our framework is suitable for the unique characteristics of speech, which differ from text:\n(1) Speech generally contains longer and more variable sequences than text, leading to uneven information density across embeddings.\n(2) Speech signals vary significantly depending on speaker identity, recording environment, and emotional state, affecting articulation and duration, even for the same content.\nTo address these challenges, we introduce a speech information bottleneck (SIB) that efficiently aligns cross-modal latent spaces while preserving key speech features.\nOur SIB encodes compressed conditional features that guide the image generation process.\nThrough extensive experiments, we establish an effective speech-based guidance for image generation by identifying the optimal combination of speech encoder, SIB, and image generator.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "models",
                    "t2i",
                    "voxstudio",
                    "image",
                    "contains"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is a unified image generation model with expressive utterance, where both linguistic and paralinguistic cues are compactly captured via the SIB module.</p>\n\n",
                "matched_terms": [
                    "image",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">VoxEmoset</span>, an automatically (and efficiently) synthesized dataset of 247k emotional spoken descriptions for sentiment images. VoxEmoset is used for both training and evaluation of S2I.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "used",
                    "training",
                    "voxemoset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> in various S2I benchmarks, including SpokenCOCO&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite>, Flickr8kAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> and VoxEmoset, and demonstrate its superiority and high fidelity over baselines.</p>\n\n",
                "matched_terms": [
                    "spokencoco",
                    "baselines",
                    "voxemoset",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conditions for image generation.</span>\nRecently, diffusion-based conditional image generative models have emerged with remarkable performance <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib44\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">44</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib40\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">40</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib41\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">41</span></a>]</cite>. Specifically, stable diffusion (SD)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite> has shown impressive results in both quality and generalizability.\nGiven that these models only take text as a condition, they have struggled to reflect individual thoughts and emotions beyond text into compelling images.\nSome methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib57\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">57</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite> have proposed emotional image generation, pointing out the importance of reacting to the user&#8217;s sentiment.\nHowever, they relied on explicit linguistic expressions (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., <span class=\"ltx_text ltx_font_italic\">&#8216;with a sense of happiness and joy&#8217;</span> in the text prompt) and focused on reflecting emotion in texture and color only&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib2\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></a>]</cite>.\nRecently, EmoGen&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib59\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">59</span></a>]</cite> and EmoEdit&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib60\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">60</span></a>]</cite> argued that emotional contents beyond color and style should be effectively expressed as semantic variations in a generated image.\nThey learned a more flexible generative model using a large-scale EmoSet dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib58\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">58</span></a>]</cite>, but still required users to explicitly specify emotion prompts.\nIn contrast, our approach automatically infers these nuances directly from the speaker&#8217;s voice.</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "image",
                    "methods",
                    "condition",
                    "performance",
                    "generative",
                    "has"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, speech naturally encodes nuanced emotion and tone&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib50\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">50</span></a>]</cite>, offering a more intuitive means for generating emotionally resonant images, yet it remains largely untapped as a conditioning signal.\nRecent audiovisual generation methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib29\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">29</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">7</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib26\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">26</span></a>]</cite> have been limited to relying only on semantic instances expressed in text, where the other expressions are excluded.\nMoreover, existing approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib54\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">54</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib55\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">55</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite> for S2I generation have used highly limited datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite>, restricting their expressive versatility.\nWe aim to design a unified and emotion-driven S2I framework as well as to introduce a large-scale dataset for both training and evaluation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training",
                    "text",
                    "methods",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Relationship between speech and image.</span>\nSpeech-image relationships have been widely explored in biometrics&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib31\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">31</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib39\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">39</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>]</cite>, linguistic alignment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>, and phonetic articulation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib56\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">56</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib9\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">9</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib10\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">10</span></a>]</cite>.\nThese studies provide valuable insights into how speech and vision interact in different contexts.\nAlso, image-speech retrieval&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> has explored the alignment between spoken descriptions and images, highlighting the importance of understanding the semantics in both modalities.\nDespite these advances, most of the existing studies focus on isolated characteristics between modalities.\nBy moving beyond traditional mappings, our work aims to bridge the gap by simultaneously leveraging natural semantic correspondences, <em class=\"ltx_emph ltx_font_italic\">i.e</em>., both linguistic and paralinguistic information, between speech and vision.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "image",
                    "has"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S2.F3\" title=\"In 2 Related Work &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the overall framework of <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, consisting of (1) a pretrained speech encoder, (2) SIB to reduce the computational cost and effectively connect heterogeneous two modalities, and (3) an image generator to synthesize an image from the compressed speech representations.\nNext, we describe each module in detail.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "image",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider two pre-trained speech encoders, SONAR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> and Whisper large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib43\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">43</span></a>]</cite>, to deploy comprehensive speech features considering both linguistic and paralinguistic information.\nBriefly reviewed, SONAR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> has learned global semantic alignment between speech and text, enabling it to encode meaning beyond phonetic content.\nWe remove its last aggregation layer to ensure that the speech embeddings retain both linguistic and paralinguistic information.\nWe also test Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib43\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">43</span></a>]</cite>, a widely used speech recognition model. Whisper is known to be capable of capturing paralinguistic information, such as emotion and speaker identity&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib64\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">64</span></a>]</cite>.\nFormally, given an input utterance <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math>, we obtain a speech embedding <math alttext=\"s\\in\\mathrm{R}^{N\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo>&#8712;</mo><msup><mi mathvariant=\"normal\">R</mi><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">s\\in\\mathrm{R}^{N\\times D}</annotation></semantics></math>, where <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> depends on the length of the speech and models, and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> is the channel dimension of the final output layer.\nWe note that our work does not employ ASR system or text encoder to explicitly map the speech into text.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "models",
                    "input",
                    "used",
                    "whisper",
                    "has",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although speech embeddings contain rich representations, they are excessively long and lead to a lower information density in each speech token compared to text (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., Whisper encodes a maximum of 1500 tokens for 30-seconds long speech, while CLIP text encoder limited to 77 tokens).\nThis low density makes direct usage challenging to condition the image generator.\nTo solve this problem, we design a Transformer-based speech information bottleneck (SIB) module.\nSIB compacts semantics in speech embeddings, motivated by previous works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib51\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">51</span></a>]</cite> applied to individual image and audio encoders.\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S2.F3\" title=\"In 2 Related Work &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, SIB reduces the number of embeddings with a strided convolution layer after a Transformer block along the time axis.\nBased on our findings, a pooling ratio of 8 provides the optimal balance, allowing us to maximize the information retention of speech features.\nAs a result, the initial embedding <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> is processed into a compressed speech condition <math alttext=\"c=f_{\\psi}(s)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>=</mo><mrow><msub><mi>f</mi><mi>&#968;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">c=f_{\\psi}(s)</annotation></semantics></math>, where <math alttext=\"c\\in\\mathrm{R}^{M\\times D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>&#8712;</mo><msup><mi mathvariant=\"normal\">R</mi><mrow><mi>M</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>D</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">c\\in\\mathrm{R}^{M\\times D^{\\prime}}</annotation></semantics></math>, <math alttext=\"M=N/8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>=</mo><mrow><mi>N</mi><mo>/</mo><mn>8</mn></mrow></mrow><annotation encoding=\"application/x-tex\">M=N/8</annotation></semantics></math> and <math alttext=\"D^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><msup><mi>D</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">D^{\\prime}</annotation></semantics></math> is the input channel of the cross-attention block in the image generator.\nThose compressed representations improve the efficiency of the S2I process while preserving both linguistic and emotional expressiveness.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "input",
                    "image",
                    "condition",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The image generator is based on the latent diffusion model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite>.\nThe speech condition <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math>, compressed through SIB, is fed into the generator as a guidance of the synthesis process.\nSpecifically, the speech embeddings are injected into the UNet through cross-attention layers to condition the image synthesis.\nThis conditioning allows the model to incorporate the emotional, semantic content of speech into the generation process.\nThe image generator and SIB are optimized with the diffusion loss&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite>.\nGiven that we do not design a specialized loss function compared to previous works such as contrastive learning in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>]</cite>, AR modeling in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>, our simple training framework ensures versatile connections for various image generators.\nIn inference, the denoised latent is decoded into the image through the decoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib30\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>]</cite>.\nGiven that we do not design a specialized loss function compared to previous works, such as contrastive learning in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>]</cite>, AR modeling in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>,\nour simple framework ensures versatile connections for various image generators.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training",
                    "image",
                    "condition",
                    "generators"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is to generate an image with a corresponding spoken description, even for emotional expression.\nHowever, prior datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite> overlooked paralinguistic features in speech, and also required significant costs for human recordings.\nOur benchmark uniquely leverages synthesized speech, enabling the natural and cost-effective creation of a large-scale dataset.\nSpecifically, VoxEmoset leverages semantic knowledge and the generative powers of pre-trained multimodal LLMs and diffusion models to generate diverse synthetic data samples.\nFirst, a multimodal LLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite> generates corresponding captions that are factual descriptions of a given emotional image based on explaining environments or objects.\nThen, a TTS model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite> generates emotional speech samples from text captions, using emotional voice samples from other datasets as references.\nConsequently, we efficiently and cheaply generate large-scale emotional utterances along with text captions, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S3.F4\" title=\"In 3.3 Image generator &#8227; 3 VoxStudio &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "utterances",
                    "voxemoset",
                    "text",
                    "models",
                    "synthesized",
                    "voxstudio",
                    "image",
                    "data",
                    "generative"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Emotional utterances are generated by a text-to-speech (TTS) system that can synthesize the speech with emotional attributes.\nThis strategy eliminates the dependency on skilled voice actors or noisy crowdsourcing.\nThrough empirical comparison of recent TTS models based on diffusion, autoregressive, and non-autoregressive architectures, F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite> demonstrates remarkable quality in both linguistic and emotional expression.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "comparison",
                    "utterances"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, to build a diverse range of emotional voice references for TTS, emotional speech data was collected from multiple datasets, including CREMA-D&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib3\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></a>]</cite>, MEAD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib53\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">53</span></a>]</cite>, and RAVDESS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib34\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">34</span></a>]</cite>.\nThese datasets contain English-spoken utterances from a variety of speakers.\nFollowing EmoBox&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>]</cite>, we split the datasets into training and test sets.\nWe validate the emotions in the generated speech using Emotion2Vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib37\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">37</span></a>]</cite> to measure emotional intensity, filtering and re-generating inadequate samples. After this process, 247k speech samples are generated.\nFurther details are provided in Appendix.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "training",
                    "utterances"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To objectively assess the quality of generated utterances, we randomly sample 10k utterances from each dataset and measure NMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib45\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">45</span></a>]</cite>. For CREMA-D, we use the entire samples.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S3.T1\" title=\"In 3.3 Image generator &#8227; 3 VoxStudio &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> shows that VoxEmoset is compatible with existing speech-image datasets such as SpokenCOCO and Flickr8kAudio in terms of speech quality (NMOS) and description quality (CLIPScore).\nHowever, only our benchmark explicitly expresses emotion in speech.\nThe last two rows in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S3.T1\" title=\"In 3.3 Image generator &#8227; 3 VoxStudio &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> validates that VoxEmoset guarantees high perceptual fidelity with clear affect, where emotion discriminability (Emo-C) is measured as the emotion classifier&#8217;s average confidence score.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "voxemoset",
                    "utterances",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets.</span>\nWe use SpokenCOCO&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite> and VoxEmoset to train <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>.\nVoxEmoset includes 208k utterances with paired 69k images for training, while SpokenCOCO contains 118k images with 591k utterances.\nFlickr8kAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> is used to evaluate zero-shot generalizability.\nEach image in SpokenCOCO and Flickr8kAudio has five voice recordings from unskilled annotators, resulting in inherently noisy audio (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., the recording may contain background noise, reading speed or volume can vary, and pronunciation may not be as clear as that of skilled voice actors as in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.SS4\" title=\"B.4 Failure cases and limitation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B.4</span></a>).\nVoxEmoset is automatically generated and less prone to recording noise.\nWe use the Karpathy split&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib27\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">27</span></a>]</cite> for SpokenCOCO and Flickr8kAudio.</p>\n\n",
                "matched_terms": [
                    "208k",
                    "training",
                    "utterances",
                    "voxemoset",
                    "591k",
                    "voxstudio",
                    "image",
                    "used",
                    "spokencoco",
                    "includes",
                    "contains",
                    "has"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation details.</span>\nTraining a high-performance image generator requires a vast amount of resources (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., SD1.5 requires 6,000 A100 GPU days&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib5\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></a>]</cite>).\nWe initialize the image generator with a pre-trained SD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite> for efficient learning.\nWe use SONAR as the speech encoder, freezing during the training, in which its last aggregation layer is removed.\nWe use AdamW <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib35\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">35</span></a>]</cite> with the learning rate of <math alttext=\"1e\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">1e</annotation></semantics></math>-<math alttext=\"6\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>, the batch size of 128 using 8 V100 GPUs.\nFP16 precision is used for all experiments.\nThe code will be released.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training",
                    "all",
                    "image",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation metrics.</span>\nWe assess the generation quality using FID&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib22\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">22</span></a>]</cite>, while content alignment between speech and generated images is measured with CLIPScore&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib21\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite> using text transcriptions.\nFor SpokenCOCO and VoxEmoset, random samples of 10k condition prompts, either speech or text, are used for evaluation.\nFor Flickr8kAudio, we use 5k test prompts for evaluation.\nWe also report emotion classification accuracy (Emo-A)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib59\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">59</span></a>]</cite> on generated images to examine whether the results reflect emotion from prompts.\nNote that we measure accuracy only with scores for the 5 emotion categories <math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math><span class=\"ltx_text ltx_font_italic\">&#8216;amusement&#8217;</span> and <span class=\"ltx_text ltx_font_italic\">&#8216;excitement&#8217;</span> are classified as the same class<math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math> in the trained emotion classifier.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "voxemoset",
                    "text",
                    "condition",
                    "spokencoco",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, despite the inherent noise in speech features and our method needs significantly lower latency compared to text-based approaches, the performance gap remains minimal in image quality and text alignment.\nMoreover, as shown in the last example in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S4.F5\" title=\"In 4.4 Dataset quality &#8227; 4 VoxEmoset Benchmark &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, the CLIP encoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib42\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">42</span></a>]</cite> often overlooks information from the latter part of a sentence&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib63\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">63</span></a>]</cite> (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., &#8216;bright yellow&#8217; in the last example).\nHowever, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> excels in conveying emotions when trained on the same datasets.\nThis advocates that speech, as a richer modality for emotional expression, provides a more effective signal to generate emotionally compelling images.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "voxstudio",
                    "image",
                    "method",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Remarkably, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> outperforms SpeechCLIP+ and TMT on SpokenCOCO, where <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> does not use Flickr8kAudio for training.\nWhile TMT additionally used huge synthesized speech data from CC3M&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib49\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">49</span></a>]</cite> and CC12M&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>]</cite> for training, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> also show comparable results on VoxEmoset.\nThis result demonstrates that our diffusion model is a powerful learner for speech-to-expressive image alignment than contrastive learning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>]</cite> and auto-regressive training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>.\nThe qualitative comparison on SpokenCOCO shows that SpeechCLIP+ and TMT often ignore keywords in the prompts, while <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> can capture the details, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F6\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training",
                    "voxemoset",
                    "synthesized",
                    "data",
                    "voxstudio",
                    "tmt",
                    "image",
                    "spokencoco",
                    "speechclip",
                    "used",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results on Flickr8kAudio.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T3\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the performance comparison on Flickr8kAudio.\nHere, while TMT and SpeechCLIP+ used Flickr8kAudio for training, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> was evaluated in a zero-shot manner.\nSurprisingly, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> outperforms existing methods by large margins.\nIt shows that end-to-end training in <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is more robust in aligning the speech-language space.\nBy contrast, speech features in <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> are more robust to the order or length of the prompt.\nMoreover, VoxEmoset might improve the robustness on generality as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F6\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training",
                    "voxemoset",
                    "voxstudio",
                    "tmt",
                    "methods",
                    "speechclip",
                    "performance",
                    "used",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human evaluation.</span>\nA user study is conducted to assess how well humans perceive the alignment between speech and image atmosphere.\n26 participants evaluated 25 images to rate how well the emotion conveyed in the image matched the given speech.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F7.sf1\" title=\"In Figure 7 &#8227; 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7(a)</span></a> shows that results from <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> are more aligned with the emotion than text-based SD in all categories.\nIn other words, with an average of 57.09% preference, the images generated by our <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> were rated as better at expressing emotions.\nIt highlights the effectiveness of speech prompts for expressive image synthesis.\nWe also carried out another human evaluation on SpokenCOCO across speech-based models.\nThis experiment is performed on 17 participants who evaluate 10 generated images for each model with a 5-point Likert scale.\nAs demonstrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F7.sf2\" title=\"In Figure 7 &#8227; 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7(b)</span></a>, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> outperforms existing approaches by generating high-quality images that accurately reflect the nuances of the input prompts.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "all",
                    "input",
                    "voxstudio",
                    "image",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effect of emotion.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F8\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a> demonstrates that the same description, when spoken with different emotions, leads to distinct visual outputs by <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>. This highlights <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>&#8217;s capability to produce emotional nuances beyond linguistic content.\nFor instance, a neutral statement spoken in a disgusted tone results in negative visual details (top-left), while an &#8220;enjoying&#8221; tone generates a more positive scene (top-right).\nThese findings show that our speech-based approach effectively leverages emotional cues, enabling more expressive and context-rich image generation.</p>\n\n",
                "matched_terms": [
                    "image",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generalization on real speech.</span>\nTo evaluate how well our model generalizes, we test on utterances in ESD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib65\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">65</span></a>]</cite> dataset, excluded from our reference samples during speech synthesis.\nWe visualize generated samples from real speakers&#8217; utterances in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F9\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a>.\nText-based generator is limited to expressing the tone in speech prompt, but <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> successfully expresses atmospheres despite ambiguous words.\nIt also proves the superiority of VoxEmoset in that <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> trained on synthesized emotional speech is well generalized in real utterances.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F8\" title=\"In B.3 Details of human evaluation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B8</span></a> also demonstrates that <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> extends naturally to various applications such as image editing by spoken prompt.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "voxemoset",
                    "utterances",
                    "synthesized",
                    "voxstudio",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training datasets.</span> <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T4\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that VoxEmoset is complementary with the real-world spoken dataset, SpokenCOCO, improving both visual fidelity and semantic relevance.</p>\n\n",
                "matched_terms": [
                    "training",
                    "voxemoset",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training strategies.</span>\nDiffusion training is usually computationally expensive.\nWe test different training strategies in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T5\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>: full finetuning, LoRA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib24\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">24</span></a>]</cite>, and freezing the model.\nWhile finetuning achieves the best performance, LoRA and frozen models show comparable results in CLIPScore and Emo-A.\nAdditionally, although speech is noisier than text, our method outperforms full finetuning for original SD1.5 (&#8216;SD(T2I)-FT&#8217; in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T5\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>) in terms of emotional expression, while maintaining the generation quality.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training",
                    "text",
                    "models",
                    "method",
                    "frozen",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scale of the image generator.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T6\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> demonstrates the performance of image generators at different scales.\nDue to the resource limit, we compare UNet of SD1.5&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite> and SDXL&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib41\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">41</span></a>]</cite> as our image generator in a frozen state during the training.\nInterestingly, although the small generator achieves a higher CLIPScore, the larger generator excels at displaying emotional nuances.\nThis finding suggests that larger-scale generators are inherently better at representing content beyond simple text cues.</p>\n\n",
                "matched_terms": [
                    "training",
                    "text",
                    "image",
                    "frozen",
                    "performance",
                    "generators"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech embedding.</span>\nWe compare SONAR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> and Whisper-Large v3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib43\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">43</span></a>]</cite> encoders as a speech input handler of our method.\nWhisper is a widely used ASR model, also known to be capable of capturing paralinguistic information&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib64\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">64</span></a>]</cite>.\nWhile SONAR is sentence-level speech-text aligned features, Whisper is trained at the phoneme-level by predicting which words are spoken in a given audio snippet.\nThis fundamental difference affects how each encoder preserves linguistic content and emotional cues when mapping speech to image descriptions.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T7\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a> demonstrates that text-aligned embeddings (<em class=\"ltx_emph ltx_font_italic\">i.e</em>., SONAR) show more robust performance on our task.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "input",
                    "image",
                    "method",
                    "performance",
                    "used",
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Architecture choices on SIB.</span>\nWe propose SIB to represent the speech condition compactly to address the issue of low information density of speech tokens.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T8\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a> compares its performance against a standard transformer structure.\nOur design choice achieves better performance with fewer parameters.\nGradually reducing the speech token length while simultaneously increasing their density across multiple layers enhances both the linguistic and paralinguistic expressiveness of the speech signal.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance",
                    "condition"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image editing using speech prompt.</span>\n<span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is built upon the SD architecture, allowing seamless integration with various extensions and applications.\nFor instance, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F8\" title=\"In B.3 Details of human evaluation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B8</span></a>, the image editing pipeline can be directly applied with speech prompts to modify input images.\nBeyond basic editing, our framework can be extended to other tasks built on SD, including personalized generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib47\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">47</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib61\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">61</span></a>]</cite> and multimodal content synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib17\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">17</span></a>]</cite>.\nIt provides a versatile foundation for future developments in S2I generation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "image",
                    "input",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is the first end-to-end S2I model that captures both linguistic and emotional nuances from speech.\nUnlike text-based methods, our approach totally leverages speech&#8217;s expressiveness to generate emotionally aligned images. VoxEmoset is built cheaply, but it is complementary with real-world datasets.\nOur experiments demonstrate that <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> not only outperforms prior speech-based methods in conveying sentiment through images, but also matches text-driven approaches in semantic alignment, despite the higher noise and lower latency of the speech modality.\nWe believe our work facilitates future research in voice-driven generative models and their applications.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "voxemoset",
                    "models",
                    "voxstudio",
                    "methods",
                    "generative"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our objective for data construction is primarily on (1) synthesizing large-scale image and speech pairs, (2) the speech will be emotional rich, and (3) closely matches the quality of real recordings while diversifying the range of speakers.\nHere we supplement the details of VoxEmoset.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "image",
                    "voxemoset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the characteristics of SpokenCOCO, we limit the length of captions and ensure they accurately describe the context of the image.\nIn <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F1\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B1</span></a>, the word count distribution remains similar to SpokenCOCO and Flickr8kAudio, but with a more structured and consistent pattern.</p>\n\n",
                "matched_terms": [
                    "image",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech generation, we use state-of-the-art F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite>, where the vocoder is trained on 24kHz.\nThe generated speech is resampled to 16kHz to use SONAR and Whisper encoders.\nTo diversify the speaker characteristics, we collect multiple emotional speech datasets: MEAD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib53\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">53</span></a>]</cite>, CREMA-D&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib3\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></a>]</cite>, and RAVDESS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib34\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">34</span></a>]</cite>, which are widely used in emotional speech synthesis and speech emotion recognition.\nMEAD is an audiovisual dataset annotated in 8 emotional categories.\nCREMA-D is a crowd-sourced actor dataset, using 6 emotion classes.\nRAVDESS contains audio and video, where the professional actors express emotion.\nAll datasets used English.\nThe split cleaned up by EmoBox&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>]</cite> is used, especially the fold 1 split for RAVDESS.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.T1\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B1</span></a> summarizes the number of emotion classes and speakers for each dataset.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "contains",
                    "all",
                    "used",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results for <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> on SpokenCOCO according to the difference of training data is shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.T2\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B2</span></a>.\nIn the results, we observe that SpokenCOCO does not fully capture the diversity of real-world scenarios.\nMost images convey neutral emotions and primarily depict scenes suitable for objective descriptions.\nHowever, real-world photographs go beyond presenting mere facts, often communicating higher-level meanings such as feelings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib59\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">59</span></a>]</cite>.\nThis demonstrates that our dataset extends beyond the distribution of SpokenCOCO and Flickr8kAudio, offering a more realistic and emotionally expressive representation.\nThis claim is further supported by Table 2 of the main paper, where models trained on the SpokenCOCO and Flickr8kAudio datasets&#8212; even TMT, which was trained on a massive amount of synthesized speech from CC3M and CC12M&#8212;fail to generalize to our dataset.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training",
                    "models",
                    "synthesized",
                    "voxstudio",
                    "tmt",
                    "spokencoco",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We show more qualitative comparisons in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F2\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F3\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B3</span></a>.\nIn those experiments, our image generators are initialized by UNet parameters in SD1.5.\nWhen training SD with a CLIP encoder using text prompts from our dataset, the model better follows the content of the text compared to zero-shot generation.\nHowever, in terms of emotional intensity and expressiveness, it performs weaker than our approach using speech prompts.\nFor example, in the last row in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F3\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B3</span></a>, generating an emotionally rich image from a sentence like &#8216;a tomato is cut into sections on a white plate&#8217; is challenging.\nBy using speech input that conveys a feeling of disgust, our model generates an image where the tomato appears distorted, conditioned on the given emotion category.\nMoreover, despite the inherent noise in speech, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> utilizes SIB to refine the information and capture its meaning, effectively following the content of the prompt.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training",
                    "text",
                    "input",
                    "voxstudio",
                    "image",
                    "generators"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F4\" title=\"In B.2 Qualitative results &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B4</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F5\" title=\"In B.2 Qualitative results &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B5</span></a> provide more results generated using the parameters of SD1.5 and SDXL, respectively.\nEspecially for the example of &#8216;a woman with blonde hair is standing in a room.&#8217;, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> poses the sadness in her facial expression.\nWhile we freeze parameters of the image generator, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F5\" title=\"In B.2 Qualitative results &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B5</span></a> shows that generated images ensure high fidelity and text relevancy.</p>\n\n",
                "matched_terms": [
                    "text",
                    "image",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess how well our model captures paralinguistic information in speech, we conducted a human evaluation to measure the alignment between the emotions perceived in the input speech and those conveyed in the generated images.\nWe had 26 locally recruited participants evaluate 25 images.\nFive images represent each emotion, but we did not provide any information about which emotion each speech sample conveyed.\nParticipants evaluated the images with randomly mixed emotion classes.\nThe instruction in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F6\" title=\"In B.3 Details of human evaluation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B6</span></a> is used in human evaluation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "used",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We recruit 17 independent evaluators to assess image quality and speech prompt fidelity on SpokenCOCO in Fig. 7(b) of the main paper.\nIn this experiment, the instructions in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F7\" title=\"In B.3 Details of human evaluation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B7</span></a> are used for evaluating 10 different images generated by SpeechCLIP+, TMT, and <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, respectively.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "voxstudio",
                    "tmt",
                    "image",
                    "spokencoco",
                    "speechclip",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although it is impossible to manually verify all samples, we found that SpokenCOCO dataset, which was created using human annotators via AMT, often contains misrecorded speech samples.\nFor example, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F9.sf1\" title=\"In Figure B9 &#8227; B.4 Failure cases and limitation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9(a)</span></a>, some recordings mispronounce the text prompt originally associated with the image.\nTherefore, using speech inputs that contain such errors is inevitably prone to performance degradation compared to using text inputs.\nAdditionally, the clarity of word representation in speech depends on the speaker&#8217;s pronunciation, making it challenging to distinguish homophones or similarly pronounced words.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "all",
                    "image",
                    "spokencoco",
                    "performance",
                    "contains"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "Seeing What You Say: Expressive Image Generation from Speech",
        "caption": "Table 3: Performance comparison on Flikr8kAudio.",
        "body": "Method\nZero-shot\n\nFIDâ\\downarrow\n\n\nCLIPScoreâ\\uparrow\n\n\n\nSpeechCLIP+\nâ\n63.19\n23.71\n\n\nTMT\nâ\n57.34\n26.98\n\n\nVoxStudio\nâ\n55.01\n30.96",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Zero-shot</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FID</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CLIPScore</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">SpeechCLIP+</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#B3B3B3;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">63.19</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.71</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">TMT</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#B3B3B3;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.34</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.98</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">VoxStudio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#004D99;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">55.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">30.96</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "flikr8kaudio",
            "zeroshot",
            "fidâdownarrow",
            "clipscoreâuparrow",
            "tmt",
            "voxstudio",
            "method",
            "speechclip",
            "performance",
            "comparison"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results on Flickr8kAudio.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T3\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the performance comparison on Flickr8kAudio.\nHere, while TMT and SpeechCLIP+ used Flickr8kAudio for training, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> was evaluated in a zero-shot manner.\nSurprisingly, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> outperforms existing methods by large margins.\nIt shows that end-to-end training in <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is more robust in aligning the speech-language space.\nBy contrast, speech features in <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> are more robust to the order or length of the prompt.\nMoreover, VoxEmoset might improve the robustness on generality as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F6\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This paper proposes <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, the first unified and end-to-end speech-to-image model that generates expressive images directly from spoken descriptions by jointly aligning linguistic and paralinguistic information.\nAt its core is a speech information bottleneck (SIB) module, which compresses raw speech into compact semantic tokens, preserving prosody and emotional nuance.\nBy operating directly on these tokens, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> eliminates the need for an additional speech-to-text system, which often ignores the hidden details beyond text, <em class=\"ltx_emph ltx_font_italic\">e.g</em>., tone or emotion.\nWe also release VoxEmoset, a large&#8208;scale paired emotional speech&#8211;image dataset built via an advanced TTS engine to affordably generate richly expressive utterances.\nComprehensive experiments on the SpokenCOCO, Flickr8kAudio, and VoxEmoset benchmarks demonstrate the feasibility of our method and highlight key challenges, including emotional consistency and linguistic ambiguity, paving the way for future research.\n<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\"/><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\"/><span class=\"ltx_tag ltx_tag_note\"/><sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup> Partly work done in NAVER AI Lab.\n<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> Corresponding author.</span></span></span></p>\n\n",
                "matched_terms": [
                    "method",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets.</span>\nWe use SpokenCOCO&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite> and VoxEmoset to train <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>.\nVoxEmoset includes 208k utterances with paired 69k images for training, while SpokenCOCO contains 118k images with 591k utterances.\nFlickr8kAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> is used to evaluate zero-shot generalizability.\nEach image in SpokenCOCO and Flickr8kAudio has five voice recordings from unskilled annotators, resulting in inherently noisy audio (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., the recording may contain background noise, reading speed or volume can vary, and pronunciation may not be as clear as that of skilled voice actors as in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.SS4\" title=\"B.4 Failure cases and limitation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B.4</span></a>).\nVoxEmoset is automatically generated and less prone to recording noise.\nWe use the Karpathy split&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib27\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">27</span></a>]</cite> for SpokenCOCO and Flickr8kAudio.</p>\n\n",
                "matched_terms": [
                    "voxstudio",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results on SpokenCOCO and VoxEmoset.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S4.T2\" title=\"In 4.4 Dataset quality &#8227; 4 VoxEmoset Benchmark &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the comparison of <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> and baselines<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>EmoGen was excluded because its pretrained weights are publicly unavailable, and our reimplementation was unable to match its reported performance.</span></span></span> on SpokenCOCO and VoxEmoset.\nSD1.5 with the text inputs (<em class=\"ltx_emph ltx_font_italic\">i.e</em>., without speech) is shown as a baseline.\nEspecially, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S4.F5\" title=\"In 4.4 Dataset quality &#8227; 4 VoxEmoset Benchmark &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> highlights the stark contrast between text- and speech-based generation.\nWhile speech conveys emotions even with the same wording, the text-based model inherently ignores these cues and focuses on fact-based generation.\nEven when trained on VoxEmoset, &#8216;SD (finetuning)&#8217; struggles to express emotions as semantic content, but speech leads to a richer and intense emotional expression.\nFor example, given the prompt &#8216;A black trash can is placed against a white wall,&#8217; our model detects disgust from spoken nuances and visually emphasizes the unpleasantness of trash, whereas the text-based model remains neutral.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "comparison",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, despite the inherent noise in speech features and our method needs significantly lower latency compared to text-based approaches, the performance gap remains minimal in image quality and text alignment.\nMoreover, as shown in the last example in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S4.F5\" title=\"In 4.4 Dataset quality &#8227; 4 VoxEmoset Benchmark &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, the CLIP encoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib42\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">42</span></a>]</cite> often overlooks information from the latter part of a sentence&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib63\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">63</span></a>]</cite> (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., &#8216;bright yellow&#8217; in the last example).\nHowever, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> excels in conveying emotions when trained on the same datasets.\nThis advocates that speech, as a richer modality for emotional expression, provides a more effective signal to generate emotionally compelling images.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "method",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Remarkably, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> outperforms SpeechCLIP+ and TMT on SpokenCOCO, where <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> does not use Flickr8kAudio for training.\nWhile TMT additionally used huge synthesized speech data from CC3M&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib49\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">49</span></a>]</cite> and CC12M&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>]</cite> for training, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> also show comparable results on VoxEmoset.\nThis result demonstrates that our diffusion model is a powerful learner for speech-to-expressive image alignment than contrastive learning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>]</cite> and auto-regressive training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>.\nThe qualitative comparison on SpokenCOCO shows that SpeechCLIP+ and TMT often ignore keywords in the prompts, while <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> can capture the details, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F6\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "tmt",
                    "speechclip",
                    "comparison",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training strategies.</span>\nDiffusion training is usually computationally expensive.\nWe test different training strategies in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T5\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>: full finetuning, LoRA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib24\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">24</span></a>]</cite>, and freezing the model.\nWhile finetuning achieves the best performance, LoRA and frozen models show comparable results in CLIPScore and Emo-A.\nAdditionally, although speech is noisier than text, our method outperforms full finetuning for original SD1.5 (&#8216;SD(T2I)-FT&#8217; in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T5\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>) in terms of emotional expression, while maintaining the generation quality.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech embedding.</span>\nWe compare SONAR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> and Whisper-Large v3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib43\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">43</span></a>]</cite> encoders as a speech input handler of our method.\nWhisper is a widely used ASR model, also known to be capable of capturing paralinguistic information&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib64\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">64</span></a>]</cite>.\nWhile SONAR is sentence-level speech-text aligned features, Whisper is trained at the phoneme-level by predicting which words are spoken in a given audio snippet.\nThis fundamental difference affects how each encoder preserves linguistic content and emotional cues when mapping speech to image descriptions.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T7\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a> demonstrates that text-aligned embeddings (<em class=\"ltx_emph ltx_font_italic\">i.e</em>., SONAR) show more robust performance on our task.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results for <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> on SpokenCOCO according to the difference of training data is shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.T2\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B2</span></a>.\nIn the results, we observe that SpokenCOCO does not fully capture the diversity of real-world scenarios.\nMost images convey neutral emotions and primarily depict scenes suitable for objective descriptions.\nHowever, real-world photographs go beyond presenting mere facts, often communicating higher-level meanings such as feelings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib59\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">59</span></a>]</cite>.\nThis demonstrates that our dataset extends beyond the distribution of SpokenCOCO and Flickr8kAudio, offering a more realistic and emotionally expressive representation.\nThis claim is further supported by Table 2 of the main paper, where models trained on the SpokenCOCO and Flickr8kAudio datasets&#8212; even TMT, which was trained on a massive amount of synthesized speech from CC3M and CC12M&#8212;fail to generalize to our dataset.</p>\n\n",
                "matched_terms": [
                    "tmt",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We show more qualitative comparisons in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F2\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F3\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B3</span></a>.\nIn those experiments, our image generators are initialized by UNet parameters in SD1.5.\nWhen training SD with a CLIP encoder using text prompts from our dataset, the model better follows the content of the text compared to zero-shot generation.\nHowever, in terms of emotional intensity and expressiveness, it performs weaker than our approach using speech prompts.\nFor example, in the last row in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F3\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B3</span></a>, generating an emotionally rich image from a sentence like &#8216;a tomato is cut into sections on a white plate&#8217; is challenging.\nBy using speech input that conveys a feeling of disgust, our model generates an image where the tomato appears distorted, conditioned on the given emotion category.\nMoreover, despite the inherent noise in speech, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> utilizes SIB to refine the information and capture its meaning, effectively following the content of the prompt.</p>\n\n",
                "matched_terms": [
                    "voxstudio",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We recruit 17 independent evaluators to assess image quality and speech prompt fidelity on SpokenCOCO in Fig. 7(b) of the main paper.\nIn this experiment, the instructions in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F7\" title=\"In B.3 Details of human evaluation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B7</span></a> are used for evaluating 10 different images generated by SpeechCLIP+, TMT, and <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, respectively.</p>\n\n",
                "matched_terms": [
                    "tmt",
                    "speechclip",
                    "voxstudio"
                ]
            }
        ]
    },
    "S5.T4": {
        "source_file": "Seeing What You Say: Expressive Image Generation from Speech",
        "caption": "Table 4: Impact of the training datasets on VoxEmoset.",
        "body": "Training data\n\nFIDâ\\downarrow\n\n\nCLIPScoreâ\\uparrow\n\n\nEmo-Aâ\\uparrow\n\n\n\nSpokenCOCO\n32.60\n26.16\n46.20\n\n\nVoxEmoset\n22.47\n27.76\n70.83\n\n\nSpokenCOCO, VoxEmoset\n19.94\n29.04\n71.70",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Training data</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FID</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CLIPScore</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Emo-A</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">SpokenCOCO</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">32.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">46.20</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">VoxEmoset</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">22.47</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.76</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.83</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">SpokenCOCO, VoxEmoset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">19.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">29.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">71.70</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "training",
            "voxemoset",
            "impact",
            "datasets",
            "emoaâuparrow",
            "spokencoco",
            "data",
            "fidâdownarrow",
            "clipscoreâuparrow"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training datasets.</span> <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T4\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that VoxEmoset is complementary with the real-world spoken dataset, SpokenCOCO, improving both visual fidelity and semantic relevance.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This paper proposes <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, the first unified and end-to-end speech-to-image model that generates expressive images directly from spoken descriptions by jointly aligning linguistic and paralinguistic information.\nAt its core is a speech information bottleneck (SIB) module, which compresses raw speech into compact semantic tokens, preserving prosody and emotional nuance.\nBy operating directly on these tokens, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> eliminates the need for an additional speech-to-text system, which often ignores the hidden details beyond text, <em class=\"ltx_emph ltx_font_italic\">e.g</em>., tone or emotion.\nWe also release VoxEmoset, a large&#8208;scale paired emotional speech&#8211;image dataset built via an advanced TTS engine to affordably generate richly expressive utterances.\nComprehensive experiments on the SpokenCOCO, Flickr8kAudio, and VoxEmoset benchmarks demonstrate the feasibility of our method and highlight key challenges, including emotional consistency and linguistic ambiguity, paving the way for future research.\n<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\"/><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\"/><span class=\"ltx_tag ltx_tag_note\"/><sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup> Partly work done in NAVER AI Lab.\n<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> Corresponding author.</span></span></span></p>\n\n",
                "matched_terms": [
                    "voxemoset",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">VoxEmoset</span>, an automatically (and efficiently) synthesized dataset of 247k emotional spoken descriptions for sentiment images. VoxEmoset is used for both training and evaluation of S2I.</p>\n\n",
                "matched_terms": [
                    "training",
                    "voxemoset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> in various S2I benchmarks, including SpokenCOCO&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite>, Flickr8kAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> and VoxEmoset, and demonstrate its superiority and high fidelity over baselines.</p>\n\n",
                "matched_terms": [
                    "voxemoset",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, speech naturally encodes nuanced emotion and tone&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib50\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">50</span></a>]</cite>, offering a more intuitive means for generating emotionally resonant images, yet it remains largely untapped as a conditioning signal.\nRecent audiovisual generation methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib29\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">29</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">7</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib26\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">26</span></a>]</cite> have been limited to relying only on semantic instances expressed in text, where the other expressions are excluded.\nMoreover, existing approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib54\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">54</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib55\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">55</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite> for S2I generation have used highly limited datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite>, restricting their expressive versatility.\nWe aim to design a unified and emotion-driven S2I framework as well as to introduce a large-scale dataset for both training and evaluation.</p>\n\n",
                "matched_terms": [
                    "training",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is to generate an image with a corresponding spoken description, even for emotional expression.\nHowever, prior datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite> overlooked paralinguistic features in speech, and also required significant costs for human recordings.\nOur benchmark uniquely leverages synthesized speech, enabling the natural and cost-effective creation of a large-scale dataset.\nSpecifically, VoxEmoset leverages semantic knowledge and the generative powers of pre-trained multimodal LLMs and diffusion models to generate diverse synthetic data samples.\nFirst, a multimodal LLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite> generates corresponding captions that are factual descriptions of a given emotional image based on explaining environments or objects.\nThen, a TTS model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite> generates emotional speech samples from text captions, using emotional voice samples from other datasets as references.\nConsequently, we efficiently and cheaply generate large-scale emotional utterances along with text captions, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S3.F4\" title=\"In 3.3 Image generator &#8227; 3 VoxStudio &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "datasets",
                    "voxemoset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, to build a diverse range of emotional voice references for TTS, emotional speech data was collected from multiple datasets, including CREMA-D&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib3\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></a>]</cite>, MEAD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib53\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">53</span></a>]</cite>, and RAVDESS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib34\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">34</span></a>]</cite>.\nThese datasets contain English-spoken utterances from a variety of speakers.\nFollowing EmoBox&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>]</cite>, we split the datasets into training and test sets.\nWe validate the emotions in the generated speech using Emotion2Vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib37\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">37</span></a>]</cite> to measure emotional intensity, filtering and re-generating inadequate samples. After this process, 247k speech samples are generated.\nFurther details are provided in Appendix.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To objectively assess the quality of generated utterances, we randomly sample 10k utterances from each dataset and measure NMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib45\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">45</span></a>]</cite>. For CREMA-D, we use the entire samples.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S3.T1\" title=\"In 3.3 Image generator &#8227; 3 VoxStudio &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> shows that VoxEmoset is compatible with existing speech-image datasets such as SpokenCOCO and Flickr8kAudio in terms of speech quality (NMOS) and description quality (CLIPScore).\nHowever, only our benchmark explicitly expresses emotion in speech.\nThe last two rows in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S3.T1\" title=\"In 3.3 Image generator &#8227; 3 VoxStudio &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> validates that VoxEmoset guarantees high perceptual fidelity with clear affect, where emotion discriminability (Emo-C) is measured as the emotion classifier&#8217;s average confidence score.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "voxemoset",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets.</span>\nWe use SpokenCOCO&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite> and VoxEmoset to train <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>.\nVoxEmoset includes 208k utterances with paired 69k images for training, while SpokenCOCO contains 118k images with 591k utterances.\nFlickr8kAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> is used to evaluate zero-shot generalizability.\nEach image in SpokenCOCO and Flickr8kAudio has five voice recordings from unskilled annotators, resulting in inherently noisy audio (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., the recording may contain background noise, reading speed or volume can vary, and pronunciation may not be as clear as that of skilled voice actors as in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.SS4\" title=\"B.4 Failure cases and limitation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B.4</span></a>).\nVoxEmoset is automatically generated and less prone to recording noise.\nWe use the Karpathy split&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib27\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">27</span></a>]</cite> for SpokenCOCO and Flickr8kAudio.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "training",
                    "voxemoset",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation metrics.</span>\nWe assess the generation quality using FID&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib22\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">22</span></a>]</cite>, while content alignment between speech and generated images is measured with CLIPScore&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib21\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite> using text transcriptions.\nFor SpokenCOCO and VoxEmoset, random samples of 10k condition prompts, either speech or text, are used for evaluation.\nFor Flickr8kAudio, we use 5k test prompts for evaluation.\nWe also report emotion classification accuracy (Emo-A)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib59\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">59</span></a>]</cite> on generated images to examine whether the results reflect emotion from prompts.\nNote that we measure accuracy only with scores for the 5 emotion categories <math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math><span class=\"ltx_text ltx_font_italic\">&#8216;amusement&#8217;</span> and <span class=\"ltx_text ltx_font_italic\">&#8216;excitement&#8217;</span> are classified as the same class<math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math> in the trained emotion classifier.</p>\n\n",
                "matched_terms": [
                    "voxemoset",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results on SpokenCOCO and VoxEmoset.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S4.T2\" title=\"In 4.4 Dataset quality &#8227; 4 VoxEmoset Benchmark &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the comparison of <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> and baselines<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>EmoGen was excluded because its pretrained weights are publicly unavailable, and our reimplementation was unable to match its reported performance.</span></span></span> on SpokenCOCO and VoxEmoset.\nSD1.5 with the text inputs (<em class=\"ltx_emph ltx_font_italic\">i.e</em>., without speech) is shown as a baseline.\nEspecially, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S4.F5\" title=\"In 4.4 Dataset quality &#8227; 4 VoxEmoset Benchmark &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> highlights the stark contrast between text- and speech-based generation.\nWhile speech conveys emotions even with the same wording, the text-based model inherently ignores these cues and focuses on fact-based generation.\nEven when trained on VoxEmoset, &#8216;SD (finetuning)&#8217; struggles to express emotions as semantic content, but speech leads to a richer and intense emotional expression.\nFor example, given the prompt &#8216;A black trash can is placed against a white wall,&#8217; our model detects disgust from spoken nuances and visually emphasizes the unpleasantness of trash, whereas the text-based model remains neutral.</p>\n\n",
                "matched_terms": [
                    "voxemoset",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Remarkably, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> outperforms SpeechCLIP+ and TMT on SpokenCOCO, where <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> does not use Flickr8kAudio for training.\nWhile TMT additionally used huge synthesized speech data from CC3M&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib49\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">49</span></a>]</cite> and CC12M&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>]</cite> for training, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> also show comparable results on VoxEmoset.\nThis result demonstrates that our diffusion model is a powerful learner for speech-to-expressive image alignment than contrastive learning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>]</cite> and auto-regressive training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>.\nThe qualitative comparison on SpokenCOCO shows that SpeechCLIP+ and TMT often ignore keywords in the prompts, while <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> can capture the details, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F6\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training",
                    "voxemoset",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results on Flickr8kAudio.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T3\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the performance comparison on Flickr8kAudio.\nHere, while TMT and SpeechCLIP+ used Flickr8kAudio for training, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> was evaluated in a zero-shot manner.\nSurprisingly, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> outperforms existing methods by large margins.\nIt shows that end-to-end training in <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is more robust in aligning the speech-language space.\nBy contrast, speech features in <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> are more robust to the order or length of the prompt.\nMoreover, VoxEmoset might improve the robustness on generality as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F6\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "voxemoset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is the first end-to-end S2I model that captures both linguistic and emotional nuances from speech.\nUnlike text-based methods, our approach totally leverages speech&#8217;s expressiveness to generate emotionally aligned images. VoxEmoset is built cheaply, but it is complementary with real-world datasets.\nOur experiments demonstrate that <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> not only outperforms prior speech-based methods in conveying sentiment through images, but also matches text-driven approaches in semantic alignment, despite the higher noise and lower latency of the speech modality.\nWe believe our work facilitates future research in voice-driven generative models and their applications.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "voxemoset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our objective for data construction is primarily on (1) synthesizing large-scale image and speech pairs, (2) the speech will be emotional rich, and (3) closely matches the quality of real recordings while diversifying the range of speakers.\nHere we supplement the details of VoxEmoset.</p>\n\n",
                "matched_terms": [
                    "data",
                    "voxemoset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results for <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> on SpokenCOCO according to the difference of training data is shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.T2\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B2</span></a>.\nIn the results, we observe that SpokenCOCO does not fully capture the diversity of real-world scenarios.\nMost images convey neutral emotions and primarily depict scenes suitable for objective descriptions.\nHowever, real-world photographs go beyond presenting mere facts, often communicating higher-level meanings such as feelings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib59\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">59</span></a>]</cite>.\nThis demonstrates that our dataset extends beyond the distribution of SpokenCOCO and Flickr8kAudio, offering a more realistic and emotionally expressive representation.\nThis claim is further supported by Table 2 of the main paper, where models trained on the SpokenCOCO and Flickr8kAudio datasets&#8212; even TMT, which was trained on a massive amount of synthesized speech from CC3M and CC12M&#8212;fail to generalize to our dataset.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training",
                    "spokencoco"
                ]
            }
        ]
    },
    "S5.T5": {
        "source_file": "Seeing What You Say: Expressive Image Generation from Speech",
        "caption": "Table 5: Effect of the training strategies for SD1.5. We report the total number of trainable parameters.",
        "body": "Training\n# Tr. Params.\nInput\n\nFIDâ\\downarrow\n\n\nCLIPScoreâ\\uparrow\n\n\nEmo-Aâ\\uparrow\n\n\n\nSD(T2I)-FT\n859.1M\nT\n18.31\n31.72\n69.38\n\n\nVoxStudio\n50.0M\nS\n25.01\n28.71\n67.09\n\n\n\nVoxStudio-LoRA\n\n50.7M\nS\n27.25\n29.88\n69.43\n\n\n\nVoxStudio-FT\n\n909.1M\nS\n19.94\n29.04\n71.70",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Training</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\"># Tr. Params.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Input</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FID</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CLIPScore</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Emo-A</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">SD(T2I)-FT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">859.1M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">T</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">18.31</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">31.72</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">69.38</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">VoxStudio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">50.0M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">S</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">67.09</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">VoxStudio</span><span class=\"ltx_text\" style=\"font-size:90%;\">-LoRA</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">50.7M</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">S</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.25</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.88</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">69.43</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">VoxStudio</span><span class=\"ltx_text\" style=\"font-size:90%;\">-FT</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">909.1M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">S</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">71.70</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "strategies",
            "training",
            "sdt2ift",
            "input",
            "voxstudio",
            "trainable",
            "clipscoreâuparrow",
            "500m",
            "507m",
            "sd15",
            "voxstudiolora",
            "fidâdownarrow",
            "report",
            "params",
            "number",
            "effect",
            "9091m",
            "8591m",
            "total",
            "emoaâuparrow",
            "parameters",
            "voxstudioft"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training strategies.</span>\nDiffusion training is usually computationally expensive.\nWe test different training strategies in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T5\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>: full finetuning, LoRA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib24\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">24</span></a>]</cite>, and freezing the model.\nWhile finetuning achieves the best performance, LoRA and frozen models show comparable results in CLIPScore and Emo-A.\nAdditionally, although speech is noisier than text, our method outperforms full finetuning for original SD1.5 (&#8216;SD(T2I)-FT&#8217; in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T5\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>) in terms of emotional expression, while maintaining the generation quality.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Although speech embeddings contain rich representations, they are excessively long and lead to a lower information density in each speech token compared to text (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., Whisper encodes a maximum of 1500 tokens for 30-seconds long speech, while CLIP text encoder limited to 77 tokens).\nThis low density makes direct usage challenging to condition the image generator.\nTo solve this problem, we design a Transformer-based speech information bottleneck (SIB) module.\nSIB compacts semantics in speech embeddings, motivated by previous works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib51\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">51</span></a>]</cite> applied to individual image and audio encoders.\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S2.F3\" title=\"In 2 Related Work &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, SIB reduces the number of embeddings with a strided convolution layer after a Transformer block along the time axis.\nBased on our findings, a pooling ratio of 8 provides the optimal balance, allowing us to maximize the information retention of speech features.\nAs a result, the initial embedding <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> is processed into a compressed speech condition <math alttext=\"c=f_{\\psi}(s)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>=</mo><mrow><msub><mi>f</mi><mi>&#968;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">c=f_{\\psi}(s)</annotation></semantics></math>, where <math alttext=\"c\\in\\mathrm{R}^{M\\times D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>&#8712;</mo><msup><mi mathvariant=\"normal\">R</mi><mrow><mi>M</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>D</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">c\\in\\mathrm{R}^{M\\times D^{\\prime}}</annotation></semantics></math>, <math alttext=\"M=N/8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>=</mo><mrow><mi>N</mi><mo>/</mo><mn>8</mn></mrow></mrow><annotation encoding=\"application/x-tex\">M=N/8</annotation></semantics></math> and <math alttext=\"D^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><msup><mi>D</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">D^{\\prime}</annotation></semantics></math> is the input channel of the cross-attention block in the image generator.\nThose compressed representations improve the efficiency of the S2I process while preserving both linguistic and emotional expressiveness.</p>\n\n",
                "matched_terms": [
                    "number",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets.</span>\nWe use SpokenCOCO&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite> and VoxEmoset to train <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>.\nVoxEmoset includes 208k utterances with paired 69k images for training, while SpokenCOCO contains 118k images with 591k utterances.\nFlickr8kAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> is used to evaluate zero-shot generalizability.\nEach image in SpokenCOCO and Flickr8kAudio has five voice recordings from unskilled annotators, resulting in inherently noisy audio (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., the recording may contain background noise, reading speed or volume can vary, and pronunciation may not be as clear as that of skilled voice actors as in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.SS4\" title=\"B.4 Failure cases and limitation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B.4</span></a>).\nVoxEmoset is automatically generated and less prone to recording noise.\nWe use the Karpathy split&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib27\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">27</span></a>]</cite> for SpokenCOCO and Flickr8kAudio.</p>\n\n",
                "matched_terms": [
                    "training",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation details.</span>\nTraining a high-performance image generator requires a vast amount of resources (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., SD1.5 requires 6,000 A100 GPU days&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib5\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></a>]</cite>).\nWe initialize the image generator with a pre-trained SD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite> for efficient learning.\nWe use SONAR as the speech encoder, freezing during the training, in which its last aggregation layer is removed.\nWe use AdamW <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib35\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">35</span></a>]</cite> with the learning rate of <math alttext=\"1e\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">1e</annotation></semantics></math>-<math alttext=\"6\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>, the batch size of 128 using 8 V100 GPUs.\nFP16 precision is used for all experiments.\nThe code will be released.</p>\n\n",
                "matched_terms": [
                    "sd15",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results on SpokenCOCO and VoxEmoset.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S4.T2\" title=\"In 4.4 Dataset quality &#8227; 4 VoxEmoset Benchmark &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the comparison of <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> and baselines<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>EmoGen was excluded because its pretrained weights are publicly unavailable, and our reimplementation was unable to match its reported performance.</span></span></span> on SpokenCOCO and VoxEmoset.\nSD1.5 with the text inputs (<em class=\"ltx_emph ltx_font_italic\">i.e</em>., without speech) is shown as a baseline.\nEspecially, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S4.F5\" title=\"In 4.4 Dataset quality &#8227; 4 VoxEmoset Benchmark &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> highlights the stark contrast between text- and speech-based generation.\nWhile speech conveys emotions even with the same wording, the text-based model inherently ignores these cues and focuses on fact-based generation.\nEven when trained on VoxEmoset, &#8216;SD (finetuning)&#8217; struggles to express emotions as semantic content, but speech leads to a richer and intense emotional expression.\nFor example, given the prompt &#8216;A black trash can is placed against a white wall,&#8217; our model detects disgust from spoken nuances and visually emphasizes the unpleasantness of trash, whereas the text-based model remains neutral.</p>\n\n",
                "matched_terms": [
                    "sd15",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Remarkably, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> outperforms SpeechCLIP+ and TMT on SpokenCOCO, where <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> does not use Flickr8kAudio for training.\nWhile TMT additionally used huge synthesized speech data from CC3M&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib49\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">49</span></a>]</cite> and CC12M&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>]</cite> for training, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> also show comparable results on VoxEmoset.\nThis result demonstrates that our diffusion model is a powerful learner for speech-to-expressive image alignment than contrastive learning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>]</cite> and auto-regressive training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>.\nThe qualitative comparison on SpokenCOCO shows that SpeechCLIP+ and TMT often ignore keywords in the prompts, while <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> can capture the details, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F6\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results on Flickr8kAudio.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T3\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the performance comparison on Flickr8kAudio.\nHere, while TMT and SpeechCLIP+ used Flickr8kAudio for training, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> was evaluated in a zero-shot manner.\nSurprisingly, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> outperforms existing methods by large margins.\nIt shows that end-to-end training in <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is more robust in aligning the speech-language space.\nBy contrast, speech features in <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> are more robust to the order or length of the prompt.\nMoreover, VoxEmoset might improve the robustness on generality as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F6\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human evaluation.</span>\nA user study is conducted to assess how well humans perceive the alignment between speech and image atmosphere.\n26 participants evaluated 25 images to rate how well the emotion conveyed in the image matched the given speech.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F7.sf1\" title=\"In Figure 7 &#8227; 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7(a)</span></a> shows that results from <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> are more aligned with the emotion than text-based SD in all categories.\nIn other words, with an average of 57.09% preference, the images generated by our <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> were rated as better at expressing emotions.\nIt highlights the effectiveness of speech prompts for expressive image synthesis.\nWe also carried out another human evaluation on SpokenCOCO across speech-based models.\nThis experiment is performed on 17 participants who evaluate 10 generated images for each model with a 5-point Likert scale.\nAs demonstrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F7.sf2\" title=\"In Figure 7 &#8227; 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7(b)</span></a>, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> outperforms existing approaches by generating high-quality images that accurately reflect the nuances of the input prompts.</p>\n\n",
                "matched_terms": [
                    "input",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effect of emotion.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F8\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a> demonstrates that the same description, when spoken with different emotions, leads to distinct visual outputs by <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>. This highlights <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>&#8217;s capability to produce emotional nuances beyond linguistic content.\nFor instance, a neutral statement spoken in a disgusted tone results in negative visual details (top-left), while an &#8220;enjoying&#8221; tone generates a more positive scene (top-right).\nThese findings show that our speech-based approach effectively leverages emotional cues, enabling more expressive and context-rich image generation.</p>\n\n",
                "matched_terms": [
                    "effect",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scale of the image generator.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T6\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> demonstrates the performance of image generators at different scales.\nDue to the resource limit, we compare UNet of SD1.5&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite> and SDXL&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib41\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">41</span></a>]</cite> as our image generator in a frozen state during the training.\nInterestingly, although the small generator achieves a higher CLIPScore, the larger generator excels at displaying emotional nuances.\nThis finding suggests that larger-scale generators are inherently better at representing content beyond simple text cues.</p>\n\n",
                "matched_terms": [
                    "sd15",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image editing using speech prompt.</span>\n<span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is built upon the SD architecture, allowing seamless integration with various extensions and applications.\nFor instance, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F8\" title=\"In B.3 Details of human evaluation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B8</span></a>, the image editing pipeline can be directly applied with speech prompts to modify input images.\nBeyond basic editing, our framework can be extended to other tasks built on SD, including personalized generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib47\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">47</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib61\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">61</span></a>]</cite> and multimodal content synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib17\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">17</span></a>]</cite>.\nIt provides a versatile foundation for future developments in S2I generation.</p>\n\n",
                "matched_terms": [
                    "input",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results for <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> on SpokenCOCO according to the difference of training data is shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.T2\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B2</span></a>.\nIn the results, we observe that SpokenCOCO does not fully capture the diversity of real-world scenarios.\nMost images convey neutral emotions and primarily depict scenes suitable for objective descriptions.\nHowever, real-world photographs go beyond presenting mere facts, often communicating higher-level meanings such as feelings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib59\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">59</span></a>]</cite>.\nThis demonstrates that our dataset extends beyond the distribution of SpokenCOCO and Flickr8kAudio, offering a more realistic and emotionally expressive representation.\nThis claim is further supported by Table 2 of the main paper, where models trained on the SpokenCOCO and Flickr8kAudio datasets&#8212; even TMT, which was trained on a massive amount of synthesized speech from CC3M and CC12M&#8212;fail to generalize to our dataset.</p>\n\n",
                "matched_terms": [
                    "training",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We show more qualitative comparisons in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F2\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F3\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B3</span></a>.\nIn those experiments, our image generators are initialized by UNet parameters in SD1.5.\nWhen training SD with a CLIP encoder using text prompts from our dataset, the model better follows the content of the text compared to zero-shot generation.\nHowever, in terms of emotional intensity and expressiveness, it performs weaker than our approach using speech prompts.\nFor example, in the last row in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F3\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B3</span></a>, generating an emotionally rich image from a sentence like &#8216;a tomato is cut into sections on a white plate&#8217; is challenging.\nBy using speech input that conveys a feeling of disgust, our model generates an image where the tomato appears distorted, conditioned on the given emotion category.\nMoreover, despite the inherent noise in speech, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> utilizes SIB to refine the information and capture its meaning, effectively following the content of the prompt.</p>\n\n",
                "matched_terms": [
                    "sd15",
                    "training",
                    "input",
                    "voxstudio",
                    "parameters"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F4\" title=\"In B.2 Qualitative results &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B4</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F5\" title=\"In B.2 Qualitative results &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B5</span></a> provide more results generated using the parameters of SD1.5 and SDXL, respectively.\nEspecially for the example of &#8216;a woman with blonde hair is standing in a room.&#8217;, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> poses the sadness in her facial expression.\nWhile we freeze parameters of the image generator, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F5\" title=\"In B.2 Qualitative results &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B5</span></a> shows that generated images ensure high fidelity and text relevancy.</p>\n\n",
                "matched_terms": [
                    "sd15",
                    "parameters",
                    "voxstudio"
                ]
            }
        ]
    },
    "S5.T6": {
        "source_file": "Seeing What You Say: Expressive Image Generation from Speech",
        "caption": "Table 6: Effect of the scale of image generator.",
        "body": "Base\nUNet Size\n\nFIDâ\\downarrow\n\n\nCLIPScoreâ\\uparrow\n\n\nEmo-Aâ\\uparrow\n\n\n\nSD1.5\n0.86B\n25.01\n28.71\n67.09\n\n\nSDXL\n2.6B\n23.12\n28.04\n69.26",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Base</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">UNet Size</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FID</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T6.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CLIPScore</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T6.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Emo-A</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T6.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">SD1.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.86B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">28.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">67.09</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">SDXL</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.6B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">23.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">69.26</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "sd15",
            "unet",
            "size",
            "scale",
            "086b",
            "sdxl",
            "base",
            "image",
            "26b",
            "emoaâuparrow",
            "generator",
            "fidâdownarrow",
            "clipscoreâuparrow",
            "effect"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We note that <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>-FT&#8217;s performance is basically reported in this section, except <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T6\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scale of the image generator.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T6\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> demonstrates the performance of image generators at different scales.\nDue to the resource limit, we compare UNet of SD1.5&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite> and SDXL&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib41\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">41</span></a>]</cite> as our image generator in a frozen state during the training.\nInterestingly, although the small generator achieves a higher CLIPScore, the larger generator excels at displaying emotional nuances.\nThis finding suggests that larger-scale generators are inherently better at representing content beyond simple text cues.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, a novel speech-to-image model that bridges the rich information in speech with the image modality space, enabling more diverse and expressive visual representations.\nBuilding upon T2I models, our framework is suitable for the unique characteristics of speech, which differ from text:\n(1) Speech generally contains longer and more variable sequences than text, leading to uneven information density across embeddings.\n(2) Speech signals vary significantly depending on speaker identity, recording environment, and emotional state, affecting articulation and duration, even for the same content.\nTo address these challenges, we introduce a speech information bottleneck (SIB) that efficiently aligns cross-modal latent spaces while preserving key speech features.\nOur SIB encodes compressed conditional features that guide the image generation process.\nThrough extensive experiments, we establish an effective speech-based guidance for image generation by identifying the optimal combination of speech encoder, SIB, and image generator.</p>\n\n",
                "matched_terms": [
                    "generator",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S2.F3\" title=\"In 2 Related Work &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the overall framework of <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, consisting of (1) a pretrained speech encoder, (2) SIB to reduce the computational cost and effectively connect heterogeneous two modalities, and (3) an image generator to synthesize an image from the compressed speech representations.\nNext, we describe each module in detail.</p>\n\n",
                "matched_terms": [
                    "generator",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although speech embeddings contain rich representations, they are excessively long and lead to a lower information density in each speech token compared to text (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., Whisper encodes a maximum of 1500 tokens for 30-seconds long speech, while CLIP text encoder limited to 77 tokens).\nThis low density makes direct usage challenging to condition the image generator.\nTo solve this problem, we design a Transformer-based speech information bottleneck (SIB) module.\nSIB compacts semantics in speech embeddings, motivated by previous works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib51\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">51</span></a>]</cite> applied to individual image and audio encoders.\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S2.F3\" title=\"In 2 Related Work &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, SIB reduces the number of embeddings with a strided convolution layer after a Transformer block along the time axis.\nBased on our findings, a pooling ratio of 8 provides the optimal balance, allowing us to maximize the information retention of speech features.\nAs a result, the initial embedding <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> is processed into a compressed speech condition <math alttext=\"c=f_{\\psi}(s)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>=</mo><mrow><msub><mi>f</mi><mi>&#968;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">c=f_{\\psi}(s)</annotation></semantics></math>, where <math alttext=\"c\\in\\mathrm{R}^{M\\times D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>&#8712;</mo><msup><mi mathvariant=\"normal\">R</mi><mrow><mi>M</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>D</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">c\\in\\mathrm{R}^{M\\times D^{\\prime}}</annotation></semantics></math>, <math alttext=\"M=N/8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>=</mo><mrow><mi>N</mi><mo>/</mo><mn>8</mn></mrow></mrow><annotation encoding=\"application/x-tex\">M=N/8</annotation></semantics></math> and <math alttext=\"D^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><msup><mi>D</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">D^{\\prime}</annotation></semantics></math> is the input channel of the cross-attention block in the image generator.\nThose compressed representations improve the efficiency of the S2I process while preserving both linguistic and emotional expressiveness.</p>\n\n",
                "matched_terms": [
                    "generator",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The image generator is based on the latent diffusion model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite>.\nThe speech condition <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math>, compressed through SIB, is fed into the generator as a guidance of the synthesis process.\nSpecifically, the speech embeddings are injected into the UNet through cross-attention layers to condition the image synthesis.\nThis conditioning allows the model to incorporate the emotional, semantic content of speech into the generation process.\nThe image generator and SIB are optimized with the diffusion loss&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite>.\nGiven that we do not design a specialized loss function compared to previous works such as contrastive learning in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>]</cite>, AR modeling in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>, our simple training framework ensures versatile connections for various image generators.\nIn inference, the denoised latent is decoded into the image through the decoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib30\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>]</cite>.\nGiven that we do not design a specialized loss function compared to previous works, such as contrastive learning in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>]</cite>, AR modeling in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>,\nour simple framework ensures versatile connections for various image generators.</p>\n\n",
                "matched_terms": [
                    "generator",
                    "image",
                    "unet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation details.</span>\nTraining a high-performance image generator requires a vast amount of resources (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., SD1.5 requires 6,000 A100 GPU days&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib5\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></a>]</cite>).\nWe initialize the image generator with a pre-trained SD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite> for efficient learning.\nWe use SONAR as the speech encoder, freezing during the training, in which its last aggregation layer is removed.\nWe use AdamW <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib35\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">35</span></a>]</cite> with the learning rate of <math alttext=\"1e\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">1e</annotation></semantics></math>-<math alttext=\"6\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>, the batch size of 128 using 8 V100 GPUs.\nFP16 precision is used for all experiments.\nThe code will be released.</p>\n\n",
                "matched_terms": [
                    "generator",
                    "image",
                    "sd15",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human evaluation.</span>\nA user study is conducted to assess how well humans perceive the alignment between speech and image atmosphere.\n26 participants evaluated 25 images to rate how well the emotion conveyed in the image matched the given speech.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F7.sf1\" title=\"In Figure 7 &#8227; 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7(a)</span></a> shows that results from <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> are more aligned with the emotion than text-based SD in all categories.\nIn other words, with an average of 57.09% preference, the images generated by our <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> were rated as better at expressing emotions.\nIt highlights the effectiveness of speech prompts for expressive image synthesis.\nWe also carried out another human evaluation on SpokenCOCO across speech-based models.\nThis experiment is performed on 17 participants who evaluate 10 generated images for each model with a 5-point Likert scale.\nAs demonstrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F7.sf2\" title=\"In Figure 7 &#8227; 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7(b)</span></a>, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> outperforms existing approaches by generating high-quality images that accurately reflect the nuances of the input prompts.</p>\n\n",
                "matched_terms": [
                    "image",
                    "scale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effect of emotion.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F8\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a> demonstrates that the same description, when spoken with different emotions, leads to distinct visual outputs by <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>. This highlights <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>&#8217;s capability to produce emotional nuances beyond linguistic content.\nFor instance, a neutral statement spoken in a disgusted tone results in negative visual details (top-left), while an &#8220;enjoying&#8221; tone generates a more positive scene (top-right).\nThese findings show that our speech-based approach effectively leverages emotional cues, enabling more expressive and context-rich image generation.</p>\n\n",
                "matched_terms": [
                    "image",
                    "effect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generalization on real speech.</span>\nTo evaluate how well our model generalizes, we test on utterances in ESD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib65\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">65</span></a>]</cite> dataset, excluded from our reference samples during speech synthesis.\nWe visualize generated samples from real speakers&#8217; utterances in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F9\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a>.\nText-based generator is limited to expressing the tone in speech prompt, but <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> successfully expresses atmospheres despite ambiguous words.\nIt also proves the superiority of VoxEmoset in that <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> trained on synthesized emotional speech is well generalized in real utterances.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F8\" title=\"In B.3 Details of human evaluation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B8</span></a> also demonstrates that <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> extends naturally to various applications such as image editing by spoken prompt.</p>\n\n",
                "matched_terms": [
                    "generator",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We show more qualitative comparisons in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F2\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F3\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B3</span></a>.\nIn those experiments, our image generators are initialized by UNet parameters in SD1.5.\nWhen training SD with a CLIP encoder using text prompts from our dataset, the model better follows the content of the text compared to zero-shot generation.\nHowever, in terms of emotional intensity and expressiveness, it performs weaker than our approach using speech prompts.\nFor example, in the last row in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F3\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B3</span></a>, generating an emotionally rich image from a sentence like &#8216;a tomato is cut into sections on a white plate&#8217; is challenging.\nBy using speech input that conveys a feeling of disgust, our model generates an image where the tomato appears distorted, conditioned on the given emotion category.\nMoreover, despite the inherent noise in speech, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> utilizes SIB to refine the information and capture its meaning, effectively following the content of the prompt.</p>\n\n",
                "matched_terms": [
                    "image",
                    "sd15",
                    "unet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F4\" title=\"In B.2 Qualitative results &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B4</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F5\" title=\"In B.2 Qualitative results &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B5</span></a> provide more results generated using the parameters of SD1.5 and SDXL, respectively.\nEspecially for the example of &#8216;a woman with blonde hair is standing in a room.&#8217;, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> poses the sadness in her facial expression.\nWhile we freeze parameters of the image generator, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F5\" title=\"In B.2 Qualitative results &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B5</span></a> shows that generated images ensure high fidelity and text relevancy.</p>\n\n",
                "matched_terms": [
                    "generator",
                    "image",
                    "sd15",
                    "sdxl"
                ]
            }
        ]
    },
    "S5.T7": {
        "source_file": "Seeing What You Say: Expressive Image Generation from Speech",
        "caption": "Table 7: Impact of the encoder choices.",
        "body": "Encoder\n# Params.\n\nFIDâ\\downarrow\n\n\nCLIPScoreâ\\uparrow\n\n\nEmo-Aâ\\uparrow\n\n\n\nWhisper-L v3\n636M\n23.57\n28.33\n67.77\n\n\nSONAR\n600M\n19.94\n29.04\n71.70",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Encoder</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\"># Params.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FID</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T7.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CLIPScore</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T7.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Emo-A</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T7.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Whisper-L v3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">636M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.57</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">67.77</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">SONAR</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">600M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">19.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">29.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">71.70</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "encoder",
            "sonar",
            "params",
            "clipscoreâuparrow",
            "600m",
            "impact",
            "emoaâuparrow",
            "whisperl",
            "fidâdownarrow",
            "choices",
            "636m"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech embedding.</span>\nWe compare SONAR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> and Whisper-Large v3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib43\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">43</span></a>]</cite> encoders as a speech input handler of our method.\nWhisper is a widely used ASR model, also known to be capable of capturing paralinguistic information&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib64\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">64</span></a>]</cite>.\nWhile SONAR is sentence-level speech-text aligned features, Whisper is trained at the phoneme-level by predicting which words are spoken in a given audio snippet.\nThis fundamental difference affects how each encoder preserves linguistic content and emotional cues when mapping speech to image descriptions.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T7\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a> demonstrates that text-aligned embeddings (<em class=\"ltx_emph ltx_font_italic\">i.e</em>., SONAR) show more robust performance on our task.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We consider two pre-trained speech encoders, SONAR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> and Whisper large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib43\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">43</span></a>]</cite>, to deploy comprehensive speech features considering both linguistic and paralinguistic information.\nBriefly reviewed, SONAR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> has learned global semantic alignment between speech and text, enabling it to encode meaning beyond phonetic content.\nWe remove its last aggregation layer to ensure that the speech embeddings retain both linguistic and paralinguistic information.\nWe also test Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib43\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">43</span></a>]</cite>, a widely used speech recognition model. Whisper is known to be capable of capturing paralinguistic information, such as emotion and speaker identity&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib64\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">64</span></a>]</cite>.\nFormally, given an input utterance <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math>, we obtain a speech embedding <math alttext=\"s\\in\\mathrm{R}^{N\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo>&#8712;</mo><msup><mi mathvariant=\"normal\">R</mi><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">s\\in\\mathrm{R}^{N\\times D}</annotation></semantics></math>, where <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> depends on the length of the speech and models, and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> is the channel dimension of the final output layer.\nWe note that our work does not employ ASR system or text encoder to explicitly map the speech into text.</p>\n\n",
                "matched_terms": [
                    "sonar",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation details.</span>\nTraining a high-performance image generator requires a vast amount of resources (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., SD1.5 requires 6,000 A100 GPU days&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib5\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></a>]</cite>).\nWe initialize the image generator with a pre-trained SD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite> for efficient learning.\nWe use SONAR as the speech encoder, freezing during the training, in which its last aggregation layer is removed.\nWe use AdamW <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib35\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">35</span></a>]</cite> with the learning rate of <math alttext=\"1e\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">1e</annotation></semantics></math>-<math alttext=\"6\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>, the batch size of 128 using 8 V100 GPUs.\nFP16 precision is used for all experiments.\nThe code will be released.</p>\n\n",
                "matched_terms": [
                    "sonar",
                    "encoder"
                ]
            }
        ]
    },
    "S5.T8": {
        "source_file": "Seeing What You Say: Expressive Image Generation from Speech",
        "caption": "Table 8: Effectivness of architecture choices of SIB.",
        "body": "SIB architecture\n# Params.\n\nFIDâ\\downarrow\n\n\nCLIPScoreâ\\uparrow\n\n\nEmo-Aâ\\uparrow\n\n\n\nTransformer\n71M\n23.12\n28.04\n69.26\n\n\nVoxStudio\n50M\n19.94\n29.04\n71.70",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">SIB architecture</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\"># Params.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FID</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T8.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CLIPScore</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T8.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Emo-A</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T8.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Transformer</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">71M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">69.26</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">VoxStudio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">50M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">19.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">29.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">71.70</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "50m",
            "effectivness",
            "params",
            "architecture",
            "voxstudio",
            "clipscoreâuparrow",
            "emoaâuparrow",
            "transformer",
            "fidâdownarrow",
            "choices",
            "sib",
            "71m"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Architecture choices on SIB.</span>\nWe propose SIB to represent the speech condition compactly to address the issue of low information density of speech tokens.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T8\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a> compares its performance against a standard transformer structure.\nOur design choice achieves better performance with fewer parameters.\nGradually reducing the speech token length while simultaneously increasing their density across multiple layers enhances both the linguistic and paralinguistic expressiveness of the speech signal.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This paper proposes <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, the first unified and end-to-end speech-to-image model that generates expressive images directly from spoken descriptions by jointly aligning linguistic and paralinguistic information.\nAt its core is a speech information bottleneck (SIB) module, which compresses raw speech into compact semantic tokens, preserving prosody and emotional nuance.\nBy operating directly on these tokens, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> eliminates the need for an additional speech-to-text system, which often ignores the hidden details beyond text, <em class=\"ltx_emph ltx_font_italic\">e.g</em>., tone or emotion.\nWe also release VoxEmoset, a large&#8208;scale paired emotional speech&#8211;image dataset built via an advanced TTS engine to affordably generate richly expressive utterances.\nComprehensive experiments on the SpokenCOCO, Flickr8kAudio, and VoxEmoset benchmarks demonstrate the feasibility of our method and highlight key challenges, including emotional consistency and linguistic ambiguity, paving the way for future research.\n<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\"/><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\"/><span class=\"ltx_tag ltx_tag_note\"/><sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup> Partly work done in NAVER AI Lab.\n<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> Corresponding author.</span></span></span></p>\n\n",
                "matched_terms": [
                    "sib",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, a novel speech-to-image model that bridges the rich information in speech with the image modality space, enabling more diverse and expressive visual representations.\nBuilding upon T2I models, our framework is suitable for the unique characteristics of speech, which differ from text:\n(1) Speech generally contains longer and more variable sequences than text, leading to uneven information density across embeddings.\n(2) Speech signals vary significantly depending on speaker identity, recording environment, and emotional state, affecting articulation and duration, even for the same content.\nTo address these challenges, we introduce a speech information bottleneck (SIB) that efficiently aligns cross-modal latent spaces while preserving key speech features.\nOur SIB encodes compressed conditional features that guide the image generation process.\nThrough extensive experiments, we establish an effective speech-based guidance for image generation by identifying the optimal combination of speech encoder, SIB, and image generator.</p>\n\n",
                "matched_terms": [
                    "sib",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is a unified image generation model with expressive utterance, where both linguistic and paralinguistic cues are compactly captured via the SIB module.</p>\n\n",
                "matched_terms": [
                    "sib",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S2.F3\" title=\"In 2 Related Work &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the overall framework of <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, consisting of (1) a pretrained speech encoder, (2) SIB to reduce the computational cost and effectively connect heterogeneous two modalities, and (3) an image generator to synthesize an image from the compressed speech representations.\nNext, we describe each module in detail.</p>\n\n",
                "matched_terms": [
                    "sib",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although speech embeddings contain rich representations, they are excessively long and lead to a lower information density in each speech token compared to text (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., Whisper encodes a maximum of 1500 tokens for 30-seconds long speech, while CLIP text encoder limited to 77 tokens).\nThis low density makes direct usage challenging to condition the image generator.\nTo solve this problem, we design a Transformer-based speech information bottleneck (SIB) module.\nSIB compacts semantics in speech embeddings, motivated by previous works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib51\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">51</span></a>]</cite> applied to individual image and audio encoders.\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S2.F3\" title=\"In 2 Related Work &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, SIB reduces the number of embeddings with a strided convolution layer after a Transformer block along the time axis.\nBased on our findings, a pooling ratio of 8 provides the optimal balance, allowing us to maximize the information retention of speech features.\nAs a result, the initial embedding <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> is processed into a compressed speech condition <math alttext=\"c=f_{\\psi}(s)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>=</mo><mrow><msub><mi>f</mi><mi>&#968;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">c=f_{\\psi}(s)</annotation></semantics></math>, where <math alttext=\"c\\in\\mathrm{R}^{M\\times D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>&#8712;</mo><msup><mi mathvariant=\"normal\">R</mi><mrow><mi>M</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>D</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">c\\in\\mathrm{R}^{M\\times D^{\\prime}}</annotation></semantics></math>, <math alttext=\"M=N/8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>=</mo><mrow><mi>N</mi><mo>/</mo><mn>8</mn></mrow></mrow><annotation encoding=\"application/x-tex\">M=N/8</annotation></semantics></math> and <math alttext=\"D^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><msup><mi>D</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">D^{\\prime}</annotation></semantics></math> is the input channel of the cross-attention block in the image generator.\nThose compressed representations improve the efficiency of the S2I process while preserving both linguistic and emotional expressiveness.</p>\n\n",
                "matched_terms": [
                    "sib",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image editing using speech prompt.</span>\n<span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is built upon the SD architecture, allowing seamless integration with various extensions and applications.\nFor instance, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F8\" title=\"In B.3 Details of human evaluation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B8</span></a>, the image editing pipeline can be directly applied with speech prompts to modify input images.\nBeyond basic editing, our framework can be extended to other tasks built on SD, including personalized generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib47\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">47</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib61\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">61</span></a>]</cite> and multimodal content synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib17\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">17</span></a>]</cite>.\nIt provides a versatile foundation for future developments in S2I generation.</p>\n\n",
                "matched_terms": [
                    "architecture",
                    "voxstudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We show more qualitative comparisons in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F2\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F3\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B3</span></a>.\nIn those experiments, our image generators are initialized by UNet parameters in SD1.5.\nWhen training SD with a CLIP encoder using text prompts from our dataset, the model better follows the content of the text compared to zero-shot generation.\nHowever, in terms of emotional intensity and expressiveness, it performs weaker than our approach using speech prompts.\nFor example, in the last row in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F3\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B3</span></a>, generating an emotionally rich image from a sentence like &#8216;a tomato is cut into sections on a white plate&#8217; is challenging.\nBy using speech input that conveys a feeling of disgust, our model generates an image where the tomato appears distorted, conditioned on the given emotion category.\nMoreover, despite the inherent noise in speech, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> utilizes SIB to refine the information and capture its meaning, effectively following the content of the prompt.</p>\n\n",
                "matched_terms": [
                    "sib",
                    "voxstudio"
                ]
            }
        ]
    },
    "A1.p4": {
        "source_file": "Seeing What You Say: Expressive Image Generation from Speech",
        "caption": "",
        "body": "Generate three disjoint captions for the given image. Each caption should:\n\n\n\n\n\n\n* Have a different sentence structure,\n\n\n\n\n\n\n* Avoid emotional or subjective expressions,\n\n\n\n\n\n\n* Describe different aspects of the image, such as objects, actions, spatial relationships, or surroundings,\n\n\n\n\n\n\n* Be between 8 and 15 words long,",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Generate three disjoint captions for the given image. Each caption should:</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">* Have a different sentence structure,</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">* Avoid emotional or subjective expressions,</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">* Describe different aspects of the image, such as objects, actions, spatial relationships, or surroundings,</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">* Be between 8 and 15 words long,</span></span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "describe",
            "actions",
            "emotional",
            "caption",
            "subjective",
            "sentence",
            "aspects",
            "words",
            "captions",
            "each",
            "three",
            "disjoint",
            "given",
            "generate",
            "between",
            "relationships",
            "have",
            "spatial",
            "avoid",
            "such",
            "structure",
            "objects",
            "image",
            "long",
            "different",
            "surroundings",
            "expressions"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This paper proposes <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, the first unified and end-to-end speech-to-image model that generates expressive images directly from spoken descriptions by jointly aligning linguistic and paralinguistic information.\nAt its core is a speech information bottleneck (SIB) module, which compresses raw speech into compact semantic tokens, preserving prosody and emotional nuance.\nBy operating directly on these tokens, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> eliminates the need for an additional speech-to-text system, which often ignores the hidden details beyond text, <em class=\"ltx_emph ltx_font_italic\">e.g</em>., tone or emotion.\nWe also release VoxEmoset, a large&#8208;scale paired emotional speech&#8211;image dataset built via an advanced TTS engine to affordably generate richly expressive utterances.\nComprehensive experiments on the SpokenCOCO, Flickr8kAudio, and VoxEmoset benchmarks demonstrate the feasibility of our method and highlight key challenges, including emotional consistency and linguistic ambiguity, paving the way for future research.\n<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\"/><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\"/><span class=\"ltx_tag ltx_tag_note\"/><sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup> Partly work done in NAVER AI Lab.\n<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> Corresponding author.</span></span></span></p>\n\n",
                "matched_terms": [
                    "emotional",
                    "generate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in text-to-image (T2I) generation have demonstrated remarkable progress, but they struggle to utilize the innate expressiveness and accessibility of speech.\nMost cascaded framework&#8212;where an utterance is first transcribed into text or textual feature and then used as input for T2I models, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S1.F2\" title=\"In 1 Introduction &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> (a, b)&#8212;encounters several significant challenges.\nFirst, speech-to-text (<em class=\"ltx_emph ltx_font_italic\">i.e</em>., ASR) transcription is limited to capturing prosody and speaker intention.\nHowever, transcription errors propagate into the image generative model, then degrade visual quality.\nSecond, this sequential approach inherently decouples speech and image generation, making it difficult to transfer crucial prosodic and temporal cues&#8212;such as speaking rate, pitch variation, and emotional style&#8212;that can influence the mood, color palette, or overall aesthetic of the generated image.\nRelying on intermediate text also excludes languages without written forms&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib50\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">50</span></a>]</cite>.\nEven for languages that do have a writing system, coverage for the cascaded approach remains far from comprehensive: there are over 7,100 languages worldwide&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib13\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">13</span></a>]</cite>, <em class=\"ltx_emph ltx_font_italic\">e.g</em>. Google API covers only 125<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1a\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://cloud.google.com/speech-to-text</span></span></span>.\nFinally, the cascaded system limits the inference speed and requires a higher cost than our unified system, as shown in the table of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S1.F2\" title=\"In 1 Introduction &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nThese limitations underscore the necessity of an end-to-end approach that directly maps raw speech to images, enabling a more seamless and expressive integration of modalities.</p>\n\n",
                "matched_terms": [
                    "image",
                    "have",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, a novel speech-to-image model that bridges the rich information in speech with the image modality space, enabling more diverse and expressive visual representations.\nBuilding upon T2I models, our framework is suitable for the unique characteristics of speech, which differ from text:\n(1) Speech generally contains longer and more variable sequences than text, leading to uneven information density across embeddings.\n(2) Speech signals vary significantly depending on speaker identity, recording environment, and emotional state, affecting articulation and duration, even for the same content.\nTo address these challenges, we introduce a speech information bottleneck (SIB) that efficiently aligns cross-modal latent spaces while preserving key speech features.\nOur SIB encodes compressed conditional features that guide the image generation process.\nThrough extensive experiments, we establish an effective speech-based guidance for image generation by identifying the optimal combination of speech encoder, SIB, and image generator.</p>\n\n",
                "matched_terms": [
                    "image",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conditions for image generation.</span>\nRecently, diffusion-based conditional image generative models have emerged with remarkable performance <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib44\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">44</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib40\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">40</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib41\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">41</span></a>]</cite>. Specifically, stable diffusion (SD)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite> has shown impressive results in both quality and generalizability.\nGiven that these models only take text as a condition, they have struggled to reflect individual thoughts and emotions beyond text into compelling images.\nSome methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib57\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">57</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite> have proposed emotional image generation, pointing out the importance of reacting to the user&#8217;s sentiment.\nHowever, they relied on explicit linguistic expressions (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., <span class=\"ltx_text ltx_font_italic\">&#8216;with a sense of happiness and joy&#8217;</span> in the text prompt) and focused on reflecting emotion in texture and color only&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib2\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></a>]</cite>.\nRecently, EmoGen&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib59\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">59</span></a>]</cite> and EmoEdit&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib60\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">60</span></a>]</cite> argued that emotional contents beyond color and style should be effectively expressed as semantic variations in a generated image.\nThey learned a more flexible generative model using a large-scale EmoSet dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib58\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">58</span></a>]</cite>, but still required users to explicitly specify emotion prompts.\nIn contrast, our approach automatically infers these nuances directly from the speaker&#8217;s voice.</p>\n\n",
                "matched_terms": [
                    "have",
                    "emotional",
                    "given",
                    "image",
                    "expressions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, speech naturally encodes nuanced emotion and tone&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib50\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">50</span></a>]</cite>, offering a more intuitive means for generating emotionally resonant images, yet it remains largely untapped as a conditioning signal.\nRecent audiovisual generation methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib29\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">29</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">7</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib26\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">26</span></a>]</cite> have been limited to relying only on semantic instances expressed in text, where the other expressions are excluded.\nMoreover, existing approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib54\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">54</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib55\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">55</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite> for S2I generation have used highly limited datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite>, restricting their expressive versatility.\nWe aim to design a unified and emotion-driven S2I framework as well as to introduce a large-scale dataset for both training and evaluation.</p>\n\n",
                "matched_terms": [
                    "have",
                    "expressions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Relationship between speech and image.</span>\nSpeech-image relationships have been widely explored in biometrics&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib31\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">31</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib39\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">39</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>]</cite>, linguistic alignment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>, and phonetic articulation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib56\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">56</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib9\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">9</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib10\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">10</span></a>]</cite>.\nThese studies provide valuable insights into how speech and vision interact in different contexts.\nAlso, image-speech retrieval&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> has explored the alignment between spoken descriptions and images, highlighting the importance of understanding the semantics in both modalities.\nDespite these advances, most of the existing studies focus on isolated characteristics between modalities.\nBy moving beyond traditional mappings, our work aims to bridge the gap by simultaneously leveraging natural semantic correspondences, <em class=\"ltx_emph ltx_font_italic\">i.e</em>., both linguistic and paralinguistic information, between speech and vision.</p>\n\n",
                "matched_terms": [
                    "have",
                    "different",
                    "image",
                    "between",
                    "relationships"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S2.F3\" title=\"In 2 Related Work &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the overall framework of <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, consisting of (1) a pretrained speech encoder, (2) SIB to reduce the computational cost and effectively connect heterogeneous two modalities, and (3) an image generator to synthesize an image from the compressed speech representations.\nNext, we describe each module in detail.</p>\n\n",
                "matched_terms": [
                    "describe",
                    "image",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider two pre-trained speech encoders, SONAR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> and Whisper large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib43\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">43</span></a>]</cite>, to deploy comprehensive speech features considering both linguistic and paralinguistic information.\nBriefly reviewed, SONAR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> has learned global semantic alignment between speech and text, enabling it to encode meaning beyond phonetic content.\nWe remove its last aggregation layer to ensure that the speech embeddings retain both linguistic and paralinguistic information.\nWe also test Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib43\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">43</span></a>]</cite>, a widely used speech recognition model. Whisper is known to be capable of capturing paralinguistic information, such as emotion and speaker identity&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib64\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">64</span></a>]</cite>.\nFormally, given an input utterance <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math>, we obtain a speech embedding <math alttext=\"s\\in\\mathrm{R}^{N\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo>&#8712;</mo><msup><mi mathvariant=\"normal\">R</mi><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">s\\in\\mathrm{R}^{N\\times D}</annotation></semantics></math>, where <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> depends on the length of the speech and models, and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> is the channel dimension of the final output layer.\nWe note that our work does not employ ASR system or text encoder to explicitly map the speech into text.</p>\n\n",
                "matched_terms": [
                    "between",
                    "given",
                    "such"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although speech embeddings contain rich representations, they are excessively long and lead to a lower information density in each speech token compared to text (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., Whisper encodes a maximum of 1500 tokens for 30-seconds long speech, while CLIP text encoder limited to 77 tokens).\nThis low density makes direct usage challenging to condition the image generator.\nTo solve this problem, we design a Transformer-based speech information bottleneck (SIB) module.\nSIB compacts semantics in speech embeddings, motivated by previous works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib51\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">51</span></a>]</cite> applied to individual image and audio encoders.\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S2.F3\" title=\"In 2 Related Work &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, SIB reduces the number of embeddings with a strided convolution layer after a Transformer block along the time axis.\nBased on our findings, a pooling ratio of 8 provides the optimal balance, allowing us to maximize the information retention of speech features.\nAs a result, the initial embedding <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> is processed into a compressed speech condition <math alttext=\"c=f_{\\psi}(s)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>=</mo><mrow><msub><mi>f</mi><mi>&#968;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">c=f_{\\psi}(s)</annotation></semantics></math>, where <math alttext=\"c\\in\\mathrm{R}^{M\\times D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>&#8712;</mo><msup><mi mathvariant=\"normal\">R</mi><mrow><mi>M</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>D</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">c\\in\\mathrm{R}^{M\\times D^{\\prime}}</annotation></semantics></math>, <math alttext=\"M=N/8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>=</mo><mrow><mi>N</mi><mo>/</mo><mn>8</mn></mrow></mrow><annotation encoding=\"application/x-tex\">M=N/8</annotation></semantics></math> and <math alttext=\"D^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><msup><mi>D</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">D^{\\prime}</annotation></semantics></math> is the input channel of the cross-attention block in the image generator.\nThose compressed representations improve the efficiency of the S2I process while preserving both linguistic and emotional expressiveness.</p>\n\n",
                "matched_terms": [
                    "long",
                    "each",
                    "image",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The image generator is based on the latent diffusion model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite>.\nThe speech condition <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math>, compressed through SIB, is fed into the generator as a guidance of the synthesis process.\nSpecifically, the speech embeddings are injected into the UNet through cross-attention layers to condition the image synthesis.\nThis conditioning allows the model to incorporate the emotional, semantic content of speech into the generation process.\nThe image generator and SIB are optimized with the diffusion loss&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite>.\nGiven that we do not design a specialized loss function compared to previous works such as contrastive learning in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>]</cite>, AR modeling in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>, our simple training framework ensures versatile connections for various image generators.\nIn inference, the denoised latent is decoded into the image through the decoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib30\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>]</cite>.\nGiven that we do not design a specialized loss function compared to previous works, such as contrastive learning in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>]</cite>, AR modeling in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>,\nour simple framework ensures versatile connections for various image generators.</p>\n\n",
                "matched_terms": [
                    "image",
                    "emotional",
                    "given",
                    "such"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is to generate an image with a corresponding spoken description, even for emotional expression.\nHowever, prior datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite> overlooked paralinguistic features in speech, and also required significant costs for human recordings.\nOur benchmark uniquely leverages synthesized speech, enabling the natural and cost-effective creation of a large-scale dataset.\nSpecifically, VoxEmoset leverages semantic knowledge and the generative powers of pre-trained multimodal LLMs and diffusion models to generate diverse synthetic data samples.\nFirst, a multimodal LLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite> generates corresponding captions that are factual descriptions of a given emotional image based on explaining environments or objects.\nThen, a TTS model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite> generates emotional speech samples from text captions, using emotional voice samples from other datasets as references.\nConsequently, we efficiently and cheaply generate large-scale emotional utterances along with text captions, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S3.F4\" title=\"In 3.3 Image generator &#8227; 3 VoxStudio &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "given",
                    "objects",
                    "generate",
                    "image",
                    "captions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While EmoSet categorized emotion classes, there is no sentence-level description for visual scenes.\nWe generate captions using the instruction prompt in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A1\" title=\"Appendix A VoxEmoset &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">A</span></a>, restricting immediate emotional expressions while focusing on factual descriptions.\nLLaVA-OneVision&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib32\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">32</span></a>]</cite>, using SigLIP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib62\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">62</span></a>]</cite> as an image encoder and Qwen-2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib8\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></a>]</cite> as LLM, generates three different captions for each image to prevent the model from simply generating emotionally biased captions, <em class=\"ltx_emph ltx_font_italic\">e.g</em>., &#8216;<span class=\"ltx_text ltx_font_italic\">a person is happy.</span>&#8217;, &#8216;<span class=\"ltx_text ltx_font_italic\">disgusting rotten egg in the plate.</span>&#8217;.\nThe word count distribution of our 247k generated captions closely matches that of existing benchmarks, indicating that they were carefully crafted to resemble real-world datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F1\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B1</span></a> in Appendix).</p>\n\n",
                "matched_terms": [
                    "each",
                    "three",
                    "emotional",
                    "different",
                    "generate",
                    "image",
                    "captions",
                    "expressions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To objectively assess the quality of generated utterances, we randomly sample 10k utterances from each dataset and measure NMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib45\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">45</span></a>]</cite>. For CREMA-D, we use the entire samples.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S3.T1\" title=\"In 3.3 Image generator &#8227; 3 VoxStudio &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> shows that VoxEmoset is compatible with existing speech-image datasets such as SpokenCOCO and Flickr8kAudio in terms of speech quality (NMOS) and description quality (CLIPScore).\nHowever, only our benchmark explicitly expresses emotion in speech.\nThe last two rows in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S3.T1\" title=\"In 3.3 Image generator &#8227; 3 VoxStudio &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> validates that VoxEmoset guarantees high perceptual fidelity with clear affect, where emotion discriminability (Emo-C) is measured as the emotion classifier&#8217;s average confidence score.</p>\n\n",
                "matched_terms": [
                    "each",
                    "such"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets.</span>\nWe use SpokenCOCO&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite> and VoxEmoset to train <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>.\nVoxEmoset includes 208k utterances with paired 69k images for training, while SpokenCOCO contains 118k images with 591k utterances.\nFlickr8kAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> is used to evaluate zero-shot generalizability.\nEach image in SpokenCOCO and Flickr8kAudio has five voice recordings from unskilled annotators, resulting in inherently noisy audio (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., the recording may contain background noise, reading speed or volume can vary, and pronunciation may not be as clear as that of skilled voice actors as in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.SS4\" title=\"B.4 Failure cases and limitation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B.4</span></a>).\nVoxEmoset is automatically generated and less prone to recording noise.\nWe use the Karpathy split&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib27\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">27</span></a>]</cite> for SpokenCOCO and Flickr8kAudio.</p>\n\n",
                "matched_terms": [
                    "each",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results on SpokenCOCO and VoxEmoset.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S4.T2\" title=\"In 4.4 Dataset quality &#8227; 4 VoxEmoset Benchmark &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the comparison of <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> and baselines<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>EmoGen was excluded because its pretrained weights are publicly unavailable, and our reimplementation was unable to match its reported performance.</span></span></span> on SpokenCOCO and VoxEmoset.\nSD1.5 with the text inputs (<em class=\"ltx_emph ltx_font_italic\">i.e</em>., without speech) is shown as a baseline.\nEspecially, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S4.F5\" title=\"In 4.4 Dataset quality &#8227; 4 VoxEmoset Benchmark &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> highlights the stark contrast between text- and speech-based generation.\nWhile speech conveys emotions even with the same wording, the text-based model inherently ignores these cues and focuses on fact-based generation.\nEven when trained on VoxEmoset, &#8216;SD (finetuning)&#8217; struggles to express emotions as semantic content, but speech leads to a richer and intense emotional expression.\nFor example, given the prompt &#8216;A black trash can is placed against a white wall,&#8217; our model detects disgust from spoken nuances and visually emphasizes the unpleasantness of trash, whereas the text-based model remains neutral.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "given",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, despite the inherent noise in speech features and our method needs significantly lower latency compared to text-based approaches, the performance gap remains minimal in image quality and text alignment.\nMoreover, as shown in the last example in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S4.F5\" title=\"In 4.4 Dataset quality &#8227; 4 VoxEmoset Benchmark &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, the CLIP encoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib42\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">42</span></a>]</cite> often overlooks information from the latter part of a sentence&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib63\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">63</span></a>]</cite> (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., &#8216;bright yellow&#8217; in the last example).\nHowever, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> excels in conveying emotions when trained on the same datasets.\nThis advocates that speech, as a richer modality for emotional expression, provides a more effective signal to generate emotionally compelling images.</p>\n\n",
                "matched_terms": [
                    "image",
                    "sentence",
                    "emotional",
                    "generate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human evaluation.</span>\nA user study is conducted to assess how well humans perceive the alignment between speech and image atmosphere.\n26 participants evaluated 25 images to rate how well the emotion conveyed in the image matched the given speech.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F7.sf1\" title=\"In Figure 7 &#8227; 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7(a)</span></a> shows that results from <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> are more aligned with the emotion than text-based SD in all categories.\nIn other words, with an average of 57.09% preference, the images generated by our <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> were rated as better at expressing emotions.\nIt highlights the effectiveness of speech prompts for expressive image synthesis.\nWe also carried out another human evaluation on SpokenCOCO across speech-based models.\nThis experiment is performed on 17 participants who evaluate 10 generated images for each model with a 5-point Likert scale.\nAs demonstrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F7.sf2\" title=\"In Figure 7 &#8227; 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7(b)</span></a>, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> outperforms existing approaches by generating high-quality images that accurately reflect the nuances of the input prompts.</p>\n\n",
                "matched_terms": [
                    "each",
                    "given",
                    "image",
                    "between",
                    "words"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effect of emotion.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F8\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a> demonstrates that the same description, when spoken with different emotions, leads to distinct visual outputs by <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>. This highlights <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>&#8217;s capability to produce emotional nuances beyond linguistic content.\nFor instance, a neutral statement spoken in a disgusted tone results in negative visual details (top-left), while an &#8220;enjoying&#8221; tone generates a more positive scene (top-right).\nThese findings show that our speech-based approach effectively leverages emotional cues, enabling more expressive and context-rich image generation.</p>\n\n",
                "matched_terms": [
                    "image",
                    "emotional",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generalization on real speech.</span>\nTo evaluate how well our model generalizes, we test on utterances in ESD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib65\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">65</span></a>]</cite> dataset, excluded from our reference samples during speech synthesis.\nWe visualize generated samples from real speakers&#8217; utterances in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F9\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a>.\nText-based generator is limited to expressing the tone in speech prompt, but <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> successfully expresses atmospheres despite ambiguous words.\nIt also proves the superiority of VoxEmoset in that <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> trained on synthesized emotional speech is well generalized in real utterances.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F8\" title=\"In B.3 Details of human evaluation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B8</span></a> also demonstrates that <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> extends naturally to various applications such as image editing by spoken prompt.</p>\n\n",
                "matched_terms": [
                    "image",
                    "emotional",
                    "such",
                    "words"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training strategies.</span>\nDiffusion training is usually computationally expensive.\nWe test different training strategies in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T5\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>: full finetuning, LoRA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib24\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">24</span></a>]</cite>, and freezing the model.\nWhile finetuning achieves the best performance, LoRA and frozen models show comparable results in CLIPScore and Emo-A.\nAdditionally, although speech is noisier than text, our method outperforms full finetuning for original SD1.5 (&#8216;SD(T2I)-FT&#8217; in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T5\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>) in terms of emotional expression, while maintaining the generation quality.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scale of the image generator.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T6\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> demonstrates the performance of image generators at different scales.\nDue to the resource limit, we compare UNet of SD1.5&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite> and SDXL&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib41\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">41</span></a>]</cite> as our image generator in a frozen state during the training.\nInterestingly, although the small generator achieves a higher CLIPScore, the larger generator excels at displaying emotional nuances.\nThis finding suggests that larger-scale generators are inherently better at representing content beyond simple text cues.</p>\n\n",
                "matched_terms": [
                    "image",
                    "emotional",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech embedding.</span>\nWe compare SONAR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> and Whisper-Large v3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib43\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">43</span></a>]</cite> encoders as a speech input handler of our method.\nWhisper is a widely used ASR model, also known to be capable of capturing paralinguistic information&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib64\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">64</span></a>]</cite>.\nWhile SONAR is sentence-level speech-text aligned features, Whisper is trained at the phoneme-level by predicting which words are spoken in a given audio snippet.\nThis fundamental difference affects how each encoder preserves linguistic content and emotional cues when mapping speech to image descriptions.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T7\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a> demonstrates that text-aligned embeddings (<em class=\"ltx_emph ltx_font_italic\">i.e</em>., SONAR) show more robust performance on our task.</p>\n\n",
                "matched_terms": [
                    "each",
                    "emotional",
                    "given",
                    "image",
                    "words"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is the first end-to-end S2I model that captures both linguistic and emotional nuances from speech.\nUnlike text-based methods, our approach totally leverages speech&#8217;s expressiveness to generate emotionally aligned images. VoxEmoset is built cheaply, but it is complementary with real-world datasets.\nOur experiments demonstrate that <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> not only outperforms prior speech-based methods in conveying sentiment through images, but also matches text-driven approaches in semantic alignment, despite the higher noise and lower latency of the speech modality.\nWe believe our work facilitates future research in voice-driven generative models and their applications.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "generate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our objective for data construction is primarily on (1) synthesizing large-scale image and speech pairs, (2) the speech will be emotional rich, and (3) closely matches the quality of real recordings while diversifying the range of speakers.\nHere we supplement the details of VoxEmoset.</p>\n\n",
                "matched_terms": [
                    "image",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the following prompt to generate text captions for images using LLaVA-OneVision&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib32\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">32</span></a>]</cite>:</p>\n\n",
                "matched_terms": [
                    "captions",
                    "generate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the characteristics of SpokenCOCO, we limit the length of captions and ensure they accurately describe the context of the image.\nIn <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F1\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B1</span></a>, the word count distribution remains similar to SpokenCOCO and Flickr8kAudio, but with a more structured and consistent pattern.</p>\n\n",
                "matched_terms": [
                    "captions",
                    "describe",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech generation, we use state-of-the-art F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite>, where the vocoder is trained on 24kHz.\nThe generated speech is resampled to 16kHz to use SONAR and Whisper encoders.\nTo diversify the speaker characteristics, we collect multiple emotional speech datasets: MEAD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib53\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">53</span></a>]</cite>, CREMA-D&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib3\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></a>]</cite>, and RAVDESS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib34\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">34</span></a>]</cite>, which are widely used in emotional speech synthesis and speech emotion recognition.\nMEAD is an audiovisual dataset annotated in 8 emotional categories.\nCREMA-D is a crowd-sourced actor dataset, using 6 emotion classes.\nRAVDESS contains audio and video, where the professional actors express emotion.\nAll datasets used English.\nThe split cleaned up by EmoBox&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>]</cite> is used, especially the fold 1 split for RAVDESS.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.T1\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B1</span></a> summarizes the number of emotion classes and speakers for each dataset.</p>\n\n",
                "matched_terms": [
                    "each",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We show more qualitative comparisons in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F2\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F3\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B3</span></a>.\nIn those experiments, our image generators are initialized by UNet parameters in SD1.5.\nWhen training SD with a CLIP encoder using text prompts from our dataset, the model better follows the content of the text compared to zero-shot generation.\nHowever, in terms of emotional intensity and expressiveness, it performs weaker than our approach using speech prompts.\nFor example, in the last row in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F3\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B3</span></a>, generating an emotionally rich image from a sentence like &#8216;a tomato is cut into sections on a white plate&#8217; is challenging.\nBy using speech input that conveys a feeling of disgust, our model generates an image where the tomato appears distorted, conditioned on the given emotion category.\nMoreover, despite the inherent noise in speech, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> utilizes SIB to refine the information and capture its meaning, effectively following the content of the prompt.</p>\n\n",
                "matched_terms": [
                    "image",
                    "sentence",
                    "emotional",
                    "given"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess how well our model captures paralinguistic information in speech, we conducted a human evaluation to measure the alignment between the emotions perceived in the input speech and those conveyed in the generated images.\nWe had 26 locally recruited participants evaluate 25 images.\nFive images represent each emotion, but we did not provide any information about which emotion each speech sample conveyed.\nParticipants evaluated the images with randomly mixed emotion classes.\nThe instruction in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F6\" title=\"In B.3 Details of human evaluation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B6</span></a> is used in human evaluation.</p>\n\n",
                "matched_terms": [
                    "each",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We recruit 17 independent evaluators to assess image quality and speech prompt fidelity on SpokenCOCO in Fig. 7(b) of the main paper.\nIn this experiment, the instructions in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F7\" title=\"In B.3 Details of human evaluation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B7</span></a> are used for evaluating 10 different images generated by SpeechCLIP+, TMT, and <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, respectively.</p>\n\n",
                "matched_terms": [
                    "image",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although it is impossible to manually verify all samples, we found that SpokenCOCO dataset, which was created using human annotators via AMT, often contains misrecorded speech samples.\nFor example, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F9.sf1\" title=\"In Figure B9 &#8227; B.4 Failure cases and limitation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9(a)</span></a>, some recordings mispronounce the text prompt originally associated with the image.\nTherefore, using speech inputs that contain such errors is inevitably prone to performance degradation compared to using text inputs.\nAdditionally, the clarity of word representation in speech depends on the speaker&#8217;s pronunciation, making it challenging to distinguish homophones or similarly pronounced words.</p>\n\n",
                "matched_terms": [
                    "image",
                    "such",
                    "words"
                ]
            }
        ]
    },
    "A2.T1": {
        "source_file": "Seeing What You Say: Expressive Image Generation from Speech",
        "caption": "Table B1: Characteristics of speech emotion datasets used as emotion and speaker condition.",
        "body": "Dataset\nClasses\n# Speakers\n\n\nRAVDESS\n8\n24\n\n\nMEAD\n8\n48\n\n\nCREMA-D\n6\n91",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Classes</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\"># Speakers</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">RAVDESS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">24</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">MEAD</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">48</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">CREMA-D</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">91</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "speech",
            "classes",
            "ravdess",
            "emotion",
            "mead",
            "dataset",
            "characteristics",
            "datasets",
            "condition",
            "speakers",
            "speaker",
            "used",
            "cremad"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">For speech generation, we use state-of-the-art F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite>, where the vocoder is trained on 24kHz.\nThe generated speech is resampled to 16kHz to use SONAR and Whisper encoders.\nTo diversify the speaker characteristics, we collect multiple emotional speech datasets: MEAD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib53\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">53</span></a>]</cite>, CREMA-D&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib3\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></a>]</cite>, and RAVDESS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib34\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">34</span></a>]</cite>, which are widely used in emotional speech synthesis and speech emotion recognition.\nMEAD is an audiovisual dataset annotated in 8 emotional categories.\nCREMA-D is a crowd-sourced actor dataset, using 6 emotion classes.\nRAVDESS contains audio and video, where the professional actors express emotion.\nAll datasets used English.\nThe split cleaned up by EmoBox&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>]</cite> is used, especially the fold 1 split for RAVDESS.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.T1\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B1</span></a> summarizes the number of emotion classes and speakers for each dataset.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This paper proposes <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, the first unified and end-to-end speech-to-image model that generates expressive images directly from spoken descriptions by jointly aligning linguistic and paralinguistic information.\nAt its core is a speech information bottleneck (SIB) module, which compresses raw speech into compact semantic tokens, preserving prosody and emotional nuance.\nBy operating directly on these tokens, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> eliminates the need for an additional speech-to-text system, which often ignores the hidden details beyond text, <em class=\"ltx_emph ltx_font_italic\">e.g</em>., tone or emotion.\nWe also release VoxEmoset, a large&#8208;scale paired emotional speech&#8211;image dataset built via an advanced TTS engine to affordably generate richly expressive utterances.\nComprehensive experiments on the SpokenCOCO, Flickr8kAudio, and VoxEmoset benchmarks demonstrate the feasibility of our method and highlight key challenges, including emotional consistency and linguistic ambiguity, paving the way for future research.\n<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\"/><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\"/><span class=\"ltx_tag ltx_tag_note\"/><sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup> Partly work done in NAVER AI Lab.\n<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> Corresponding author.</span></span></span></p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotion",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans naturally &#8220;imagine&#8221; vivid mental images when listening to speech, which conveys not only semantics but also emotion, tone, and intent.\nSpeech-to-image (S2I) generation taps this rich, multimodal expressiveness to produce visuals that are more nuanced and emotionally resonant than those driven by text alone. By translating spoken descriptions directly into images, S2I can unlock applications in accessibility, creative media, and voice&#8208;driven interfaces&#8212;treating speech as a first&#8208;class modality for content creation rather than a mere precursor to text.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in text-to-image (T2I) generation have demonstrated remarkable progress, but they struggle to utilize the innate expressiveness and accessibility of speech.\nMost cascaded framework&#8212;where an utterance is first transcribed into text or textual feature and then used as input for T2I models, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S1.F2\" title=\"In 1 Introduction &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> (a, b)&#8212;encounters several significant challenges.\nFirst, speech-to-text (<em class=\"ltx_emph ltx_font_italic\">i.e</em>., ASR) transcription is limited to capturing prosody and speaker intention.\nHowever, transcription errors propagate into the image generative model, then degrade visual quality.\nSecond, this sequential approach inherently decouples speech and image generation, making it difficult to transfer crucial prosodic and temporal cues&#8212;such as speaking rate, pitch variation, and emotional style&#8212;that can influence the mood, color palette, or overall aesthetic of the generated image.\nRelying on intermediate text also excludes languages without written forms&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib50\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">50</span></a>]</cite>.\nEven for languages that do have a writing system, coverage for the cascaded approach remains far from comprehensive: there are over 7,100 languages worldwide&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib13\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">13</span></a>]</cite>, <em class=\"ltx_emph ltx_font_italic\">e.g</em>. Google API covers only 125<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1a\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://cloud.google.com/speech-to-text</span></span></span>.\nFinally, the cascaded system limits the inference speed and requires a higher cost than our unified system, as shown in the table of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S1.F2\" title=\"In 1 Introduction &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nThese limitations underscore the necessity of an end-to-end approach that directly maps raw speech to images, enabling a more seamless and expressive integration of modalities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, a novel speech-to-image model that bridges the rich information in speech with the image modality space, enabling more diverse and expressive visual representations.\nBuilding upon T2I models, our framework is suitable for the unique characteristics of speech, which differ from text:\n(1) Speech generally contains longer and more variable sequences than text, leading to uneven information density across embeddings.\n(2) Speech signals vary significantly depending on speaker identity, recording environment, and emotional state, affecting articulation and duration, even for the same content.\nTo address these challenges, we introduce a speech information bottleneck (SIB) that efficiently aligns cross-modal latent spaces while preserving key speech features.\nOur SIB encodes compressed conditional features that guide the image generation process.\nThrough extensive experiments, we establish an effective speech-based guidance for image generation by identifying the optimal combination of speech encoder, SIB, and image generator.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "characteristics",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">VoxEmoset</span>, an automatically (and efficiently) synthesized dataset of 247k emotional spoken descriptions for sentiment images. VoxEmoset is used for both training and evaluation of S2I.</p>\n\n",
                "matched_terms": [
                    "used",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conditions for image generation.</span>\nRecently, diffusion-based conditional image generative models have emerged with remarkable performance <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib44\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">44</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib40\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">40</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib41\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">41</span></a>]</cite>. Specifically, stable diffusion (SD)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite> has shown impressive results in both quality and generalizability.\nGiven that these models only take text as a condition, they have struggled to reflect individual thoughts and emotions beyond text into compelling images.\nSome methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib57\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">57</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite> have proposed emotional image generation, pointing out the importance of reacting to the user&#8217;s sentiment.\nHowever, they relied on explicit linguistic expressions (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., <span class=\"ltx_text ltx_font_italic\">&#8216;with a sense of happiness and joy&#8217;</span> in the text prompt) and focused on reflecting emotion in texture and color only&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib2\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></a>]</cite>.\nRecently, EmoGen&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib59\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">59</span></a>]</cite> and EmoEdit&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib60\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">60</span></a>]</cite> argued that emotional contents beyond color and style should be effectively expressed as semantic variations in a generated image.\nThey learned a more flexible generative model using a large-scale EmoSet dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib58\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">58</span></a>]</cite>, but still required users to explicitly specify emotion prompts.\nIn contrast, our approach automatically infers these nuances directly from the speaker&#8217;s voice.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "condition",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, speech naturally encodes nuanced emotion and tone&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib50\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">50</span></a>]</cite>, offering a more intuitive means for generating emotionally resonant images, yet it remains largely untapped as a conditioning signal.\nRecent audiovisual generation methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib29\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">29</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">7</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib26\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">26</span></a>]</cite> have been limited to relying only on semantic instances expressed in text, where the other expressions are excluded.\nMoreover, existing approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib54\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">54</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib55\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">55</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite> for S2I generation have used highly limited datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite>, restricting their expressive versatility.\nWe aim to design a unified and emotion-driven S2I framework as well as to introduce a large-scale dataset for both training and evaluation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotion",
                    "dataset",
                    "datasets",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Relationship between speech and image.</span>\nSpeech-image relationships have been widely explored in biometrics&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib31\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">31</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib39\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">39</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>]</cite>, linguistic alignment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>, and phonetic articulation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib56\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">56</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib9\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">9</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib10\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">10</span></a>]</cite>.\nThese studies provide valuable insights into how speech and vision interact in different contexts.\nAlso, image-speech retrieval&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> has explored the alignment between spoken descriptions and images, highlighting the importance of understanding the semantics in both modalities.\nDespite these advances, most of the existing studies focus on isolated characteristics between modalities.\nBy moving beyond traditional mappings, our work aims to bridge the gap by simultaneously leveraging natural semantic correspondences, <em class=\"ltx_emph ltx_font_italic\">i.e</em>., both linguistic and paralinguistic information, between speech and vision.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "characteristics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider two pre-trained speech encoders, SONAR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> and Whisper large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib43\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">43</span></a>]</cite>, to deploy comprehensive speech features considering both linguistic and paralinguistic information.\nBriefly reviewed, SONAR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> has learned global semantic alignment between speech and text, enabling it to encode meaning beyond phonetic content.\nWe remove its last aggregation layer to ensure that the speech embeddings retain both linguistic and paralinguistic information.\nWe also test Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib43\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">43</span></a>]</cite>, a widely used speech recognition model. Whisper is known to be capable of capturing paralinguistic information, such as emotion and speaker identity&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib64\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">64</span></a>]</cite>.\nFormally, given an input utterance <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math>, we obtain a speech embedding <math alttext=\"s\\in\\mathrm{R}^{N\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo>&#8712;</mo><msup><mi mathvariant=\"normal\">R</mi><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">s\\in\\mathrm{R}^{N\\times D}</annotation></semantics></math>, where <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> depends on the length of the speech and models, and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> is the channel dimension of the final output layer.\nWe note that our work does not employ ASR system or text encoder to explicitly map the speech into text.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotion",
                    "used",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although speech embeddings contain rich representations, they are excessively long and lead to a lower information density in each speech token compared to text (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., Whisper encodes a maximum of 1500 tokens for 30-seconds long speech, while CLIP text encoder limited to 77 tokens).\nThis low density makes direct usage challenging to condition the image generator.\nTo solve this problem, we design a Transformer-based speech information bottleneck (SIB) module.\nSIB compacts semantics in speech embeddings, motivated by previous works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib51\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">51</span></a>]</cite> applied to individual image and audio encoders.\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S2.F3\" title=\"In 2 Related Work &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, SIB reduces the number of embeddings with a strided convolution layer after a Transformer block along the time axis.\nBased on our findings, a pooling ratio of 8 provides the optimal balance, allowing us to maximize the information retention of speech features.\nAs a result, the initial embedding <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> is processed into a compressed speech condition <math alttext=\"c=f_{\\psi}(s)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>=</mo><mrow><msub><mi>f</mi><mi>&#968;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">c=f_{\\psi}(s)</annotation></semantics></math>, where <math alttext=\"c\\in\\mathrm{R}^{M\\times D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>&#8712;</mo><msup><mi mathvariant=\"normal\">R</mi><mrow><mi>M</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>D</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">c\\in\\mathrm{R}^{M\\times D^{\\prime}}</annotation></semantics></math>, <math alttext=\"M=N/8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>=</mo><mrow><mi>N</mi><mo>/</mo><mn>8</mn></mrow></mrow><annotation encoding=\"application/x-tex\">M=N/8</annotation></semantics></math> and <math alttext=\"D^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><msup><mi>D</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">D^{\\prime}</annotation></semantics></math> is the input channel of the cross-attention block in the image generator.\nThose compressed representations improve the efficiency of the S2I process while preserving both linguistic and emotional expressiveness.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "condition"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The image generator is based on the latent diffusion model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite>.\nThe speech condition <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math>, compressed through SIB, is fed into the generator as a guidance of the synthesis process.\nSpecifically, the speech embeddings are injected into the UNet through cross-attention layers to condition the image synthesis.\nThis conditioning allows the model to incorporate the emotional, semantic content of speech into the generation process.\nThe image generator and SIB are optimized with the diffusion loss&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite>.\nGiven that we do not design a specialized loss function compared to previous works such as contrastive learning in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>]</cite>, AR modeling in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>, our simple training framework ensures versatile connections for various image generators.\nIn inference, the denoised latent is decoded into the image through the decoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib30\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>]</cite>.\nGiven that we do not design a specialized loss function compared to previous works, such as contrastive learning in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>]</cite>, AR modeling in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>,\nour simple framework ensures versatile connections for various image generators.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "condition"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is to generate an image with a corresponding spoken description, even for emotional expression.\nHowever, prior datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite> overlooked paralinguistic features in speech, and also required significant costs for human recordings.\nOur benchmark uniquely leverages synthesized speech, enabling the natural and cost-effective creation of a large-scale dataset.\nSpecifically, VoxEmoset leverages semantic knowledge and the generative powers of pre-trained multimodal LLMs and diffusion models to generate diverse synthetic data samples.\nFirst, a multimodal LLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite> generates corresponding captions that are factual descriptions of a given emotional image based on explaining environments or objects.\nThen, a TTS model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite> generates emotional speech samples from text captions, using emotional voice samples from other datasets as references.\nConsequently, we efficiently and cheaply generate large-scale emotional utterances along with text captions, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S3.F4\" title=\"In 3.3 Image generator &#8227; 3 VoxStudio &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "datasets",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our benchmark uses images in EmoSet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib58\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">58</span></a>]</cite>, the large-scale visual emotion dataset annotated with Mikels model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib38\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">38</span></a>]</cite>.\nWe use the partial of 118k subset labeled by humans and machines, including six categories: amusement, excitement, anger, disgust, fear, and sadness.\nIn line with <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib14\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">14</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib48\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">48</span></a>]</cite>, we group amusement and excitement into a single emotion category, &#8216;enjoyment&#8217;, because these two categories are difficult to distinguish solely through voice expression.\nOn the other hand, we exclude &#8216;awe&#8217; and &#8216;contentment&#8217; emotion categories which are hard to express in voice.\nThe final number of images in VoxEmoset is shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S3.T1\" title=\"In 3.3 Image generator &#8227; 3 VoxStudio &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While EmoSet categorized emotion classes, there is no sentence-level description for visual scenes.\nWe generate captions using the instruction prompt in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A1\" title=\"Appendix A VoxEmoset &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">A</span></a>, restricting immediate emotional expressions while focusing on factual descriptions.\nLLaVA-OneVision&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib32\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">32</span></a>]</cite>, using SigLIP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib62\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">62</span></a>]</cite> as an image encoder and Qwen-2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib8\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></a>]</cite> as LLM, generates three different captions for each image to prevent the model from simply generating emotionally biased captions, <em class=\"ltx_emph ltx_font_italic\">e.g</em>., &#8216;<span class=\"ltx_text ltx_font_italic\">a person is happy.</span>&#8217;, &#8216;<span class=\"ltx_text ltx_font_italic\">disgusting rotten egg in the plate.</span>&#8217;.\nThe word count distribution of our 247k generated captions closely matches that of existing benchmarks, indicating that they were carefully crafted to resemble real-world datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F1\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B1</span></a> in Appendix).</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "classes",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, to build a diverse range of emotional voice references for TTS, emotional speech data was collected from multiple datasets, including CREMA-D&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib3\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></a>]</cite>, MEAD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib53\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">53</span></a>]</cite>, and RAVDESS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib34\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">34</span></a>]</cite>.\nThese datasets contain English-spoken utterances from a variety of speakers.\nFollowing EmoBox&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>]</cite>, we split the datasets into training and test sets.\nWe validate the emotions in the generated speech using Emotion2Vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib37\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">37</span></a>]</cite> to measure emotional intensity, filtering and re-generating inadequate samples. After this process, 247k speech samples are generated.\nFurther details are provided in Appendix.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "ravdess",
                    "mead",
                    "datasets",
                    "speakers",
                    "cremad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To objectively assess the quality of generated utterances, we randomly sample 10k utterances from each dataset and measure NMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib45\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">45</span></a>]</cite>. For CREMA-D, we use the entire samples.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S3.T1\" title=\"In 3.3 Image generator &#8227; 3 VoxStudio &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> shows that VoxEmoset is compatible with existing speech-image datasets such as SpokenCOCO and Flickr8kAudio in terms of speech quality (NMOS) and description quality (CLIPScore).\nHowever, only our benchmark explicitly expresses emotion in speech.\nThe last two rows in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S3.T1\" title=\"In 3.3 Image generator &#8227; 3 VoxStudio &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> validates that VoxEmoset guarantees high perceptual fidelity with clear affect, where emotion discriminability (Emo-C) is measured as the emotion classifier&#8217;s average confidence score.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotion",
                    "dataset",
                    "datasets",
                    "cremad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets.</span>\nWe use SpokenCOCO&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite> and VoxEmoset to train <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>.\nVoxEmoset includes 208k utterances with paired 69k images for training, while SpokenCOCO contains 118k images with 591k utterances.\nFlickr8kAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> is used to evaluate zero-shot generalizability.\nEach image in SpokenCOCO and Flickr8kAudio has five voice recordings from unskilled annotators, resulting in inherently noisy audio (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., the recording may contain background noise, reading speed or volume can vary, and pronunciation may not be as clear as that of skilled voice actors as in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.SS4\" title=\"B.4 Failure cases and limitation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B.4</span></a>).\nVoxEmoset is automatically generated and less prone to recording noise.\nWe use the Karpathy split&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib27\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">27</span></a>]</cite> for SpokenCOCO and Flickr8kAudio.</p>\n\n",
                "matched_terms": [
                    "used",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation details.</span>\nTraining a high-performance image generator requires a vast amount of resources (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., SD1.5 requires 6,000 A100 GPU days&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib5\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></a>]</cite>).\nWe initialize the image generator with a pre-trained SD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib46\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">46</span></a>]</cite> for efficient learning.\nWe use SONAR as the speech encoder, freezing during the training, in which its last aggregation layer is removed.\nWe use AdamW <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib35\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">35</span></a>]</cite> with the learning rate of <math alttext=\"1e\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">1e</annotation></semantics></math>-<math alttext=\"6\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>, the batch size of 128 using 8 V100 GPUs.\nFP16 precision is used for all experiments.\nThe code will be released.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation metrics.</span>\nWe assess the generation quality using FID&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib22\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">22</span></a>]</cite>, while content alignment between speech and generated images is measured with CLIPScore&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib21\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite> using text transcriptions.\nFor SpokenCOCO and VoxEmoset, random samples of 10k condition prompts, either speech or text, are used for evaluation.\nFor Flickr8kAudio, we use 5k test prompts for evaluation.\nWe also report emotion classification accuracy (Emo-A)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib59\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">59</span></a>]</cite> on generated images to examine whether the results reflect emotion from prompts.\nNote that we measure accuracy only with scores for the 5 emotion categories <math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math><span class=\"ltx_text ltx_font_italic\">&#8216;amusement&#8217;</span> and <span class=\"ltx_text ltx_font_italic\">&#8216;excitement&#8217;</span> are classified as the same class<math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math> in the trained emotion classifier.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotion",
                    "used",
                    "condition"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, despite the inherent noise in speech features and our method needs significantly lower latency compared to text-based approaches, the performance gap remains minimal in image quality and text alignment.\nMoreover, as shown in the last example in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S4.F5\" title=\"In 4.4 Dataset quality &#8227; 4 VoxEmoset Benchmark &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, the CLIP encoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib42\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">42</span></a>]</cite> often overlooks information from the latter part of a sentence&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib63\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">63</span></a>]</cite> (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., &#8216;bright yellow&#8217; in the last example).\nHowever, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> excels in conveying emotions when trained on the same datasets.\nThis advocates that speech, as a richer modality for emotional expression, provides a more effective signal to generate emotionally compelling images.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Remarkably, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> outperforms SpeechCLIP+ and TMT on SpokenCOCO, where <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> does not use Flickr8kAudio for training.\nWhile TMT additionally used huge synthesized speech data from CC3M&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib49\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">49</span></a>]</cite> and CC12M&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>]</cite> for training, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> also show comparable results on VoxEmoset.\nThis result demonstrates that our diffusion model is a powerful learner for speech-to-expressive image alignment than contrastive learning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>]</cite> and auto-regressive training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>.\nThe qualitative comparison on SpokenCOCO shows that SpeechCLIP+ and TMT often ignore keywords in the prompts, while <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> can capture the details, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F6\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results on Flickr8kAudio.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T3\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the performance comparison on Flickr8kAudio.\nHere, while TMT and SpeechCLIP+ used Flickr8kAudio for training, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> was evaluated in a zero-shot manner.\nSurprisingly, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> outperforms existing methods by large margins.\nIt shows that end-to-end training in <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is more robust in aligning the speech-language space.\nBy contrast, speech features in <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> are more robust to the order or length of the prompt.\nMoreover, VoxEmoset might improve the robustness on generality as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F6\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human evaluation.</span>\nA user study is conducted to assess how well humans perceive the alignment between speech and image atmosphere.\n26 participants evaluated 25 images to rate how well the emotion conveyed in the image matched the given speech.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F7.sf1\" title=\"In Figure 7 &#8227; 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7(a)</span></a> shows that results from <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> are more aligned with the emotion than text-based SD in all categories.\nIn other words, with an average of 57.09% preference, the images generated by our <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> were rated as better at expressing emotions.\nIt highlights the effectiveness of speech prompts for expressive image synthesis.\nWe also carried out another human evaluation on SpokenCOCO across speech-based models.\nThis experiment is performed on 17 participants who evaluate 10 generated images for each model with a 5-point Likert scale.\nAs demonstrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F7.sf2\" title=\"In Figure 7 &#8227; 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7(b)</span></a>, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> outperforms existing approaches by generating high-quality images that accurately reflect the nuances of the input prompts.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generalization on real speech.</span>\nTo evaluate how well our model generalizes, we test on utterances in ESD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib65\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">65</span></a>]</cite> dataset, excluded from our reference samples during speech synthesis.\nWe visualize generated samples from real speakers&#8217; utterances in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F9\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a>.\nText-based generator is limited to expressing the tone in speech prompt, but <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> successfully expresses atmospheres despite ambiguous words.\nIt also proves the superiority of VoxEmoset in that <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> trained on synthesized emotional speech is well generalized in real utterances.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F8\" title=\"In B.3 Details of human evaluation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B8</span></a> also demonstrates that <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> extends naturally to various applications such as image editing by spoken prompt.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training datasets.</span> <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T4\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that VoxEmoset is complementary with the real-world spoken dataset, SpokenCOCO, improving both visual fidelity and semantic relevance.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech embedding.</span>\nWe compare SONAR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> and Whisper-Large v3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib43\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">43</span></a>]</cite> encoders as a speech input handler of our method.\nWhisper is a widely used ASR model, also known to be capable of capturing paralinguistic information&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib64\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">64</span></a>]</cite>.\nWhile SONAR is sentence-level speech-text aligned features, Whisper is trained at the phoneme-level by predicting which words are spoken in a given audio snippet.\nThis fundamental difference affects how each encoder preserves linguistic content and emotional cues when mapping speech to image descriptions.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T7\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a> demonstrates that text-aligned embeddings (<em class=\"ltx_emph ltx_font_italic\">i.e</em>., SONAR) show more robust performance on our task.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Architecture choices on SIB.</span>\nWe propose SIB to represent the speech condition compactly to address the issue of low information density of speech tokens.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T8\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a> compares its performance against a standard transformer structure.\nOur design choice achieves better performance with fewer parameters.\nGradually reducing the speech token length while simultaneously increasing their density across multiple layers enhances both the linguistic and paralinguistic expressiveness of the speech signal.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "condition"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is the first end-to-end S2I model that captures both linguistic and emotional nuances from speech.\nUnlike text-based methods, our approach totally leverages speech&#8217;s expressiveness to generate emotionally aligned images. VoxEmoset is built cheaply, but it is complementary with real-world datasets.\nOur experiments demonstrate that <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> not only outperforms prior speech-based methods in conveying sentiment through images, but also matches text-driven approaches in semantic alignment, despite the higher noise and lower latency of the speech modality.\nWe believe our work facilitates future research in voice-driven generative models and their applications.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our objective for data construction is primarily on (1) synthesizing large-scale image and speech pairs, (2) the speech will be emotional rich, and (3) closely matches the quality of real recordings while diversifying the range of speakers.\nHere we supplement the details of VoxEmoset.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speakers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results for <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> on SpokenCOCO according to the difference of training data is shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.T2\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B2</span></a>.\nIn the results, we observe that SpokenCOCO does not fully capture the diversity of real-world scenarios.\nMost images convey neutral emotions and primarily depict scenes suitable for objective descriptions.\nHowever, real-world photographs go beyond presenting mere facts, often communicating higher-level meanings such as feelings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib59\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">59</span></a>]</cite>.\nThis demonstrates that our dataset extends beyond the distribution of SpokenCOCO and Flickr8kAudio, offering a more realistic and emotionally expressive representation.\nThis claim is further supported by Table 2 of the main paper, where models trained on the SpokenCOCO and Flickr8kAudio datasets&#8212; even TMT, which was trained on a massive amount of synthesized speech from CC3M and CC12M&#8212;fail to generalize to our dataset.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We show more qualitative comparisons in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F2\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F3\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B3</span></a>.\nIn those experiments, our image generators are initialized by UNet parameters in SD1.5.\nWhen training SD with a CLIP encoder using text prompts from our dataset, the model better follows the content of the text compared to zero-shot generation.\nHowever, in terms of emotional intensity and expressiveness, it performs weaker than our approach using speech prompts.\nFor example, in the last row in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F3\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B3</span></a>, generating an emotionally rich image from a sentence like &#8216;a tomato is cut into sections on a white plate&#8217; is challenging.\nBy using speech input that conveys a feeling of disgust, our model generates an image where the tomato appears distorted, conditioned on the given emotion category.\nMoreover, despite the inherent noise in speech, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> utilizes SIB to refine the information and capture its meaning, effectively following the content of the prompt.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotion",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess how well our model captures paralinguistic information in speech, we conducted a human evaluation to measure the alignment between the emotions perceived in the input speech and those conveyed in the generated images.\nWe had 26 locally recruited participants evaluate 25 images.\nFive images represent each emotion, but we did not provide any information about which emotion each speech sample conveyed.\nParticipants evaluated the images with randomly mixed emotion classes.\nThe instruction in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F6\" title=\"In B.3 Details of human evaluation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B6</span></a> is used in human evaluation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotion",
                    "classes",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We recruit 17 independent evaluators to assess image quality and speech prompt fidelity on SpokenCOCO in Fig. 7(b) of the main paper.\nIn this experiment, the instructions in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F7\" title=\"In B.3 Details of human evaluation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B7</span></a> are used for evaluating 10 different images generated by SpeechCLIP+, TMT, and <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, respectively.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although it is impossible to manually verify all samples, we found that SpokenCOCO dataset, which was created using human annotators via AMT, often contains misrecorded speech samples.\nFor example, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.F9.sf1\" title=\"In Figure B9 &#8227; B.4 Failure cases and limitation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9(a)</span></a>, some recordings mispronounce the text prompt originally associated with the image.\nTherefore, using speech inputs that contain such errors is inevitably prone to performance degradation compared to using text inputs.\nAdditionally, the clarity of word representation in speech depends on the speaker&#8217;s pronunciation, making it challenging to distinguish homophones or similarly pronounced words.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset"
                ]
            }
        ]
    },
    "A2.T2": {
        "source_file": "Seeing What You Say: Expressive Image Generation from Speech",
        "caption": "Table B2: Impact of the training datasets on SpokenCOCO.",
        "body": "SpokenCOCO\nVoxEmoset\n\nFIDâ\\downarrow\n\n\nCLIPScoreâ\\uparrow\n\n\n\nâ\nâ\n24.95\n29.04\n\n\nâ\nâ\n32.59\n25.32\n\n\nâ\nâ\n27.15\n27.27",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">SpokenCOCO</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">VoxEmoset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FID</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T2.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CLIPScore</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T2.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#004D99;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#B3B3B3;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">24.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">29.04</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#B3B3B3;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#004D99;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">32.59</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.32</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#004D99;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#004D99;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.27</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "training",
            "voxemoset",
            "impact",
            "datasets",
            "spokencoco",
            "fidâdownarrow",
            "clipscoreâuparrow"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The results for <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> on SpokenCOCO according to the difference of training data is shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.T2\" title=\"In B.1 Ablation study &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B2</span></a>.\nIn the results, we observe that SpokenCOCO does not fully capture the diversity of real-world scenarios.\nMost images convey neutral emotions and primarily depict scenes suitable for objective descriptions.\nHowever, real-world photographs go beyond presenting mere facts, often communicating higher-level meanings such as feelings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib59\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">59</span></a>]</cite>.\nThis demonstrates that our dataset extends beyond the distribution of SpokenCOCO and Flickr8kAudio, offering a more realistic and emotionally expressive representation.\nThis claim is further supported by Table 2 of the main paper, where models trained on the SpokenCOCO and Flickr8kAudio datasets&#8212; even TMT, which was trained on a massive amount of synthesized speech from CC3M and CC12M&#8212;fail to generalize to our dataset.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This paper proposes <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>, the first unified and end-to-end speech-to-image model that generates expressive images directly from spoken descriptions by jointly aligning linguistic and paralinguistic information.\nAt its core is a speech information bottleneck (SIB) module, which compresses raw speech into compact semantic tokens, preserving prosody and emotional nuance.\nBy operating directly on these tokens, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> eliminates the need for an additional speech-to-text system, which often ignores the hidden details beyond text, <em class=\"ltx_emph ltx_font_italic\">e.g</em>., tone or emotion.\nWe also release VoxEmoset, a large&#8208;scale paired emotional speech&#8211;image dataset built via an advanced TTS engine to affordably generate richly expressive utterances.\nComprehensive experiments on the SpokenCOCO, Flickr8kAudio, and VoxEmoset benchmarks demonstrate the feasibility of our method and highlight key challenges, including emotional consistency and linguistic ambiguity, paving the way for future research.\n<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\"/><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\"/><span class=\"ltx_tag ltx_tag_note\"/><sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup> Partly work done in NAVER AI Lab.\n<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> Corresponding author.</span></span></span></p>\n\n",
                "matched_terms": [
                    "voxemoset",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">VoxEmoset</span>, an automatically (and efficiently) synthesized dataset of 247k emotional spoken descriptions for sentiment images. VoxEmoset is used for both training and evaluation of S2I.</p>\n\n",
                "matched_terms": [
                    "training",
                    "voxemoset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> in various S2I benchmarks, including SpokenCOCO&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite>, Flickr8kAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> and VoxEmoset, and demonstrate its superiority and high fidelity over baselines.</p>\n\n",
                "matched_terms": [
                    "voxemoset",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, speech naturally encodes nuanced emotion and tone&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib50\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">50</span></a>]</cite>, offering a more intuitive means for generating emotionally resonant images, yet it remains largely untapped as a conditioning signal.\nRecent audiovisual generation methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib29\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">29</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">7</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib26\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">26</span></a>]</cite> have been limited to relying only on semantic instances expressed in text, where the other expressions are excluded.\nMoreover, existing approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib54\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">54</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib55\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">55</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite> for S2I generation have used highly limited datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite>, restricting their expressive versatility.\nWe aim to design a unified and emotion-driven S2I framework as well as to introduce a large-scale dataset for both training and evaluation.</p>\n\n",
                "matched_terms": [
                    "training",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is to generate an image with a corresponding spoken description, even for emotional expression.\nHowever, prior datasets&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite> overlooked paralinguistic features in speech, and also required significant costs for human recordings.\nOur benchmark uniquely leverages synthesized speech, enabling the natural and cost-effective creation of a large-scale dataset.\nSpecifically, VoxEmoset leverages semantic knowledge and the generative powers of pre-trained multimodal LLMs and diffusion models to generate diverse synthetic data samples.\nFirst, a multimodal LLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite> generates corresponding captions that are factual descriptions of a given emotional image based on explaining environments or objects.\nThen, a TTS model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite> generates emotional speech samples from text captions, using emotional voice samples from other datasets as references.\nConsequently, we efficiently and cheaply generate large-scale emotional utterances along with text captions, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S3.F4\" title=\"In 3.3 Image generator &#8227; 3 VoxStudio &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "voxemoset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, to build a diverse range of emotional voice references for TTS, emotional speech data was collected from multiple datasets, including CREMA-D&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib3\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></a>]</cite>, MEAD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib53\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">53</span></a>]</cite>, and RAVDESS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib34\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">34</span></a>]</cite>.\nThese datasets contain English-spoken utterances from a variety of speakers.\nFollowing EmoBox&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>]</cite>, we split the datasets into training and test sets.\nWe validate the emotions in the generated speech using Emotion2Vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib37\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">37</span></a>]</cite> to measure emotional intensity, filtering and re-generating inadequate samples. After this process, 247k speech samples are generated.\nFurther details are provided in Appendix.</p>\n\n",
                "matched_terms": [
                    "training",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To objectively assess the quality of generated utterances, we randomly sample 10k utterances from each dataset and measure NMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib45\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">45</span></a>]</cite>. For CREMA-D, we use the entire samples.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S3.T1\" title=\"In 3.3 Image generator &#8227; 3 VoxStudio &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> shows that VoxEmoset is compatible with existing speech-image datasets such as SpokenCOCO and Flickr8kAudio in terms of speech quality (NMOS) and description quality (CLIPScore).\nHowever, only our benchmark explicitly expresses emotion in speech.\nThe last two rows in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S3.T1\" title=\"In 3.3 Image generator &#8227; 3 VoxStudio &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> validates that VoxEmoset guarantees high perceptual fidelity with clear affect, where emotion discriminability (Emo-C) is measured as the emotion classifier&#8217;s average confidence score.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "voxemoset",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets.</span>\nWe use SpokenCOCO&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite> and VoxEmoset to train <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span>.\nVoxEmoset includes 208k utterances with paired 69k images for training, while SpokenCOCO contains 118k images with 591k utterances.\nFlickr8kAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> is used to evaluate zero-shot generalizability.\nEach image in SpokenCOCO and Flickr8kAudio has five voice recordings from unskilled annotators, resulting in inherently noisy audio (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., the recording may contain background noise, reading speed or volume can vary, and pronunciation may not be as clear as that of skilled voice actors as in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#A2.SS4\" title=\"B.4 Failure cases and limitation &#8227; Appendix B More Results &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B.4</span></a>).\nVoxEmoset is automatically generated and less prone to recording noise.\nWe use the Karpathy split&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib27\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">27</span></a>]</cite> for SpokenCOCO and Flickr8kAudio.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "training",
                    "voxemoset",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation metrics.</span>\nWe assess the generation quality using FID&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib22\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">22</span></a>]</cite>, while content alignment between speech and generated images is measured with CLIPScore&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib21\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite> using text transcriptions.\nFor SpokenCOCO and VoxEmoset, random samples of 10k condition prompts, either speech or text, are used for evaluation.\nFor Flickr8kAudio, we use 5k test prompts for evaluation.\nWe also report emotion classification accuracy (Emo-A)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib59\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">59</span></a>]</cite> on generated images to examine whether the results reflect emotion from prompts.\nNote that we measure accuracy only with scores for the 5 emotion categories <math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math><span class=\"ltx_text ltx_font_italic\">&#8216;amusement&#8217;</span> and <span class=\"ltx_text ltx_font_italic\">&#8216;excitement&#8217;</span> are classified as the same class<math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math> in the trained emotion classifier.</p>\n\n",
                "matched_terms": [
                    "voxemoset",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results on SpokenCOCO and VoxEmoset.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S4.T2\" title=\"In 4.4 Dataset quality &#8227; 4 VoxEmoset Benchmark &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the comparison of <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> and baselines<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>EmoGen was excluded because its pretrained weights are publicly unavailable, and our reimplementation was unable to match its reported performance.</span></span></span> on SpokenCOCO and VoxEmoset.\nSD1.5 with the text inputs (<em class=\"ltx_emph ltx_font_italic\">i.e</em>., without speech) is shown as a baseline.\nEspecially, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S4.F5\" title=\"In 4.4 Dataset quality &#8227; 4 VoxEmoset Benchmark &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> highlights the stark contrast between text- and speech-based generation.\nWhile speech conveys emotions even with the same wording, the text-based model inherently ignores these cues and focuses on fact-based generation.\nEven when trained on VoxEmoset, &#8216;SD (finetuning)&#8217; struggles to express emotions as semantic content, but speech leads to a richer and intense emotional expression.\nFor example, given the prompt &#8216;A black trash can is placed against a white wall,&#8217; our model detects disgust from spoken nuances and visually emphasizes the unpleasantness of trash, whereas the text-based model remains neutral.</p>\n\n",
                "matched_terms": [
                    "voxemoset",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Remarkably, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> outperforms SpeechCLIP+ and TMT on SpokenCOCO, where <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> does not use Flickr8kAudio for training.\nWhile TMT additionally used huge synthesized speech data from CC3M&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib49\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">49</span></a>]</cite> and CC12M&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>]</cite> for training, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> also show comparable results on VoxEmoset.\nThis result demonstrates that our diffusion model is a powerful learner for speech-to-expressive image alignment than contrastive learning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib52\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></a>]</cite> and auto-regressive training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>.\nThe qualitative comparison on SpokenCOCO shows that SpeechCLIP+ and TMT often ignore keywords in the prompts, while <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> can capture the details, as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F6\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "voxemoset",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results on Flickr8kAudio.</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T3\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the performance comparison on Flickr8kAudio.\nHere, while TMT and SpeechCLIP+ used Flickr8kAudio for training, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> was evaluated in a zero-shot manner.\nSurprisingly, <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> outperforms existing methods by large margins.\nIt shows that end-to-end training in <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is more robust in aligning the speech-language space.\nBy contrast, speech features in <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> are more robust to the order or length of the prompt.\nMoreover, VoxEmoset might improve the robustness on generality as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.F6\" title=\"In 5.2 Results &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "voxemoset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training datasets.</span> <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03423v1#S5.T4\" title=\"In 5.3 Discussion &#8227; 5 Experiments &#8227; Seeing What You Say: Expressive Image Generation from Speech\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that VoxEmoset is complementary with the real-world spoken dataset, SpokenCOCO, improving both visual fidelity and semantic relevance.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "training",
                    "voxemoset",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> is the first end-to-end S2I model that captures both linguistic and emotional nuances from speech.\nUnlike text-based methods, our approach totally leverages speech&#8217;s expressiveness to generate emotionally aligned images. VoxEmoset is built cheaply, but it is complementary with real-world datasets.\nOur experiments demonstrate that <span class=\"ltx_text ltx_font_typewriter\">VoxStudio</span> not only outperforms prior speech-based methods in conveying sentiment through images, but also matches text-driven approaches in semantic alignment, despite the higher noise and lower latency of the speech modality.\nWe believe our work facilitates future research in voice-driven generative models and their applications.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "voxemoset"
                ]
            }
        ]
    }
}