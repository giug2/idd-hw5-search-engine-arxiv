{
    "S4.T1": {
        "caption": "Table 1: \nDataset Example (English Translations): Water Levels and Texts",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\">Water level Ratio</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Observation Text</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">4<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m1\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">There&#8217;s barely any water out here.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">40<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m2\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center\">The river&#8217;s flowing really gently today.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">80<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m3\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center\">The water&#8217;s pretty high&#8230; hope it&#8217;s okay.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">98<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m4\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">Almost flooding&#8230; this is scary!</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "high…",
            "out",
            "example",
            "level",
            "water’s",
            "water",
            "any",
            "flowing",
            "observation",
            "hope",
            "scary",
            "ratio",
            "text",
            "it’s",
            "today",
            "english",
            "barely",
            "river’s",
            "there’s",
            "okay",
            "really",
            "pretty",
            "here",
            "texts",
            "almost",
            "flooding…",
            "gently",
            "translations",
            "dataset",
            "levels"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We conducted a questionnaire survey on Yahoo! Crowdsourcing, an online platform in Japan, asking participants to imagine posting on SNS when observing a river at a given water level ratio.\nExamples of the resulting pairs of water level ratios and observation texts\nare shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11285v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Dataset for Numerical Experiments &#8227; 4 Numerical Experiment &#8227; Language-Aided State Estimation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Natural language data, such as text and speech, have become readily available through social networking services and chat platforms.\nBy leveraging human observations expressed in natural language, this paper addresses the problem of state estimation for physical systems, in which humans act as sensing agents.\nTo this end, we propose a Language-Aided Particle Filter (LAPF), a particle filter framework that structures human observations via natural language processing and incorporates them into the update step of the state estimation.\nFinally,\nthe LAPF is applied to the water level estimation problem in an irrigation canal and its effectiveness is demonstrated.\n</p>\n\n",
                "matched_terms": [
                    "level",
                    "text",
                    "water"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notation:\nThe symbol <math alttext=\"\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p8.m1\" intent=\":literal\"><semantics><mi>&#8477;</mi><annotation encoding=\"application/x-tex\">\\mathbb{R}</annotation></semantics></math> denotes the set of real numbers, the symbol <math alttext=\"\\mathbb{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p8.m2\" intent=\":literal\"><semantics><mi>&#120139;</mi><annotation encoding=\"application/x-tex\">\\mathbb{T}</annotation></semantics></math> denotes the set of natural language texts, and the symbol <math alttext=\"\\phi\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p8.m3\" intent=\":literal\"><semantics><mi>&#981;</mi><annotation encoding=\"application/x-tex\">\\phi</annotation></semantics></math> denotes the empty set.\nElements of the set <math alttext=\"\\mathbb{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p8.m4\" intent=\":literal\"><semantics><mi>&#120139;</mi><annotation encoding=\"application/x-tex\">\\mathbb{T}</annotation></semantics></math> include, for example,\n&#8220;IFAC2026 will be held in Busan.&#8221;\nThe symbol <math alttext=\"I_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p8.m5\" intent=\":literal\"><semantics><msub><mi>I</mi><mi>n</mi></msub><annotation encoding=\"application/x-tex\">I_{n}</annotation></semantics></math> denote the <math alttext=\"n\\times n\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p8.m6\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n\\times n</annotation></semantics></math> identity matrix, and the symbol <math alttext=\"0_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p8.m7\" intent=\":literal\"><semantics><msub><mn>0</mn><mi>n</mi></msub><annotation encoding=\"application/x-tex\">0_{n}</annotation></semantics></math> denotes the <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p8.m8\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-dimensional zero vector.\nThe symbol <math alttext=\"\\mathcal{N}(\\mu,\\Sigma)\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p8.m9\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#956;</mi><mo>,</mo><mi mathvariant=\"normal\">&#931;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(\\mu,\\Sigma)</annotation></semantics></math> denotes the normal distribution with mean <math alttext=\"\\mu\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p8.m10\" intent=\":literal\"><semantics><mi>&#956;</mi><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math> and covariance matrix <math alttext=\"\\Sigma\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p8.m11\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#931;</mi><annotation encoding=\"application/x-tex\">\\Sigma</annotation></semantics></math>.\nThe expression <math alttext=\"x\\sim p(x)\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p8.m12\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>&#8764;</mo><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">x\\sim p(x)</annotation></semantics></math> denotes sampling <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p8.m13\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> from the probability distribution <math alttext=\"p(x)\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p8.m14\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p(x)</annotation></semantics></math>.\nThe symbol <math alttext=\"x_{1:T}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p8.m15\" intent=\":literal\"><semantics><msub><mi>x</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">x_{1:T}</annotation></semantics></math> denotes the time sequence <math alttext=\"\\{x_{1},x_{2},\\dots,x_{T}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p8.m16\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{x_{1},x_{2},\\dots,x_{T}\\}</annotation></semantics></math>.\nFinally, the symbol <math alttext=\"\\delta(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p8.m17\" intent=\":literal\"><semantics><mrow><mi>&#948;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\delta(\\cdot)</annotation></semantics></math> denotes the Dirac delta function.</p>\n\n",
                "matched_terms": [
                    "texts",
                    "example"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To model the human sensor,\nwe begin by focusing on the observation process through which a human sensing agent perceives a part of the state <math alttext=\"x_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">x_{k}</annotation></semantics></math> and reports it as a natural language observation <math alttext=\"s_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">s_{k}</annotation></semantics></math>.\nFor simplicity, in this subsection we restrict our attention to the case of a single\nagent\nwho produces a single text <math alttext=\"s_{k}\\in\\mathbb{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>k</mi></msub><mo>&#8712;</mo><mi>&#120139;</mi></mrow><annotation encoding=\"application/x-tex\">s_{k}\\in\\mathbb{T}</annotation></semantics></math>.\nAs illustrated in the upper block of Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11285v1#S2.F1\" title=\"Figure 1 &#8227; 2.2 human sensor &#119982;_H &#8227; 2 Problem Setting &#8227; Language-Aided State Estimation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>,\nwe model the observation process of the human sensor as a cascade of two modules: the\ncognitive module <math alttext=\"\\mathcal{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119966;</mi><annotation encoding=\"application/x-tex\">\\mathcal{C}</annotation></semantics></math> and the\nexpression module <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "observation",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <math alttext=\"\\mathcal{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119966;</mi><annotation encoding=\"application/x-tex\">\\mathcal{C}</annotation></semantics></math>, the agent perceives the state <math alttext=\"x_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">x_{k}</annotation></semantics></math> and forms an internal cognitive value <math alttext=\"y_{\\mathrm{H},k}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>y</mi><mrow><mi mathvariant=\"normal\">H</mi><mo>,</mo><mi>k</mi></mrow></msub><mo>&#8712;</mo><mi>&#8477;</mi></mrow><annotation encoding=\"application/x-tex\">y_{\\mathrm{H},k}\\in\\mathbb{R}</annotation></semantics></math>.\nThen, in <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math>, this cognitive value is expressed as the observation text <math alttext=\"s_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m5\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">s_{k}</annotation></semantics></math>.\nIn the following, we describe the models of\n<math alttext=\"\\mathcal{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119966;</mi><annotation encoding=\"application/x-tex\">\\mathcal{C}</annotation></semantics></math> and <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m7\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "observation",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in\nthe lower block of Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11285v1#S2.F1\" title=\"Figure 1 &#8227; 2.2 human sensor &#119982;_H &#8227; 2 Problem Setting &#8227; Language-Aided State Estimation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>,\nwe further model the expression module <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math> as being composed of a quantizer and a verbalizer.\nIn other words, the model\n<math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math> assumes that a quantization process mediates the verbalization of cognitive information by human agents.\nThis reflects the limitation of human linguistic ability, which make it impossible for an agent to verbalize the cognitive value <math alttext=\"y_{\\mathrm{H},k}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>y</mi><mrow><mi mathvariant=\"normal\">H</mi><mo>,</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">y_{\\mathrm{H},k}</annotation></semantics></math> at high resolution.\nFor example, as shown in\nFig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11285v1#S2.F1\" title=\"Figure 1 &#8227; 2.2 human sensor &#119982;_H &#8227; 2 Problem Setting &#8227; Language-Aided State Estimation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, it is reasonable to assume that human agents cannot distinguish between the cognitive values <math alttext=\"0.27~\\mathrm{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m4\" intent=\":literal\"><semantics><mrow><mn>0.27</mn><mo lspace=\"0.330em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">L</mi></mrow><annotation encoding=\"application/x-tex\">0.27~\\mathrm{L}</annotation></semantics></math> and <math alttext=\"0.28~\\mathrm{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m5\" intent=\":literal\"><semantics><mrow><mn>0.28</mn><mo lspace=\"0.330em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">L</mi></mrow><annotation encoding=\"application/x-tex\">0.28~\\mathrm{L}</annotation></semantics></math> when generating observation texts.\nWe further assume that the quantizer has <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m6\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> quantization levels and outputs a corresponding quantization label <math alttext=\"q_{k}\\in\\{1,\\dots,m\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m7\" intent=\":literal\"><semantics><mrow><msub><mi>q</mi><mi>k</mi></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>m</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">q_{k}\\in\\{1,\\dots,m\\}</annotation></semantics></math>.\nIn summary, the expression module <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m8\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math>\nis described as follows:</p>\n\n",
                "matched_terms": [
                    "observation",
                    "texts",
                    "example",
                    "levels"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where the function <math alttext=\"Q_{m}:\\mathbb{R}\\to\\{1,\\dots,m\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m9\" intent=\":literal\"><semantics><mrow><msub><mi>Q</mi><mi>m</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi>&#8477;</mi><mo stretchy=\"false\">&#8594;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>m</mi><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">Q_{m}:\\mathbb{R}\\to\\{1,\\dots,m\\}</annotation></semantics></math> denotes the quantizer that maps the cognitive value <math alttext=\"y_{\\mathrm{H},k}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m10\" intent=\":literal\"><semantics><msub><mi>y</mi><mrow><mi mathvariant=\"normal\">H</mi><mo>,</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">y_{\\mathrm{H},k}</annotation></semantics></math> to a quantized label <math alttext=\"q_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m11\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">q_{k}</annotation></semantics></math> with <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m12\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> discrete levels.\nThe function <math alttext=\"\\mathrm{Label2Prob}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m13\" intent=\":literal\"><semantics><mi>Label2Prob</mi><annotation encoding=\"application/x-tex\">\\mathrm{Label2Prob}</annotation></semantics></math> generates a probability distribution over observation texts <math alttext=\"s_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m14\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">s_{k}</annotation></semantics></math> corresponding to the quantized label <math alttext=\"q_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m15\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">q_{k}</annotation></semantics></math>.\nNote that <math alttext=\"\\mathrm{Label2Prob}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m16\" intent=\":literal\"><semantics><mi>Label2Prob</mi><annotation encoding=\"application/x-tex\">\\mathrm{Label2Prob}</annotation></semantics></math> is introduced as an abstract function to formally describe the process of human\nobservation.\nWhile we assume the existence of <math alttext=\"\\mathrm{Label2Prob}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m17\" intent=\":literal\"><semantics><mi>Label2Prob</mi><annotation encoding=\"application/x-tex\">\\mathrm{Label2Prob}</annotation></semantics></math>, its explicit functional form is not used in the subsequent discussion.\nEquation&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11285v1#S2.E3.2\" title=\"In 3 &#8227; 2.2 human sensor &#119982;_H &#8227; 2 Problem Setting &#8227; Language-Aided State Estimation\"><span class=\"ltx_text ltx_ref_tag\">3b</span></a>) describes the generation of the observation text <math alttext=\"s_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m18\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">s_{k}</annotation></semantics></math> as a sample drawn from the distribution given by <math alttext=\"\\mathrm{Label2Prob}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m19\" intent=\":literal\"><semantics><mi>Label2Prob</mi><annotation encoding=\"application/x-tex\">\\mathrm{Label2Prob}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "observation",
                    "levels",
                    "text",
                    "texts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">Consider the plant system <math alttext=\"\\mathcal{P}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmthm1.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><annotation encoding=\"application/x-tex\">\\mathcal{P}</annotation></semantics></math>, given in (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11285v1#S2.E1\" title=\"In 2.1 Plant System &#119979; &#8227; 2 Problem Setting &#8227; Language-Aided State Estimation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>), and the human sensor <math alttext=\"S_{\\mathrm{H}}=\\{\\mathcal{C},\\mathcal{E}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmthm1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>S</mi><mi mathvariant=\"normal\">H</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mi class=\"ltx_font_mathcaligraphic\">&#119966;</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">S_{\\mathrm{H}}=\\{\\mathcal{C},\\mathcal{E}\\}</annotation></semantics></math>, given in (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11285v1#S2.E2\" title=\"In 2.2 human sensor &#119982;_H &#8227; 2 Problem Setting &#8227; Language-Aided State Estimation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11285v1#S2.E3\" title=\"In 2.2 human sensor &#119982;_H &#8227; 2 Problem Setting &#8227; Language-Aided State Estimation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11285v1#S2.E4\" title=\"In 2.2 human sensor &#119982;_H &#8227; 2 Problem Setting &#8227; Language-Aided State Estimation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), and (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11285v1#S2.E5\" title=\"In 2.2 human sensor &#119982;_H &#8227; 2 Problem Setting &#8227; Language-Aided State Estimation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>).\nGiven the distribution of the initial state <math alttext=\"\\pi(x_{0})\" class=\"ltx_Math\" display=\"inline\" id=\"Thmthm1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#960;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\pi(x_{0})</annotation></semantics></math> and the sequence of observation texts <math alttext=\"s_{1:k}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmthm1.p1.m4\" intent=\":literal\"><semantics><msub><mi>s</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">s_{1:k}</annotation></semantics></math>,\nestimate the distribution of the state <math alttext=\"\\pi(x_{k})\" class=\"ltx_Math\" display=\"inline\" id=\"Thmthm1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#960;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\pi(x_{k})</annotation></semantics></math>.</span>\n</p>\n\n",
                "matched_terms": [
                    "observation",
                    "texts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where\n<math alttext=\"w_{k}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>w</mi><mi>k</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">w_{k}^{i}</annotation></semantics></math> denote i.i.d. samples drawn from the process noise distribution\n<math alttext=\"\\mathcal{W}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119986;</mi><annotation encoding=\"application/x-tex\">\\mathcal{W}</annotation></semantics></math>.\nAt the second step,\ngiven an observation of the state, such as observation text <math alttext=\"s_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m6\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">s_{k}</annotation></semantics></math> in the problem addressed in this paper,\neach weight <math alttext=\"\\alpha_{k}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m7\" intent=\":literal\"><semantics><msubsup><mi>&#945;</mi><mi>k</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">\\alpha_{k}^{i}</annotation></semantics></math> of the predicted particles <math alttext=\"\\{x_{k|k-1}^{i}\\}_{i=1}^{N_{\\mathrm{p}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m8\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>x</mi><mrow><mi>k</mi><mo fence=\"false\">|</mo><mrow><mi>k</mi><mo>&#8722;</mo><mn>1</mn></mrow></mrow><mi>i</mi></msubsup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>N</mi><mi mathvariant=\"normal\">p</mi></msub></msubsup><annotation encoding=\"application/x-tex\">\\{x_{k|k-1}^{i}\\}_{i=1}^{N_{\\mathrm{p}}}</annotation></semantics></math> is\nupdated\naccording to the likelihood <math alttext=\"p(s_{k}\\mid x_{k\\mid k-1}^{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m9\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>s</mi><mi>k</mi></msub><mo>&#8739;</mo><msubsup><mi>x</mi><mrow><mi>k</mi><mo>&#8739;</mo><mrow><mi>k</mi><mo>&#8722;</mo><mn>1</mn></mrow></mrow><mi>i</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p(s_{k}\\mid x_{k\\mid k-1}^{i})</annotation></semantics></math> as</p>\n\n",
                "matched_terms": [
                    "observation",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First,\na text encoder processes the observation text <math alttext=\"s_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m1\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">s_{k}</annotation></semantics></math> to produce an embedding <math alttext=\"e_{k}\\in\\mathbb{R}^{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m2\" intent=\":literal\"><semantics><mrow><msub><mi>e</mi><mi>k</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>d</mi></msup></mrow><annotation encoding=\"application/x-tex\">e_{k}\\in\\mathbb{R}^{d}</annotation></semantics></math>.\nA text encoder is a machine learning model that maps an input text into a vector representation capturing its semantics and context.\nExamples include text embedding models or pretrained language models, and the dimensionality <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m3\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> of the embedding <math alttext=\"e_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m4\" intent=\":literal\"><semantics><msub><mi>e</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">e_{k}</annotation></semantics></math> is typically high, ranging from several hundred to several thousand.\nSecond,\na neural network maps the embedding <math alttext=\"e_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m5\" intent=\":literal\"><semantics><msub><mi>e</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">e_{k}</annotation></semantics></math> to an <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m6\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>-dimensional feature vector <math alttext=\"\\psi_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m7\" intent=\":literal\"><semantics><msub><mi>&#968;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">\\psi_{k}</annotation></semantics></math>.\nFinally,\nthe softmax function applied to <math alttext=\"\\psi_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m8\" intent=\":literal\"><semantics><msub><mi>&#968;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">\\psi_{k}</annotation></semantics></math> yields the probability distribution <math alttext=\"p(q_{k}|s_{k})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m9\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>q</mi><mi>k</mi></msub><mo fence=\"false\">|</mo><msub><mi>s</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p(q_{k}|s_{k})</annotation></semantics></math>.\nThis model can be trained using supervised learning with a dataset of text-label pairs <math alttext=\"\\{s^{i},q^{i}\\}_{i=1}^{N_{\\mathrm{d}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m10\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msup><mi>s</mi><mi>i</mi></msup><mo>,</mo><msup><mi>q</mi><mi>i</mi></msup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>N</mi><mi mathvariant=\"normal\">d</mi></msub></msubsup><annotation encoding=\"application/x-tex\">\\{s^{i},q^{i}\\}_{i=1}^{N_{\\mathrm{d}}}</annotation></semantics></math>.\nSuch a dataset can be constructed by conducting questionnaires, as\ndetailed in Subsection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11285v1#S4.SS2\" title=\"4.2 Dataset for Numerical Experiments &#8227; 4 Numerical Experiment &#8227; Language-Aided State Estimation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "observation",
                    "text",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">Computing the probability distribution <math alttext=\"p(q_{k}|s_{k})\" class=\"ltx_Math\" display=\"inline\" id=\"Thmthm4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>q</mi><mi>k</mi></msub><mo fence=\"false\">|</mo><msub><mi>s</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p(q_{k}|s_{k})</annotation></semantics></math>, rather than directly predicting a single label <math alttext=\"q_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmthm4.p1.m2\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">q_{k}</annotation></semantics></math>, has a benefit in handling out-of-domain observation texts <math alttext=\"s_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmthm4.p1.m3\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">s_{k}</annotation></semantics></math>.\nFor example, when an observation text <math alttext=\"s_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmthm4.p1.m4\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">s_{k}</annotation></semantics></math> contains features not present in the training data, the model outputs a low-confidence distribution instead of a forced high-confidence label, thereby preventing overconfident and incorrect likelihood updates.\nThis idea is inspired by the approach of <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sitdhipol2025spatial</span></cite>, which focuses on spatial-language expressions.\nIn contrast, we provide a method applicable to general natural language observations.</span>\n</p>\n\n",
                "matched_terms": [
                    "observation",
                    "text",
                    "example",
                    "texts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"p(s_{k}^{(1)}|s_{k}^{(1)},s_{k}^{(0)},x_{k|k-1}^{i})=p(s_{k}^{(1)}|x_{k|k-1}^{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>s</mi><mi>k</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo fence=\"false\">|</mo><mrow><msubsup><mi>s</mi><mi>k</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>s</mi><mi>k</mi><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>x</mi><mrow><mi>k</mi><mo fence=\"false\">|</mo><mrow><mi>k</mi><mo>&#8722;</mo><mn>1</mn></mrow></mrow><mi>i</mi></msubsup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>s</mi><mi>k</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo fence=\"false\">|</mo><msubsup><mi>x</mi><mrow><mi>k</mi><mo fence=\"false\">|</mo><mrow><mi>k</mi><mo>&#8722;</mo><mn>1</mn></mrow></mrow><mi>i</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">p(s_{k}^{(1)}|s_{k}^{(1)},s_{k}^{(0)},x_{k|k-1}^{i})=p(s_{k}^{(1)}|x_{k|k-1}^{i})</annotation></semantics></math>.\nAssuming that the observation texts <math alttext=\"s_{k}^{(j)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><msubsup><mi>s</mi><mi>k</mi><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">s_{k}^{(j)}</annotation></semantics></math> are independent of each other, (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11285v1#S3.E17\" title=\"In 3.2 The case of multiple human sensors &#8227; 3 Language-Aided Particle Filter &#8227; Language-Aided State Estimation\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>) simplifies to</p>\n\n",
                "matched_terms": [
                    "observation",
                    "texts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note that each <math alttext=\"p(s_{k}^{(j)}|x_{k|k-1}^{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>s</mi><mi>k</mi><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo fence=\"false\">|</mo><msubsup><mi>x</mi><mrow><mi>k</mi><mo fence=\"false\">|</mo><mrow><mi>k</mi><mo>&#8722;</mo><mn>1</mn></mrow></mrow><mi>i</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p(s_{k}^{(j)}|x_{k|k-1}^{i})</annotation></semantics></math> corresponds to the single-sensor likelihood <math alttext=\"p(s_{k}|x_{k|k-1}^{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>s</mi><mi>k</mi></msub><mo fence=\"false\">|</mo><msubsup><mi>x</mi><mrow><mi>k</mi><mo fence=\"false\">|</mo><mrow><mi>k</mi><mo>&#8722;</mo><mn>1</mn></mrow></mrow><mi>i</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p(s_{k}|x_{k|k-1}^{i})</annotation></semantics></math>\naddressed in Subsection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11285v1#S3.SS1\" title=\"3.1 The case of a single human sensor &#8227; 3 Language-Aided Particle Filter &#8227; Language-Aided State Estimation\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> and\nis given by (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11285v1#S3.E10\" title=\"In Proposition 3 &#8227; 3.1 The case of a single human sensor &#8227; 3 Language-Aided Particle Filter &#8227; Language-Aided State Estimation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>).\nTherefore, in the case of multiple human sensors <math alttext=\"\\mathcal{S}_{\\mathrm{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi mathvariant=\"normal\">H</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{S}_{\\mathrm{H}}</annotation></semantics></math>, the overall likelihood is obtained by computing the likelihood for each observation text <math alttext=\"s_{k}^{(j)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>s</mi><mi>k</mi><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">s_{k}^{(j)}</annotation></semantics></math> and taking the product of these terms.</p>\n\n",
                "matched_terms": [
                    "observation",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we demonstrate the effectiveness of the proposed LAPF through numerical experiments.\nTo this end, we address a water level state estimation problem in an irrigation canal,\nwhich is illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11285v1#S4.F3\" title=\"Figure 3 &#8227; 4 Numerical Experiment &#8227; Language-Aided State Estimation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "level",
                    "water"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In such canals, accurate prediction and control of the water level are crucial to ensure appropriate water allocation to agricultural fields.\nHowever, in many practical situations, physical sensors are only sparsely deployed, as pointed out by <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sadowska2015human</span></cite>.\nTherefore, rather than relying\nsolely on physical sensors,\nthe state estimation that utilizes additional sources of information is needed.\nIn this paper, we address the problem of estimating the canal water level by combining a physical model of the canal with human observations from farmers,\nand present the LAPF approach.\nTo benchmark the LAPF, we additionally perform experiments with an External DNN-Aided Particle Filter (EDAPF).\nThe EDAPF was devised as a reference method, inspired by the concept of external deep-neural-network(DNN) architectures introduced in <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shlezinger2024ai</span></cite>.</p>\n\n",
                "matched_terms": [
                    "level",
                    "out",
                    "water"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where the state vector <math alttext=\"x_{k}=[x_{k}^{(1)}\\;\\cdots\\;x_{k}^{(5)}]^{\\top}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>x</mi><mi>k</mi></msub><mo>=</mo><msup><mrow><mo stretchy=\"false\">[</mo><mrow><msubsup><mi>x</mi><mi>k</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0.280em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>x</mi><mi>k</mi><mrow><mo stretchy=\"false\">(</mo><mn>5</mn><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo stretchy=\"false\">]</mo></mrow><mo>&#8868;</mo></msup></mrow><annotation encoding=\"application/x-tex\">x_{k}=[x_{k}^{(1)}\\;\\cdots\\;x_{k}^{(5)}]^{\\top}</annotation></semantics></math> represents the water levels at five locations, and the function <math alttext=\"\\mathrm{proj}_{[a,b]}:\\mathbb{R}^{n}\\to\\mathbb{R}^{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>proj</mi><mrow><mo stretchy=\"false\">[</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy=\"false\">]</mo></mrow></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi>&#8477;</mi><mi>n</mi></msup><mo stretchy=\"false\">&#8594;</mo><msup><mi>&#8477;</mi><mi>n</mi></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{proj}_{[a,b]}:\\mathbb{R}^{n}\\to\\mathbb{R}^{n}</annotation></semantics></math> denotes the component-wise projection of a vector onto the interval <math alttext=\"[a,b]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[a,b]</annotation></semantics></math>.\nThus,\nthe model&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11285v1#S4.E19\" title=\"In 4.1 Experiment Setting &#8227; 4 Numerical Experiment &#8227; Language-Aided State Estimation\"><span class=\"ltx_text ltx_ref_tag\">19</span></a>) describes the temporal evolution of river water levels, where the maximum water level at each location is bounded by&#160;<math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m5\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math>.\nThe system matrix <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m6\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> is given by</p>\n\n",
                "matched_terms": [
                    "levels",
                    "level",
                    "water"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The process noise <math alttext=\"w_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m7\" intent=\":literal\"><semantics><msub><mi>w</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">w_{k}</annotation></semantics></math> is modeled as a Gaussian distribution <math alttext=\"\\mathcal{N}(u_{k},Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m8\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>u</mi><mi>k</mi></msub><mo>,</mo><mi>Q</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(u_{k},Q)</annotation></semantics></math> with mean <math alttext=\"u_{k}=[1\\;0\\;0\\;0\\;0]^{\\top}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m9\" intent=\":literal\"><semantics><mrow><msub><mi>u</mi><mi>k</mi></msub><mo>=</mo><msup><mrow><mo stretchy=\"false\">[</mo><mn>1&#8196;0&#8196;0&#8196;0&#8196;0</mn><mo stretchy=\"false\">]</mo></mrow><mo>&#8868;</mo></msup></mrow><annotation encoding=\"application/x-tex\">u_{k}=[1\\;0\\;0\\;0\\;0]^{\\top}</annotation></semantics></math> and covariance matrix <math alttext=\"Q=\\mathrm{diag}(1.0,0.1,0.1,0.1,0.1)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m10\" intent=\":literal\"><semantics><mrow><mi>Q</mi><mo>=</mo><mrow><mi>diag</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>1.0</mn><mo>,</mo><mn>0.1</mn><mo>,</mo><mn>0.1</mn><mo>,</mo><mn>0.1</mn><mo>,</mo><mn>0.1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">Q=\\mathrm{diag}(1.0,0.1,0.1,0.1,0.1)</annotation></semantics></math>.\nThe initial water level at each location <math alttext=\"x_{0}^{(i)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m11\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mn>0</mn><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">x_{0}^{(i)}</annotation></semantics></math> is set to <math alttext=\"2.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m12\" intent=\":literal\"><semantics><mn>2.5</mn><annotation encoding=\"application/x-tex\">2.5</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "level",
                    "water"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where the projection operator <math alttext=\"\\mathrm{proj}_{[0,5]}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><msub><mi>proj</mi><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>5</mn><mo stretchy=\"false\">]</mo></mrow></msub><annotation encoding=\"application/x-tex\">\\mathrm{proj}_{[0,5]}</annotation></semantics></math> in (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11285v1#S4.E21.1\" title=\"In 21 &#8227; 4.1 Experiment Setting &#8227; 4 Numerical Experiment &#8227; Language-Aided State Estimation\"><span class=\"ltx_text ltx_ref_tag\">21a</span></a>) ensures that the perceived water level remains within the range <math alttext=\"[0,5]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>5</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0,5]</annotation></semantics></math>, reflecting the fact that\nan agent\ncannot perceive values outside this range.\nThe cognitive matrix\n<math alttext=\"C_{\\mathrm{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><msub><mi>C</mi><mi mathvariant=\"normal\">H</mi></msub><annotation encoding=\"application/x-tex\">C_{\\mathrm{H}}</annotation></semantics></math>\nis set to\n<math alttext=\"[1\\;0\\;0\\;0\\;0]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>1&#8196;0&#8196;0&#8196;0&#8196;0</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[1\\;0\\;0\\;0\\;0]</annotation></semantics></math>, meaning that the\nagent\nperceives only the water level at the first location <math alttext=\"x_{k}^{(1)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m5\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>k</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">x_{k}^{(1)}</annotation></semantics></math>.\nThe cognitive noise <math alttext=\"v_{\\mathrm{H},k}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m6\" intent=\":literal\"><semantics><msub><mi>v</mi><mrow><mi mathvariant=\"normal\">H</mi><mo>,</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">v_{\\mathrm{H},k}</annotation></semantics></math> is modeled as Gaussian with distribution <math alttext=\"\\mathcal{N}(0,1.0)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m7\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1.0</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(0,1.0)</annotation></semantics></math>.\nIn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11285v1#S4.E21.2\" title=\"In 21 &#8227; 4.1 Experiment Setting &#8227; 4 Numerical Experiment &#8227; Language-Aided State Estimation\"><span class=\"ltx_text ltx_ref_tag\">21b</span></a>), the function <math alttext=\"\\mathrm{Y2Prob}:\\mathbb{R}\\to\\mathbb{R}^{N_{\\mathrm{d}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m8\" intent=\":literal\"><semantics><mrow><mi>Y2Prob</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi>&#8477;</mi><mo stretchy=\"false\">&#8594;</mo><msup><mi>&#8477;</mi><msub><mi>N</mi><mi mathvariant=\"normal\">d</mi></msub></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{Y2Prob}:\\mathbb{R}\\to\\mathbb{R}^{N_{\\mathrm{d}}}</annotation></semantics></math> generates a probability distribution over possible observation texts <math alttext=\"s_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m9\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">s_{k}</annotation></semantics></math> corresponding to the cognitive value <math alttext=\"y_{\\mathrm{H},k}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m10\" intent=\":literal\"><semantics><msub><mi>y</mi><mrow><mi mathvariant=\"normal\">H</mi><mo>,</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">y_{\\mathrm{H},k}</annotation></semantics></math>.\nIn practice, we implement this function with a dataset of <math alttext=\"N_{\\mathrm{d}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m11\" intent=\":literal\"><semantics><msub><mi>N</mi><mi mathvariant=\"normal\">d</mi></msub><annotation encoding=\"application/x-tex\">N_{\\mathrm{d}}</annotation></semantics></math> pairs <math alttext=\"\\{y_{\\mathrm{H}}^{i},s^{i}\\}_{i=1}^{N_{\\mathrm{d}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m12\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>y</mi><mi mathvariant=\"normal\">H</mi><mi>i</mi></msubsup><mo>,</mo><msup><mi>s</mi><mi>i</mi></msup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>N</mi><mi mathvariant=\"normal\">d</mi></msub></msubsup><annotation encoding=\"application/x-tex\">\\{y_{\\mathrm{H}}^{i},s^{i}\\}_{i=1}^{N_{\\mathrm{d}}}</annotation></semantics></math> : given\na cognitive value <math alttext=\"y_{\\mathrm{H},k}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m13\" intent=\":literal\"><semantics><msub><mi>y</mi><mrow><mi mathvariant=\"normal\">H</mi><mo>,</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">y_{\\mathrm{H},k}</annotation></semantics></math>, the corresponding observation text <math alttext=\"s_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m14\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">s_{k}</annotation></semantics></math> is obtained by randomly sampling one of the texts <math alttext=\"s^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m15\" intent=\":literal\"><semantics><msup><mi>s</mi><mi>i</mi></msup><annotation encoding=\"application/x-tex\">s^{i}</annotation></semantics></math> linked to\nthe value of <math alttext=\"y_{\\mathrm{H},k}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m16\" intent=\":literal\"><semantics><msub><mi>y</mi><mrow><mi mathvariant=\"normal\">H</mi><mo>,</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">y_{\\mathrm{H},k}</annotation></semantics></math>\nThe details of this dataset are provided in Subsection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11285v1#S4.SS2\" title=\"4.2 Dataset for Numerical Experiments &#8227; 4 Numerical Experiment &#8227; Language-Aided State Estimation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "texts",
                    "observation",
                    "level",
                    "dataset",
                    "text",
                    "water"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We collected observation texts from <math alttext=\"50\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mn>50</mn><annotation encoding=\"application/x-tex\">50</annotation></semantics></math> participants for each water level ratio between <math alttext=\"0\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>0</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0\\%</annotation></semantics></math> and <math alttext=\"100\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mn>100</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">100\\%</annotation></semantics></math> in increments of <math alttext=\"2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2\\%</annotation></semantics></math>, resulting in a dataset of <math alttext=\"2,454\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>,</mo><mn>454</mn></mrow><annotation encoding=\"application/x-tex\">2,454</annotation></semantics></math> pairs.\nAmong these, <math alttext=\"1,882\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m6\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>,</mo><mn>882</mn></mrow><annotation encoding=\"application/x-tex\">1,882</annotation></semantics></math> pairs were used for training, <math alttext=\"205\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m7\" intent=\":literal\"><semantics><mn>205</mn><annotation encoding=\"application/x-tex\">205</annotation></semantics></math> for validation, and <math alttext=\"289\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m8\" intent=\":literal\"><semantics><mn>289</mn><annotation encoding=\"application/x-tex\">289</annotation></semantics></math> for testing.\nThe training and validation sets were used to train the quantized-label classification model in LAPF and an external DNN in EDAPF.\nThe test set was used to implement the language generation process of the true system described in (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11285v1#S4.E21.2\" title=\"In 21 &#8227; 4.1 Experiment Setting &#8227; 4 Numerical Experiment &#8227; Language-Aided State Estimation\"><span class=\"ltx_text ltx_ref_tag\">21b</span></a>).</p>\n\n",
                "matched_terms": [
                    "texts",
                    "observation",
                    "level",
                    "dataset",
                    "ratio",
                    "water"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this subsection, we describe the training of the quantized-label classification model used in LAPF.\nIn the experiments, we constructed and trained a quantized-label classification model that predicts, from the observation text <math alttext=\"s_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">s_{k}</annotation></semantics></math>, which of five quantization intervals the cognitive value <math alttext=\"y_{\\mathrm{H},k}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>y</mi><mrow><mi mathvariant=\"normal\">H</mi><mo>,</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">y_{\\mathrm{H},k}</annotation></semantics></math> belongs to.\nSpecifically, the range <math alttext=\"\\Lambda=[0,5]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#923;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>5</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\Lambda=[0,5]</annotation></semantics></math> was divided into five equal partitions <math alttext=\"\\{\\Lambda_{i}\\}_{i=1}^{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m4\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi mathvariant=\"normal\">&#923;</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mn>5</mn></msubsup><annotation encoding=\"application/x-tex\">\\{\\Lambda_{i}\\}_{i=1}^{5}</annotation></semantics></math>, and the model assigns <math alttext=\"y_{\\mathrm{H},k}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m5\" intent=\":literal\"><semantics><msub><mi>y</mi><mrow><mi mathvariant=\"normal\">H</mi><mo>,</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">y_{\\mathrm{H},k}</annotation></semantics></math> to the corresponding <math alttext=\"\\Lambda_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m6\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#923;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\Lambda_{i}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "observation",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We performed an additional experiment to evaluate the robustness of the LAPF under &#8220;out-of-domain&#8221; observation texts.\nTo this end, we injected dialectal expressions,\nwhich is not included in the training data, as out-of-domain observations only when <math alttext=\"y_{\\mathrm{H},k}&lt;0.2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p4.m1\" intent=\":literal\"><semantics><mrow><msub><mi>y</mi><mrow><mi mathvariant=\"normal\">H</mi><mo>,</mo><mi>k</mi></mrow></msub><mo>&lt;</mo><mn>0.2</mn></mrow><annotation encoding=\"application/x-tex\">y_{\\mathrm{H},k}&lt;0.2</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "observation",
                    "texts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11285v1#S4.F5\" title=\"Figure 5 &#8227; 4.4 Result &#8227; 4 Numerical Experiment &#8227; Language-Aided State Estimation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the LAPF retained lower MSE than the EDAPF even under out-of-domain observations, indicating higher robustness to domain shifts in the observation texts.\nOver all trials, the average MSE was <math alttext=\"0.53\\pm 0.08\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p5.m1\" intent=\":literal\"><semantics><mrow><mn>0.53</mn><mo>&#177;</mo><mn>0.08</mn></mrow><annotation encoding=\"application/x-tex\">0.53\\pm 0.08</annotation></semantics></math> for the LAPF and <math alttext=\"0.75\\pm 0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p5.m2\" intent=\":literal\"><semantics><mrow><mn>0.75</mn><mo>&#177;</mo><mn>0.15</mn></mrow><annotation encoding=\"application/x-tex\">0.75\\pm 0.15</annotation></semantics></math> for the EDAPF.</p>\n\n",
                "matched_terms": [
                    "observation",
                    "texts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This robustness can be attributed to the different ways in which the two methods incorporate observation texts <math alttext=\"s_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p6.m1\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">s_{k}</annotation></semantics></math>.\nWhile the LAPF uses a probability distribution <math alttext=\"p(q_{k}|s_{k})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p6.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>q</mi><mi>k</mi></msub><mo fence=\"false\">|</mo><msub><mi>s</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p(q_{k}|s_{k})</annotation></semantics></math> inferred from the observation text, the EDAPF uses only a single predicted value of <math alttext=\"y_{\\mathrm{H},k}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p6.m3\" intent=\":literal\"><semantics><msub><mi>y</mi><mrow><mi mathvariant=\"normal\">H</mi><mo>,</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">y_{\\mathrm{H},k}</annotation></semantics></math>.\nFor example,\nconsider that\nan observation text with dialectal expression is injected.\nThen,\nthe LAPF produced a distribution such as <math alttext=\"\\begin{bmatrix}0.34&amp;0.19&amp;0.11&amp;0.15&amp;0.21\\end{bmatrix}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p6.m4\" intent=\":literal\"><semantics><mrow><mo>[</mo><mtable columnspacing=\"5pt\"><mtr><mtd><mn>0.34</mn></mtd><mtd><mn>0.19</mn></mtd><mtd><mn>0.11</mn></mtd><mtd><mn>0.15</mn></mtd><mtd><mn>0.21</mn></mtd></mtr></mtable><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">\\begin{bmatrix}0.34&amp;0.19&amp;0.11&amp;0.15&amp;0.21\\end{bmatrix}</annotation></semantics></math>,\nwhile the EDAPF yielded a single predicted value of <math alttext=\"2.45\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p6.m5\" intent=\":literal\"><semantics><mn>2.45</mn><annotation encoding=\"application/x-tex\">2.45</annotation></semantics></math>.\nThis suggests that representing observations as a distribution rather than a single point estimate contributes to the higher robustness of the LAPF under domain shifts.</p>\n\n",
                "matched_terms": [
                    "observation",
                    "text",
                    "example",
                    "texts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper\npresented the LAPF, a particle filter framework that enables the use of general\nnatural language observations by human agents in state estimation.\nWe conducted a comparative experiment between the LAPF and the EDAPF, and the results demonstrated that the LAPF achieved higher estimation accuracy.\nFurthermore, in the additional experiment under out-of-domain observation texts, the LAPF exhibited smaller degradation in performance than the EDAPF, indicating its higher robustness to domain shifts in observation texts.\n</p>\n\n",
                "matched_terms": [
                    "observation",
                    "texts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we introduce the EDAPF as a comparative method to the LAPF.\nThe EDAPF is a method that uses an external DNN to predict the cognitive value <math alttext=\"y_{\\mathrm{H},k}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><msub><mi>y</mi><mrow><mi mathvariant=\"normal\">H</mi><mo>,</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">y_{\\mathrm{H},k}</annotation></semantics></math> from the observation text <math alttext=\"s_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">s_{k}</annotation></semantics></math> directly, and treats the predicted value <math alttext=\"\\tilde{y}_{\\mathrm{H},k}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m3\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>y</mi><mo>~</mo></mover><mrow><mi mathvariant=\"normal\">H</mi><mo>,</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\tilde{y}_{\\mathrm{H},k}</annotation></semantics></math> as a pseudo-observation for running the particle filter.\nSpecifically, the predicted value <math alttext=\"\\tilde{y}_{\\mathrm{H},k}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>y</mi><mo>~</mo></mover><mrow><mi mathvariant=\"normal\">H</mi><mo>,</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\tilde{y}_{\\mathrm{H},k}</annotation></semantics></math> is regarded as the observation in the following observation equation, and the particle filter is applied:</p>\n\n",
                "matched_terms": [
                    "observation",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We describe the structure of the external DNN.\nFirst, the observation text <math alttext=\"s_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m1\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">s_{k}</annotation></semantics></math> is mapped to a text embedding <math alttext=\"e_{k}\\in\\mathbb{R}^{768}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>e</mi><mi>k</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mn>768</mn></msup></mrow><annotation encoding=\"application/x-tex\">e_{k}\\in\\mathbb{R}^{768}</annotation></semantics></math> using the Japanese embedding model &#8220;sentence-bert-base-ja-mean-tokens-v2.&#8221;\nThe embedding is then passed through a neural network with two hidden layers of 128 and 64 units, each followed by a ReLU activation.\nA final linear layer outputs a scalar feature <math alttext=\"\\psi_{k}^{\\prime}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m3\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#968;</mi><mi>k</mi><mo>&#8242;</mo></msubsup><mo>&#8712;</mo><mi>&#8477;</mi></mrow><annotation encoding=\"application/x-tex\">\\psi_{k}^{\\prime}\\in\\mathbb{R}</annotation></semantics></math>, which is passed through a sigmoid function and scaled by the maximum water level of <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m4\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> to produce the prediction <math alttext=\"\\tilde{y}_{\\mathrm{H},k}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m5\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>y</mi><mo>~</mo></mover><mrow><mi mathvariant=\"normal\">H</mi><mo>,</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\tilde{y}_{\\mathrm{H},k}</annotation></semantics></math></p>\n\n",
                "matched_terms": [
                    "observation",
                    "level",
                    "text",
                    "water"
                ]
            }
        ]
    }
}