{
    "S3.T1": {
        "source_file": "Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage",
        "caption": "Table 1: Performance comparison of accuracy, first-token latency, and latency savings (as a percentage of tool use latency, which is 3.37 seconds as reported in Table 3) for three models—Qwen2.5-7B, OpusLM, and Kimi Audio—across three settings: Closed Book (without tool usage), Open Book (with tool usage), and Streaming RAG (Model-Triggered Streaming RAG). We evaluate all models on both the AudioCRAG-Synthetic (Syn.) and AudioCRAG-Human (Hum.). Streaming RAG is not applied to Kimi-Audio as it can handle only a restricted length of tool result references (S. 12). *: OpusLM currently does not support taking tool result references in speech-out settings in zero-shot.",
        "body": "Setting\nRef length\nModel\nAccuracy\nLatency\n\n\n\n\n\nSyn.\nHum.\nFirst Token (s)\n% Savings\n\n\n\n\n\n\n\nSyn.\nHum.\nSyn.\nHum.\n\n\nClosed Book\n0\nQwen2.5-7B\n11.1\n13.1\n1.34\n1.24\n✗\n✗\n\n\n\n0\nOpusLM\n18.4\n15.5\n5.67\n7.07\n✗\n✗\n\n\n\n0\nKimi Audio\n16.7\n16.0\n0.85\n0.89\n✗\n✗\n\n\nOpen Book\n23K\nQwen2.5-7B\n26.3\n26.9\n5.90\n5.40\n✗\n✗\n\n\n(S. 3.1)\n15K\nOpusLM*\n0.0\n0.0\n9.05\n10.44\n✗\n✗\n\n\n\n500\nKimi Audio\n21.8\n19.6\n4.22\n4.22\n✗\n✗\n\n\nStreaming RAG\n23K\nQwen2.5-7B\n34.2\n37.4\n5.32\n3.60\n20.7%\n53.4%\n\n\n(S. 3.2.2)\n15K\nOpusLM\n23.6\n22.8\n8.63\n9.04\n14.8%\n41.5%",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">Setting</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">Ref length</th>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">Accuracy</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\">Latency</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">Syn.</td>\n<td class=\"ltx_td ltx_align_left\">Hum.</td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\">First Token (s)</td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\">% Savings</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">Syn.</td>\n<td class=\"ltx_td ltx_align_left\">Hum.</td>\n<td class=\"ltx_td ltx_align_left\">Syn.</td>\n<td class=\"ltx_td ltx_align_left\">Hum.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Closed Book</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">0</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Qwen2.5-7B</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">11.1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">13.1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">1.34</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">1.24</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">&#10007;</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">&#10007;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">0</th>\n<td class=\"ltx_td ltx_align_left\">OpusLM</td>\n<td class=\"ltx_td ltx_align_left\">18.4</td>\n<td class=\"ltx_td ltx_align_left\">15.5</td>\n<td class=\"ltx_td ltx_align_left\">5.67</td>\n<td class=\"ltx_td ltx_align_left\">7.07</td>\n<td class=\"ltx_td ltx_align_left\">&#10007;</td>\n<td class=\"ltx_td ltx_align_left\">&#10007;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">0</th>\n<td class=\"ltx_td ltx_align_left\">Kimi Audio</td>\n<td class=\"ltx_td ltx_align_left\">16.7</td>\n<td class=\"ltx_td ltx_align_left\">16.0</td>\n<td class=\"ltx_td ltx_align_left\">0.85</td>\n<td class=\"ltx_td ltx_align_left\">0.89</td>\n<td class=\"ltx_td ltx_align_left\">&#10007;</td>\n<td class=\"ltx_td ltx_align_left\">&#10007;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Open Book</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">23K</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Qwen2.5-7B</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">26.3</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">26.9</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">5.90</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">5.40</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">&#10007;</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">&#10007;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">(S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>)</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">15K</th>\n<td class=\"ltx_td ltx_align_left\">OpusLM*</td>\n<td class=\"ltx_td ltx_align_left\">0.0</td>\n<td class=\"ltx_td ltx_align_left\">0.0</td>\n<td class=\"ltx_td ltx_align_left\">9.05</td>\n<td class=\"ltx_td ltx_align_left\">10.44</td>\n<td class=\"ltx_td ltx_align_left\">&#10007;</td>\n<td class=\"ltx_td ltx_align_left\">&#10007;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">500</th>\n<td class=\"ltx_td ltx_align_left\">Kimi Audio</td>\n<td class=\"ltx_td ltx_align_left\">21.8</td>\n<td class=\"ltx_td ltx_align_left\">19.6</td>\n<td class=\"ltx_td ltx_align_left\">4.22</td>\n<td class=\"ltx_td ltx_align_left\">4.22</td>\n<td class=\"ltx_td ltx_align_left\">&#10007;</td>\n<td class=\"ltx_td ltx_align_left\">&#10007;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Streaming RAG</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">23K</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Qwen2.5-7B</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">34.2</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">37.4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">5.32</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">3.60</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">20.7%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">53.4%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">(S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>)</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">15K</th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">OpusLM</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">23.6</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">22.8</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">8.63</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">9.04</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">14.8%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">41.5%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "kimi",
            "speechout",
            "tool",
            "evaluate",
            "usage",
            "closed",
            "percentage",
            "seconds",
            "length",
            "qwen257b",
            "audio—across",
            "first",
            "three",
            "applied",
            "which",
            "restricted",
            "hum",
            "opuslm",
            "all",
            "references",
            "zeroshot",
            "both",
            "book",
            "latency",
            "open",
            "not",
            "23k",
            "taking",
            "handle",
            "support",
            "streaming",
            "model",
            "rag",
            "settings",
            "without",
            "result",
            "token",
            "does",
            "kimiaudio",
            "performance",
            "only",
            "accuracy",
            "ref",
            "reported",
            "setting",
            "models",
            "currently",
            "savings",
            "audiocraghuman",
            "15k",
            "use",
            "modeltriggered",
            "firsttoken",
            "audiocragsynthetic",
            "comparison",
            "audio",
            "syn",
            "models—qwen257b"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution 2</span>: To address this, we propose <em class=\"ltx_emph ltx_font_italic\">Streaming Retrieval-Augmented Generation</em> (<em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>), the <em class=\"ltx_emph ltx_font_italic\">first</em> framework that empowers the system to trigger tool queries in parallel with user speech, even before the user finishes speaking. Within this framework, we introduce two novel approaches: (1) <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em>, which issues tool queries at regular intervals during speech input and carefully examines quality of retrieval results on the full query to guarantee response quality, and can be incorporated into any speech-in, speech-out model without post-training; (2) <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, which post-trains the model to intelligently determine optimal query timing based on the evolving user utterance to save computation resources.\nOur results demonstrate that our proposed <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> delivers over 200% relative improvement in accuracy (from 11.1% to 34.2% absolute in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) compared to the no-tool baseline, while also reducing tool result generation latency by 20%. Though designed for speech-in speech-out systems, streaming RAG can also be adapted in cascaded SDS, or even chatbots as users type.</p>\n\n",
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a comprehensive performance comparison of three models, Qwen2.5-7B, OpusLM, and Kimi Audio, evaluated across three settings: Closed Book, Open Book, and Streaming RAG. All models are assessed on both the AudioCRAG-Synthetic (Syn.) and AudioCRAG-Human (Hum.). In the Closed Book setting, where models rely solely on their internal knowledge without access to external tools (reference length = 0), all models achieve accuracy scores below 20%. These results highlight the inherent limitations of closed-book approaches in handling complex queries.\nThe Open Book setting, which provides models with access to external information, demonstrates the clear benefits of tool integration. Qwen2.5-7B and Kimi Audio&#8217;s accuracy rises substantially, underscoring the value of leveraging external context. As expected, latency increases due to the additional overhead of retrieving information from external tools (See T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> for detailed latency analysis).</p>\n\n",
            "<p class=\"ltx_p\">Similar to previous work <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>, we employ model-based automatic evaluation. We use a three-way scoring system, assigning scores of 1, -1, and 0 for accurate, incorrect, and missing answers, respectively. The evaluation is conducted using the Llama 4-maverick LLM evaluator. For speech outputs, we first transcribe the audio using Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib27\" title=\"\">2023</a>)</cite> before passing the transcriptions to the LLM evaluator. In this study, our primary focus is on enhancing system accuracy; therefore, we report average accuracy values in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Evaluated SOTA Speech-in Speech-out Models &#8227; 4 Experiment Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. For additional context, we also provide the average rates of accurate, hallucinated, and missing responses, as well as overall truthfulness scores for each system in the Open Book Setting (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S10.T6\" title=\"Table 6 &#8227; 10 Tool Usage Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>). Notably, our results indicate that Qwen-OMNI was less likely to generate hallucinated responses compared to OpusLM and Kimi Audio.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">End-to-end speech-in speech-out dialogue systems are emerging as a powerful alternative to traditional ASR&#8211;LLM&#8211;TTS pipelines, generating more natural, expressive responses with significantly lower latency. However, these systems remain prone to hallucinations due to limited factual grounding. While text-based dialogue systems address this challenge by integrating tools such as web search and knowledge graph APIs, we introduce the first approach to extend tool use directly into speech-in speech-out systems. A key challenge is that tool integration substantially increases response latency, disrupting conversational flow. To mitigate this, we propose Streaming Retrieval-Augmented Generation (<em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>), a novel framework that reduces user-perceived latency by predicting tool queries in parallel with user speech, even before the user finishes speaking. Specifically, we develop a post-training pipeline that teaches the model when to <em class=\"ltx_emph ltx_font_italic\">issue tool calls</em> during ongoing speech and how to generate spoken summaries that fuse audio queries with retrieved text results, thereby improving both <em class=\"ltx_emph ltx_font_italic\">accuracy and responsiveness</em>. To evaluate our approach, we construct AudioCRAG, a benchmark created by converting queries from the publicly available CRAG dataset into speech form. Experimental results demonstrate that our <em class=\"ltx_emph ltx_font_italic\">streaming RAG</em> approach increases QA accuracy by up to 200% relative (from 11.1% to 34.2% absolute) and further enhances user experience by reducing tool use latency by 20%. Importantly, our <em class=\"ltx_emph ltx_font_italic\">streaming RAG</em> approach is modality-agnostic and can be applied equally to typed input, paving the way for more agentic, real-time AI assistants.</p>\n\n",
                "matched_terms": [
                    "speechout",
                    "applied",
                    "tool",
                    "evaluate",
                    "model",
                    "rag",
                    "both",
                    "use",
                    "latency",
                    "accuracy",
                    "audio",
                    "first",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spoken Dialogue Systems (SDS) are foundational to many everyday technologies, powering intelligent assistants such as Alexa and Siri, as well as interactive voice response systems in customer service. With the rapid expansion of SDS capabilities to mobile phones and wearable devices, the need for robust, scalable, and generalizable solutions has never been greater. Traditionally, SDS have relied on cascaded pipelines composed of multiple modules&#8212;including voice activity detection (VAD), automatic speech recognition (ASR), natural language understanding (NLU), natural language generation (NLG), and text-to-speech (TTS) synthesis&#8212;each introducing potential points of failure and latency <cite class=\"ltx_cite ltx_citemacro_citep\">(Glass, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib12\" title=\"\">1999</a>; Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib13\" title=\"\">2024</a>)</cite>.\nRecently, end-to-end (E2E) SDS <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib34\" title=\"\">2024</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib22\" title=\"\">2023</a>; Meng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib20\" title=\"\">2024</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib40\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib2\" title=\"\">2025b</a>)</cite> have been proposed, which directly generate spoken responses from speech input within a unified architecture. This E2E approach not only mitigates error propagation across modules but also captures non-phonemic information more effectively, resulting in significantly lower inference time and computational overhead, and paving the way for more natural and efficient conversational experiences.</p>\n\n",
                "matched_terms": [
                    "only",
                    "not",
                    "which",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, current E2E SDS are fundamentally constrained by their reliance on internalized knowledge from static training data, which often results in responses that lack factual grounding or fail to reflect the most up to date information. This shortcoming is particularly critical for action-oriented or knowledge-seeking tasks, such as booking hotels or answering questions about current events. In contrast, text-based conversational assistants have begun to overcome these limitations by integrating external tools through Retrieval-Augmented Generation (RAG)<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>; Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib3\" title=\"\">2023a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib5\" title=\"\">c</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib10\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib11\" title=\"\">b</a>)</cite>, dynamically retrieving relevant information from sources like web search, knowledge graphs (KG), and real-time APIs. Yet, the integration of such tool use into E2E SDS remains largely unexplored.\nA key challenge is that while external tools can substantially improve factual accuracy, invoking them often introduces additional latency, leading to awkward silences that disrupt the natural conversational flow. This raises a research question: <span class=\"ltx_text ltx_font_italic\">How can we trade-off between accuracy and responsiveness for developing SDS that feel both intelligent and natural?</span></p>\n\n",
                "matched_terms": [
                    "tool",
                    "which",
                    "both",
                    "use",
                    "latency",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present, to the best of our knowledge, the first <span class=\"ltx_text ltx_font_italic\">speech-in, speech-out language model that seamlessly integrates external tool invocation with low latency</span>. The key idea is a <span class=\"ltx_text ltx_font_bold\">Streaming RAG</span> strategy that generates tool queries in parallel with user speech, often times even before the user has finished speaking (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). A naive implementation of streaming queries, however, faces two challenges: (1) queries issued from partial speech may be suboptimal, yielding distracting tool outputs and inaccurate responses; and (2) unnecessary tool calls may be triggered, wasting computational resources. We introduce effective modeling techniques to address these challenges and make the following contributions.</p>\n\n",
                "matched_terms": [
                    "speechout",
                    "tool",
                    "model",
                    "rag",
                    "latency",
                    "first",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution 1</span>: We introduce a formal framework for tool integration in speech-in speech-out systems and empirically show that leveraging web search and KG APIs significantly enhances factual question answering. Evaluating three state-of-the-art (SOTA) models, Qwen-OMNI <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>)</cite>, OpusLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib30\" title=\"\">2025</a>)</cite>, and Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib8\" title=\"\">2025</a>)</cite>, we find that external tool integration delivers substantial performance gains, boosting accuracy by up to 140% relative (from 11.1% to 26.3% absolute). However, tool usage also introduces considerable latency, increasing first-token response time by 2.3x.</p>\n\n",
                "matched_terms": [
                    "speechout",
                    "three",
                    "tool",
                    "usage",
                    "opuslm",
                    "models",
                    "latency",
                    "kimiaudio",
                    "firsttoken",
                    "performance",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution 3</span>: Finally, we introduce AudioCRAG, a benchmark created by recording spoken queries from the CRAG <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite> dataset, enabling robust evaluation of tool usage capabilities in speech-in speech-out systems.\nTo support future research, we will open source our training code and AudioCRAG-Human benchmark, supporting future research in tool-integrated voice assistants.</p>\n\n",
                "matched_terms": [
                    "speechout",
                    "tool",
                    "usage",
                    "audiocraghuman",
                    "open",
                    "support"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in benchmarking text-based dialogue systems for tool usage <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib4\" title=\"\">2023b</a>; Ouyang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib24\" title=\"\">2025</a>; Cheng and Dou, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib6\" title=\"\">2025</a>; Cohen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib7\" title=\"\">2025</a>; Xiong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib35\" title=\"\">2024a</a>)</cite> have primarily focused on evaluating factual question answering and task completion within simulated environments (detailed related work discussion in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S7\" title=\"7 Benchmarking Text Dialogue Systems for Tool Usage &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S8\" title=\"8 Multimodal Benchmarks for Tool Usage &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>). The CRAG benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite> is a leading example, featuring 4,409 question-answer pairs and providing mock APIs for both web and KG search. Recent benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Meta CRAG-MM Challenge Organizers, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib21\" title=\"\">2025</a>; Ma et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib17\" title=\"\">2024</a>; Jang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib14\" title=\"\">2025</a>)</cite> have extended tool-augmented dialogue evaluation to multimodal input and longer-context scenarios. While these benchmarks have significantly advanced the evaluation of tool-augmented dialogue systems, they remain largely limited to text-based outputs and do not fully address the unique challenges presented by speech-in speech-out systems.</p>\n\n",
                "matched_terms": [
                    "speechout",
                    "tool",
                    "usage",
                    "both",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several E2E spoken dialogue systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>; Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib34\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib1\" title=\"\">2025a</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib22\" title=\"\">2023</a>; Meng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib20\" title=\"\">2024</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib40\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib2\" title=\"\">2025b</a>)</cite> have recently been introduced, demonstrating impressive semantic understanding and high audio quality in their responses. However, these systems have not yet been trained or evaluated for their ability to <em class=\"ltx_emph ltx_font_italic\">use external tools</em>.\nAnother research direction <cite class=\"ltx_cite ltx_citemacro_citep\">(Feng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib9\" title=\"\">2025</a>)</cite> explores E2E RAG for direct speech-to-text retrieval, utilizing multimodal embeddings for enabling speech utterances to directly retrieve relevant text. While this method outperforms models without RAG, it is primarily limited to <em class=\"ltx_emph ltx_font_italic\">speech-to-text</em> scenarios. Its ability to access KGs and other APIs, critical for real-world applications, remains unproven. Furthermore, the retrieval scope is limited, as experiments are conducted with retrieval restricted to just 10 paragraphs, whereas modern RAG benchmarks require searching across thousands of web pages.\nAlthough recent studies <cite class=\"ltx_cite ltx_citemacro_citep\">(Maben et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib18\" title=\"\">2025</a>)</cite> have developed web interfaces that integrate tools into speech-in <em class=\"ltx_emph ltx_font_italic\">speech-out</em> scenarios using cascaded pipelines, comprehensive empirical investigations of E2E speech-in speech-out systems and, crucially, systematic analyses of user-perceived latency, remain largely unexplored. In this work, we address these gaps by developing a comprehensive framework for tool integration in E2E speech-in speech-out systems and designing benchmarks to quantitatively assess the tool usage capabilities of SOTA models. Furthermore, recognizing the importance of latency for natural conversational flow, we introduce novel streaming RAG methods that not only enhance tool usage performance but also reduce user-perceived latency.</p>\n\n",
                "matched_terms": [
                    "speechout",
                    "tool",
                    "restricted",
                    "usage",
                    "rag",
                    "only",
                    "models",
                    "without",
                    "not",
                    "latency",
                    "use",
                    "performance",
                    "audio",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A RAG spoken conversation system takes an audio question <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math> as input and outputs a spoken answer <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>. Let the ASR transcript of audio question be <math alttext=\"X^{\\text{asr}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>asr</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}</annotation></semantics></math> and the ASR transcript of the spoken answer be <math alttext=\"X^{\\text{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>res</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{res}}</annotation></semantics></math>.\nAnswers are generated by speech-in speech-out models, leveraging both the model&#8217;s internal knowledge and information retrieved from external sources.\nTo incorporate external information, the model needs to formulate a tool query <math alttext=\"Q^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><msup><mi>Q</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">Q^{T}</annotation></semantics></math> to retrieve relevant results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m6\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from an external tool <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m7\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speechout",
                    "tool",
                    "model",
                    "rag",
                    "models",
                    "both",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our proposed formulation for integrating external tools into speech-in speech-out systems. We introduce a two-stage inference approach: <em class=\"ltx_emph ltx_font_italic\">Query Generation</em> and <em class=\"ltx_emph ltx_font_italic\">Response Generation</em>. In the Query Generation stage, the system processes an audio question and generates queries for each external tool to retrieve relevant information by maximizing the posterior distribution <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> (Examples of generated queries are provided in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T7\" title=\"Table 7 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> in the Appendix.). In the Response Generation stage, the retrieved results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from these tools are combined with the original audio question and input into the model to generate the final spoken response by maximizing the posterior distribution <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math>. By conditioning the final output generation on the input audio, this formulation not only provides a simple and effective mechanism for interacting with text-based APIs, but also preserves the key advantages of speech-in speech-out systems, mitigating error propagation and enabling the model to capture non-phonemic information (such as prosody and speaker intent) more effectively.</p>\n\n",
                "matched_terms": [
                    "speechout",
                    "tool",
                    "model",
                    "not",
                    "only",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">RAG-based systems, as proposed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, can significantly improve factual accuracy by incorporating external tools. However, these tool calls often introduce substantial latency, which is particularly problematic in speech-in, speech-out applications where users expect rapid, conversational responses and even brief silences can disrupt the natural flow of dialogue.\nOne way to address this challenge lies in the nature of audio inputs, which arrive as a continuous stream. This streaming property enables tool calls to be initiated before the user has finished speaking, offering a unique opportunity to mitigate latency.</p>\n\n",
                "matched_terms": [
                    "speechout",
                    "tool",
                    "which",
                    "latency",
                    "accuracy",
                    "audio",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To minimize user-perceived latency, we introduce <em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>: the first framework to generate and issue tool queries in parallel as audio input is received. This novel approach is built on three key design components:\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> Trigger: When to initiate a new tool query;\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> Threads: The number of parallel tool query threads;\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic3\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">3</span></span></foreignobject></g></g></svg></span> Reflector: The module that determines whether intermediate tool results are sufficient for generating the final output. By exploring different design choices for each component, we introduce two complementary approaches for streaming tool query generation in the following subsections: a fixed-interval trigger method and a model-based trigger method.</p>\n\n",
                "matched_terms": [
                    "three",
                    "tool",
                    "rag",
                    "latency",
                    "audio",
                    "first",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this approach, the <em class=\"ltx_emph ltx_font_italic\">trigger</em> is set to fire tool calls at fixed chunk intervals during audio input.\nThe input speech <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math> is divided into a sequence of <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> blocks, <math alttext=\"Q=\\{Q_{b}\\mid b=1,\\dots,B\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>Q</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>Q</mi><mi>b</mi></msub><mo fence=\"true\" lspace=\"0em\" rspace=\"0em\">&#8739;</mo><mrow><mi>b</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>B</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Q=\\{Q_{b}\\mid b=1,\\dots,B\\}</annotation></semantics></math>, with each block containing <math alttext=\"N_{\\text{block}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>N</mi><mtext>block</mtext></msub><annotation encoding=\"application/x-tex\">N_{\\text{block}}</annotation></semantics></math> frames.\nTo approximate <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> as described in S. 3.1, we follow a block-wise prediction strategy. In this approach, after processing each audio block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m6\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, the model predicts a tool query <math alttext=\"\\hat{Q}_{b}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m7\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}_{b}^{T}</annotation></semantics></math> by conditioning on the input speech accumulated up to block b, specifically <math alttext=\"Q_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">Q_{1:b}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This strategy results in <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m9\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> parallel tool call threads running simultaneously (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F3.sf1\" title=\"Figure 3(a) &#8227; Figure 3 &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>), where each thread generates a tool query prediction <math alttext=\"\\hat{Q}_{b}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m10\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}_{b}^{T}</annotation></semantics></math> for its corresponding block <math alttext=\"b\\in[1,B]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m11\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mi>B</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b\\in[1,B]</annotation></semantics></math>.\nThe tool queries <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m12\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> generated after each block are then stored in cache. Given the high latency of tool calls (See T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), users typically complete their utterances before tool responses are ready. Upon utterance completion, an explicit <em class=\"ltx_emph ltx_font_italic\">reflector</em> module &#8220;<math alttext=\"\\text{reflect}()\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m13\" intent=\":literal\"><semantics><mrow><mtext>reflect</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{reflect}()</annotation></semantics></math>&#8221; evaluates the cached intermediate queries <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m14\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> against final query <math alttext=\"\\hat{Q}^{T}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m15\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{B}</annotation></semantics></math> to determine whether an early intermediate tool call provides sufficient information to answer the user&#8217;s question <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m16\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math>. The reflector module systematically evaluates all intermediate queries in the cache and identifies the <em class=\"ltx_emph ltx_font_italic\">earliest</em> sufficient tool call <math alttext=\"b^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m17\" intent=\":literal\"><semantics><msup><mi>b</mi><mo>&#8902;</mo></msup><annotation encoding=\"application/x-tex\">b^{\\star}</annotation></semantics></math> where the intermediate query <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m18\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> will give the same result as final query <math alttext=\"\\hat{Q}^{T}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m19\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{B}</annotation></semantics></math> as shown:</p>\n\n",
                "matched_terms": [
                    "all",
                    "tool",
                    "result",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All subsequent parallel tool calls after <math alttext=\"b^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m20\" intent=\":literal\"><semantics><msup><mi>b</mi><mo>&#8902;</mo></msup><annotation encoding=\"application/x-tex\">b^{\\star}</annotation></semantics></math> are promptly terminated, and the retrieved results <math alttext=\"R_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m21\" intent=\":literal\"><semantics><msub><mi>R</mi><msup><mi>b</mi><mo>&#8902;</mo></msup></msub><annotation encoding=\"application/x-tex\">R_{b^{\\star}}</annotation></semantics></math> from this intermediate call <math alttext=\"\\hat{Q}^{T}_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m22\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><msup><mi>b</mi><mo>&#8902;</mo></msup><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b^{\\star}}</annotation></semantics></math> are used to generate the final spoken response <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m23\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by maximizing the posterior distribution <math alttext=\"P(A|Q,R_{b^{\\star}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m24\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><msub><mi>R</mi><msup><mi>b</mi><mo>&#8902;</mo></msup></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R_{b^{\\star}})</annotation></semantics></math> (instead of <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m25\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math> in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>).\nWe employ a reflector module that uses simple yet effective heuristics: (a) if the top 5 web documents for an intermediate web query match those of the final web query, and (b) if the KG results for intermediate and final KG queries are identical.\nThese heuristics ensure that the information retrieved from an early tool call using <math alttext=\"\\hat{Q}^{T}_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m26\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><msup><mi>b</mi><mo>&#8902;</mo></msup><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b^{\\star}}</annotation></semantics></math> is consistent with what would have been obtained by waiting for the final tool call using <math alttext=\"\\hat{Q}^{T}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m27\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{B}</annotation></semantics></math>, thereby providing a strong quality guarantee. Since most tool call latency arises from chunking and reranking the chunks of web documents, these checks enable significant latency savings without any compromise in model performance.\nA key advantage of this strategy is its plug-and-play nature: it requires no additional post-training for speech-in, speech-out models and can be directly applied at inference time across a variety of architectures. However, there are important considerations. First, generating parallel tool calls at every fixed interval (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E1\" title=\"Equation 1 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) increases computational overhead, which may pose challenges for deployment on resource-constrained devices such as wearables devices. Second, reliance on an external reflector module (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E2\" title=\"Equation 2 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) to determine the sufficiency of intermediate tool calls may limit the extent of achievable latency improvements.</p>\n\n",
                "matched_terms": [
                    "speechout",
                    "applied",
                    "tool",
                    "model",
                    "which",
                    "models",
                    "all",
                    "without",
                    "savings",
                    "latency",
                    "performance",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the limitations of Fixed-Interval Streaming RAG and further optimize both efficiency and responsiveness, we propose a more adaptive approach: <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>.\nHere, the <em class=\"ltx_emph ltx_font_italic\">trigger</em> is learned: the model is trained to autonomously determine the optimal moments to initiate tool queries, issuing a query only when it encounters new or additional information as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F3.sf2\" title=\"Figure 3(b) &#8227; Figure 3 &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>. In this formulation, the model receives user input in fixed chunk intervals as before and intelligently determines whether a tool call is needed after each block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>. The model can either:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p1.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> Predict\nNO_QUERY if a new tool query is unnecessary, or\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p1.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> Generate a new tool query. To make this decision, the model conditions on both the accumulated input speech <math alttext=\"Q_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">Q_{1:b}</annotation></semantics></math> (see Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E1\" title=\"Equation 1 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) as well as most recent tool query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}=\\hat{Q}^{T}_{\\max\\{\\,i&lt;b:\\hat{Q}^{T}_{i}\\neq\\text{NO\\_QUERY}\\,\\}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><mo>=</mo><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">{</mo><mrow><mrow><mi>i</mi><mo>&lt;</mo><mi>b</mi></mrow><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>i</mi><mi>T</mi></msubsup><mo>&#8800;</mo><mtext>NO_QUERY</mtext></mrow></mrow><mo lspace=\"0.170em\" stretchy=\"false\">}</mo></mrow></mrow><mi>T</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}=\\hat{Q}^{T}_{\\max\\{\\,i&lt;b:\\hat{Q}^{T}_{i}\\neq\\text{NO\\_QUERY}\\,\\}}</annotation></semantics></math> as shown:</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "rag",
                    "both",
                    "modeltriggered",
                    "only",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When a new query <math alttext=\"\\hat{Q}^{T}_{b}\\neq\\text{NO\\_QUERY}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m4\" intent=\":literal\"><semantics><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><mo>&#8800;</mo><mtext>NO_QUERY</mtext></mrow><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}\\neq\\text{NO\\_QUERY}</annotation></semantics></math>, the system immediately terminates any ongoing tool calls for the previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m5\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, ensuring that only a <em class=\"ltx_emph ltx_font_italic\">single tool call thread</em> runs at any given time. (Examples of generated <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m6\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> are provided in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T8\" title=\"Table 8 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> in the Appendix.)\nThis approach offers several key advantages. First, it effectively eliminating redundant parallel threads and significantly reducing computational overhead. This is especially important for deployment on resource-constrained devices.\nSecond, this formulation removes the need for an external <em class=\"ltx_emph ltx_font_italic\">reflector</em> (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E2\" title=\"Equation 2 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) module. The model confidently relies on the results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from the most recent tool call using <math alttext=\"\\hat{Q}^{\\text{prev}}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m8\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{B}</annotation></semantics></math> to generate the spoken response <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m9\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by maximizing <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math>, reducing system complexity.</p>\n\n",
                "matched_terms": [
                    "only",
                    "tool",
                    "model",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-training</span>: To train the model, we transform text-based tool usage benchmarks into spoken format (See S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13\" title=\"13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>). Word-level timestamps are computed using a pre-trained ASR model. For each partial ASR transcript <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math> up to block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, we generate corresponding queries for each tool <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m3\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> using an LLM as pseudo ground truth (GT).\nTo create effective training labels that teach the model when to trigger new queries, we employ a similarity-based labeling strategy where we compare the current pseudo GT query <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m4\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> with the most recent non-empty tool query label <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m5\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math> (see Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E3\" title=\"Equation 3 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) before block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m6\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>.\nOur labeling function assigns the training label <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m7\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E3\" title=\"Equation 3 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) for the tool query after block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m8\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> as follows:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p2.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> when the current query is sufficiently similar to the previous query (as determined by manually defined heuristics <math alttext=\"f(\\cdots)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m9\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8943;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f(\\cdots)</annotation></semantics></math>), we assign the special label NO_QUERY to teach the model that no new tool call is needed.  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p2.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> When the queries are sufficiently different, we assign the actual pseudo ground truth query\n<math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m10\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> as the label to teach the model to trigger a new tool call:</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "usage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a multi-task fine-tuning strategy targeting two key capabilities.\nFirst, we train the model on Streaming Tool Query Generation by optimizing <math alttext=\"P(Q^{T}_{b}|Q_{1:b},\\hat{Q}^{\\text{prev}}_{b})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo fence=\"false\">|</mo><mrow><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><mo>,</mo><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}_{b}|Q_{1:b},\\hat{Q}^{\\text{prev}}_{b})</annotation></semantics></math> for <math alttext=\"b\\in[1,B]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m2\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mi>B</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b\\in[1,B]</annotation></semantics></math>, enabling intelligent decisions about when to trigger tool queries. Second, we fine-tune on Response Generation by optimizing <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m3\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) to improve the intelligibility of the speech output.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "first",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An important aspect of our post-training is enhancing the model&#8217;s ability to recover from errors in intermediate query predictions. For example, when presented with the audio question, &#8220;Who founded Rare Beauty in 2019?&#8221;, we observed that an initial misinterpretation of <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m1\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, such as &#8220;Red Bull founder&#8221;, can lead the model to subsequently generate NO_QUERY labels, effectively halting further attempts to retrieve the correct information. This issue arises because, during training, the model is always provided with correct previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m2\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, whereas during inference, it may make errors due to partial utterances <math alttext=\"{Q}_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">{Q}_{1:b}</annotation></semantics></math> being ambiguous. Thus the model lacks the ability to recover from such mistakes during inference.\nTo overcome this, we introduce a novel strategy in which we deliberately inject negative samples during post-training by substituting the previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m4\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math> in Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E4\" title=\"Equation 4 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> with incorrect ones <math alttext=\"Q^{\\text{neg}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m5\" intent=\":literal\"><semantics><msubsup><mi>Q</mi><mi>b</mi><mtext>neg</mtext></msubsup><annotation encoding=\"application/x-tex\">Q^{\\text{neg}}_{b}</annotation></semantics></math>. Crucially, when we perform negative sampling, we fall back to the pseudo ground truth query <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> as the training label:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This approach explicitly teaches the model to recover from errors in intermediate query prediction, thereby maintaining accuracy (see T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T5\" title=\"Table 5 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> for ablation) while achieving latency savings.</p>\n\n",
                "matched_terms": [
                    "savings",
                    "model",
                    "latency",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To rigorously evaluate our proposed approach, we construct comprehensive benchmark datasets featuring spoken queries paired with simulated tool interactions. We begin with the CRAG dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>, which form the basis for our spoken version of CRAG, which we term <em class=\"ltx_emph ltx_font_italic\">AudioCRAG</em>. It consists of 2 distinct variants:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S4.SS1.p1.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> <span class=\"ltx_text ltx_font_bold\">Audio CRAG Synthetic</span>: To generate spoken queries, we use our in-house TTS system. We then apply a rigorous filtering procedure which results in a high-quality set of 1,862 spoken queries, which we refer to as the <em class=\"ltx_emph ltx_font_italic\">AudioCRAG-Synthetic</em> benchmark.  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S4.SS1.p1.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> <span class=\"ltx_text ltx_font_bold\">Audio CRAG Human</span>: To further enhance the realism and diversity of our evaluation, we introduce the AudioCRAG-Human benchmark, which consists of 618 human-recorded spoken queries. We will release\nAudio CRAG Human benchmark upon acceptance to supports the development of more natural and reliable voice assistants.\nFurther details on the construction of these benchmarks are provided in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S9\" title=\"9 Audio CRAG Benchmark &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. We follow the CRAG setup to incorporate both web and KG-based tools, and adopt its robust evaluation methodology, as described in Secs. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S10\" title=\"10 Tool Usage Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S11\" title=\"11 Evaluation Setting &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.\nAdditionally, we leverage a random subset of 16,000 questions from the text-based factual question answering dataset TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib15\" title=\"\">2017</a>)</cite> to post-train our speech-in, speech-out models, as detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13\" title=\"13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "speechout",
                    "tool",
                    "evaluate",
                    "which",
                    "models",
                    "audiocraghuman",
                    "both",
                    "use",
                    "audiocragsynthetic",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present a comprehensive benchmark of three SOTA speech-in, speech-out conversational systems: Qwen-OMNI <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>)</cite>, Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib8\" title=\"\">2025</a>)</cite> and OpusLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib30\" title=\"\">2025</a>)</cite>.\nWe evaluate them under both tool-augmented and non-tool-augmented conditions. Further details about our experimental setup are provided in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S12\" title=\"12 Experiment Setup of SOTA speech-in speech-out models &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.</p>\n\n",
                "matched_terms": [
                    "speechout",
                    "three",
                    "evaluate",
                    "opuslm",
                    "both",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform an ablation study (referred to as &#8220;Tool Integration&#8221; in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) where we post-train the model on sequential query generation (i.e. <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) and output generation, to assess the impact of streaming RAG post-training versus standard post-training on final response generation.\nAdditionally, we conduct an ablation study on open book setting (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) using a self-cascade approach with a three-stage inference pipeline: (1) the audio question is used to generate a tool query <math alttext=\"Q^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><msup><mi>Q</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">Q^{T}</annotation></semantics></math> (corresponding to the &#8220;Query Generation&#8221; stage described in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>); (2) the audio question, and tool results are combined to produce the final text output <math alttext=\"X^{\\text{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>res</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{res}}</annotation></semantics></math> by maximizing <math alttext=\"P(X^{\\text{res}}|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mtext>res</mtext></msup><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(X^{\\text{res}}|Q,R)</annotation></semantics></math>; and (3) the audio question, and final text output are used to generate the final speech output <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by optimizing <math alttext=\"P(A|Q,X^{\\text{res}})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m6\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><msup><mi>X</mi><mtext>res</mtext></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,X^{\\text{res}})</annotation></semantics></math>. Since we teacher-force the text output to obtain the final speech output in stage (3), this self-cascade approach can only be applied to a &#8220;thinker-talker&#8221; architecture (eq. Qwen-OMNI) or Chain-of-Thought (CoT) style architectures (eg. OpusLM).\nThe motivation for this ablation is to investigate whether the inclusion of RAG references <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> during the &#8220;Response Generation&#8221; stage (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) affects the quality of the generated speech <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m8\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "applied",
                    "tool",
                    "model",
                    "rag",
                    "setting",
                    "opuslm",
                    "references",
                    "book",
                    "open",
                    "only",
                    "audio",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most notably, our post-training approach to build <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, as described in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>, delivers significant advancements in both accuracy and efficiency. Qwen2.5-7B and OpusLM achieve significant accuracy improvements across both benchmarks. While the absolute accuracy scores may appear low, they are consistent with the evaluation results observed in the CRAG benchmark. Notably, Qwen2.5-7B with <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> achieves accuracy comparable to the open book performance of similarly sized LLMs reported in the original CRAG paper (i.e., 34.2 for Qwen2.5-7B vs. 32.1 for LLAMA-3 8B Instruct in <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>).\nImportantly, although post-training is performed exclusively on the synthetic dataset, we observe consistent and even greater improvements on the human-spoken benchmark, demonstrating the robustness and strong generalization capabilities of our method. This setting also introduces substantial latency savings compared to the Open Book configuration, with Qwen2.5-7B and OpusLM achieving 20.7% and 14.8% reductions in first-token latency on the synthetic benchmark, and even greater savings on the human benchmark\n<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Our latency calculations on synthetic audio exclude end-point detection latency, which is required in all production systems. Since our streaming RAG approach enables processing without waiting for end-point detection, including this factor would further amplify the observed latency savings, as observed in higher latency savings for human-spoken audio, where endpoint detection errors often introduce trailing silence.</span></span></span>. These results demonstrate that our streaming RAG approach not only advances the accuracy of speech-in speech-out systems, but also optimizes response efficiency by enabling earlier and more effective prediction of tool queries.</p>\n\n",
                "matched_terms": [
                    "speechout",
                    "tool",
                    "qwen257b",
                    "which",
                    "opuslm",
                    "all",
                    "both",
                    "book",
                    "latency",
                    "open",
                    "not",
                    "streaming",
                    "rag",
                    "without",
                    "only",
                    "performance",
                    "accuracy",
                    "reported",
                    "setting",
                    "savings",
                    "modeltriggered",
                    "firsttoken",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Evaluated SOTA Speech-in Speech-out Models &#8227; 4 Experiment Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides a comparative evaluation of the three SOTA models in generating either text or speech outputs from speech inputs, both with and without the integration of external tool results. In the absence of tool results (Ref length = 0), all models achieve higher accuracy when generating text outputs compared to speech outputs. The incorporation of tool results generally leads to improved text generation performance, with Kimi Audio achieving the highest accuracy. In contrast, the accuracy for speech output remains consistently lower across all models and conditions. The self-cascade approach, in which the model first generates an intermediate text response before producing the final speech output, provides moderate improvements in speech output accuracy for both Qwen2.5-7B and OpusLM.\nHowever, it still underperforms compared to the text-out baseline, primarily due to errors in accurately generating answers involving uncommon entity nouns.\nOverall, these findings underscore a persistent challenge in direct speech generation, particularly when tool results are integrated, as this appears to negatively impact the quality of generated speech responses.\nOur <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> demonstrates clear advantages: it maintains comparable performance for text output and delivers substantial improvements for speech output, even outperforming the self-cascade approach. These results underscore the effectiveness of post-training in overcoming challenges in direct speech generation in tool-augmented scenarios.</p>\n\n",
                "matched_terms": [
                    "kimi",
                    "tool",
                    "length",
                    "qwen257b",
                    "first",
                    "three",
                    "which",
                    "opuslm",
                    "all",
                    "both",
                    "streaming",
                    "model",
                    "rag",
                    "without",
                    "performance",
                    "ref",
                    "accuracy",
                    "models",
                    "modeltriggered",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> provides a comprehensive breakdown of latency measurements for the Qwen2.5-7B speech-to-speech model, evaluated in both the Open Book setting and our proposed <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> setting on AudioCRAG-Synthetic. We report both median (P50) and 90th percentile (P90) values for each stage of the processing pipeline. The latency is decomposed into three main components: tool query generation, tool result generation, and speech response synthesis, with measurements provided for both the first token outputs (We also provide latency measurements for last token output in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T9\" title=\"Table 9 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>). For both tool query and tool result generation, it is assumed that all tools are accessed in parallel; thus, the reported latency corresponds to the maximum query or result generation time among all tools. The majority of this latency arises from leveraging external web pages, which introduces significant delays, most notably, increasing the first token latency by 2.3x in Open Book Setting.\nNotably, our <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> setting enables early generation of tool results, successfully reducing P50 first token latency by 9.8% and tool use latency by 20.7%.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "qwen257b",
                    "first",
                    "three",
                    "which",
                    "all",
                    "both",
                    "book",
                    "latency",
                    "open",
                    "streaming",
                    "model",
                    "rag",
                    "result",
                    "token",
                    "reported",
                    "setting",
                    "use",
                    "modeltriggered",
                    "audiocragsynthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Streaming RAG approaches</span>: Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> highlights the substantial latency savings enabled by the <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>), compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS1\" title=\"3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>). Three key metrics are evaluated: overall latency savings, the percentage of queries benefiting from reduced latency, and the number of parallel threads required.\nEven without any post-training, the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach already reduces tool usage latency (3.37s in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) by 10.7% for Open Book Qwen-OMNI, demonstrating its flexibility and plug-and-play compatibility with any existing speech-in speech-out model.\nThe <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> method, utilizing Qwen-OMNI, consistently delivers superior performance. It achieves greater average latency reductions and benefits a higher proportion of queries with improved response times. Notably, <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> require only a single parallel thread, representing a significant advancement in resource efficiency compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach, which demands multiple parallel threads.\nThese findings highlight the effectiveness of <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> in not only minimizing latency, validating our intuition from Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>, but also in optimizing system efficiency and resource utilization.</p>\n\n",
                "matched_terms": [
                    "speechout",
                    "three",
                    "tool",
                    "model",
                    "which",
                    "rag",
                    "usage",
                    "only",
                    "percentage",
                    "without",
                    "savings",
                    "book",
                    "latency",
                    "open",
                    "not",
                    "modeltriggered",
                    "performance",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-training Strategies</span>: Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents performance comparison under different post-training conditions. Incorporating streaming tool query generation during post-training (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>) results in comparable performance for both models. These results suggest that <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> can be integrated into post-training process without negatively impacting model performance.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "rag",
                    "models",
                    "without",
                    "both",
                    "modeltriggered",
                    "performance",
                    "comparison",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation Results for Negative Sampling Strategy</span>: Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T5\" title=\"Table 5 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the ablation results evaluating the impact of deliberately injecting negative samples during post-training (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E5\" title=\"Equation 5 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). The findings highlight that, without negative sampling, streaming tool query generation can reduce final accuracy in text output settings, primarily due to errors in final query generation, as detailed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>. In contrast, our negative sampling approach significantly enhances the model&#8217;s robustness, enabling it to recover from intermediate prediction errors. This leads to consistently high accuracy while also achieving notable latency reductions (Tab. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "tool",
                    "settings",
                    "without",
                    "latency",
                    "accuracy",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced the first comprehensive approach for integrating external tool usage directly into E2E speech-in, speech-out dialogue systems. By incorporating <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> pipeline, we further enhanced the model&#8217;s ability to leverage retrieved information and autonomously decide when to trigger new tool queries, resulting in improved accuracy and responsiveness.\nEmpirical evaluation on the newly introduced AudioCRAG benchmark demonstrated that tool integration can more than double factual question answering accuracy compared to closed-book models. Additionally, our streaming RAG approach achieved a 20% reduction in tool usage latency, thereby preserving natural conversational flow.\nOverall, our contributions advance the state of the art in spoken dialogue systems by enabling accurate, real-time, and tool-augmented voice assistants. In addition, we are committed to open science and will release our training code and Audio CRAG Human benchmark to support future research and development in this area.</p>\n\n",
                "matched_terms": [
                    "speechout",
                    "tool",
                    "rag",
                    "usage",
                    "support",
                    "models",
                    "modeltriggered",
                    "latency",
                    "open",
                    "accuracy",
                    "audio",
                    "first",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in benchmarking text-based dialogue systems for tool usage <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib4\" title=\"\">2023b</a>; Ouyang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib24\" title=\"\">2025</a>; Cheng and Dou, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib6\" title=\"\">2025</a>; Cohen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib7\" title=\"\">2025</a>; Xiong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib35\" title=\"\">2024a</a>; Vu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib31\" title=\"\">2023</a>; Xiong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib36\" title=\"\">2024b</a>; Peng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib25\" title=\"\">2024</a>; Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib29\" title=\"\">2025</a>; Ni et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib23\" title=\"\">2025</a>)</cite> have primarily focused on evaluating factual question answering and task completion within simulated environments. The CRAG benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite> is a leading example, featuring 4,409 question-answer pairs and providing mock APIs for both web and knowledge graph (KG) search. CRAG supports a range of KG and web retrieval tasks, and highlights key challenges such as hallucinations in retrieval-augmented generation (RAG) and the importance of leveraging KGs and search ranking to improve factual accuracy. Evaluation is conducted automatically using two LLM judges. SimpleQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib32\" title=\"\">2024</a>)</cite> is another widely adopted benchmark, designed to assess language models on short, fact-seeking questions. With 4,326 adversarially collected questions spanning diverse topics and a straightforward grading scheme based on single, indisputable answers, SimpleQA provides a robust testbed for factual accuracy. Moving beyond question answering, WebArena <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib41\" title=\"\">2024</a>)</cite> offers a simulated environment for evaluating dialogue agents on web-based tasks using fully functional websites, enabling assessment of more complex, action-oriented behaviors. While these benchmarks have significantly advanced the evaluation of tool-augmented dialogue systems, they remain largely limited to text-based interactions and do not fully address the unique challenges presented by speech-in speech-out systems.</p>\n\n",
                "matched_terms": [
                    "speechout",
                    "tool",
                    "rag",
                    "usage",
                    "models",
                    "both",
                    "not",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent benchmarks <cite class=\"ltx_cite ltx_citemacro_cite\">Mei et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib19\" title=\"\">2025</a>); Yu et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib39\" title=\"\">2025</a>); Luo et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib16\" title=\"\">2024</a>)</cite> have extended tool-augmented dialogue evaluation to multimodal and longer-context scenarios. The m&amp;m&#8217;s benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib17\" title=\"\">2024</a>)</cite> evaluates LLMs on multi-step, multi-modal tasks using a diverse set of 33 tools, including public APIs and multimodal models such as off-the-shelf automatic speech recognition (ASR) models, highlighting the potential for developing agents that leverage audio-based tools.\nCRAG_MM <cite class=\"ltx_cite ltx_citemacro_citep\">(Meta CRAG-MM Challenge Organizers, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib21\" title=\"\">2025</a>)</cite> builds on the original CRAG benchmark by introducing visual question answering (QA) tasks that combine images and text-based queries, utilizing mock APIs for both image descriptions and web search.\nFor video understanding and long-context reasoning, the Video Web Arena <cite class=\"ltx_cite ltx_citemacro_citep\">(Jang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib14\" title=\"\">2025</a>)</cite> benchmark evaluates multimodal agents on tasks involving 2,021 manually crafted tutorial videos.\nWhile these benchmarks advance the field by incorporating multimodal tools, they still do not evaluate systems in speech-in speech-out scenarios.</p>\n\n",
                "matched_terms": [
                    "speechout",
                    "evaluate",
                    "models",
                    "both",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We begin with the CRAG dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>, which contains 2,706 text queries. Since these queries are not directly suitable for speech-based evaluation, we first identify those requiring adaptation before TTS conversion. Through careful manual inspection, we determine that queries containing dates or brackets benefit from rewriting to ensure naturalness and clarity in spoken form. In total, we identify 569 such queries and rewrite them using a large language model (LLAMA-4 Maverick). The resulting 569 rewritten queries, combined with the remaining original queries, form the basis for our spoken version of CRAG, which we term <em class=\"ltx_emph ltx_font_italic\">AudioCRAG</em>. We follow the CRAG setup to incorporate web and KG-based tools and adopt its robust evaluation setup, as detailed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S10\" title=\"10 Tool Usage Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S11\" title=\"11 Evaluation Setting &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "not",
                    "model",
                    "which",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio CRAG Synthetic</span>:\nTo generate spoken queries, we process all 2,706 queries through our in-house TTS system. We then apply a rigorous filtering procedure to remove queries with intelligibility issues, specifically excluding any utterances for which Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib27\" title=\"\">2023</a>)</cite> hypotheses exhibit a non-zero word error rate. We also remove utterances with suboptimal audio quality, as determined by UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib28\" title=\"\">2022</a>)</cite> scores below 3.5. This results in a high-quality set of 1,862 spoken queries, which we refer to as the <em class=\"ltx_emph ltx_font_italic\">AudioCRAG-Synthetic</em> benchmark.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "all",
                    "audiocragsynthetic",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio CRAG Human</span>:\nTo further enhance the realism and diversity of our evaluation, we introduce the AudioCRAG-Human benchmark, which consists of 618 human-recorded spoken queries. These queries are recorded by a diverse pool of participants to capture natural variations in speech, accent, and prosody. The inclusion of human-recorded audio enables a more comprehensive assessment of speech-in speech-out systems under real-world conditions, providing valuable insights into model robustness and generalization beyond synthetic speech. This benchmark serves as a critical resource for evaluating the effectiveness of tool integration in conversational AI systems.</p>\n\n",
                "matched_terms": [
                    "speechout",
                    "tool",
                    "model",
                    "which",
                    "audiocraghuman",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable effective tool usage, we build upon the CRAG framework by integrating two complementary information sources: web search, which provides access to fresh and dynamic content, and a knowledge graph, which offers structured and reliable information. For web search, we aggregate all 100,000 documents from the CRAG corpus and employ a BGE-based re-ranker<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/BAAI/bge-large-en-v1.5\" title=\"\">https://huggingface.co/BAAI/bge-large-en-v1.5</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Xiao et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib33\" title=\"\">2023</a>)</cite> to index and retrieve the top 50 most relevant documents for each query. These documents are then segmented into chunks and re-ranked using the same BGE model based on their similarity to the query, ensuring highly contextually relevant retrieval. Meanwhile, queries to the knowledge graph are performed via a simulated API, adhering to the methodology established in CRAG <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/facebookresearch/CRAG/tree/main/mock_api\" title=\"\">https://github.com/facebookresearch/CRAG/tree/main/mock_api</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "which",
                    "usage",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qwen-OMNI</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>)</cite> is a end-to-end multimodal model that seamlessly integrates diverse input modalities&#8212;including text, images, audio, and video&#8212;and generates both text and natural speech responses in a real-time streaming fashion. It leverages an innovative Thinker-Talker architecture, where the Thinker module performs high-level reasoning to produce a text response, which is then used by the Talker module, conditioning on both the text and the Thinker&#8217;s hidden representations, to generate streaming speech output.</p>\n\n",
                "matched_terms": [
                    "model",
                    "which",
                    "both",
                    "audio",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OpusLM</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib30\" title=\"\">2025</a>)</cite> is an open-source speech-in, speech-out model post-trained to directly answer complex semantic and factual questions from raw audio inputs, through Chain-of-Thought reasoning.</p>\n\n",
                "matched_terms": [
                    "speechout",
                    "opuslm",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Kimi Audio</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib8\" title=\"\">2025</a>)</cite> is a universal audio foundation model that unifies audio understanding, generation, and conversational abilities within a single framework. Pre-trained on over 13 million hours of diverse audio and text data, Kimi Audio achieves state-of-the-art performance across a wide range of audio benchmarks, including audio understanding and speech conversation tasks.</p>\n\n",
                "matched_terms": [
                    "kimi",
                    "performance",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For tool-augmented scenarios, retrieval results are provided up to each model&#8217;s maximum token limit (&#8220;Ref length&#8221; in Tables), maintaining a 2:1 ratio of web page to KG results.\nSpecifically, we observe that Kimi-Audio is currently optimized for handling tool result references up to a certain length. When this limit is exceeded, an error arises during the audio detokenization process, specifically within the rotary embedding mechanism, highlighting an architectural constraint in processing longer input sequences or larger reference contexts. Addressing this limitation presents a valuable opportunity for future model enhancements.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "references",
                    "currently",
                    "result",
                    "token",
                    "length",
                    "kimiaudio",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation encompasses both speech-in-text-out and speech-in-speech-out scenarios. For the Fixed-Interval Streaming RAG setting (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS1\" title=\"3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>), intermediate tool queries are generated at consistent 1-second intervals. In the Model-Triggered Streaming RAG setting, the model dynamically determines the need for a tool call after processing each 500ms block. This approach allows us to utilize a smaller chunk size, as only a single tool call thread is required for Model-Triggered Streaming RAG, thereby enabling more efficient and responsive processing.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "rag",
                    "setting",
                    "both",
                    "modeltriggered",
                    "only",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This subsection details the experimental setup for post-training the pretrained speech-in, speech-out model to significantly enhance its tool usage capabilities, as outlined in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>.\nWe leverage a random subset of 16,000 questions from the text-based factual question answering dataset TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib15\" title=\"\">2017</a>)</cite>, which contains 97,000 questions and 662,659 associated web documents.\nFor <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, we further compute word-level timestamps using a pre-trained ASR model, OWSM CTC v4 1B <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib26\" title=\"\">2025</a>)</cite>, enabling us to generate partial ASR transcripts <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m1\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math> at 500 ms intervals. Note that if word occurs at boundary of block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, it is excluded from <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math>. For each partial transcript <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math>, we generate corresponding psuedo ground truth queries <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m5\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> using LLAMA-4-Maverick to simulate incremental user input.\nTo simulate realistic tool usage, we concatenate all documents and employ a web query reranker to retrieve the top 50 most relevant documents for each query. The text questions from this 16k subset are converted into discrete speech tokens using text-to-speech synthesis with the corresponding pretrained speech-in, speech-out model. Recognizing that TriviaQA answers are typically single named entities, we further transform the queries into a conversational style using LLAMA-4-Maverick, making them more suitable for dialogue-based evaluation.</p>\n\n",
                "matched_terms": [
                    "speechout",
                    "tool",
                    "model",
                    "which",
                    "rag",
                    "usage",
                    "all",
                    "modeltriggered",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT = \"\"\"\nYou are given an Audio Question, References and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". The references may or may not help answer the question. Your task is to answer the question in as few words as possible.\n<br class=\"ltx_break\"/>Please follow these guidelines when formulating your answer:\n<br class=\"ltx_break\"/>1. If the question contains a false premise or assumption, answer &#8220;invalid question&#8221;.\n<br class=\"ltx_break\"/>2. If you are uncertain or don&#8217;t know the answer, respond with &#8220;I don&#8217;t know&#8221;.\n<br class=\"ltx_break\"/>### Question\n<br class=\"ltx_break\"/>{query}\n<br class=\"ltx_break\"/>### Query Time\n<br class=\"ltx_break\"/>{query_time}\n<br class=\"ltx_break\"/>### References\n<br class=\"ltx_break\"/># web\n<br class=\"ltx_break\"/>{web_results}\n<br class=\"ltx_break\"/># knowledge graph\n<br class=\"ltx_break\"/>{kg_response}\n<br class=\"ltx_break\"/>### Answer\n<br class=\"ltx_break\"/>\"\"\"</p>\n\n",
                "matched_terms": [
                    "audio",
                    "references",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT = \"\"\"\nYou are an agent that only outputs JSON. You are given a Query and Query Time. Do the following:\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>1) Determine the domain the query is about. The domain should be one of the following: \"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". If none of the domains apply, use \"other\". Use \"domain\" as the key in the result json.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>2) Extract structured information from the query. Include different keys into the result json depending on the domains, and put them DIRECTLY in the result json. Here are the rules:\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;encyclopedia&#8217; and &#8216;other&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;main_entity&#8217;: extract the main entity of the query.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;finance&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;market_identifier&#8217;: stock identifiers including individual company names, stock symbols.\n<br class=\"ltx_break\"/>- &#8216;metric&#8217;: financial metrics that the query is asking about. This must be one of the following: &#8216;price&#8217;, &#8216;dividend&#8217;, &#8216;P/E ratio&#8217;, &#8216;EPS&#8217;, &#8216;marketCap&#8217;, and &#8216;other&#8217;.\n<br class=\"ltx_break\"/>- &#8216;datetime&#8217;: time frame that the query asks about. When datetime is not explicitly mentioned, use &#8216;Query Time&#8217; as default.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;movie&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;movie_name&#8217;: name of the movie\n<br class=\"ltx_break\"/>- &#8216;movie_aspect&#8217;: if the query is about a movie, which movie aspect the query asks. This must be one of the following: &#8216;budget&#8217;, &#8216;genres&#8217;, &#8216;original_language&#8217;, &#8216;original_title&#8217;, &#8216;release_date&#8217;, &#8216;revenue&#8217;, &#8216;title&#8217;, &#8216;cast&#8217;, &#8216;crew&#8217;, &#8216;rating&#8217;, &#8216;length&#8217;.\n<br class=\"ltx_break\"/>- &#8216;person&#8217;: person name related to moves\n<br class=\"ltx_break\"/>- &#8216;person_aspect&#8217;: if the query is about a person, which person aspect the query asks. This must be one of the following: &#8216;acted_movies&#8217;, &#8216;directed_movies&#8217;, &#8216;oscar_awards&#8217;, &#8216;birthday&#8217;.\n<br class=\"ltx_break\"/>- &#8216;year&#8217;: if the query is about movies released in a specific year, extract the year\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;music&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;artist_name&#8217;: name of the artist\n<br class=\"ltx_break\"/>- &#8216;artist_aspect&#8217;: if the query is about an artist, extract the aspect of the artist. This must be one of the following: &#8216;member&#8217;, &#8216;birth place&#8217;, &#8216;birth date&#8217;, &#8216;lifespan&#8217;, &#8216;artist work&#8217;, &#8216;grammy award count&#8217;, &#8216;grammy award date&#8217;.\n<br class=\"ltx_break\"/>- &#8216;song_name&#8217;: name of the song\n<br class=\"ltx_break\"/>- &#8216;song_aspect&#8217;: if the query is about a song, extract the aspect of the song. This must be one of the following: &#8216;author&#8217;, &#8216;grammy award count&#8217;, &#8216;release country&#8217;, &#8216;release date&#8217;.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;sports&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;sport_type&#8217;: one of &#8216;basketball&#8216;, &#8216;soccer&#8216;, &#8216;other&#8216;\n<br class=\"ltx_break\"/>- &#8216;tournament&#8217;: NBA, World Cup, Olympic.\n<br class=\"ltx_break\"/>- &#8216;team&#8217;: teams that users are interested in.\n<br class=\"ltx_break\"/>- &#8216;datetime&#8217;: time frame that the user is interested in. When datetime is not explicitly mentioned, use &#8216;Query Time&#8217; as default.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Return the results in a FLAT json.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n<br class=\"ltx_break\"/>\"\"\"</p>\n\n",
                "matched_terms": [
                    "which",
                    "result",
                    "use",
                    "not",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT = \"\"\"\nYou are an agent that only outputs JSON. You are given an Audio\nQuery, Previously generated JSON result (&#8217;Previous Result&#8217;) and Query Time. Do the following:</p>\n\n",
                "matched_terms": [
                    "only",
                    "audio",
                    "result"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">1) Determine the domain the query is about. The domain should be one of the following:\nf&#776;inance,&#776; s&#776;ports,&#776; m&#776;usic,&#776; m&#776;ovie,&#776; &#235;ncyclopedia.&#776; If none of the domains apply, use &#246;ther.&#776; Use\nd&#776;omain&#228;s the key in the result json.</p>\n\n",
                "matched_terms": [
                    "use",
                    "result"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8216;finance&#8217; queries, these are possible keys:\n- &#8216;market_identifier&#8217;: stock identifiers including individual company names, stock symbols.\n- &#8216;metric&#8217;: financial metrics that the query is asking about. This must be one of the following: &#8216;price&#8217;,\n&#8216;dividend&#8217;, &#8216;P/E ratio&#8217;, &#8216;EPS&#8217;, &#8216;marketCap&#8217;, and &#8216;other&#8217;.\n- &#8216;datetime&#8217;: time frame that the query asks about. When datetime is not explicitly mentioned, use\n&#8216;Query Time&#8217; as default.</p>\n\n",
                "matched_terms": [
                    "use",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8216;sports&#8217; queries, these are possible keys:\n- &#8216;sport_type&#8217;: one of &#8216;basketball&#8216;, &#8216;soccer&#8216;, &#8216;other&#8216;\n- &#8216;tournament&#8217;: NBA, World Cup, Olympic.\n- &#8216;team&#8217;: teams that users are interested in.\n- &#8216;datetime&#8217;: time frame that the user is interested in. When datetime is not explicitly mentioned, use\n&#8216;Query Time&#8217; as default.\nReturn the results in a FLAT json.\n<br class=\"ltx_break\"/>*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*</p>\n\n",
                "matched_terms": [
                    "use",
                    "only",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">3) Compare your newly generated result to the &#8217;Previous Result&#8217;. **If your new result would be exactly the same as the &#8217;Previous Result&#8217;, output only NO_QUERY.**\nReturn the results in a FLAT json.</p>\n\n",
                "matched_terms": [
                    "only",
                    "result"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT = \"\"\" You are given an Audio Query and Query Time. Your task is to generate a web query that can be used to retrieve relevant web pages. Rewrite the following query into a short and succinct form, focusing on the main topic or domain (e.g. finance, sports, music, movie, encyclopedia), key entities mentioned (e.g. people, organizations, locations), and specific aspects of those entities (e.g. performance metrics, relationships, events). Ensure the rewritten query is clear, concise, and easy to understand.\nNote that simply outputting the original query is not acceptable. You must rephrase the query to make it more concise and focused on the key information that will help retrieve relevant web pages.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "performance",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8217;sports&#8217; queries, focus on:\n- Sports Type (eg. basketball, soccer)\n- Teams, players\n- Statistics or performance metrics (e.g. scores, wins, losses)\n- Specific events or tournaments (eg. NBA, World Cup, Olympic)\n- Time frame that the user is interested in; if no timeframe is specified, use the Query Time as default</p>\n\n",
                "matched_terms": [
                    "performance",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When rewriting the query, ensure that it captures all important information from the original question that could impact the retrieval results. Do not omit any crucial details, such as specific dates, locations, or relationships between entities. Also, do not invent any new details on your own. If necessary, use the Query Time to provide context for the query. The goal is to create a concise and accurate query that effectively conveys the user&#8217;s intent and retrieves relevant information.\n*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT THE REWRITTEN QUERY!!!*\n\"\"\"</p>\n\n",
                "matched_terms": [
                    "only",
                    "all",
                    "not",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8217;sports&#8217; queries, focus on:\n- Sports Type (eg. basketball, soccer)\n- Teams, players\n- Statistics or performance metrics (e.g. scores, wins, losses)\n- Specific events or tournaments (eg. NBA, World Cup, Olympic)\n- Time frame that the user is interested in; if no timeframe is specified, use the Query Time as default</p>\n\n",
                "matched_terms": [
                    "performance",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When rewriting the query, ensure that it captures all important information from the original question that could impact the retrieval results. Do not omit any crucial details, such as specific dates, locations, or relationships between entities. Also, do not invent any new details on your own. If necessary, use the Query Time to provide context for the query. The goal is to create a concise and accurate query that effectively conveys the user&#8217;s intent and retrieves relevant information.\nNow, compare the new web query to the previously generated web query (&#8217;Previous Result&#8217;).</p>\n\n",
                "matched_terms": [
                    "use",
                    "all",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT=\"\"\"\n<br class=\"ltx_break\"/>Assume you are a human expert in grading predictions given by a model. You are given a question and a model prediction. Judge if the prediction matches the ground truth answer by following these steps:\n1: Take it as granted that the Ground Truth is always correct.\n2: If the Prediction indicates it is not sure about the answer, \"score\" should be \"0\"; otherwise, go the next step.\n3: If the Prediction exactly matches the Ground Truth, \"score\" is 1.\n4: If the Prediction does not exactly match the Ground Truth, go through the following steps and likely give a score as 0.\n5: If the Ground Truth is a number, \"score\" is 1 if and only if the Prediction gives a number that almost exactly matches the ground truth.\n6: If the Prediction is self-contradictory, \"score\" must be 0.\n7: If the prediction is not answering the question, \"score\" must be 0.\n8: If the prediction is a concise and correct summary of the ground truth, \"score\" is 1.\n9: If ground truth contains a set of items, prediction must contain exactly same items for the score to be 1.\n10: Otherwise, \"score\" is 0.</p>\n\n",
                "matched_terms": [
                    "only",
                    "model",
                    "not",
                    "does"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">You should make the judgment based on provided examples.\nExamples:\nQuestion: \"which company has higher eps, btu or cma?\"\nGround Truth: \"cma\"\nPrediction: \"it is not possible to determine which company has a higher eps.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction is not sure about the answer.\"</p>\n\n",
                "matched_terms": [
                    "not",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the height of the building where citigroup is headquartered?\"\nGround Truth: \"151 m\"\nPrediction: \"the height is 915 feet (279 m).\"\nOutput: \"score\": 0, \"explanation\": \"The prediction, 151 m, does not match the ground truth, 279 m.\"</p>\n\n",
                "matched_terms": [
                    "not",
                    "does"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who were the last 3 prime ministers of uk?\"\nGround Truth: \"rishi sunak, liz truss, and boris johnson\"\nPrediction: \"1. liz truss, 2. boris johnson, 3.theresa may\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not contain item, rishi sunak, that is in the grount truth.\"</p>\n\n",
                "matched_terms": [
                    "does",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"on which days did xxx distribute dividends in the last year?\"\nGround Truth: \"2023-01-13, 2023-03-25, 2023-11-21\"\nPrediction: \"xxx distributed dividends on 1. 2023-01-13, 2. 2023-03-25, 3. 2023-10-21.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction contains an item, 2023-10-21, which is not in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "not",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the current market cap of appl?\"\nGround Truth: \"2.81 trillion\"\nPrediction: \"2.667 trillion\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the number in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "not",
                    "does"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the current pe ratio of appl?\"\nGround Truth: \"28.3\"\nPrediction: \"the current pe ratio of apple is 26.66\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the number in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "not",
                    "does"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how much is tesla&#8217;s stock price down from its all-time high?\"\nGround Truth: \"$221.83\"\nPrediction: \"209.52\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the number in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "does",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the length of amazon river?\"\nGround Truth: \"over 4000 miles\"\nPrediction: \"the length of amazon river is 4,000 miles\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not say Amazon River is longer than 4000 miles.\"</p>\n\n",
                "matched_terms": [
                    "not",
                    "does",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how many copies x were sold?\"\nGround Truth: \"2 million.\"\nPrediction: \"it is over 2 million\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "not",
                    "does"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the population of country x?\"\nGround Truth: \"3,576,873\"\nPrediction: \"the population of country x is 3.3 million.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction, 3.3 M, does not match the number, 3.6 M, in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "not",
                    "does"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the current market value of stock x?\"\nGround Truth: \"$2,237,578,268\"\nPrediction: \"$2.16 billion.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction, 2.16 B, does not match the number, 2.2 B, in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "not",
                    "does"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how old is michael jordan?\"\nGround Truth: \"61\"\nPrediction: \"mj was born on February 17, 1963.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not explicitly say the age.\"</p>\n\n",
                "matched_terms": [
                    "not",
                    "does"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how long did amy teach as a teacher?\"\nGround Truth: \"24 years and 255 days\"\nPrediction: \"25 years.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not exactly match the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "does",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"at what age did usher perform in the half time show of superbowl?\"\nGround Truth: \"45 years old\"\nPrediction: \"usher has not performed at a super bowl halftime show.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "does",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who is younger, timothee chalamet or tom holland?\"\nGround Truth: \"tom holland\"\nPrediction: \"timoth&#233;e chalamet is younger than tom holland.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "not",
                    "does"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is xxx&#8217;s birthdate?\"\nGround Truth: \"1996-01-01.\"\nPrediction: \"02/01/1996\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "not",
                    "does"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how much has spotify&#8217;s user base increased by since 2020 in na?\"\nGround Truth: \"spotify&#8217;s user base increased by 34 million since 2020.\"\nPrediction: \"spotify&#8217;s north american user base increased from 36 million in 2020 to 85 million by 2021\"\nOutput: \"score\": 0, \"explanation\": \"The prediction is not answering the question as it only gives the increase from 2020 to 2021.\"\n<br class=\"ltx_break\"/>\"\"\"</p>\n\n",
                "matched_terms": [
                    "only",
                    "not"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage",
        "caption": "Table 2: Results on AudioCRAG-Synthetic for Qwen2.5-7B, OpusLM, and Kimi Audio comparing text vs. speech output across Closed Book, Open Book, and Model-Triggered Streaming RAG.",
        "body": "Setting\nRef length\nOutput\nModel\nAcc.\n\n\nClosed Book\n0\nText\nQwen2.5-7B\n15.0\n\n\n\n0\nSpeech\nQwen2.5-7B\n11.1\n\n\n\n0\nText\nOpusLM\n20.1\n\n\n\n0\nSpeech\nOpusLM\n18.4\n\n\n\n0\nText\nKimi Audio\n24.2\n\n\n\n0\nSpeech\nKimi Audio\n16.7\n\n\nOpen Book\n23K\nText\nQwen2.5-7B\n39.6\n\n\n(S. 3.1)\n23K\nSpeech\nQwen2.5-7B\n26.3\n\n\n\n23K\nSpeech (self-cascade)\nQwen2.5-7B\n33.8\n\n\n\n15K\nText\nOpusLM\n26.3\n\n\n\n15K\nSpeech\nOpusLM\n0.0\n\n\n\n15K\nSpeech (self-cascade)\nOpusLM\n21.2\n\n\n\n5K\nText\nKimi Audio\n45.8\n\n\n\n500\nSpeech\nKimi Audio\n21.8\n\n\nStreaming RAG\n23K\nText\nQwen2.5-7B\n39.8\n\n\n(S. 3.2.2)\n23K\nSpeech\nQwen2.5-7B\n34.2\n\n\n\n15K\nText\nOpusLM\n29.7\n\n\n\n15K\nSpeech\nOpusLM\n23.6",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">Setting</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">Ref length</th>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Output</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Model</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Acc.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Closed Book</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">0</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Text</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Qwen2.5-7B</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">15.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">0</th>\n<td class=\"ltx_td ltx_align_left\">Speech</td>\n<td class=\"ltx_td ltx_align_left\">Qwen2.5-7B</td>\n<td class=\"ltx_td ltx_align_left\">11.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">0</th>\n<td class=\"ltx_td ltx_align_left\">Text</td>\n<td class=\"ltx_td ltx_align_left\">OpusLM</td>\n<td class=\"ltx_td ltx_align_left\">20.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">0</th>\n<td class=\"ltx_td ltx_align_left\">Speech</td>\n<td class=\"ltx_td ltx_align_left\">OpusLM</td>\n<td class=\"ltx_td ltx_align_left\">18.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">0</th>\n<td class=\"ltx_td ltx_align_left\">Text</td>\n<td class=\"ltx_td ltx_align_left\">Kimi Audio</td>\n<td class=\"ltx_td ltx_align_left\">24.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">0</th>\n<td class=\"ltx_td ltx_align_left\">Speech</td>\n<td class=\"ltx_td ltx_align_left\">Kimi Audio</td>\n<td class=\"ltx_td ltx_align_left\">16.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Open Book</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">23K</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Text</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Qwen2.5-7B</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">39.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">(S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>)</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">23K</th>\n<td class=\"ltx_td ltx_align_left\">Speech</td>\n<td class=\"ltx_td ltx_align_left\">Qwen2.5-7B</td>\n<td class=\"ltx_td ltx_align_left\">26.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">23K</th>\n<td class=\"ltx_td ltx_align_left\">Speech (self-cascade)</td>\n<td class=\"ltx_td ltx_align_left\">Qwen2.5-7B</td>\n<td class=\"ltx_td ltx_align_left\">33.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">15K</th>\n<td class=\"ltx_td ltx_align_left\">Text</td>\n<td class=\"ltx_td ltx_align_left\">OpusLM</td>\n<td class=\"ltx_td ltx_align_left\">26.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">15K</th>\n<td class=\"ltx_td ltx_align_left\">Speech</td>\n<td class=\"ltx_td ltx_align_left\">OpusLM</td>\n<td class=\"ltx_td ltx_align_left\">0.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">15K</th>\n<td class=\"ltx_td ltx_align_left\">Speech (self-cascade)</td>\n<td class=\"ltx_td ltx_align_left\">OpusLM</td>\n<td class=\"ltx_td ltx_align_left\">21.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">5K</th>\n<td class=\"ltx_td ltx_align_left\">Text</td>\n<td class=\"ltx_td ltx_align_left\">Kimi Audio</td>\n<td class=\"ltx_td ltx_align_left\">45.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">500</th>\n<td class=\"ltx_td ltx_align_left\">Speech</td>\n<td class=\"ltx_td ltx_align_left\">Kimi Audio</td>\n<td class=\"ltx_td ltx_align_left\">21.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Streaming RAG</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">23K</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Text</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Qwen2.5-7B</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">39.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">(S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>)</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">23K</th>\n<td class=\"ltx_td ltx_align_left\">Speech</td>\n<td class=\"ltx_td ltx_align_left\">Qwen2.5-7B</td>\n<td class=\"ltx_td ltx_align_left\">34.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">15K</th>\n<td class=\"ltx_td ltx_align_left\">Text</td>\n<td class=\"ltx_td ltx_align_left\">OpusLM</td>\n<td class=\"ltx_td ltx_align_left\">29.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_bb\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">15K</th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Speech</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">OpusLM</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">23.6</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "kimi",
            "text",
            "closed",
            "output",
            "qwen257b",
            "length",
            "speech",
            "across",
            "opuslm",
            "book",
            "open",
            "23k",
            "results",
            "streaming",
            "acc",
            "model",
            "rag",
            "ref",
            "comparing",
            "selfcascade",
            "setting",
            "15k",
            "modeltriggered",
            "audiocragsynthetic",
            "audio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Evaluated SOTA Speech-in Speech-out Models &#8227; 4 Experiment Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides a comparative evaluation of the three SOTA models in generating either text or speech outputs from speech inputs, both with and without the integration of external tool results. In the absence of tool results (Ref length = 0), all models achieve higher accuracy when generating text outputs compared to speech outputs. The incorporation of tool results generally leads to improved text generation performance, with Kimi Audio achieving the highest accuracy. In contrast, the accuracy for speech output remains consistently lower across all models and conditions. The self-cascade approach, in which the model first generates an intermediate text response before producing the final speech output, provides moderate improvements in speech output accuracy for both Qwen2.5-7B and OpusLM.\nHowever, it still underperforms compared to the text-out baseline, primarily due to errors in accurately generating answers involving uncommon entity nouns.\nOverall, these findings underscore a persistent challenge in direct speech generation, particularly when tool results are integrated, as this appears to negatively impact the quality of generated speech responses.\nOur <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> demonstrates clear advantages: it maintains comparable performance for text output and delivers substantial improvements for speech output, even outperforming the self-cascade approach. These results underscore the effectiveness of post-training in overcoming challenges in direct speech generation in tool-augmented scenarios.</p>\n\n",
            "<p class=\"ltx_p\">Similar to previous work <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>, we employ model-based automatic evaluation. We use a three-way scoring system, assigning scores of 1, -1, and 0 for accurate, incorrect, and missing answers, respectively. The evaluation is conducted using the Llama 4-maverick LLM evaluator. For speech outputs, we first transcribe the audio using Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib27\" title=\"\">2023</a>)</cite> before passing the transcriptions to the LLM evaluator. In this study, our primary focus is on enhancing system accuracy; therefore, we report average accuracy values in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Evaluated SOTA Speech-in Speech-out Models &#8227; 4 Experiment Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. For additional context, we also provide the average rates of accurate, hallucinated, and missing responses, as well as overall truthfulness scores for each system in the Open Book Setting (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S10.T6\" title=\"Table 6 &#8227; 10 Tool Usage Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>). Notably, our results indicate that Qwen-OMNI was less likely to generate hallucinated responses compared to OpusLM and Kimi Audio.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">End-to-end speech-in speech-out dialogue systems are emerging as a powerful alternative to traditional ASR&#8211;LLM&#8211;TTS pipelines, generating more natural, expressive responses with significantly lower latency. However, these systems remain prone to hallucinations due to limited factual grounding. While text-based dialogue systems address this challenge by integrating tools such as web search and knowledge graph APIs, we introduce the first approach to extend tool use directly into speech-in speech-out systems. A key challenge is that tool integration substantially increases response latency, disrupting conversational flow. To mitigate this, we propose Streaming Retrieval-Augmented Generation (<em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>), a novel framework that reduces user-perceived latency by predicting tool queries in parallel with user speech, even before the user finishes speaking. Specifically, we develop a post-training pipeline that teaches the model when to <em class=\"ltx_emph ltx_font_italic\">issue tool calls</em> during ongoing speech and how to generate spoken summaries that fuse audio queries with retrieved text results, thereby improving both <em class=\"ltx_emph ltx_font_italic\">accuracy and responsiveness</em>. To evaluate our approach, we construct AudioCRAG, a benchmark created by converting queries from the publicly available CRAG dataset into speech form. Experimental results demonstrate that our <em class=\"ltx_emph ltx_font_italic\">streaming RAG</em> approach increases QA accuracy by up to 200% relative (from 11.1% to 34.2% absolute) and further enhances user experience by reducing tool use latency by 20%. Importantly, our <em class=\"ltx_emph ltx_font_italic\">streaming RAG</em> approach is modality-agnostic and can be applied equally to typed input, paving the way for more agentic, real-time AI assistants.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "rag",
                    "text",
                    "results",
                    "audio",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spoken Dialogue Systems (SDS) are foundational to many everyday technologies, powering intelligent assistants such as Alexa and Siri, as well as interactive voice response systems in customer service. With the rapid expansion of SDS capabilities to mobile phones and wearable devices, the need for robust, scalable, and generalizable solutions has never been greater. Traditionally, SDS have relied on cascaded pipelines composed of multiple modules&#8212;including voice activity detection (VAD), automatic speech recognition (ASR), natural language understanding (NLU), natural language generation (NLG), and text-to-speech (TTS) synthesis&#8212;each introducing potential points of failure and latency <cite class=\"ltx_cite ltx_citemacro_citep\">(Glass, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib12\" title=\"\">1999</a>; Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib13\" title=\"\">2024</a>)</cite>.\nRecently, end-to-end (E2E) SDS <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib34\" title=\"\">2024</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib22\" title=\"\">2023</a>; Meng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib20\" title=\"\">2024</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib40\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib2\" title=\"\">2025b</a>)</cite> have been proposed, which directly generate spoken responses from speech input within a unified architecture. This E2E approach not only mitigates error propagation across modules but also captures non-phonemic information more effectively, resulting in significantly lower inference time and computational overhead, and paving the way for more natural and efficient conversational experiences.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present, to the best of our knowledge, the first <span class=\"ltx_text ltx_font_italic\">speech-in, speech-out language model that seamlessly integrates external tool invocation with low latency</span>. The key idea is a <span class=\"ltx_text ltx_font_bold\">Streaming RAG</span> strategy that generates tool queries in parallel with user speech, often times even before the user has finished speaking (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). A naive implementation of streaming queries, however, faces two challenges: (1) queries issued from partial speech may be suboptimal, yielding distracting tool outputs and inaccurate responses; and (2) unnecessary tool calls may be triggered, wasting computational resources. We introduce effective modeling techniques to address these challenges and make the following contributions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "rag",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution 2</span>: To address this, we propose <em class=\"ltx_emph ltx_font_italic\">Streaming Retrieval-Augmented Generation</em> (<em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>), the <em class=\"ltx_emph ltx_font_italic\">first</em> framework that empowers the system to trigger tool queries in parallel with user speech, even before the user finishes speaking. Within this framework, we introduce two novel approaches: (1) <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em>, which issues tool queries at regular intervals during speech input and carefully examines quality of retrieval results on the full query to guarantee response quality, and can be incorporated into any speech-in, speech-out model without post-training; (2) <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, which post-trains the model to intelligently determine optimal query timing based on the evolving user utterance to save computation resources.\nOur results demonstrate that our proposed <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> delivers over 200% relative improvement in accuracy (from 11.1% to 34.2% absolute in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) compared to the no-tool baseline, while also reducing tool result generation latency by 20%. Though designed for speech-in speech-out systems, streaming RAG can also be adapted in cascaded SDS, or even chatbots as users type.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "rag",
                    "modeltriggered",
                    "results",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several E2E spoken dialogue systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>; Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib34\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib1\" title=\"\">2025a</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib22\" title=\"\">2023</a>; Meng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib20\" title=\"\">2024</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib40\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib2\" title=\"\">2025b</a>)</cite> have recently been introduced, demonstrating impressive semantic understanding and high audio quality in their responses. However, these systems have not yet been trained or evaluated for their ability to <em class=\"ltx_emph ltx_font_italic\">use external tools</em>.\nAnother research direction <cite class=\"ltx_cite ltx_citemacro_citep\">(Feng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib9\" title=\"\">2025</a>)</cite> explores E2E RAG for direct speech-to-text retrieval, utilizing multimodal embeddings for enabling speech utterances to directly retrieve relevant text. While this method outperforms models without RAG, it is primarily limited to <em class=\"ltx_emph ltx_font_italic\">speech-to-text</em> scenarios. Its ability to access KGs and other APIs, critical for real-world applications, remains unproven. Furthermore, the retrieval scope is limited, as experiments are conducted with retrieval restricted to just 10 paragraphs, whereas modern RAG benchmarks require searching across thousands of web pages.\nAlthough recent studies <cite class=\"ltx_cite ltx_citemacro_citep\">(Maben et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib18\" title=\"\">2025</a>)</cite> have developed web interfaces that integrate tools into speech-in <em class=\"ltx_emph ltx_font_italic\">speech-out</em> scenarios using cascaded pipelines, comprehensive empirical investigations of E2E speech-in speech-out systems and, crucially, systematic analyses of user-perceived latency, remain largely unexplored. In this work, we address these gaps by developing a comprehensive framework for tool integration in E2E speech-in speech-out systems and designing benchmarks to quantitatively assess the tool usage capabilities of SOTA models. Furthermore, recognizing the importance of latency for natural conversational flow, we introduce novel streaming RAG methods that not only enhance tool usage performance but also reduce user-perceived latency.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "rag",
                    "across",
                    "text",
                    "audio",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A RAG spoken conversation system takes an audio question <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math> as input and outputs a spoken answer <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>. Let the ASR transcript of audio question be <math alttext=\"X^{\\text{asr}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>asr</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}</annotation></semantics></math> and the ASR transcript of the spoken answer be <math alttext=\"X^{\\text{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>res</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{res}}</annotation></semantics></math>.\nAnswers are generated by speech-in speech-out models, leveraging both the model&#8217;s internal knowledge and information retrieved from external sources.\nTo incorporate external information, the model needs to formulate a tool query <math alttext=\"Q^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><msup><mi>Q</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">Q^{T}</annotation></semantics></math> to retrieve relevant results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m6\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from an external tool <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m7\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "audio",
                    "rag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our proposed formulation for integrating external tools into speech-in speech-out systems. We introduce a two-stage inference approach: <em class=\"ltx_emph ltx_font_italic\">Query Generation</em> and <em class=\"ltx_emph ltx_font_italic\">Response Generation</em>. In the Query Generation stage, the system processes an audio question and generates queries for each external tool to retrieve relevant information by maximizing the posterior distribution <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> (Examples of generated queries are provided in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T7\" title=\"Table 7 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> in the Appendix.). In the Response Generation stage, the retrieved results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from these tools are combined with the original audio question and input into the model to generate the final spoken response by maximizing the posterior distribution <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math>. By conditioning the final output generation on the input audio, this formulation not only provides a simple and effective mechanism for interacting with text-based APIs, but also preserves the key advantages of speech-in speech-out systems, mitigating error propagation and enabling the model to capture non-phonemic information (such as prosody and speaker intent) more effectively.</p>\n\n",
                "matched_terms": [
                    "model",
                    "output",
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">RAG-based systems, as proposed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, can significantly improve factual accuracy by incorporating external tools. However, these tool calls often introduce substantial latency, which is particularly problematic in speech-in, speech-out applications where users expect rapid, conversational responses and even brief silences can disrupt the natural flow of dialogue.\nOne way to address this challenge lies in the nature of audio inputs, which arrive as a continuous stream. This streaming property enables tool calls to be initiated before the user has finished speaking, offering a unique opportunity to mitigate latency.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To minimize user-perceived latency, we introduce <em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>: the first framework to generate and issue tool queries in parallel as audio input is received. This novel approach is built on three key design components:\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> Trigger: When to initiate a new tool query;\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> Threads: The number of parallel tool query threads;\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic3\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">3</span></span></foreignobject></g></g></svg></span> Reflector: The module that determines whether intermediate tool results are sufficient for generating the final output. By exploring different design choices for each component, we introduce two complementary approaches for streaming tool query generation in the following subsections: a fixed-interval trigger method and a model-based trigger method.</p>\n\n",
                "matched_terms": [
                    "rag",
                    "output",
                    "results",
                    "audio",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this approach, the <em class=\"ltx_emph ltx_font_italic\">trigger</em> is set to fire tool calls at fixed chunk intervals during audio input.\nThe input speech <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math> is divided into a sequence of <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> blocks, <math alttext=\"Q=\\{Q_{b}\\mid b=1,\\dots,B\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>Q</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>Q</mi><mi>b</mi></msub><mo fence=\"true\" lspace=\"0em\" rspace=\"0em\">&#8739;</mo><mrow><mi>b</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>B</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Q=\\{Q_{b}\\mid b=1,\\dots,B\\}</annotation></semantics></math>, with each block containing <math alttext=\"N_{\\text{block}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>N</mi><mtext>block</mtext></msub><annotation encoding=\"application/x-tex\">N_{\\text{block}}</annotation></semantics></math> frames.\nTo approximate <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> as described in S. 3.1, we follow a block-wise prediction strategy. In this approach, after processing each audio block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m6\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, the model predicts a tool query <math alttext=\"\\hat{Q}_{b}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m7\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}_{b}^{T}</annotation></semantics></math> by conditioning on the input speech accumulated up to block b, specifically <math alttext=\"Q_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">Q_{1:b}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All subsequent parallel tool calls after <math alttext=\"b^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m20\" intent=\":literal\"><semantics><msup><mi>b</mi><mo>&#8902;</mo></msup><annotation encoding=\"application/x-tex\">b^{\\star}</annotation></semantics></math> are promptly terminated, and the retrieved results <math alttext=\"R_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m21\" intent=\":literal\"><semantics><msub><mi>R</mi><msup><mi>b</mi><mo>&#8902;</mo></msup></msub><annotation encoding=\"application/x-tex\">R_{b^{\\star}}</annotation></semantics></math> from this intermediate call <math alttext=\"\\hat{Q}^{T}_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m22\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><msup><mi>b</mi><mo>&#8902;</mo></msup><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b^{\\star}}</annotation></semantics></math> are used to generate the final spoken response <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m23\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by maximizing the posterior distribution <math alttext=\"P(A|Q,R_{b^{\\star}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m24\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><msub><mi>R</mi><msup><mi>b</mi><mo>&#8902;</mo></msup></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R_{b^{\\star}})</annotation></semantics></math> (instead of <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m25\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math> in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>).\nWe employ a reflector module that uses simple yet effective heuristics: (a) if the top 5 web documents for an intermediate web query match those of the final web query, and (b) if the KG results for intermediate and final KG queries are identical.\nThese heuristics ensure that the information retrieved from an early tool call using <math alttext=\"\\hat{Q}^{T}_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m26\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><msup><mi>b</mi><mo>&#8902;</mo></msup><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b^{\\star}}</annotation></semantics></math> is consistent with what would have been obtained by waiting for the final tool call using <math alttext=\"\\hat{Q}^{T}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m27\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{B}</annotation></semantics></math>, thereby providing a strong quality guarantee. Since most tool call latency arises from chunking and reranking the chunks of web documents, these checks enable significant latency savings without any compromise in model performance.\nA key advantage of this strategy is its plug-and-play nature: it requires no additional post-training for speech-in, speech-out models and can be directly applied at inference time across a variety of architectures. However, there are important considerations. First, generating parallel tool calls at every fixed interval (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E1\" title=\"Equation 1 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) increases computational overhead, which may pose challenges for deployment on resource-constrained devices such as wearables devices. Second, reliance on an external reflector module (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E2\" title=\"Equation 2 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) to determine the sufficiency of intermediate tool calls may limit the extent of achievable latency improvements.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the limitations of Fixed-Interval Streaming RAG and further optimize both efficiency and responsiveness, we propose a more adaptive approach: <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>.\nHere, the <em class=\"ltx_emph ltx_font_italic\">trigger</em> is learned: the model is trained to autonomously determine the optimal moments to initiate tool queries, issuing a query only when it encounters new or additional information as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F3.sf2\" title=\"Figure 3(b) &#8227; Figure 3 &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>. In this formulation, the model receives user input in fixed chunk intervals as before and intelligently determines whether a tool call is needed after each block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>. The model can either:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p1.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> Predict\nNO_QUERY if a new tool query is unnecessary, or\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p1.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> Generate a new tool query. To make this decision, the model conditions on both the accumulated input speech <math alttext=\"Q_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">Q_{1:b}</annotation></semantics></math> (see Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E1\" title=\"Equation 1 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) as well as most recent tool query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}=\\hat{Q}^{T}_{\\max\\{\\,i&lt;b:\\hat{Q}^{T}_{i}\\neq\\text{NO\\_QUERY}\\,\\}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><mo>=</mo><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">{</mo><mrow><mrow><mi>i</mi><mo>&lt;</mo><mi>b</mi></mrow><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>i</mi><mi>T</mi></msubsup><mo>&#8800;</mo><mtext>NO_QUERY</mtext></mrow></mrow><mo lspace=\"0.170em\" stretchy=\"false\">}</mo></mrow></mrow><mi>T</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}=\\hat{Q}^{T}_{\\max\\{\\,i&lt;b:\\hat{Q}^{T}_{i}\\neq\\text{NO\\_QUERY}\\,\\}}</annotation></semantics></math> as shown:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "rag",
                    "modeltriggered",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When a new query <math alttext=\"\\hat{Q}^{T}_{b}\\neq\\text{NO\\_QUERY}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m4\" intent=\":literal\"><semantics><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><mo>&#8800;</mo><mtext>NO_QUERY</mtext></mrow><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}\\neq\\text{NO\\_QUERY}</annotation></semantics></math>, the system immediately terminates any ongoing tool calls for the previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m5\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, ensuring that only a <em class=\"ltx_emph ltx_font_italic\">single tool call thread</em> runs at any given time. (Examples of generated <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m6\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> are provided in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T8\" title=\"Table 8 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> in the Appendix.)\nThis approach offers several key advantages. First, it effectively eliminating redundant parallel threads and significantly reducing computational overhead. This is especially important for deployment on resource-constrained devices.\nSecond, this formulation removes the need for an external <em class=\"ltx_emph ltx_font_italic\">reflector</em> (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E2\" title=\"Equation 2 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) module. The model confidently relies on the results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from the most recent tool call using <math alttext=\"\\hat{Q}^{\\text{prev}}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m8\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{B}</annotation></semantics></math> to generate the spoken response <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m9\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by maximizing <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math>, reducing system complexity.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a multi-task fine-tuning strategy targeting two key capabilities.\nFirst, we train the model on Streaming Tool Query Generation by optimizing <math alttext=\"P(Q^{T}_{b}|Q_{1:b},\\hat{Q}^{\\text{prev}}_{b})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo fence=\"false\">|</mo><mrow><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><mo>,</mo><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}_{b}|Q_{1:b},\\hat{Q}^{\\text{prev}}_{b})</annotation></semantics></math> for <math alttext=\"b\\in[1,B]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m2\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mi>B</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b\\in[1,B]</annotation></semantics></math>, enabling intelligent decisions about when to trigger tool queries. Second, we fine-tune on Response Generation by optimizing <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m3\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) to improve the intelligibility of the speech output.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "output",
                    "model",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An important aspect of our post-training is enhancing the model&#8217;s ability to recover from errors in intermediate query predictions. For example, when presented with the audio question, &#8220;Who founded Rare Beauty in 2019?&#8221;, we observed that an initial misinterpretation of <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m1\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, such as &#8220;Red Bull founder&#8221;, can lead the model to subsequently generate NO_QUERY labels, effectively halting further attempts to retrieve the correct information. This issue arises because, during training, the model is always provided with correct previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m2\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, whereas during inference, it may make errors due to partial utterances <math alttext=\"{Q}_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">{Q}_{1:b}</annotation></semantics></math> being ambiguous. Thus the model lacks the ability to recover from such mistakes during inference.\nTo overcome this, we introduce a novel strategy in which we deliberately inject negative samples during post-training by substituting the previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m4\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math> in Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E4\" title=\"Equation 4 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> with incorrect ones <math alttext=\"Q^{\\text{neg}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m5\" intent=\":literal\"><semantics><msubsup><mi>Q</mi><mi>b</mi><mtext>neg</mtext></msubsup><annotation encoding=\"application/x-tex\">Q^{\\text{neg}}_{b}</annotation></semantics></math>. Crucially, when we perform negative sampling, we fall back to the pseudo ground truth query <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> as the training label:</p>\n\n",
                "matched_terms": [
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To rigorously evaluate our proposed approach, we construct comprehensive benchmark datasets featuring spoken queries paired with simulated tool interactions. We begin with the CRAG dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>, which form the basis for our spoken version of CRAG, which we term <em class=\"ltx_emph ltx_font_italic\">AudioCRAG</em>. It consists of 2 distinct variants:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S4.SS1.p1.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> <span class=\"ltx_text ltx_font_bold\">Audio CRAG Synthetic</span>: To generate spoken queries, we use our in-house TTS system. We then apply a rigorous filtering procedure which results in a high-quality set of 1,862 spoken queries, which we refer to as the <em class=\"ltx_emph ltx_font_italic\">AudioCRAG-Synthetic</em> benchmark.  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S4.SS1.p1.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> <span class=\"ltx_text ltx_font_bold\">Audio CRAG Human</span>: To further enhance the realism and diversity of our evaluation, we introduce the AudioCRAG-Human benchmark, which consists of 618 human-recorded spoken queries. We will release\nAudio CRAG Human benchmark upon acceptance to supports the development of more natural and reliable voice assistants.\nFurther details on the construction of these benchmarks are provided in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S9\" title=\"9 Audio CRAG Benchmark &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. We follow the CRAG setup to incorporate both web and KG-based tools, and adopt its robust evaluation methodology, as described in Secs. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S10\" title=\"10 Tool Usage Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S11\" title=\"11 Evaluation Setting &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.\nAdditionally, we leverage a random subset of 16,000 questions from the text-based factual question answering dataset TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib15\" title=\"\">2017</a>)</cite> to post-train our speech-in, speech-out models, as detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13\" title=\"13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "audiocragsynthetic",
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform an ablation study (referred to as &#8220;Tool Integration&#8221; in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) where we post-train the model on sequential query generation (i.e. <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) and output generation, to assess the impact of streaming RAG post-training versus standard post-training on final response generation.\nAdditionally, we conduct an ablation study on open book setting (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) using a self-cascade approach with a three-stage inference pipeline: (1) the audio question is used to generate a tool query <math alttext=\"Q^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><msup><mi>Q</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">Q^{T}</annotation></semantics></math> (corresponding to the &#8220;Query Generation&#8221; stage described in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>); (2) the audio question, and tool results are combined to produce the final text output <math alttext=\"X^{\\text{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>res</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{res}}</annotation></semantics></math> by maximizing <math alttext=\"P(X^{\\text{res}}|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mtext>res</mtext></msup><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(X^{\\text{res}}|Q,R)</annotation></semantics></math>; and (3) the audio question, and final text output are used to generate the final speech output <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by optimizing <math alttext=\"P(A|Q,X^{\\text{res}})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m6\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><msup><mi>X</mi><mtext>res</mtext></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,X^{\\text{res}})</annotation></semantics></math>. Since we teacher-force the text output to obtain the final speech output in stage (3), this self-cascade approach can only be applied to a &#8220;thinker-talker&#8221; architecture (eq. Qwen-OMNI) or Chain-of-Thought (CoT) style architectures (eg. OpusLM).\nThe motivation for this ablation is to investigate whether the inclusion of RAG references <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> during the &#8220;Response Generation&#8221; stage (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) affects the quality of the generated speech <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m8\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "rag",
                    "setting",
                    "text",
                    "opuslm",
                    "output",
                    "book",
                    "open",
                    "selfcascade",
                    "results",
                    "audio",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a comprehensive performance comparison of three models, Qwen2.5-7B, OpusLM, and Kimi Audio, evaluated across three settings: Closed Book, Open Book, and Streaming RAG. All models are assessed on both the AudioCRAG-Synthetic (Syn.) and AudioCRAG-Human (Hum.). In the Closed Book setting, where models rely solely on their internal knowledge without access to external tools (reference length = 0), all models achieve accuracy scores below 20%. These results highlight the inherent limitations of closed-book approaches in handling complex queries.\nThe Open Book setting, which provides models with access to external information, demonstrates the clear benefits of tool integration. Qwen2.5-7B and Kimi Audio&#8217;s accuracy rises substantially, underscoring the value of leveraging external context. As expected, latency increases due to the additional overhead of retrieving information from external tools (See T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> for detailed latency analysis).</p>\n\n",
                "matched_terms": [
                    "kimi",
                    "across",
                    "rag",
                    "closed",
                    "opuslm",
                    "setting",
                    "open",
                    "book",
                    "length",
                    "qwen257b",
                    "audiocragsynthetic",
                    "results",
                    "audio",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most notably, our post-training approach to build <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, as described in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>, delivers significant advancements in both accuracy and efficiency. Qwen2.5-7B and OpusLM achieve significant accuracy improvements across both benchmarks. While the absolute accuracy scores may appear low, they are consistent with the evaluation results observed in the CRAG benchmark. Notably, Qwen2.5-7B with <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> achieves accuracy comparable to the open book performance of similarly sized LLMs reported in the original CRAG paper (i.e., 34.2 for Qwen2.5-7B vs. 32.1 for LLAMA-3 8B Instruct in <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>).\nImportantly, although post-training is performed exclusively on the synthetic dataset, we observe consistent and even greater improvements on the human-spoken benchmark, demonstrating the robustness and strong generalization capabilities of our method. This setting also introduces substantial latency savings compared to the Open Book configuration, with Qwen2.5-7B and OpusLM achieving 20.7% and 14.8% reductions in first-token latency on the synthetic benchmark, and even greater savings on the human benchmark\n<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Our latency calculations on synthetic audio exclude end-point detection latency, which is required in all production systems. Since our streaming RAG approach enables processing without waiting for end-point detection, including this factor would further amplify the observed latency savings, as observed in higher latency savings for human-spoken audio, where endpoint detection errors often introduce trailing silence.</span></span></span>. These results demonstrate that our streaming RAG approach not only advances the accuracy of speech-in speech-out systems, but also optimizes response efficiency by enabling earlier and more effective prediction of tool queries.</p>\n\n",
                "matched_terms": [
                    "across",
                    "rag",
                    "setting",
                    "opuslm",
                    "book",
                    "qwen257b",
                    "open",
                    "modeltriggered",
                    "results",
                    "audio",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> provides a comprehensive breakdown of latency measurements for the Qwen2.5-7B speech-to-speech model, evaluated in both the Open Book setting and our proposed <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> setting on AudioCRAG-Synthetic. We report both median (P50) and 90th percentile (P90) values for each stage of the processing pipeline. The latency is decomposed into three main components: tool query generation, tool result generation, and speech response synthesis, with measurements provided for both the first token outputs (We also provide latency measurements for last token output in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T9\" title=\"Table 9 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>). For both tool query and tool result generation, it is assumed that all tools are accessed in parallel; thus, the reported latency corresponds to the maximum query or result generation time among all tools. The majority of this latency arises from leveraging external web pages, which introduces significant delays, most notably, increasing the first token latency by 2.3x in Open Book Setting.\nNotably, our <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> setting enables early generation of tool results, successfully reducing P50 first token latency by 9.8% and tool use latency by 20.7%.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "rag",
                    "setting",
                    "output",
                    "book",
                    "qwen257b",
                    "open",
                    "modeltriggered",
                    "audiocragsynthetic",
                    "results",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Streaming RAG approaches</span>: Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> highlights the substantial latency savings enabled by the <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>), compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS1\" title=\"3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>). Three key metrics are evaluated: overall latency savings, the percentage of queries benefiting from reduced latency, and the number of parallel threads required.\nEven without any post-training, the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach already reduces tool usage latency (3.37s in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) by 10.7% for Open Book Qwen-OMNI, demonstrating its flexibility and plug-and-play compatibility with any existing speech-in speech-out model.\nThe <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> method, utilizing Qwen-OMNI, consistently delivers superior performance. It achieves greater average latency reductions and benefits a higher proportion of queries with improved response times. Notably, <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> require only a single parallel thread, representing a significant advancement in resource efficiency compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach, which demands multiple parallel threads.\nThese findings highlight the effectiveness of <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> in not only minimizing latency, validating our intuition from Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>, but also in optimizing system efficiency and resource utilization.</p>\n\n",
                "matched_terms": [
                    "model",
                    "rag",
                    "book",
                    "modeltriggered",
                    "open",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-training Strategies</span>: Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents performance comparison under different post-training conditions. Incorporating streaming tool query generation during post-training (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>) results in comparable performance for both models. These results suggest that <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> can be integrated into post-training process without negatively impacting model performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "rag",
                    "modeltriggered",
                    "results",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation Results for Negative Sampling Strategy</span>: Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T5\" title=\"Table 5 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the ablation results evaluating the impact of deliberately injecting negative samples during post-training (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E5\" title=\"Equation 5 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). The findings highlight that, without negative sampling, streaming tool query generation can reduce final accuracy in text output settings, primarily due to errors in final query generation, as detailed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>. In contrast, our negative sampling approach significantly enhances the model&#8217;s robustness, enabling it to recover from intermediate prediction errors. This leads to consistently high accuracy while also achieving notable latency reductions (Tab. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "text",
                    "output",
                    "results",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced the first comprehensive approach for integrating external tool usage directly into E2E speech-in, speech-out dialogue systems. By incorporating <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> pipeline, we further enhanced the model&#8217;s ability to leverage retrieved information and autonomously decide when to trigger new tool queries, resulting in improved accuracy and responsiveness.\nEmpirical evaluation on the newly introduced AudioCRAG benchmark demonstrated that tool integration can more than double factual question answering accuracy compared to closed-book models. Additionally, our streaming RAG approach achieved a 20% reduction in tool usage latency, thereby preserving natural conversational flow.\nOverall, our contributions advance the state of the art in spoken dialogue systems by enabling accurate, real-time, and tool-augmented voice assistants. In addition, we are committed to open science and will release our training code and Audio CRAG Human benchmark to support future research and development in this area.</p>\n\n",
                "matched_terms": [
                    "rag",
                    "modeltriggered",
                    "open",
                    "audio",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We begin with the CRAG dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>, which contains 2,706 text queries. Since these queries are not directly suitable for speech-based evaluation, we first identify those requiring adaptation before TTS conversion. Through careful manual inspection, we determine that queries containing dates or brackets benefit from rewriting to ensure naturalness and clarity in spoken form. In total, we identify 569 such queries and rewrite them using a large language model (LLAMA-4 Maverick). The resulting 569 rewritten queries, combined with the remaining original queries, form the basis for our spoken version of CRAG, which we term <em class=\"ltx_emph ltx_font_italic\">AudioCRAG</em>. We follow the CRAG setup to incorporate web and KG-based tools and adopt its robust evaluation setup, as detailed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S10\" title=\"10 Tool Usage Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S11\" title=\"11 Evaluation Setting &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio CRAG Synthetic</span>:\nTo generate spoken queries, we process all 2,706 queries through our in-house TTS system. We then apply a rigorous filtering procedure to remove queries with intelligibility issues, specifically excluding any utterances for which Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib27\" title=\"\">2023</a>)</cite> hypotheses exhibit a non-zero word error rate. We also remove utterances with suboptimal audio quality, as determined by UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib28\" title=\"\">2022</a>)</cite> scores below 3.5. This results in a high-quality set of 1,862 spoken queries, which we refer to as the <em class=\"ltx_emph ltx_font_italic\">AudioCRAG-Synthetic</em> benchmark.</p>\n\n",
                "matched_terms": [
                    "audiocragsynthetic",
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio CRAG Human</span>:\nTo further enhance the realism and diversity of our evaluation, we introduce the AudioCRAG-Human benchmark, which consists of 618 human-recorded spoken queries. These queries are recorded by a diverse pool of participants to capture natural variations in speech, accent, and prosody. The inclusion of human-recorded audio enables a more comprehensive assessment of speech-in speech-out systems under real-world conditions, providing valuable insights into model robustness and generalization beyond synthetic speech. This benchmark serves as a critical resource for evaluating the effectiveness of tool integration in conversational AI systems.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qwen-OMNI</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>)</cite> is a end-to-end multimodal model that seamlessly integrates diverse input modalities&#8212;including text, images, audio, and video&#8212;and generates both text and natural speech responses in a real-time streaming fashion. It leverages an innovative Thinker-Talker architecture, where the Thinker module performs high-level reasoning to produce a text response, which is then used by the Talker module, conditioning on both the text and the Thinker&#8217;s hidden representations, to generate streaming speech output.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "text",
                    "output",
                    "audio",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OpusLM</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib30\" title=\"\">2025</a>)</cite> is an open-source speech-in, speech-out model post-trained to directly answer complex semantic and factual questions from raw audio inputs, through Chain-of-Thought reasoning.</p>\n\n",
                "matched_terms": [
                    "opuslm",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Kimi Audio</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib8\" title=\"\">2025</a>)</cite> is a universal audio foundation model that unifies audio understanding, generation, and conversational abilities within a single framework. Pre-trained on over 13 million hours of diverse audio and text data, Kimi Audio achieves state-of-the-art performance across a wide range of audio benchmarks, including audio understanding and speech conversation tasks.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "kimi",
                    "model",
                    "across",
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For tool-augmented scenarios, retrieval results are provided up to each model&#8217;s maximum token limit (&#8220;Ref length&#8221; in Tables), maintaining a 2:1 ratio of web page to KG results.\nSpecifically, we observe that Kimi-Audio is currently optimized for handling tool result references up to a certain length. When this limit is exceeded, an error arises during the audio detokenization process, specifically within the rotary embedding mechanism, highlighting an architectural constraint in processing longer input sequences or larger reference contexts. Addressing this limitation presents a valuable opportunity for future model enhancements.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "audio",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation encompasses both speech-in-text-out and speech-in-speech-out scenarios. For the Fixed-Interval Streaming RAG setting (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS1\" title=\"3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>), intermediate tool queries are generated at consistent 1-second intervals. In the Model-Triggered Streaming RAG setting, the model dynamically determines the need for a tool call after processing each 500ms block. This approach allows us to utilize a smaller chunk size, as only a single tool call thread is required for Model-Triggered Streaming RAG, thereby enabling more efficient and responsive processing.</p>\n\n",
                "matched_terms": [
                    "model",
                    "setting",
                    "rag",
                    "modeltriggered",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This subsection details the experimental setup for post-training the pretrained speech-in, speech-out model to significantly enhance its tool usage capabilities, as outlined in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>.\nWe leverage a random subset of 16,000 questions from the text-based factual question answering dataset TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib15\" title=\"\">2017</a>)</cite>, which contains 97,000 questions and 662,659 associated web documents.\nFor <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, we further compute word-level timestamps using a pre-trained ASR model, OWSM CTC v4 1B <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib26\" title=\"\">2025</a>)</cite>, enabling us to generate partial ASR transcripts <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m1\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math> at 500 ms intervals. Note that if word occurs at boundary of block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, it is excluded from <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math>. For each partial transcript <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math>, we generate corresponding psuedo ground truth queries <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m5\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> using LLAMA-4-Maverick to simulate incremental user input.\nTo simulate realistic tool usage, we concatenate all documents and employ a web query reranker to retrieve the top 50 most relevant documents for each query. The text questions from this 16k subset are converted into discrete speech tokens using text-to-speech synthesis with the corresponding pretrained speech-in, speech-out model. Recognizing that TriviaQA answers are typically single named entities, we further transform the queries into a conversational style using LLAMA-4-Maverick, making them more suitable for dialogue-based evaluation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "rag",
                    "text",
                    "modeltriggered",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT = \"\"\"\nYou are an agent that only outputs JSON. You are given a Query and Query Time. Do the following:\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>1) Determine the domain the query is about. The domain should be one of the following: \"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". If none of the domains apply, use \"other\". Use \"domain\" as the key in the result json.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>2) Extract structured information from the query. Include different keys into the result json depending on the domains, and put them DIRECTLY in the result json. Here are the rules:\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;encyclopedia&#8217; and &#8216;other&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;main_entity&#8217;: extract the main entity of the query.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;finance&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;market_identifier&#8217;: stock identifiers including individual company names, stock symbols.\n<br class=\"ltx_break\"/>- &#8216;metric&#8217;: financial metrics that the query is asking about. This must be one of the following: &#8216;price&#8217;, &#8216;dividend&#8217;, &#8216;P/E ratio&#8217;, &#8216;EPS&#8217;, &#8216;marketCap&#8217;, and &#8216;other&#8217;.\n<br class=\"ltx_break\"/>- &#8216;datetime&#8217;: time frame that the query asks about. When datetime is not explicitly mentioned, use &#8216;Query Time&#8217; as default.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;movie&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;movie_name&#8217;: name of the movie\n<br class=\"ltx_break\"/>- &#8216;movie_aspect&#8217;: if the query is about a movie, which movie aspect the query asks. This must be one of the following: &#8216;budget&#8217;, &#8216;genres&#8217;, &#8216;original_language&#8217;, &#8216;original_title&#8217;, &#8216;release_date&#8217;, &#8216;revenue&#8217;, &#8216;title&#8217;, &#8216;cast&#8217;, &#8216;crew&#8217;, &#8216;rating&#8217;, &#8216;length&#8217;.\n<br class=\"ltx_break\"/>- &#8216;person&#8217;: person name related to moves\n<br class=\"ltx_break\"/>- &#8216;person_aspect&#8217;: if the query is about a person, which person aspect the query asks. This must be one of the following: &#8216;acted_movies&#8217;, &#8216;directed_movies&#8217;, &#8216;oscar_awards&#8217;, &#8216;birthday&#8217;.\n<br class=\"ltx_break\"/>- &#8216;year&#8217;: if the query is about movies released in a specific year, extract the year\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;music&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;artist_name&#8217;: name of the artist\n<br class=\"ltx_break\"/>- &#8216;artist_aspect&#8217;: if the query is about an artist, extract the aspect of the artist. This must be one of the following: &#8216;member&#8217;, &#8216;birth place&#8217;, &#8216;birth date&#8217;, &#8216;lifespan&#8217;, &#8216;artist work&#8217;, &#8216;grammy award count&#8217;, &#8216;grammy award date&#8217;.\n<br class=\"ltx_break\"/>- &#8216;song_name&#8217;: name of the song\n<br class=\"ltx_break\"/>- &#8216;song_aspect&#8217;: if the query is about a song, extract the aspect of the song. This must be one of the following: &#8216;author&#8217;, &#8216;grammy award count&#8217;, &#8216;release country&#8217;, &#8216;release date&#8217;.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;sports&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;sport_type&#8217;: one of &#8216;basketball&#8216;, &#8216;soccer&#8216;, &#8216;other&#8216;\n<br class=\"ltx_break\"/>- &#8216;tournament&#8217;: NBA, World Cup, Olympic.\n<br class=\"ltx_break\"/>- &#8216;team&#8217;: teams that users are interested in.\n<br class=\"ltx_break\"/>- &#8216;datetime&#8217;: time frame that the user is interested in. When datetime is not explicitly mentioned, use &#8216;Query Time&#8217; as default.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Return the results in a FLAT json.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n<br class=\"ltx_break\"/>\"\"\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8216;sports&#8217; queries, these are possible keys:\n- &#8216;sport_type&#8217;: one of &#8216;basketball&#8216;, &#8216;soccer&#8216;, &#8216;other&#8216;\n- &#8216;tournament&#8217;: NBA, World Cup, Olympic.\n- &#8216;team&#8217;: teams that users are interested in.\n- &#8216;datetime&#8217;: time frame that the user is interested in. When datetime is not explicitly mentioned, use\n&#8216;Query Time&#8217; as default.\nReturn the results in a FLAT json.\n<br class=\"ltx_break\"/>*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*</p>\n\n",
                "matched_terms": [
                    "output",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">3) Compare your newly generated result to the &#8217;Previous Result&#8217;. **If your new result would be exactly the same as the &#8217;Previous Result&#8217;, output only NO_QUERY.**\nReturn the results in a FLAT json.</p>\n\n",
                "matched_terms": [
                    "output",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When rewriting the query, ensure that it captures all important information from the original question that could impact the retrieval results. Do not omit any crucial details, such as specific dates, locations, or relationships between entities. Also, do not invent any new details on your own. If necessary, use the Query Time to provide context for the query. The goal is to create a concise and accurate query that effectively conveys the user&#8217;s intent and retrieves relevant information.\n*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT THE REWRITTEN QUERY!!!*\n\"\"\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the length of amazon river?\"\nGround Truth: \"over 4000 miles\"\nPrediction: \"the length of amazon river is 4,000 miles\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not say Amazon River is longer than 4000 miles.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "length"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage",
        "caption": "Table 3: First Token Latency breakdown, showing median (P50) and 90th percentile (P90) timings, for the Qwen2.5-7B on AudioCRAG-Synthetic.",
        "body": "Model\nSetting\nP\nLatency (sec)\n\n\nTool Use Latency\nResponse Gen\nTotal\n\n\nQuery Gen\nTool Results Gen\n\n\nQwen2.5-7B\nOpen Book\nP50\n0.59\n2.78\n\n02.52\n5.90\n\n\n(S. 3.1)\nP90\n0.85\n4.90\n\n03.25\n9.00\n\n\nStreaming RAG\nP50\n0.59\n2.20\n\n02.52\n5.32\n\n\n(S. 3.2.2)\nP90\n0.85\n4.37\n\n03.25\n8.47",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\">Model</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"3\">Setting</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"3\">P</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"4\">Latency (sec)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">Tool Use Latency</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\">Response Gen</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\">Total</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Query Gen</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Tool Results Gen</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"4\">Qwen2.5-7B</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Open Book</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">P50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span class=\"ltx_text ltx_phantom\"><span style=\"visibility:hidden\">0</span></span>2.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">5.90</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">(S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">P90</td>\n<td class=\"ltx_td ltx_align_center\">0.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">4.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<span class=\"ltx_text ltx_phantom\"><span style=\"visibility:hidden\">0</span></span>3.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">9.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Streaming RAG</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">P50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span class=\"ltx_text ltx_phantom\"><span style=\"visibility:hidden\">0</span></span>2.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">5.32</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\">(S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">P90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">4.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">\n<span class=\"ltx_text ltx_phantom\"><span style=\"visibility:hidden\">0</span></span>3.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">8.47</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "tool",
            "90th",
            "qwen257b",
            "response",
            "first",
            "book",
            "latency",
            "median",
            "percentile",
            "query",
            "open",
            "results",
            "streaming",
            "p90",
            "model",
            "rag",
            "gen",
            "token",
            "p50",
            "showing",
            "breakdown",
            "setting",
            "timings",
            "total",
            "use",
            "sec",
            "audiocragsynthetic"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">This strategy results in <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m9\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> parallel tool call threads running simultaneously (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F3.sf1\" title=\"Figure 3(a) &#8227; Figure 3 &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>), where each thread generates a tool query prediction <math alttext=\"\\hat{Q}_{b}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m10\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}_{b}^{T}</annotation></semantics></math> for its corresponding block <math alttext=\"b\\in[1,B]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m11\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mi>B</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b\\in[1,B]</annotation></semantics></math>.\nThe tool queries <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m12\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> generated after each block are then stored in cache. Given the high latency of tool calls (See T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), users typically complete their utterances before tool responses are ready. Upon utterance completion, an explicit <em class=\"ltx_emph ltx_font_italic\">reflector</em> module &#8220;<math alttext=\"\\text{reflect}()\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m13\" intent=\":literal\"><semantics><mrow><mtext>reflect</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{reflect}()</annotation></semantics></math>&#8221; evaluates the cached intermediate queries <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m14\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> against final query <math alttext=\"\\hat{Q}^{T}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m15\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{B}</annotation></semantics></math> to determine whether an early intermediate tool call provides sufficient information to answer the user&#8217;s question <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m16\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math>. The reflector module systematically evaluates all intermediate queries in the cache and identifies the <em class=\"ltx_emph ltx_font_italic\">earliest</em> sufficient tool call <math alttext=\"b^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m17\" intent=\":literal\"><semantics><msup><mi>b</mi><mo>&#8902;</mo></msup><annotation encoding=\"application/x-tex\">b^{\\star}</annotation></semantics></math> where the intermediate query <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m18\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> will give the same result as final query <math alttext=\"\\hat{Q}^{T}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m19\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{B}</annotation></semantics></math> as shown:</p>\n\n",
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a comprehensive performance comparison of three models, Qwen2.5-7B, OpusLM, and Kimi Audio, evaluated across three settings: Closed Book, Open Book, and Streaming RAG. All models are assessed on both the AudioCRAG-Synthetic (Syn.) and AudioCRAG-Human (Hum.). In the Closed Book setting, where models rely solely on their internal knowledge without access to external tools (reference length = 0), all models achieve accuracy scores below 20%. These results highlight the inherent limitations of closed-book approaches in handling complex queries.\nThe Open Book setting, which provides models with access to external information, demonstrates the clear benefits of tool integration. Qwen2.5-7B and Kimi Audio&#8217;s accuracy rises substantially, underscoring the value of leveraging external context. As expected, latency increases due to the additional overhead of retrieving information from external tools (See T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> for detailed latency analysis).</p>\n\n",
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> provides a comprehensive breakdown of latency measurements for the Qwen2.5-7B speech-to-speech model, evaluated in both the Open Book setting and our proposed <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> setting on AudioCRAG-Synthetic. We report both median (P50) and 90th percentile (P90) values for each stage of the processing pipeline. The latency is decomposed into three main components: tool query generation, tool result generation, and speech response synthesis, with measurements provided for both the first token outputs (We also provide latency measurements for last token output in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T9\" title=\"Table 9 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>). For both tool query and tool result generation, it is assumed that all tools are accessed in parallel; thus, the reported latency corresponds to the maximum query or result generation time among all tools. The majority of this latency arises from leveraging external web pages, which introduces significant delays, most notably, increasing the first token latency by 2.3x in Open Book Setting.\nNotably, our <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> setting enables early generation of tool results, successfully reducing P50 first token latency by 9.8% and tool use latency by 20.7%.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Streaming RAG approaches</span>: Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> highlights the substantial latency savings enabled by the <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>), compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS1\" title=\"3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>). Three key metrics are evaluated: overall latency savings, the percentage of queries benefiting from reduced latency, and the number of parallel threads required.\nEven without any post-training, the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach already reduces tool usage latency (3.37s in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) by 10.7% for Open Book Qwen-OMNI, demonstrating its flexibility and plug-and-play compatibility with any existing speech-in speech-out model.\nThe <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> method, utilizing Qwen-OMNI, consistently delivers superior performance. It achieves greater average latency reductions and benefits a higher proportion of queries with improved response times. Notably, <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> require only a single parallel thread, representing a significant advancement in resource efficiency compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach, which demands multiple parallel threads.\nThese findings highlight the effectiveness of <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> in not only minimizing latency, validating our intuition from Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>, but also in optimizing system efficiency and resource utilization.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation Results for Negative Sampling Strategy</span>: Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T5\" title=\"Table 5 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the ablation results evaluating the impact of deliberately injecting negative samples during post-training (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E5\" title=\"Equation 5 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). The findings highlight that, without negative sampling, streaming tool query generation can reduce final accuracy in text output settings, primarily due to errors in final query generation, as detailed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>. In contrast, our negative sampling approach significantly enhances the model&#8217;s robustness, enabling it to recover from intermediate prediction errors. This leads to consistently high accuracy while also achieving notable latency reductions (Tab. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">End-to-end speech-in speech-out dialogue systems are emerging as a powerful alternative to traditional ASR&#8211;LLM&#8211;TTS pipelines, generating more natural, expressive responses with significantly lower latency. However, these systems remain prone to hallucinations due to limited factual grounding. While text-based dialogue systems address this challenge by integrating tools such as web search and knowledge graph APIs, we introduce the first approach to extend tool use directly into speech-in speech-out systems. A key challenge is that tool integration substantially increases response latency, disrupting conversational flow. To mitigate this, we propose Streaming Retrieval-Augmented Generation (<em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>), a novel framework that reduces user-perceived latency by predicting tool queries in parallel with user speech, even before the user finishes speaking. Specifically, we develop a post-training pipeline that teaches the model when to <em class=\"ltx_emph ltx_font_italic\">issue tool calls</em> during ongoing speech and how to generate spoken summaries that fuse audio queries with retrieved text results, thereby improving both <em class=\"ltx_emph ltx_font_italic\">accuracy and responsiveness</em>. To evaluate our approach, we construct AudioCRAG, a benchmark created by converting queries from the publicly available CRAG dataset into speech form. Experimental results demonstrate that our <em class=\"ltx_emph ltx_font_italic\">streaming RAG</em> approach increases QA accuracy by up to 200% relative (from 11.1% to 34.2% absolute) and further enhances user experience by reducing tool use latency by 20%. Importantly, our <em class=\"ltx_emph ltx_font_italic\">streaming RAG</em> approach is modality-agnostic and can be applied equally to typed input, paving the way for more agentic, real-time AI assistants.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "rag",
                    "use",
                    "latency",
                    "response",
                    "results",
                    "first",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spoken Dialogue Systems (SDS) are foundational to many everyday technologies, powering intelligent assistants such as Alexa and Siri, as well as interactive voice response systems in customer service. With the rapid expansion of SDS capabilities to mobile phones and wearable devices, the need for robust, scalable, and generalizable solutions has never been greater. Traditionally, SDS have relied on cascaded pipelines composed of multiple modules&#8212;including voice activity detection (VAD), automatic speech recognition (ASR), natural language understanding (NLU), natural language generation (NLG), and text-to-speech (TTS) synthesis&#8212;each introducing potential points of failure and latency <cite class=\"ltx_cite ltx_citemacro_citep\">(Glass, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib12\" title=\"\">1999</a>; Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib13\" title=\"\">2024</a>)</cite>.\nRecently, end-to-end (E2E) SDS <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib34\" title=\"\">2024</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib22\" title=\"\">2023</a>; Meng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib20\" title=\"\">2024</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib40\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib2\" title=\"\">2025b</a>)</cite> have been proposed, which directly generate spoken responses from speech input within a unified architecture. This E2E approach not only mitigates error propagation across modules but also captures non-phonemic information more effectively, resulting in significantly lower inference time and computational overhead, and paving the way for more natural and efficient conversational experiences.</p>\n\n",
                "matched_terms": [
                    "response",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, current E2E SDS are fundamentally constrained by their reliance on internalized knowledge from static training data, which often results in responses that lack factual grounding or fail to reflect the most up to date information. This shortcoming is particularly critical for action-oriented or knowledge-seeking tasks, such as booking hotels or answering questions about current events. In contrast, text-based conversational assistants have begun to overcome these limitations by integrating external tools through Retrieval-Augmented Generation (RAG)<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>; Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib3\" title=\"\">2023a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib5\" title=\"\">c</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib10\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib11\" title=\"\">b</a>)</cite>, dynamically retrieving relevant information from sources like web search, knowledge graphs (KG), and real-time APIs. Yet, the integration of such tool use into E2E SDS remains largely unexplored.\nA key challenge is that while external tools can substantially improve factual accuracy, invoking them often introduces additional latency, leading to awkward silences that disrupt the natural conversational flow. This raises a research question: <span class=\"ltx_text ltx_font_italic\">How can we trade-off between accuracy and responsiveness for developing SDS that feel both intelligent and natural?</span></p>\n\n",
                "matched_terms": [
                    "tool",
                    "results",
                    "use",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present, to the best of our knowledge, the first <span class=\"ltx_text ltx_font_italic\">speech-in, speech-out language model that seamlessly integrates external tool invocation with low latency</span>. The key idea is a <span class=\"ltx_text ltx_font_bold\">Streaming RAG</span> strategy that generates tool queries in parallel with user speech, often times even before the user has finished speaking (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). A naive implementation of streaming queries, however, faces two challenges: (1) queries issued from partial speech may be suboptimal, yielding distracting tool outputs and inaccurate responses; and (2) unnecessary tool calls may be triggered, wasting computational resources. We introduce effective modeling techniques to address these challenges and make the following contributions.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "rag",
                    "latency",
                    "first",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution 1</span>: We introduce a formal framework for tool integration in speech-in speech-out systems and empirically show that leveraging web search and KG APIs significantly enhances factual question answering. Evaluating three state-of-the-art (SOTA) models, Qwen-OMNI <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>)</cite>, OpusLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib30\" title=\"\">2025</a>)</cite>, and Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib8\" title=\"\">2025</a>)</cite>, we find that external tool integration delivers substantial performance gains, boosting accuracy by up to 140% relative (from 11.1% to 26.3% absolute). However, tool usage also introduces considerable latency, increasing first-token response time by 2.3x.</p>\n\n",
                "matched_terms": [
                    "response",
                    "tool",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution 2</span>: To address this, we propose <em class=\"ltx_emph ltx_font_italic\">Streaming Retrieval-Augmented Generation</em> (<em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>), the <em class=\"ltx_emph ltx_font_italic\">first</em> framework that empowers the system to trigger tool queries in parallel with user speech, even before the user finishes speaking. Within this framework, we introduce two novel approaches: (1) <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em>, which issues tool queries at regular intervals during speech input and carefully examines quality of retrieval results on the full query to guarantee response quality, and can be incorporated into any speech-in, speech-out model without post-training; (2) <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, which post-trains the model to intelligently determine optimal query timing based on the evolving user utterance to save computation resources.\nOur results demonstrate that our proposed <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> delivers over 200% relative improvement in accuracy (from 11.1% to 34.2% absolute in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) compared to the no-tool baseline, while also reducing tool result generation latency by 20%. Though designed for speech-in speech-out systems, streaming RAG can also be adapted in cascaded SDS, or even chatbots as users type.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "rag",
                    "latency",
                    "query",
                    "response",
                    "results",
                    "first",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution 3</span>: Finally, we introduce AudioCRAG, a benchmark created by recording spoken queries from the CRAG <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite> dataset, enabling robust evaluation of tool usage capabilities in speech-in speech-out systems.\nTo support future research, we will open source our training code and AudioCRAG-Human benchmark, supporting future research in tool-integrated voice assistants.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "open"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several E2E spoken dialogue systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>; Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib34\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib1\" title=\"\">2025a</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib22\" title=\"\">2023</a>; Meng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib20\" title=\"\">2024</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib40\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib2\" title=\"\">2025b</a>)</cite> have recently been introduced, demonstrating impressive semantic understanding and high audio quality in their responses. However, these systems have not yet been trained or evaluated for their ability to <em class=\"ltx_emph ltx_font_italic\">use external tools</em>.\nAnother research direction <cite class=\"ltx_cite ltx_citemacro_citep\">(Feng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib9\" title=\"\">2025</a>)</cite> explores E2E RAG for direct speech-to-text retrieval, utilizing multimodal embeddings for enabling speech utterances to directly retrieve relevant text. While this method outperforms models without RAG, it is primarily limited to <em class=\"ltx_emph ltx_font_italic\">speech-to-text</em> scenarios. Its ability to access KGs and other APIs, critical for real-world applications, remains unproven. Furthermore, the retrieval scope is limited, as experiments are conducted with retrieval restricted to just 10 paragraphs, whereas modern RAG benchmarks require searching across thousands of web pages.\nAlthough recent studies <cite class=\"ltx_cite ltx_citemacro_citep\">(Maben et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib18\" title=\"\">2025</a>)</cite> have developed web interfaces that integrate tools into speech-in <em class=\"ltx_emph ltx_font_italic\">speech-out</em> scenarios using cascaded pipelines, comprehensive empirical investigations of E2E speech-in speech-out systems and, crucially, systematic analyses of user-perceived latency, remain largely unexplored. In this work, we address these gaps by developing a comprehensive framework for tool integration in E2E speech-in speech-out systems and designing benchmarks to quantitatively assess the tool usage capabilities of SOTA models. Furthermore, recognizing the importance of latency for natural conversational flow, we introduce novel streaming RAG methods that not only enhance tool usage performance but also reduce user-perceived latency.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "rag",
                    "use",
                    "latency",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A RAG spoken conversation system takes an audio question <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math> as input and outputs a spoken answer <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>. Let the ASR transcript of audio question be <math alttext=\"X^{\\text{asr}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>asr</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}</annotation></semantics></math> and the ASR transcript of the spoken answer be <math alttext=\"X^{\\text{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>res</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{res}}</annotation></semantics></math>.\nAnswers are generated by speech-in speech-out models, leveraging both the model&#8217;s internal knowledge and information retrieved from external sources.\nTo incorporate external information, the model needs to formulate a tool query <math alttext=\"Q^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><msup><mi>Q</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">Q^{T}</annotation></semantics></math> to retrieve relevant results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m6\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from an external tool <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m7\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "rag",
                    "query",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our proposed formulation for integrating external tools into speech-in speech-out systems. We introduce a two-stage inference approach: <em class=\"ltx_emph ltx_font_italic\">Query Generation</em> and <em class=\"ltx_emph ltx_font_italic\">Response Generation</em>. In the Query Generation stage, the system processes an audio question and generates queries for each external tool to retrieve relevant information by maximizing the posterior distribution <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> (Examples of generated queries are provided in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T7\" title=\"Table 7 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> in the Appendix.). In the Response Generation stage, the retrieved results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from these tools are combined with the original audio question and input into the model to generate the final spoken response by maximizing the posterior distribution <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math>. By conditioning the final output generation on the input audio, this formulation not only provides a simple and effective mechanism for interacting with text-based APIs, but also preserves the key advantages of speech-in speech-out systems, mitigating error propagation and enabling the model to capture non-phonemic information (such as prosody and speaker intent) more effectively.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "query",
                    "response",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">RAG-based systems, as proposed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, can significantly improve factual accuracy by incorporating external tools. However, these tool calls often introduce substantial latency, which is particularly problematic in speech-in, speech-out applications where users expect rapid, conversational responses and even brief silences can disrupt the natural flow of dialogue.\nOne way to address this challenge lies in the nature of audio inputs, which arrive as a continuous stream. This streaming property enables tool calls to be initiated before the user has finished speaking, offering a unique opportunity to mitigate latency.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "latency",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To minimize user-perceived latency, we introduce <em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>: the first framework to generate and issue tool queries in parallel as audio input is received. This novel approach is built on three key design components:\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> Trigger: When to initiate a new tool query;\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> Threads: The number of parallel tool query threads;\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic3\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">3</span></span></foreignobject></g></g></svg></span> Reflector: The module that determines whether intermediate tool results are sufficient for generating the final output. By exploring different design choices for each component, we introduce two complementary approaches for streaming tool query generation in the following subsections: a fixed-interval trigger method and a model-based trigger method.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "rag",
                    "latency",
                    "query",
                    "results",
                    "first",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this approach, the <em class=\"ltx_emph ltx_font_italic\">trigger</em> is set to fire tool calls at fixed chunk intervals during audio input.\nThe input speech <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math> is divided into a sequence of <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> blocks, <math alttext=\"Q=\\{Q_{b}\\mid b=1,\\dots,B\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>Q</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>Q</mi><mi>b</mi></msub><mo fence=\"true\" lspace=\"0em\" rspace=\"0em\">&#8739;</mo><mrow><mi>b</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>B</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Q=\\{Q_{b}\\mid b=1,\\dots,B\\}</annotation></semantics></math>, with each block containing <math alttext=\"N_{\\text{block}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>N</mi><mtext>block</mtext></msub><annotation encoding=\"application/x-tex\">N_{\\text{block}}</annotation></semantics></math> frames.\nTo approximate <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> as described in S. 3.1, we follow a block-wise prediction strategy. In this approach, after processing each audio block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m6\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, the model predicts a tool query <math alttext=\"\\hat{Q}_{b}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m7\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}_{b}^{T}</annotation></semantics></math> by conditioning on the input speech accumulated up to block b, specifically <math alttext=\"Q_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">Q_{1:b}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "query",
                    "tool",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All subsequent parallel tool calls after <math alttext=\"b^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m20\" intent=\":literal\"><semantics><msup><mi>b</mi><mo>&#8902;</mo></msup><annotation encoding=\"application/x-tex\">b^{\\star}</annotation></semantics></math> are promptly terminated, and the retrieved results <math alttext=\"R_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m21\" intent=\":literal\"><semantics><msub><mi>R</mi><msup><mi>b</mi><mo>&#8902;</mo></msup></msub><annotation encoding=\"application/x-tex\">R_{b^{\\star}}</annotation></semantics></math> from this intermediate call <math alttext=\"\\hat{Q}^{T}_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m22\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><msup><mi>b</mi><mo>&#8902;</mo></msup><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b^{\\star}}</annotation></semantics></math> are used to generate the final spoken response <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m23\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by maximizing the posterior distribution <math alttext=\"P(A|Q,R_{b^{\\star}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m24\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><msub><mi>R</mi><msup><mi>b</mi><mo>&#8902;</mo></msup></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R_{b^{\\star}})</annotation></semantics></math> (instead of <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m25\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math> in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>).\nWe employ a reflector module that uses simple yet effective heuristics: (a) if the top 5 web documents for an intermediate web query match those of the final web query, and (b) if the KG results for intermediate and final KG queries are identical.\nThese heuristics ensure that the information retrieved from an early tool call using <math alttext=\"\\hat{Q}^{T}_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m26\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><msup><mi>b</mi><mo>&#8902;</mo></msup><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b^{\\star}}</annotation></semantics></math> is consistent with what would have been obtained by waiting for the final tool call using <math alttext=\"\\hat{Q}^{T}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m27\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{B}</annotation></semantics></math>, thereby providing a strong quality guarantee. Since most tool call latency arises from chunking and reranking the chunks of web documents, these checks enable significant latency savings without any compromise in model performance.\nA key advantage of this strategy is its plug-and-play nature: it requires no additional post-training for speech-in, speech-out models and can be directly applied at inference time across a variety of architectures. However, there are important considerations. First, generating parallel tool calls at every fixed interval (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E1\" title=\"Equation 1 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) increases computational overhead, which may pose challenges for deployment on resource-constrained devices such as wearables devices. Second, reliance on an external reflector module (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E2\" title=\"Equation 2 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) to determine the sufficiency of intermediate tool calls may limit the extent of achievable latency improvements.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "latency",
                    "query",
                    "response",
                    "results",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the limitations of Fixed-Interval Streaming RAG and further optimize both efficiency and responsiveness, we propose a more adaptive approach: <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>.\nHere, the <em class=\"ltx_emph ltx_font_italic\">trigger</em> is learned: the model is trained to autonomously determine the optimal moments to initiate tool queries, issuing a query only when it encounters new or additional information as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F3.sf2\" title=\"Figure 3(b) &#8227; Figure 3 &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>. In this formulation, the model receives user input in fixed chunk intervals as before and intelligently determines whether a tool call is needed after each block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>. The model can either:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p1.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> Predict\nNO_QUERY if a new tool query is unnecessary, or\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p1.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> Generate a new tool query. To make this decision, the model conditions on both the accumulated input speech <math alttext=\"Q_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">Q_{1:b}</annotation></semantics></math> (see Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E1\" title=\"Equation 1 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) as well as most recent tool query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}=\\hat{Q}^{T}_{\\max\\{\\,i&lt;b:\\hat{Q}^{T}_{i}\\neq\\text{NO\\_QUERY}\\,\\}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><mo>=</mo><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">{</mo><mrow><mrow><mi>i</mi><mo>&lt;</mo><mi>b</mi></mrow><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>i</mi><mi>T</mi></msubsup><mo>&#8800;</mo><mtext>NO_QUERY</mtext></mrow></mrow><mo lspace=\"0.170em\" stretchy=\"false\">}</mo></mrow></mrow><mi>T</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}=\\hat{Q}^{T}_{\\max\\{\\,i&lt;b:\\hat{Q}^{T}_{i}\\neq\\text{NO\\_QUERY}\\,\\}}</annotation></semantics></math> as shown:</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "rag",
                    "query",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When a new query <math alttext=\"\\hat{Q}^{T}_{b}\\neq\\text{NO\\_QUERY}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m4\" intent=\":literal\"><semantics><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><mo>&#8800;</mo><mtext>NO_QUERY</mtext></mrow><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}\\neq\\text{NO\\_QUERY}</annotation></semantics></math>, the system immediately terminates any ongoing tool calls for the previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m5\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, ensuring that only a <em class=\"ltx_emph ltx_font_italic\">single tool call thread</em> runs at any given time. (Examples of generated <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m6\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> are provided in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T8\" title=\"Table 8 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> in the Appendix.)\nThis approach offers several key advantages. First, it effectively eliminating redundant parallel threads and significantly reducing computational overhead. This is especially important for deployment on resource-constrained devices.\nSecond, this formulation removes the need for an external <em class=\"ltx_emph ltx_font_italic\">reflector</em> (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E2\" title=\"Equation 2 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) module. The model confidently relies on the results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from the most recent tool call using <math alttext=\"\\hat{Q}^{\\text{prev}}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m8\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{B}</annotation></semantics></math> to generate the spoken response <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m9\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by maximizing <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math>, reducing system complexity.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "query",
                    "response",
                    "results",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-training</span>: To train the model, we transform text-based tool usage benchmarks into spoken format (See S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13\" title=\"13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>). Word-level timestamps are computed using a pre-trained ASR model. For each partial ASR transcript <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math> up to block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, we generate corresponding queries for each tool <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m3\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> using an LLM as pseudo ground truth (GT).\nTo create effective training labels that teach the model when to trigger new queries, we employ a similarity-based labeling strategy where we compare the current pseudo GT query <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m4\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> with the most recent non-empty tool query label <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m5\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math> (see Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E3\" title=\"Equation 3 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) before block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m6\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>.\nOur labeling function assigns the training label <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m7\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E3\" title=\"Equation 3 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) for the tool query after block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m8\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> as follows:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p2.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> when the current query is sufficiently similar to the previous query (as determined by manually defined heuristics <math alttext=\"f(\\cdots)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m9\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8943;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f(\\cdots)</annotation></semantics></math>), we assign the special label NO_QUERY to teach the model that no new tool call is needed.  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p2.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> When the queries are sufficiently different, we assign the actual pseudo ground truth query\n<math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m10\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> as the label to teach the model to trigger a new tool call:</p>\n\n",
                "matched_terms": [
                    "query",
                    "tool",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a multi-task fine-tuning strategy targeting two key capabilities.\nFirst, we train the model on Streaming Tool Query Generation by optimizing <math alttext=\"P(Q^{T}_{b}|Q_{1:b},\\hat{Q}^{\\text{prev}}_{b})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo fence=\"false\">|</mo><mrow><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><mo>,</mo><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}_{b}|Q_{1:b},\\hat{Q}^{\\text{prev}}_{b})</annotation></semantics></math> for <math alttext=\"b\\in[1,B]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m2\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mi>B</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b\\in[1,B]</annotation></semantics></math>, enabling intelligent decisions about when to trigger tool queries. Second, we fine-tune on Response Generation by optimizing <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m3\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) to improve the intelligibility of the speech output.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "query",
                    "response",
                    "first",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An important aspect of our post-training is enhancing the model&#8217;s ability to recover from errors in intermediate query predictions. For example, when presented with the audio question, &#8220;Who founded Rare Beauty in 2019?&#8221;, we observed that an initial misinterpretation of <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m1\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, such as &#8220;Red Bull founder&#8221;, can lead the model to subsequently generate NO_QUERY labels, effectively halting further attempts to retrieve the correct information. This issue arises because, during training, the model is always provided with correct previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m2\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, whereas during inference, it may make errors due to partial utterances <math alttext=\"{Q}_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">{Q}_{1:b}</annotation></semantics></math> being ambiguous. Thus the model lacks the ability to recover from such mistakes during inference.\nTo overcome this, we introduce a novel strategy in which we deliberately inject negative samples during post-training by substituting the previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m4\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math> in Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E4\" title=\"Equation 4 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> with incorrect ones <math alttext=\"Q^{\\text{neg}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m5\" intent=\":literal\"><semantics><msubsup><mi>Q</mi><mi>b</mi><mtext>neg</mtext></msubsup><annotation encoding=\"application/x-tex\">Q^{\\text{neg}}_{b}</annotation></semantics></math>. Crucially, when we perform negative sampling, we fall back to the pseudo ground truth query <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> as the training label:</p>\n\n",
                "matched_terms": [
                    "query",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This approach explicitly teaches the model to recover from errors in intermediate query prediction, thereby maintaining accuracy (see T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T5\" title=\"Table 5 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> for ablation) while achieving latency savings.</p>\n\n",
                "matched_terms": [
                    "query",
                    "model",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To rigorously evaluate our proposed approach, we construct comprehensive benchmark datasets featuring spoken queries paired with simulated tool interactions. We begin with the CRAG dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>, which form the basis for our spoken version of CRAG, which we term <em class=\"ltx_emph ltx_font_italic\">AudioCRAG</em>. It consists of 2 distinct variants:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S4.SS1.p1.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> <span class=\"ltx_text ltx_font_bold\">Audio CRAG Synthetic</span>: To generate spoken queries, we use our in-house TTS system. We then apply a rigorous filtering procedure which results in a high-quality set of 1,862 spoken queries, which we refer to as the <em class=\"ltx_emph ltx_font_italic\">AudioCRAG-Synthetic</em> benchmark.  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S4.SS1.p1.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> <span class=\"ltx_text ltx_font_bold\">Audio CRAG Human</span>: To further enhance the realism and diversity of our evaluation, we introduce the AudioCRAG-Human benchmark, which consists of 618 human-recorded spoken queries. We will release\nAudio CRAG Human benchmark upon acceptance to supports the development of more natural and reliable voice assistants.\nFurther details on the construction of these benchmarks are provided in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S9\" title=\"9 Audio CRAG Benchmark &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. We follow the CRAG setup to incorporate both web and KG-based tools, and adopt its robust evaluation methodology, as described in Secs. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S10\" title=\"10 Tool Usage Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S11\" title=\"11 Evaluation Setting &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.\nAdditionally, we leverage a random subset of 16,000 questions from the text-based factual question answering dataset TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib15\" title=\"\">2017</a>)</cite> to post-train our speech-in, speech-out models, as detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13\" title=\"13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "use",
                    "sec",
                    "audiocragsynthetic",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform an ablation study (referred to as &#8220;Tool Integration&#8221; in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) where we post-train the model on sequential query generation (i.e. <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) and output generation, to assess the impact of streaming RAG post-training versus standard post-training on final response generation.\nAdditionally, we conduct an ablation study on open book setting (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) using a self-cascade approach with a three-stage inference pipeline: (1) the audio question is used to generate a tool query <math alttext=\"Q^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><msup><mi>Q</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">Q^{T}</annotation></semantics></math> (corresponding to the &#8220;Query Generation&#8221; stage described in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>); (2) the audio question, and tool results are combined to produce the final text output <math alttext=\"X^{\\text{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>res</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{res}}</annotation></semantics></math> by maximizing <math alttext=\"P(X^{\\text{res}}|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mtext>res</mtext></msup><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(X^{\\text{res}}|Q,R)</annotation></semantics></math>; and (3) the audio question, and final text output are used to generate the final speech output <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by optimizing <math alttext=\"P(A|Q,X^{\\text{res}})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m6\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><msup><mi>X</mi><mtext>res</mtext></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,X^{\\text{res}})</annotation></semantics></math>. Since we teacher-force the text output to obtain the final speech output in stage (3), this self-cascade approach can only be applied to a &#8220;thinker-talker&#8221; architecture (eq. Qwen-OMNI) or Chain-of-Thought (CoT) style architectures (eg. OpusLM).\nThe motivation for this ablation is to investigate whether the inclusion of RAG references <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> during the &#8220;Response Generation&#8221; stage (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) affects the quality of the generated speech <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m8\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "rag",
                    "setting",
                    "book",
                    "open",
                    "query",
                    "response",
                    "results",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most notably, our post-training approach to build <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, as described in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>, delivers significant advancements in both accuracy and efficiency. Qwen2.5-7B and OpusLM achieve significant accuracy improvements across both benchmarks. While the absolute accuracy scores may appear low, they are consistent with the evaluation results observed in the CRAG benchmark. Notably, Qwen2.5-7B with <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> achieves accuracy comparable to the open book performance of similarly sized LLMs reported in the original CRAG paper (i.e., 34.2 for Qwen2.5-7B vs. 32.1 for LLAMA-3 8B Instruct in <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>).\nImportantly, although post-training is performed exclusively on the synthetic dataset, we observe consistent and even greater improvements on the human-spoken benchmark, demonstrating the robustness and strong generalization capabilities of our method. This setting also introduces substantial latency savings compared to the Open Book configuration, with Qwen2.5-7B and OpusLM achieving 20.7% and 14.8% reductions in first-token latency on the synthetic benchmark, and even greater savings on the human benchmark\n<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Our latency calculations on synthetic audio exclude end-point detection latency, which is required in all production systems. Since our streaming RAG approach enables processing without waiting for end-point detection, including this factor would further amplify the observed latency savings, as observed in higher latency savings for human-spoken audio, where endpoint detection errors often introduce trailing silence.</span></span></span>. These results demonstrate that our streaming RAG approach not only advances the accuracy of speech-in speech-out systems, but also optimizes response efficiency by enabling earlier and more effective prediction of tool queries.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "rag",
                    "setting",
                    "open",
                    "book",
                    "latency",
                    "qwen257b",
                    "response",
                    "results",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Evaluated SOTA Speech-in Speech-out Models &#8227; 4 Experiment Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides a comparative evaluation of the three SOTA models in generating either text or speech outputs from speech inputs, both with and without the integration of external tool results. In the absence of tool results (Ref length = 0), all models achieve higher accuracy when generating text outputs compared to speech outputs. The incorporation of tool results generally leads to improved text generation performance, with Kimi Audio achieving the highest accuracy. In contrast, the accuracy for speech output remains consistently lower across all models and conditions. The self-cascade approach, in which the model first generates an intermediate text response before producing the final speech output, provides moderate improvements in speech output accuracy for both Qwen2.5-7B and OpusLM.\nHowever, it still underperforms compared to the text-out baseline, primarily due to errors in accurately generating answers involving uncommon entity nouns.\nOverall, these findings underscore a persistent challenge in direct speech generation, particularly when tool results are integrated, as this appears to negatively impact the quality of generated speech responses.\nOur <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> demonstrates clear advantages: it maintains comparable performance for text output and delivers substantial improvements for speech output, even outperforming the self-cascade approach. These results underscore the effectiveness of post-training in overcoming challenges in direct speech generation in tool-augmented scenarios.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "rag",
                    "qwen257b",
                    "response",
                    "results",
                    "first",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-training Strategies</span>: Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents performance comparison under different post-training conditions. Incorporating streaming tool query generation during post-training (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>) results in comparable performance for both models. These results suggest that <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> can be integrated into post-training process without negatively impacting model performance.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "rag",
                    "query",
                    "results",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced the first comprehensive approach for integrating external tool usage directly into E2E speech-in, speech-out dialogue systems. By incorporating <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> pipeline, we further enhanced the model&#8217;s ability to leverage retrieved information and autonomously decide when to trigger new tool queries, resulting in improved accuracy and responsiveness.\nEmpirical evaluation on the newly introduced AudioCRAG benchmark demonstrated that tool integration can more than double factual question answering accuracy compared to closed-book models. Additionally, our streaming RAG approach achieved a 20% reduction in tool usage latency, thereby preserving natural conversational flow.\nOverall, our contributions advance the state of the art in spoken dialogue systems by enabling accurate, real-time, and tool-augmented voice assistants. In addition, we are committed to open science and will release our training code and Audio CRAG Human benchmark to support future research and development in this area.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "rag",
                    "latency",
                    "open",
                    "first",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in benchmarking text-based dialogue systems for tool usage <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib4\" title=\"\">2023b</a>; Ouyang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib24\" title=\"\">2025</a>; Cheng and Dou, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib6\" title=\"\">2025</a>; Cohen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib7\" title=\"\">2025</a>; Xiong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib35\" title=\"\">2024a</a>; Vu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib31\" title=\"\">2023</a>; Xiong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib36\" title=\"\">2024b</a>; Peng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib25\" title=\"\">2024</a>; Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib29\" title=\"\">2025</a>; Ni et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib23\" title=\"\">2025</a>)</cite> have primarily focused on evaluating factual question answering and task completion within simulated environments. The CRAG benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite> is a leading example, featuring 4,409 question-answer pairs and providing mock APIs for both web and knowledge graph (KG) search. CRAG supports a range of KG and web retrieval tasks, and highlights key challenges such as hallucinations in retrieval-augmented generation (RAG) and the importance of leveraging KGs and search ranking to improve factual accuracy. Evaluation is conducted automatically using two LLM judges. SimpleQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib32\" title=\"\">2024</a>)</cite> is another widely adopted benchmark, designed to assess language models on short, fact-seeking questions. With 4,326 adversarially collected questions spanning diverse topics and a straightforward grading scheme based on single, indisputable answers, SimpleQA provides a robust testbed for factual accuracy. Moving beyond question answering, WebArena <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib41\" title=\"\">2024</a>)</cite> offers a simulated environment for evaluating dialogue agents on web-based tasks using fully functional websites, enabling assessment of more complex, action-oriented behaviors. While these benchmarks have significantly advanced the evaluation of tool-augmented dialogue systems, they remain largely limited to text-based interactions and do not fully address the unique challenges presented by speech-in speech-out systems.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "rag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We begin with the CRAG dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>, which contains 2,706 text queries. Since these queries are not directly suitable for speech-based evaluation, we first identify those requiring adaptation before TTS conversion. Through careful manual inspection, we determine that queries containing dates or brackets benefit from rewriting to ensure naturalness and clarity in spoken form. In total, we identify 569 such queries and rewrite them using a large language model (LLAMA-4 Maverick). The resulting 569 rewritten queries, combined with the remaining original queries, form the basis for our spoken version of CRAG, which we term <em class=\"ltx_emph ltx_font_italic\">AudioCRAG</em>. We follow the CRAG setup to incorporate web and KG-based tools and adopt its robust evaluation setup, as detailed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S10\" title=\"10 Tool Usage Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S11\" title=\"11 Evaluation Setting &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "total",
                    "model",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio CRAG Synthetic</span>:\nTo generate spoken queries, we process all 2,706 queries through our in-house TTS system. We then apply a rigorous filtering procedure to remove queries with intelligibility issues, specifically excluding any utterances for which Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib27\" title=\"\">2023</a>)</cite> hypotheses exhibit a non-zero word error rate. We also remove utterances with suboptimal audio quality, as determined by UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib28\" title=\"\">2022</a>)</cite> scores below 3.5. This results in a high-quality set of 1,862 spoken queries, which we refer to as the <em class=\"ltx_emph ltx_font_italic\">AudioCRAG-Synthetic</em> benchmark.</p>\n\n",
                "matched_terms": [
                    "audiocragsynthetic",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio CRAG Human</span>:\nTo further enhance the realism and diversity of our evaluation, we introduce the AudioCRAG-Human benchmark, which consists of 618 human-recorded spoken queries. These queries are recorded by a diverse pool of participants to capture natural variations in speech, accent, and prosody. The inclusion of human-recorded audio enables a more comprehensive assessment of speech-in speech-out systems under real-world conditions, providing valuable insights into model robustness and generalization beyond synthetic speech. This benchmark serves as a critical resource for evaluating the effectiveness of tool integration in conversational AI systems.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable effective tool usage, we build upon the CRAG framework by integrating two complementary information sources: web search, which provides access to fresh and dynamic content, and a knowledge graph, which offers structured and reliable information. For web search, we aggregate all 100,000 documents from the CRAG corpus and employ a BGE-based re-ranker<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/BAAI/bge-large-en-v1.5\" title=\"\">https://huggingface.co/BAAI/bge-large-en-v1.5</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Xiao et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib33\" title=\"\">2023</a>)</cite> to index and retrieve the top 50 most relevant documents for each query. These documents are then segmented into chunks and re-ranked using the same BGE model based on their similarity to the query, ensuring highly contextually relevant retrieval. Meanwhile, queries to the knowledge graph are performed via a simulated API, adhering to the methodology established in CRAG <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/facebookresearch/CRAG/tree/main/mock_api\" title=\"\">https://github.com/facebookresearch/CRAG/tree/main/mock_api</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "query",
                    "tool",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Similar to previous work <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>, we employ model-based automatic evaluation. We use a three-way scoring system, assigning scores of 1, -1, and 0 for accurate, incorrect, and missing answers, respectively. The evaluation is conducted using the Llama 4-maverick LLM evaluator. For speech outputs, we first transcribe the audio using Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib27\" title=\"\">2023</a>)</cite> before passing the transcriptions to the LLM evaluator. In this study, our primary focus is on enhancing system accuracy; therefore, we report average accuracy values in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Evaluated SOTA Speech-in Speech-out Models &#8227; 4 Experiment Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. For additional context, we also provide the average rates of accurate, hallucinated, and missing responses, as well as overall truthfulness scores for each system in the Open Book Setting (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S10.T6\" title=\"Table 6 &#8227; 10 Tool Usage Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>). Notably, our results indicate that Qwen-OMNI was less likely to generate hallucinated responses compared to OpusLM and Kimi Audio.</p>\n\n",
                "matched_terms": [
                    "setting",
                    "book",
                    "use",
                    "open",
                    "results",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qwen-OMNI</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>)</cite> is a end-to-end multimodal model that seamlessly integrates diverse input modalities&#8212;including text, images, audio, and video&#8212;and generates both text and natural speech responses in a real-time streaming fashion. It leverages an innovative Thinker-Talker architecture, where the Thinker module performs high-level reasoning to produce a text response, which is then used by the Talker module, conditioning on both the text and the Thinker&#8217;s hidden representations, to generate streaming speech output.</p>\n\n",
                "matched_terms": [
                    "response",
                    "model",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For tool-augmented scenarios, retrieval results are provided up to each model&#8217;s maximum token limit (&#8220;Ref length&#8221; in Tables), maintaining a 2:1 ratio of web page to KG results.\nSpecifically, we observe that Kimi-Audio is currently optimized for handling tool result references up to a certain length. When this limit is exceeded, an error arises during the audio detokenization process, specifically within the rotary embedding mechanism, highlighting an architectural constraint in processing longer input sequences or larger reference contexts. Addressing this limitation presents a valuable opportunity for future model enhancements.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tool",
                    "results",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation encompasses both speech-in-text-out and speech-in-speech-out scenarios. For the Fixed-Interval Streaming RAG setting (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS1\" title=\"3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>), intermediate tool queries are generated at consistent 1-second intervals. In the Model-Triggered Streaming RAG setting, the model dynamically determines the need for a tool call after processing each 500ms block. This approach allows us to utilize a smaller chunk size, as only a single tool call thread is required for Model-Triggered Streaming RAG, thereby enabling more efficient and responsive processing.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "setting",
                    "rag",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This subsection details the experimental setup for post-training the pretrained speech-in, speech-out model to significantly enhance its tool usage capabilities, as outlined in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>.\nWe leverage a random subset of 16,000 questions from the text-based factual question answering dataset TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib15\" title=\"\">2017</a>)</cite>, which contains 97,000 questions and 662,659 associated web documents.\nFor <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, we further compute word-level timestamps using a pre-trained ASR model, OWSM CTC v4 1B <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib26\" title=\"\">2025</a>)</cite>, enabling us to generate partial ASR transcripts <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m1\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math> at 500 ms intervals. Note that if word occurs at boundary of block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, it is excluded from <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math>. For each partial transcript <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math>, we generate corresponding psuedo ground truth queries <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m5\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> using LLAMA-4-Maverick to simulate incremental user input.\nTo simulate realistic tool usage, we concatenate all documents and employ a web query reranker to retrieve the top 50 most relevant documents for each query. The text questions from this 16k subset are converted into discrete speech tokens using text-to-speech synthesis with the corresponding pretrained speech-in, speech-out model. Recognizing that TriviaQA answers are typically single named entities, we further transform the queries into a conversational style using LLAMA-4-Maverick, making them more suitable for dialogue-based evaluation.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "rag",
                    "query",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT = \"\"\"\nYou are an agent that only outputs JSON. You are given a Query and Query Time. Do the following:\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>1) Determine the domain the query is about. The domain should be one of the following: \"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". If none of the domains apply, use \"other\". Use \"domain\" as the key in the result json.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>2) Extract structured information from the query. Include different keys into the result json depending on the domains, and put them DIRECTLY in the result json. Here are the rules:\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;encyclopedia&#8217; and &#8216;other&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;main_entity&#8217;: extract the main entity of the query.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;finance&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;market_identifier&#8217;: stock identifiers including individual company names, stock symbols.\n<br class=\"ltx_break\"/>- &#8216;metric&#8217;: financial metrics that the query is asking about. This must be one of the following: &#8216;price&#8217;, &#8216;dividend&#8217;, &#8216;P/E ratio&#8217;, &#8216;EPS&#8217;, &#8216;marketCap&#8217;, and &#8216;other&#8217;.\n<br class=\"ltx_break\"/>- &#8216;datetime&#8217;: time frame that the query asks about. When datetime is not explicitly mentioned, use &#8216;Query Time&#8217; as default.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;movie&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;movie_name&#8217;: name of the movie\n<br class=\"ltx_break\"/>- &#8216;movie_aspect&#8217;: if the query is about a movie, which movie aspect the query asks. This must be one of the following: &#8216;budget&#8217;, &#8216;genres&#8217;, &#8216;original_language&#8217;, &#8216;original_title&#8217;, &#8216;release_date&#8217;, &#8216;revenue&#8217;, &#8216;title&#8217;, &#8216;cast&#8217;, &#8216;crew&#8217;, &#8216;rating&#8217;, &#8216;length&#8217;.\n<br class=\"ltx_break\"/>- &#8216;person&#8217;: person name related to moves\n<br class=\"ltx_break\"/>- &#8216;person_aspect&#8217;: if the query is about a person, which person aspect the query asks. This must be one of the following: &#8216;acted_movies&#8217;, &#8216;directed_movies&#8217;, &#8216;oscar_awards&#8217;, &#8216;birthday&#8217;.\n<br class=\"ltx_break\"/>- &#8216;year&#8217;: if the query is about movies released in a specific year, extract the year\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;music&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;artist_name&#8217;: name of the artist\n<br class=\"ltx_break\"/>- &#8216;artist_aspect&#8217;: if the query is about an artist, extract the aspect of the artist. This must be one of the following: &#8216;member&#8217;, &#8216;birth place&#8217;, &#8216;birth date&#8217;, &#8216;lifespan&#8217;, &#8216;artist work&#8217;, &#8216;grammy award count&#8217;, &#8216;grammy award date&#8217;.\n<br class=\"ltx_break\"/>- &#8216;song_name&#8217;: name of the song\n<br class=\"ltx_break\"/>- &#8216;song_aspect&#8217;: if the query is about a song, extract the aspect of the song. This must be one of the following: &#8216;author&#8217;, &#8216;grammy award count&#8217;, &#8216;release country&#8217;, &#8216;release date&#8217;.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;sports&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;sport_type&#8217;: one of &#8216;basketball&#8216;, &#8216;soccer&#8216;, &#8216;other&#8216;\n<br class=\"ltx_break\"/>- &#8216;tournament&#8217;: NBA, World Cup, Olympic.\n<br class=\"ltx_break\"/>- &#8216;team&#8217;: teams that users are interested in.\n<br class=\"ltx_break\"/>- &#8216;datetime&#8217;: time frame that the user is interested in. When datetime is not explicitly mentioned, use &#8216;Query Time&#8217; as default.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Return the results in a FLAT json.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n<br class=\"ltx_break\"/>\"\"\"</p>\n\n",
                "matched_terms": [
                    "query",
                    "results",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">1) Determine the domain the query is about. The domain should be one of the following:\nf&#776;inance,&#776; s&#776;ports,&#776; m&#776;usic,&#776; m&#776;ovie,&#776; &#235;ncyclopedia.&#776; If none of the domains apply, use &#246;ther.&#776; Use\nd&#776;omain&#228;s the key in the result json.</p>\n\n",
                "matched_terms": [
                    "query",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8216;finance&#8217; queries, these are possible keys:\n- &#8216;market_identifier&#8217;: stock identifiers including individual company names, stock symbols.\n- &#8216;metric&#8217;: financial metrics that the query is asking about. This must be one of the following: &#8216;price&#8217;,\n&#8216;dividend&#8217;, &#8216;P/E ratio&#8217;, &#8216;EPS&#8217;, &#8216;marketCap&#8217;, and &#8216;other&#8217;.\n- &#8216;datetime&#8217;: time frame that the query asks about. When datetime is not explicitly mentioned, use\n&#8216;Query Time&#8217; as default.</p>\n\n",
                "matched_terms": [
                    "query",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8216;sports&#8217; queries, these are possible keys:\n- &#8216;sport_type&#8217;: one of &#8216;basketball&#8216;, &#8216;soccer&#8216;, &#8216;other&#8216;\n- &#8216;tournament&#8217;: NBA, World Cup, Olympic.\n- &#8216;team&#8217;: teams that users are interested in.\n- &#8216;datetime&#8217;: time frame that the user is interested in. When datetime is not explicitly mentioned, use\n&#8216;Query Time&#8217; as default.\nReturn the results in a FLAT json.\n<br class=\"ltx_break\"/>*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*</p>\n\n",
                "matched_terms": [
                    "results",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8217;finance&#8217; queries, focus on:\n- Company names or stock symbols\n- Financial metrics (e.g. price, dividend, P/E ratio, EPS, marketCap)\n- Specific timeframes or events; if no timeframe is specified, use the Query Time as default</p>\n\n",
                "matched_terms": [
                    "query",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8217;sports&#8217; queries, focus on:\n- Sports Type (eg. basketball, soccer)\n- Teams, players\n- Statistics or performance metrics (e.g. scores, wins, losses)\n- Specific events or tournaments (eg. NBA, World Cup, Olympic)\n- Time frame that the user is interested in; if no timeframe is specified, use the Query Time as default</p>\n\n",
                "matched_terms": [
                    "query",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When rewriting the query, ensure that it captures all important information from the original question that could impact the retrieval results. Do not omit any crucial details, such as specific dates, locations, or relationships between entities. Also, do not invent any new details on your own. If necessary, use the Query Time to provide context for the query. The goal is to create a concise and accurate query that effectively conveys the user&#8217;s intent and retrieves relevant information.\n*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT THE REWRITTEN QUERY!!!*\n\"\"\"</p>\n\n",
                "matched_terms": [
                    "query",
                    "results",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8217;finance&#8217; queries, focus on:\n- Company names or stock symbols\n- Financial metrics (e.g. price, dividend, P/E ratio, EPS, marketCap)\n- Specific timeframes or events; if no timeframe is specified, use the Query Time as default</p>\n\n",
                "matched_terms": [
                    "query",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8217;sports&#8217; queries, focus on:\n- Sports Type (eg. basketball, soccer)\n- Teams, players\n- Statistics or performance metrics (e.g. scores, wins, losses)\n- Specific events or tournaments (eg. NBA, World Cup, Olympic)\n- Time frame that the user is interested in; if no timeframe is specified, use the Query Time as default</p>\n\n",
                "matched_terms": [
                    "query",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When rewriting the query, ensure that it captures all important information from the original question that could impact the retrieval results. Do not omit any crucial details, such as specific dates, locations, or relationships between entities. Also, do not invent any new details on your own. If necessary, use the Query Time to provide context for the query. The goal is to create a concise and accurate query that effectively conveys the user&#8217;s intent and retrieves relevant information.\nNow, compare the new web query to the previously generated web query (&#8217;Previous Result&#8217;).</p>\n\n",
                "matched_terms": [
                    "query",
                    "results",
                    "use"
                ]
            }
        ]
    },
    "S5.T4": {
        "source_file": "Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage",
        "caption": "Table 4: Comparison of speech-to-speech model under different post-training conditions",
        "body": "Post-Training\nRef length\nPost Train Data\nModel\nAcc.\n\n\n\n\nTool Integration (S. 4.2)\n15K\n16K\nOpusLM\n22.4\n\n\nStreaming RAG (S. 3.2.2)\n15K\n16K\nOpusLM\n23.6\n\n\nTool Integration (S. 4.2)\n23K\n16K\nQwen2.5-7B\n34.9\n\n\nStreaming RAG (S. 3.2.2)\n23K\n16K\nQwen2.5-7B\n34.2",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Post-Training</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Ref length</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Post Train Data</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Model</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Acc.</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Tool Integration (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S4.SS2\" title=\"4.2 Evaluated SOTA Speech-in Speech-out Models &#8227; 4 Experiment Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>)</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">15K</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">16K</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">OpusLM</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">22.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Streaming RAG (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>)</th>\n<td class=\"ltx_td ltx_align_left\">15K</td>\n<td class=\"ltx_td ltx_align_left\">16K</td>\n<td class=\"ltx_td ltx_align_left\">OpusLM</td>\n<td class=\"ltx_td ltx_align_left\">23.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Tool Integration (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S4.SS2\" title=\"4.2 Evaluated SOTA Speech-in Speech-out Models &#8227; 4 Experiment Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>)</th>\n<td class=\"ltx_td ltx_align_left\">23K</td>\n<td class=\"ltx_td ltx_align_left\">16K</td>\n<td class=\"ltx_td ltx_align_left\">Qwen2.5-7B</td>\n<td class=\"ltx_td ltx_align_left\">34.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Streaming RAG (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>)</th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">23K</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">16K</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Qwen2.5-7B</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">34.2</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "tool",
            "integration",
            "length",
            "qwen257b",
            "speechtospeech",
            "16k",
            "opuslm",
            "conditions",
            "23k",
            "post",
            "streaming",
            "acc",
            "model",
            "rag",
            "train",
            "ref",
            "posttraining",
            "under",
            "different",
            "15k",
            "data",
            "comparison"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We perform an ablation study (referred to as &#8220;Tool Integration&#8221; in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) where we post-train the model on sequential query generation (i.e. <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) and output generation, to assess the impact of streaming RAG post-training versus standard post-training on final response generation.\nAdditionally, we conduct an ablation study on open book setting (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) using a self-cascade approach with a three-stage inference pipeline: (1) the audio question is used to generate a tool query <math alttext=\"Q^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><msup><mi>Q</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">Q^{T}</annotation></semantics></math> (corresponding to the &#8220;Query Generation&#8221; stage described in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>); (2) the audio question, and tool results are combined to produce the final text output <math alttext=\"X^{\\text{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>res</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{res}}</annotation></semantics></math> by maximizing <math alttext=\"P(X^{\\text{res}}|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mtext>res</mtext></msup><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(X^{\\text{res}}|Q,R)</annotation></semantics></math>; and (3) the audio question, and final text output are used to generate the final speech output <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by optimizing <math alttext=\"P(A|Q,X^{\\text{res}})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m6\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><msup><mi>X</mi><mtext>res</mtext></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,X^{\\text{res}})</annotation></semantics></math>. Since we teacher-force the text output to obtain the final speech output in stage (3), this self-cascade approach can only be applied to a &#8220;thinker-talker&#8221; architecture (eq. Qwen-OMNI) or Chain-of-Thought (CoT) style architectures (eg. OpusLM).\nThe motivation for this ablation is to investigate whether the inclusion of RAG references <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> during the &#8220;Response Generation&#8221; stage (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) affects the quality of the generated speech <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m8\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-training Strategies</span>: Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents performance comparison under different post-training conditions. Incorporating streaming tool query generation during post-training (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>) results in comparable performance for both models. These results suggest that <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> can be integrated into post-training process without negatively impacting model performance.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">End-to-end speech-in speech-out dialogue systems are emerging as a powerful alternative to traditional ASR&#8211;LLM&#8211;TTS pipelines, generating more natural, expressive responses with significantly lower latency. However, these systems remain prone to hallucinations due to limited factual grounding. While text-based dialogue systems address this challenge by integrating tools such as web search and knowledge graph APIs, we introduce the first approach to extend tool use directly into speech-in speech-out systems. A key challenge is that tool integration substantially increases response latency, disrupting conversational flow. To mitigate this, we propose Streaming Retrieval-Augmented Generation (<em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>), a novel framework that reduces user-perceived latency by predicting tool queries in parallel with user speech, even before the user finishes speaking. Specifically, we develop a post-training pipeline that teaches the model when to <em class=\"ltx_emph ltx_font_italic\">issue tool calls</em> during ongoing speech and how to generate spoken summaries that fuse audio queries with retrieved text results, thereby improving both <em class=\"ltx_emph ltx_font_italic\">accuracy and responsiveness</em>. To evaluate our approach, we construct AudioCRAG, a benchmark created by converting queries from the publicly available CRAG dataset into speech form. Experimental results demonstrate that our <em class=\"ltx_emph ltx_font_italic\">streaming RAG</em> approach increases QA accuracy by up to 200% relative (from 11.1% to 34.2% absolute) and further enhances user experience by reducing tool use latency by 20%. Importantly, our <em class=\"ltx_emph ltx_font_italic\">streaming RAG</em> approach is modality-agnostic and can be applied equally to typed input, paving the way for more agentic, real-time AI assistants.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "rag",
                    "integration",
                    "posttraining",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, current E2E SDS are fundamentally constrained by their reliance on internalized knowledge from static training data, which often results in responses that lack factual grounding or fail to reflect the most up to date information. This shortcoming is particularly critical for action-oriented or knowledge-seeking tasks, such as booking hotels or answering questions about current events. In contrast, text-based conversational assistants have begun to overcome these limitations by integrating external tools through Retrieval-Augmented Generation (RAG)<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>; Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib3\" title=\"\">2023a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib5\" title=\"\">c</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib10\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib11\" title=\"\">b</a>)</cite>, dynamically retrieving relevant information from sources like web search, knowledge graphs (KG), and real-time APIs. Yet, the integration of such tool use into E2E SDS remains largely unexplored.\nA key challenge is that while external tools can substantially improve factual accuracy, invoking them often introduces additional latency, leading to awkward silences that disrupt the natural conversational flow. This raises a research question: <span class=\"ltx_text ltx_font_italic\">How can we trade-off between accuracy and responsiveness for developing SDS that feel both intelligent and natural?</span></p>\n\n",
                "matched_terms": [
                    "data",
                    "integration",
                    "tool"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present, to the best of our knowledge, the first <span class=\"ltx_text ltx_font_italic\">speech-in, speech-out language model that seamlessly integrates external tool invocation with low latency</span>. The key idea is a <span class=\"ltx_text ltx_font_bold\">Streaming RAG</span> strategy that generates tool queries in parallel with user speech, often times even before the user has finished speaking (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). A naive implementation of streaming queries, however, faces two challenges: (1) queries issued from partial speech may be suboptimal, yielding distracting tool outputs and inaccurate responses; and (2) unnecessary tool calls may be triggered, wasting computational resources. We introduce effective modeling techniques to address these challenges and make the following contributions.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "rag",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution 1</span>: We introduce a formal framework for tool integration in speech-in speech-out systems and empirically show that leveraging web search and KG APIs significantly enhances factual question answering. Evaluating three state-of-the-art (SOTA) models, Qwen-OMNI <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>)</cite>, OpusLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib30\" title=\"\">2025</a>)</cite>, and Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib8\" title=\"\">2025</a>)</cite>, we find that external tool integration delivers substantial performance gains, boosting accuracy by up to 140% relative (from 11.1% to 26.3% absolute). However, tool usage also introduces considerable latency, increasing first-token response time by 2.3x.</p>\n\n",
                "matched_terms": [
                    "opuslm",
                    "integration",
                    "tool"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution 2</span>: To address this, we propose <em class=\"ltx_emph ltx_font_italic\">Streaming Retrieval-Augmented Generation</em> (<em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>), the <em class=\"ltx_emph ltx_font_italic\">first</em> framework that empowers the system to trigger tool queries in parallel with user speech, even before the user finishes speaking. Within this framework, we introduce two novel approaches: (1) <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em>, which issues tool queries at regular intervals during speech input and carefully examines quality of retrieval results on the full query to guarantee response quality, and can be incorporated into any speech-in, speech-out model without post-training; (2) <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, which post-trains the model to intelligently determine optimal query timing based on the evolving user utterance to save computation resources.\nOur results demonstrate that our proposed <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> delivers over 200% relative improvement in accuracy (from 11.1% to 34.2% absolute in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) compared to the no-tool baseline, while also reducing tool result generation latency by 20%. Though designed for speech-in speech-out systems, streaming RAG can also be adapted in cascaded SDS, or even chatbots as users type.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "rag",
                    "streaming",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several E2E spoken dialogue systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>; Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib34\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib1\" title=\"\">2025a</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib22\" title=\"\">2023</a>; Meng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib20\" title=\"\">2024</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib40\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib2\" title=\"\">2025b</a>)</cite> have recently been introduced, demonstrating impressive semantic understanding and high audio quality in their responses. However, these systems have not yet been trained or evaluated for their ability to <em class=\"ltx_emph ltx_font_italic\">use external tools</em>.\nAnother research direction <cite class=\"ltx_cite ltx_citemacro_citep\">(Feng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib9\" title=\"\">2025</a>)</cite> explores E2E RAG for direct speech-to-text retrieval, utilizing multimodal embeddings for enabling speech utterances to directly retrieve relevant text. While this method outperforms models without RAG, it is primarily limited to <em class=\"ltx_emph ltx_font_italic\">speech-to-text</em> scenarios. Its ability to access KGs and other APIs, critical for real-world applications, remains unproven. Furthermore, the retrieval scope is limited, as experiments are conducted with retrieval restricted to just 10 paragraphs, whereas modern RAG benchmarks require searching across thousands of web pages.\nAlthough recent studies <cite class=\"ltx_cite ltx_citemacro_citep\">(Maben et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib18\" title=\"\">2025</a>)</cite> have developed web interfaces that integrate tools into speech-in <em class=\"ltx_emph ltx_font_italic\">speech-out</em> scenarios using cascaded pipelines, comprehensive empirical investigations of E2E speech-in speech-out systems and, crucially, systematic analyses of user-perceived latency, remain largely unexplored. In this work, we address these gaps by developing a comprehensive framework for tool integration in E2E speech-in speech-out systems and designing benchmarks to quantitatively assess the tool usage capabilities of SOTA models. Furthermore, recognizing the importance of latency for natural conversational flow, we introduce novel streaming RAG methods that not only enhance tool usage performance but also reduce user-perceived latency.</p>\n\n",
                "matched_terms": [
                    "integration",
                    "tool",
                    "rag",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A RAG spoken conversation system takes an audio question <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math> as input and outputs a spoken answer <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>. Let the ASR transcript of audio question be <math alttext=\"X^{\\text{asr}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>asr</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}</annotation></semantics></math> and the ASR transcript of the spoken answer be <math alttext=\"X^{\\text{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>res</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{res}}</annotation></semantics></math>.\nAnswers are generated by speech-in speech-out models, leveraging both the model&#8217;s internal knowledge and information retrieved from external sources.\nTo incorporate external information, the model needs to formulate a tool query <math alttext=\"Q^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><msup><mi>Q</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">Q^{T}</annotation></semantics></math> to retrieve relevant results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m6\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from an external tool <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m7\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "rag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our proposed formulation for integrating external tools into speech-in speech-out systems. We introduce a two-stage inference approach: <em class=\"ltx_emph ltx_font_italic\">Query Generation</em> and <em class=\"ltx_emph ltx_font_italic\">Response Generation</em>. In the Query Generation stage, the system processes an audio question and generates queries for each external tool to retrieve relevant information by maximizing the posterior distribution <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> (Examples of generated queries are provided in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T7\" title=\"Table 7 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> in the Appendix.). In the Response Generation stage, the retrieved results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from these tools are combined with the original audio question and input into the model to generate the final spoken response by maximizing the posterior distribution <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math>. By conditioning the final output generation on the input audio, this formulation not only provides a simple and effective mechanism for interacting with text-based APIs, but also preserves the key advantages of speech-in speech-out systems, mitigating error propagation and enabling the model to capture non-phonemic information (such as prosody and speaker intent) more effectively.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">RAG-based systems, as proposed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, can significantly improve factual accuracy by incorporating external tools. However, these tool calls often introduce substantial latency, which is particularly problematic in speech-in, speech-out applications where users expect rapid, conversational responses and even brief silences can disrupt the natural flow of dialogue.\nOne way to address this challenge lies in the nature of audio inputs, which arrive as a continuous stream. This streaming property enables tool calls to be initiated before the user has finished speaking, offering a unique opportunity to mitigate latency.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To minimize user-perceived latency, we introduce <em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>: the first framework to generate and issue tool queries in parallel as audio input is received. This novel approach is built on three key design components:\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> Trigger: When to initiate a new tool query;\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> Threads: The number of parallel tool query threads;\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic3\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">3</span></span></foreignobject></g></g></svg></span> Reflector: The module that determines whether intermediate tool results are sufficient for generating the final output. By exploring different design choices for each component, we introduce two complementary approaches for streaming tool query generation in the following subsections: a fixed-interval trigger method and a model-based trigger method.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "different",
                    "rag",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this approach, the <em class=\"ltx_emph ltx_font_italic\">trigger</em> is set to fire tool calls at fixed chunk intervals during audio input.\nThe input speech <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math> is divided into a sequence of <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> blocks, <math alttext=\"Q=\\{Q_{b}\\mid b=1,\\dots,B\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>Q</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>Q</mi><mi>b</mi></msub><mo fence=\"true\" lspace=\"0em\" rspace=\"0em\">&#8739;</mo><mrow><mi>b</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>B</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Q=\\{Q_{b}\\mid b=1,\\dots,B\\}</annotation></semantics></math>, with each block containing <math alttext=\"N_{\\text{block}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>N</mi><mtext>block</mtext></msub><annotation encoding=\"application/x-tex\">N_{\\text{block}}</annotation></semantics></math> frames.\nTo approximate <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> as described in S. 3.1, we follow a block-wise prediction strategy. In this approach, after processing each audio block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m6\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, the model predicts a tool query <math alttext=\"\\hat{Q}_{b}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m7\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}_{b}^{T}</annotation></semantics></math> by conditioning on the input speech accumulated up to block b, specifically <math alttext=\"Q_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">Q_{1:b}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All subsequent parallel tool calls after <math alttext=\"b^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m20\" intent=\":literal\"><semantics><msup><mi>b</mi><mo>&#8902;</mo></msup><annotation encoding=\"application/x-tex\">b^{\\star}</annotation></semantics></math> are promptly terminated, and the retrieved results <math alttext=\"R_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m21\" intent=\":literal\"><semantics><msub><mi>R</mi><msup><mi>b</mi><mo>&#8902;</mo></msup></msub><annotation encoding=\"application/x-tex\">R_{b^{\\star}}</annotation></semantics></math> from this intermediate call <math alttext=\"\\hat{Q}^{T}_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m22\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><msup><mi>b</mi><mo>&#8902;</mo></msup><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b^{\\star}}</annotation></semantics></math> are used to generate the final spoken response <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m23\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by maximizing the posterior distribution <math alttext=\"P(A|Q,R_{b^{\\star}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m24\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><msub><mi>R</mi><msup><mi>b</mi><mo>&#8902;</mo></msup></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R_{b^{\\star}})</annotation></semantics></math> (instead of <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m25\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math> in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>).\nWe employ a reflector module that uses simple yet effective heuristics: (a) if the top 5 web documents for an intermediate web query match those of the final web query, and (b) if the KG results for intermediate and final KG queries are identical.\nThese heuristics ensure that the information retrieved from an early tool call using <math alttext=\"\\hat{Q}^{T}_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m26\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><msup><mi>b</mi><mo>&#8902;</mo></msup><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b^{\\star}}</annotation></semantics></math> is consistent with what would have been obtained by waiting for the final tool call using <math alttext=\"\\hat{Q}^{T}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m27\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{B}</annotation></semantics></math>, thereby providing a strong quality guarantee. Since most tool call latency arises from chunking and reranking the chunks of web documents, these checks enable significant latency savings without any compromise in model performance.\nA key advantage of this strategy is its plug-and-play nature: it requires no additional post-training for speech-in, speech-out models and can be directly applied at inference time across a variety of architectures. However, there are important considerations. First, generating parallel tool calls at every fixed interval (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E1\" title=\"Equation 1 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) increases computational overhead, which may pose challenges for deployment on resource-constrained devices such as wearables devices. Second, reliance on an external reflector module (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E2\" title=\"Equation 2 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) to determine the sufficiency of intermediate tool calls may limit the extent of achievable latency improvements.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the limitations of Fixed-Interval Streaming RAG and further optimize both efficiency and responsiveness, we propose a more adaptive approach: <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>.\nHere, the <em class=\"ltx_emph ltx_font_italic\">trigger</em> is learned: the model is trained to autonomously determine the optimal moments to initiate tool queries, issuing a query only when it encounters new or additional information as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F3.sf2\" title=\"Figure 3(b) &#8227; Figure 3 &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>. In this formulation, the model receives user input in fixed chunk intervals as before and intelligently determines whether a tool call is needed after each block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>. The model can either:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p1.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> Predict\nNO_QUERY if a new tool query is unnecessary, or\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p1.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> Generate a new tool query. To make this decision, the model conditions on both the accumulated input speech <math alttext=\"Q_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">Q_{1:b}</annotation></semantics></math> (see Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E1\" title=\"Equation 1 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) as well as most recent tool query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}=\\hat{Q}^{T}_{\\max\\{\\,i&lt;b:\\hat{Q}^{T}_{i}\\neq\\text{NO\\_QUERY}\\,\\}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><mo>=</mo><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">{</mo><mrow><mrow><mi>i</mi><mo>&lt;</mo><mi>b</mi></mrow><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>i</mi><mi>T</mi></msubsup><mo>&#8800;</mo><mtext>NO_QUERY</mtext></mrow></mrow><mo lspace=\"0.170em\" stretchy=\"false\">}</mo></mrow></mrow><mi>T</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}=\\hat{Q}^{T}_{\\max\\{\\,i&lt;b:\\hat{Q}^{T}_{i}\\neq\\text{NO\\_QUERY}\\,\\}}</annotation></semantics></math> as shown:</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "rag",
                    "conditions",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When a new query <math alttext=\"\\hat{Q}^{T}_{b}\\neq\\text{NO\\_QUERY}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m4\" intent=\":literal\"><semantics><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><mo>&#8800;</mo><mtext>NO_QUERY</mtext></mrow><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}\\neq\\text{NO\\_QUERY}</annotation></semantics></math>, the system immediately terminates any ongoing tool calls for the previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m5\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, ensuring that only a <em class=\"ltx_emph ltx_font_italic\">single tool call thread</em> runs at any given time. (Examples of generated <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m6\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> are provided in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T8\" title=\"Table 8 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> in the Appendix.)\nThis approach offers several key advantages. First, it effectively eliminating redundant parallel threads and significantly reducing computational overhead. This is especially important for deployment on resource-constrained devices.\nSecond, this formulation removes the need for an external <em class=\"ltx_emph ltx_font_italic\">reflector</em> (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E2\" title=\"Equation 2 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) module. The model confidently relies on the results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from the most recent tool call using <math alttext=\"\\hat{Q}^{\\text{prev}}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m8\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{B}</annotation></semantics></math> to generate the spoken response <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m9\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by maximizing <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math>, reducing system complexity.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-training</span>: To train the model, we transform text-based tool usage benchmarks into spoken format (See S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13\" title=\"13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>). Word-level timestamps are computed using a pre-trained ASR model. For each partial ASR transcript <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math> up to block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, we generate corresponding queries for each tool <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m3\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> using an LLM as pseudo ground truth (GT).\nTo create effective training labels that teach the model when to trigger new queries, we employ a similarity-based labeling strategy where we compare the current pseudo GT query <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m4\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> with the most recent non-empty tool query label <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m5\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math> (see Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E3\" title=\"Equation 3 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) before block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m6\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>.\nOur labeling function assigns the training label <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m7\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E3\" title=\"Equation 3 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) for the tool query after block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m8\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> as follows:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p2.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> when the current query is sufficiently similar to the previous query (as determined by manually defined heuristics <math alttext=\"f(\\cdots)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m9\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8943;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f(\\cdots)</annotation></semantics></math>), we assign the special label NO_QUERY to teach the model that no new tool call is needed.  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p2.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> When the queries are sufficiently different, we assign the actual pseudo ground truth query\n<math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m10\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> as the label to teach the model to trigger a new tool call:</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "train",
                    "different",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a multi-task fine-tuning strategy targeting two key capabilities.\nFirst, we train the model on Streaming Tool Query Generation by optimizing <math alttext=\"P(Q^{T}_{b}|Q_{1:b},\\hat{Q}^{\\text{prev}}_{b})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo fence=\"false\">|</mo><mrow><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><mo>,</mo><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}_{b}|Q_{1:b},\\hat{Q}^{\\text{prev}}_{b})</annotation></semantics></math> for <math alttext=\"b\\in[1,B]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m2\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mi>B</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b\\in[1,B]</annotation></semantics></math>, enabling intelligent decisions about when to trigger tool queries. Second, we fine-tune on Response Generation by optimizing <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m3\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) to improve the intelligibility of the speech output.</p>\n\n",
                "matched_terms": [
                    "train",
                    "tool",
                    "model",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An important aspect of our post-training is enhancing the model&#8217;s ability to recover from errors in intermediate query predictions. For example, when presented with the audio question, &#8220;Who founded Rare Beauty in 2019?&#8221;, we observed that an initial misinterpretation of <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m1\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, such as &#8220;Red Bull founder&#8221;, can lead the model to subsequently generate NO_QUERY labels, effectively halting further attempts to retrieve the correct information. This issue arises because, during training, the model is always provided with correct previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m2\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, whereas during inference, it may make errors due to partial utterances <math alttext=\"{Q}_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">{Q}_{1:b}</annotation></semantics></math> being ambiguous. Thus the model lacks the ability to recover from such mistakes during inference.\nTo overcome this, we introduce a novel strategy in which we deliberately inject negative samples during post-training by substituting the previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m4\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math> in Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E4\" title=\"Equation 4 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> with incorrect ones <math alttext=\"Q^{\\text{neg}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m5\" intent=\":literal\"><semantics><msubsup><mi>Q</mi><mi>b</mi><mtext>neg</mtext></msubsup><annotation encoding=\"application/x-tex\">Q^{\\text{neg}}_{b}</annotation></semantics></math>. Crucially, when we perform negative sampling, we fall back to the pseudo ground truth query <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> as the training label:</p>\n\n",
                "matched_terms": [
                    "model",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present a comprehensive benchmark of three SOTA speech-in, speech-out conversational systems: Qwen-OMNI <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>)</cite>, Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib8\" title=\"\">2025</a>)</cite> and OpusLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib30\" title=\"\">2025</a>)</cite>.\nWe evaluate them under both tool-augmented and non-tool-augmented conditions. Further details about our experimental setup are provided in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S12\" title=\"12 Experiment Setup of SOTA speech-in speech-out models &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.</p>\n\n",
                "matched_terms": [
                    "opuslm",
                    "under",
                    "conditions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a comprehensive performance comparison of three models, Qwen2.5-7B, OpusLM, and Kimi Audio, evaluated across three settings: Closed Book, Open Book, and Streaming RAG. All models are assessed on both the AudioCRAG-Synthetic (Syn.) and AudioCRAG-Human (Hum.). In the Closed Book setting, where models rely solely on their internal knowledge without access to external tools (reference length = 0), all models achieve accuracy scores below 20%. These results highlight the inherent limitations of closed-book approaches in handling complex queries.\nThe Open Book setting, which provides models with access to external information, demonstrates the clear benefits of tool integration. Qwen2.5-7B and Kimi Audio&#8217;s accuracy rises substantially, underscoring the value of leveraging external context. As expected, latency increases due to the additional overhead of retrieving information from external tools (See T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> for detailed latency analysis).</p>\n\n",
                "matched_terms": [
                    "tool",
                    "rag",
                    "opuslm",
                    "integration",
                    "length",
                    "qwen257b",
                    "comparison",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most notably, our post-training approach to build <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, as described in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>, delivers significant advancements in both accuracy and efficiency. Qwen2.5-7B and OpusLM achieve significant accuracy improvements across both benchmarks. While the absolute accuracy scores may appear low, they are consistent with the evaluation results observed in the CRAG benchmark. Notably, Qwen2.5-7B with <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> achieves accuracy comparable to the open book performance of similarly sized LLMs reported in the original CRAG paper (i.e., 34.2 for Qwen2.5-7B vs. 32.1 for LLAMA-3 8B Instruct in <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>).\nImportantly, although post-training is performed exclusively on the synthetic dataset, we observe consistent and even greater improvements on the human-spoken benchmark, demonstrating the robustness and strong generalization capabilities of our method. This setting also introduces substantial latency savings compared to the Open Book configuration, with Qwen2.5-7B and OpusLM achieving 20.7% and 14.8% reductions in first-token latency on the synthetic benchmark, and even greater savings on the human benchmark\n<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Our latency calculations on synthetic audio exclude end-point detection latency, which is required in all production systems. Since our streaming RAG approach enables processing without waiting for end-point detection, including this factor would further amplify the observed latency savings, as observed in higher latency savings for human-spoken audio, where endpoint detection errors often introduce trailing silence.</span></span></span>. These results demonstrate that our streaming RAG approach not only advances the accuracy of speech-in speech-out systems, but also optimizes response efficiency by enabling earlier and more effective prediction of tool queries.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "rag",
                    "opuslm",
                    "qwen257b",
                    "posttraining",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Evaluated SOTA Speech-in Speech-out Models &#8227; 4 Experiment Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides a comparative evaluation of the three SOTA models in generating either text or speech outputs from speech inputs, both with and without the integration of external tool results. In the absence of tool results (Ref length = 0), all models achieve higher accuracy when generating text outputs compared to speech outputs. The incorporation of tool results generally leads to improved text generation performance, with Kimi Audio achieving the highest accuracy. In contrast, the accuracy for speech output remains consistently lower across all models and conditions. The self-cascade approach, in which the model first generates an intermediate text response before producing the final speech output, provides moderate improvements in speech output accuracy for both Qwen2.5-7B and OpusLM.\nHowever, it still underperforms compared to the text-out baseline, primarily due to errors in accurately generating answers involving uncommon entity nouns.\nOverall, these findings underscore a persistent challenge in direct speech generation, particularly when tool results are integrated, as this appears to negatively impact the quality of generated speech responses.\nOur <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> demonstrates clear advantages: it maintains comparable performance for text output and delivers substantial improvements for speech output, even outperforming the self-cascade approach. These results underscore the effectiveness of post-training in overcoming challenges in direct speech generation in tool-augmented scenarios.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "rag",
                    "opuslm",
                    "conditions",
                    "integration",
                    "length",
                    "qwen257b",
                    "ref",
                    "posttraining",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> provides a comprehensive breakdown of latency measurements for the Qwen2.5-7B speech-to-speech model, evaluated in both the Open Book setting and our proposed <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> setting on AudioCRAG-Synthetic. We report both median (P50) and 90th percentile (P90) values for each stage of the processing pipeline. The latency is decomposed into three main components: tool query generation, tool result generation, and speech response synthesis, with measurements provided for both the first token outputs (We also provide latency measurements for last token output in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T9\" title=\"Table 9 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>). For both tool query and tool result generation, it is assumed that all tools are accessed in parallel; thus, the reported latency corresponds to the maximum query or result generation time among all tools. The majority of this latency arises from leveraging external web pages, which introduces significant delays, most notably, increasing the first token latency by 2.3x in Open Book Setting.\nNotably, our <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> setting enables early generation of tool results, successfully reducing P50 first token latency by 9.8% and tool use latency by 20.7%.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "rag",
                    "qwen257b",
                    "speechtospeech",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Streaming RAG approaches</span>: Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> highlights the substantial latency savings enabled by the <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>), compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS1\" title=\"3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>). Three key metrics are evaluated: overall latency savings, the percentage of queries benefiting from reduced latency, and the number of parallel threads required.\nEven without any post-training, the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach already reduces tool usage latency (3.37s in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) by 10.7% for Open Book Qwen-OMNI, demonstrating its flexibility and plug-and-play compatibility with any existing speech-in speech-out model.\nThe <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> method, utilizing Qwen-OMNI, consistently delivers superior performance. It achieves greater average latency reductions and benefits a higher proportion of queries with improved response times. Notably, <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> require only a single parallel thread, representing a significant advancement in resource efficiency compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach, which demands multiple parallel threads.\nThese findings highlight the effectiveness of <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> in not only minimizing latency, validating our intuition from Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>, but also in optimizing system efficiency and resource utilization.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "rag",
                    "streaming",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation Results for Negative Sampling Strategy</span>: Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T5\" title=\"Table 5 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the ablation results evaluating the impact of deliberately injecting negative samples during post-training (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E5\" title=\"Equation 5 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). The findings highlight that, without negative sampling, streaming tool query generation can reduce final accuracy in text output settings, primarily due to errors in final query generation, as detailed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>. In contrast, our negative sampling approach significantly enhances the model&#8217;s robustness, enabling it to recover from intermediate prediction errors. This leads to consistently high accuracy while also achieving notable latency reductions (Tab. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "tool",
                    "posttraining",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced the first comprehensive approach for integrating external tool usage directly into E2E speech-in, speech-out dialogue systems. By incorporating <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> pipeline, we further enhanced the model&#8217;s ability to leverage retrieved information and autonomously decide when to trigger new tool queries, resulting in improved accuracy and responsiveness.\nEmpirical evaluation on the newly introduced AudioCRAG benchmark demonstrated that tool integration can more than double factual question answering accuracy compared to closed-book models. Additionally, our streaming RAG approach achieved a 20% reduction in tool usage latency, thereby preserving natural conversational flow.\nOverall, our contributions advance the state of the art in spoken dialogue systems by enabling accurate, real-time, and tool-augmented voice assistants. In addition, we are committed to open science and will release our training code and Audio CRAG Human benchmark to support future research and development in this area.</p>\n\n",
                "matched_terms": [
                    "integration",
                    "tool",
                    "rag",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in benchmarking text-based dialogue systems for tool usage <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib4\" title=\"\">2023b</a>; Ouyang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib24\" title=\"\">2025</a>; Cheng and Dou, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib6\" title=\"\">2025</a>; Cohen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib7\" title=\"\">2025</a>; Xiong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib35\" title=\"\">2024a</a>; Vu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib31\" title=\"\">2023</a>; Xiong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib36\" title=\"\">2024b</a>; Peng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib25\" title=\"\">2024</a>; Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib29\" title=\"\">2025</a>; Ni et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib23\" title=\"\">2025</a>)</cite> have primarily focused on evaluating factual question answering and task completion within simulated environments. The CRAG benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite> is a leading example, featuring 4,409 question-answer pairs and providing mock APIs for both web and knowledge graph (KG) search. CRAG supports a range of KG and web retrieval tasks, and highlights key challenges such as hallucinations in retrieval-augmented generation (RAG) and the importance of leveraging KGs and search ranking to improve factual accuracy. Evaluation is conducted automatically using two LLM judges. SimpleQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib32\" title=\"\">2024</a>)</cite> is another widely adopted benchmark, designed to assess language models on short, fact-seeking questions. With 4,326 adversarially collected questions spanning diverse topics and a straightforward grading scheme based on single, indisputable answers, SimpleQA provides a robust testbed for factual accuracy. Moving beyond question answering, WebArena <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib41\" title=\"\">2024</a>)</cite> offers a simulated environment for evaluating dialogue agents on web-based tasks using fully functional websites, enabling assessment of more complex, action-oriented behaviors. While these benchmarks have significantly advanced the evaluation of tool-augmented dialogue systems, they remain largely limited to text-based interactions and do not fully address the unique challenges presented by speech-in speech-out systems.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "rag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio CRAG Human</span>:\nTo further enhance the realism and diversity of our evaluation, we introduce the AudioCRAG-Human benchmark, which consists of 618 human-recorded spoken queries. These queries are recorded by a diverse pool of participants to capture natural variations in speech, accent, and prosody. The inclusion of human-recorded audio enables a more comprehensive assessment of speech-in speech-out systems under real-world conditions, providing valuable insights into model robustness and generalization beyond synthetic speech. This benchmark serves as a critical resource for evaluating the effectiveness of tool integration in conversational AI systems.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "under",
                    "conditions",
                    "integration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable effective tool usage, we build upon the CRAG framework by integrating two complementary information sources: web search, which provides access to fresh and dynamic content, and a knowledge graph, which offers structured and reliable information. For web search, we aggregate all 100,000 documents from the CRAG corpus and employ a BGE-based re-ranker<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/BAAI/bge-large-en-v1.5\" title=\"\">https://huggingface.co/BAAI/bge-large-en-v1.5</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Xiao et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib33\" title=\"\">2023</a>)</cite> to index and retrieve the top 50 most relevant documents for each query. These documents are then segmented into chunks and re-ranked using the same BGE model based on their similarity to the query, ensuring highly contextually relevant retrieval. Meanwhile, queries to the knowledge graph are performed via a simulated API, adhering to the methodology established in CRAG <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/facebookresearch/CRAG/tree/main/mock_api\" title=\"\">https://github.com/facebookresearch/CRAG/tree/main/mock_api</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qwen-OMNI</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>)</cite> is a end-to-end multimodal model that seamlessly integrates diverse input modalities&#8212;including text, images, audio, and video&#8212;and generates both text and natural speech responses in a real-time streaming fashion. It leverages an innovative Thinker-Talker architecture, where the Thinker module performs high-level reasoning to produce a text response, which is then used by the Talker module, conditioning on both the text and the Thinker&#8217;s hidden representations, to generate streaming speech output.</p>\n\n",
                "matched_terms": [
                    "model",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OpusLM</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib30\" title=\"\">2025</a>)</cite> is an open-source speech-in, speech-out model post-trained to directly answer complex semantic and factual questions from raw audio inputs, through Chain-of-Thought reasoning.</p>\n\n",
                "matched_terms": [
                    "opuslm",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Kimi Audio</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib8\" title=\"\">2025</a>)</cite> is a universal audio foundation model that unifies audio understanding, generation, and conversational abilities within a single framework. Pre-trained on over 13 million hours of diverse audio and text data, Kimi Audio achieves state-of-the-art performance across a wide range of audio benchmarks, including audio understanding and speech conversation tasks.</p>\n\n",
                "matched_terms": [
                    "data",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For tool-augmented scenarios, retrieval results are provided up to each model&#8217;s maximum token limit (&#8220;Ref length&#8221; in Tables), maintaining a 2:1 ratio of web page to KG results.\nSpecifically, we observe that Kimi-Audio is currently optimized for handling tool result references up to a certain length. When this limit is exceeded, an error arises during the audio detokenization process, specifically within the rotary embedding mechanism, highlighting an architectural constraint in processing longer input sequences or larger reference contexts. Addressing this limitation presents a valuable opportunity for future model enhancements.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation encompasses both speech-in-text-out and speech-in-speech-out scenarios. For the Fixed-Interval Streaming RAG setting (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS1\" title=\"3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>), intermediate tool queries are generated at consistent 1-second intervals. In the Model-Triggered Streaming RAG setting, the model dynamically determines the need for a tool call after processing each 500ms block. This approach allows us to utilize a smaller chunk size, as only a single tool call thread is required for Model-Triggered Streaming RAG, thereby enabling more efficient and responsive processing.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "rag",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This subsection details the experimental setup for post-training the pretrained speech-in, speech-out model to significantly enhance its tool usage capabilities, as outlined in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>.\nWe leverage a random subset of 16,000 questions from the text-based factual question answering dataset TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib15\" title=\"\">2017</a>)</cite>, which contains 97,000 questions and 662,659 associated web documents.\nFor <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, we further compute word-level timestamps using a pre-trained ASR model, OWSM CTC v4 1B <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib26\" title=\"\">2025</a>)</cite>, enabling us to generate partial ASR transcripts <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m1\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math> at 500 ms intervals. Note that if word occurs at boundary of block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, it is excluded from <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math>. For each partial transcript <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math>, we generate corresponding psuedo ground truth queries <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m5\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> using LLAMA-4-Maverick to simulate incremental user input.\nTo simulate realistic tool usage, we concatenate all documents and employ a web query reranker to retrieve the top 50 most relevant documents for each query. The text questions from this 16k subset are converted into discrete speech tokens using text-to-speech synthesis with the corresponding pretrained speech-in, speech-out model. Recognizing that TriviaQA answers are typically single named entities, we further transform the queries into a conversational style using LLAMA-4-Maverick, making them more suitable for dialogue-based evaluation.</p>\n\n",
                "matched_terms": [
                    "16k",
                    "tool",
                    "model",
                    "rag",
                    "posttraining",
                    "streaming"
                ]
            }
        ]
    },
    "S5.T5": {
        "source_file": "Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage",
        "caption": "Table 5: Ablation Results for Negative Sampling Strategy in Model-Triggered Streaming RAG (S. 3.2.2).",
        "body": "Scenario\nRef Length\nPost Train Data\nOutput\nModel\nAcc.\n\n\nOpen Book\n15K\n0\nText\nQwen2.5-7B\n39.6\n\n\n\n\nPost-train (S. 3.2.2)\n15K\n16K\nText\nQwen2.5-7B\n39.8\n\n\n- Negative sampling\n15K\n16K\nText\nQwen2.5-7B\n36.5",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Scenario</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Ref Length</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Post Train Data</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Output</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Acc.</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\">Open Book</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">15K</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Text</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Qwen2.5-7B</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">39.6</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Post-train (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">15K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">16K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Qwen2.5-7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">- Negative sampling</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">15K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">16K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Qwen2.5-7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">36.5</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "strategy",
            "ablation",
            "text",
            "output",
            "length",
            "qwen257b",
            "scenario",
            "16k",
            "book",
            "open",
            "results",
            "post",
            "streaming",
            "acc",
            "model",
            "rag",
            "train",
            "posttrain",
            "ref",
            "negative",
            "sampling",
            "15k",
            "modeltriggered",
            "data"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">This approach explicitly teaches the model to recover from errors in intermediate query prediction, thereby maintaining accuracy (see T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T5\" title=\"Table 5 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> for ablation) while achieving latency savings.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation Results for Negative Sampling Strategy</span>: Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T5\" title=\"Table 5 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the ablation results evaluating the impact of deliberately injecting negative samples during post-training (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E5\" title=\"Equation 5 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). The findings highlight that, without negative sampling, streaming tool query generation can reduce final accuracy in text output settings, primarily due to errors in final query generation, as detailed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>. In contrast, our negative sampling approach significantly enhances the model&#8217;s robustness, enabling it to recover from intermediate prediction errors. This leads to consistently high accuracy while also achieving notable latency reductions (Tab. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">End-to-end speech-in speech-out dialogue systems are emerging as a powerful alternative to traditional ASR&#8211;LLM&#8211;TTS pipelines, generating more natural, expressive responses with significantly lower latency. However, these systems remain prone to hallucinations due to limited factual grounding. While text-based dialogue systems address this challenge by integrating tools such as web search and knowledge graph APIs, we introduce the first approach to extend tool use directly into speech-in speech-out systems. A key challenge is that tool integration substantially increases response latency, disrupting conversational flow. To mitigate this, we propose Streaming Retrieval-Augmented Generation (<em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>), a novel framework that reduces user-perceived latency by predicting tool queries in parallel with user speech, even before the user finishes speaking. Specifically, we develop a post-training pipeline that teaches the model when to <em class=\"ltx_emph ltx_font_italic\">issue tool calls</em> during ongoing speech and how to generate spoken summaries that fuse audio queries with retrieved text results, thereby improving both <em class=\"ltx_emph ltx_font_italic\">accuracy and responsiveness</em>. To evaluate our approach, we construct AudioCRAG, a benchmark created by converting queries from the publicly available CRAG dataset into speech form. Experimental results demonstrate that our <em class=\"ltx_emph ltx_font_italic\">streaming RAG</em> approach increases QA accuracy by up to 200% relative (from 11.1% to 34.2% absolute) and further enhances user experience by reducing tool use latency by 20%. Importantly, our <em class=\"ltx_emph ltx_font_italic\">streaming RAG</em> approach is modality-agnostic and can be applied equally to typed input, paving the way for more agentic, real-time AI assistants.</p>\n\n",
                "matched_terms": [
                    "model",
                    "rag",
                    "text",
                    "results",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, current E2E SDS are fundamentally constrained by their reliance on internalized knowledge from static training data, which often results in responses that lack factual grounding or fail to reflect the most up to date information. This shortcoming is particularly critical for action-oriented or knowledge-seeking tasks, such as booking hotels or answering questions about current events. In contrast, text-based conversational assistants have begun to overcome these limitations by integrating external tools through Retrieval-Augmented Generation (RAG)<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>; Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib3\" title=\"\">2023a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib5\" title=\"\">c</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib10\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib11\" title=\"\">b</a>)</cite>, dynamically retrieving relevant information from sources like web search, knowledge graphs (KG), and real-time APIs. Yet, the integration of such tool use into E2E SDS remains largely unexplored.\nA key challenge is that while external tools can substantially improve factual accuracy, invoking them often introduces additional latency, leading to awkward silences that disrupt the natural conversational flow. This raises a research question: <span class=\"ltx_text ltx_font_italic\">How can we trade-off between accuracy and responsiveness for developing SDS that feel both intelligent and natural?</span></p>\n\n",
                "matched_terms": [
                    "data",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present, to the best of our knowledge, the first <span class=\"ltx_text ltx_font_italic\">speech-in, speech-out language model that seamlessly integrates external tool invocation with low latency</span>. The key idea is a <span class=\"ltx_text ltx_font_bold\">Streaming RAG</span> strategy that generates tool queries in parallel with user speech, often times even before the user has finished speaking (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). A naive implementation of streaming queries, however, faces two challenges: (1) queries issued from partial speech may be suboptimal, yielding distracting tool outputs and inaccurate responses; and (2) unnecessary tool calls may be triggered, wasting computational resources. We introduce effective modeling techniques to address these challenges and make the following contributions.</p>\n\n",
                "matched_terms": [
                    "model",
                    "strategy",
                    "rag",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution 2</span>: To address this, we propose <em class=\"ltx_emph ltx_font_italic\">Streaming Retrieval-Augmented Generation</em> (<em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>), the <em class=\"ltx_emph ltx_font_italic\">first</em> framework that empowers the system to trigger tool queries in parallel with user speech, even before the user finishes speaking. Within this framework, we introduce two novel approaches: (1) <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em>, which issues tool queries at regular intervals during speech input and carefully examines quality of retrieval results on the full query to guarantee response quality, and can be incorporated into any speech-in, speech-out model without post-training; (2) <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, which post-trains the model to intelligently determine optimal query timing based on the evolving user utterance to save computation resources.\nOur results demonstrate that our proposed <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> delivers over 200% relative improvement in accuracy (from 11.1% to 34.2% absolute in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) compared to the no-tool baseline, while also reducing tool result generation latency by 20%. Though designed for speech-in speech-out systems, streaming RAG can also be adapted in cascaded SDS, or even chatbots as users type.</p>\n\n",
                "matched_terms": [
                    "model",
                    "rag",
                    "modeltriggered",
                    "results",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several E2E spoken dialogue systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>; Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib34\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib1\" title=\"\">2025a</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib22\" title=\"\">2023</a>; Meng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib20\" title=\"\">2024</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib40\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib2\" title=\"\">2025b</a>)</cite> have recently been introduced, demonstrating impressive semantic understanding and high audio quality in their responses. However, these systems have not yet been trained or evaluated for their ability to <em class=\"ltx_emph ltx_font_italic\">use external tools</em>.\nAnother research direction <cite class=\"ltx_cite ltx_citemacro_citep\">(Feng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib9\" title=\"\">2025</a>)</cite> explores E2E RAG for direct speech-to-text retrieval, utilizing multimodal embeddings for enabling speech utterances to directly retrieve relevant text. While this method outperforms models without RAG, it is primarily limited to <em class=\"ltx_emph ltx_font_italic\">speech-to-text</em> scenarios. Its ability to access KGs and other APIs, critical for real-world applications, remains unproven. Furthermore, the retrieval scope is limited, as experiments are conducted with retrieval restricted to just 10 paragraphs, whereas modern RAG benchmarks require searching across thousands of web pages.\nAlthough recent studies <cite class=\"ltx_cite ltx_citemacro_citep\">(Maben et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib18\" title=\"\">2025</a>)</cite> have developed web interfaces that integrate tools into speech-in <em class=\"ltx_emph ltx_font_italic\">speech-out</em> scenarios using cascaded pipelines, comprehensive empirical investigations of E2E speech-in speech-out systems and, crucially, systematic analyses of user-perceived latency, remain largely unexplored. In this work, we address these gaps by developing a comprehensive framework for tool integration in E2E speech-in speech-out systems and designing benchmarks to quantitatively assess the tool usage capabilities of SOTA models. Furthermore, recognizing the importance of latency for natural conversational flow, we introduce novel streaming RAG methods that not only enhance tool usage performance but also reduce user-perceived latency.</p>\n\n",
                "matched_terms": [
                    "text",
                    "rag",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A RAG spoken conversation system takes an audio question <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math> as input and outputs a spoken answer <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>. Let the ASR transcript of audio question be <math alttext=\"X^{\\text{asr}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>asr</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}</annotation></semantics></math> and the ASR transcript of the spoken answer be <math alttext=\"X^{\\text{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>res</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{res}}</annotation></semantics></math>.\nAnswers are generated by speech-in speech-out models, leveraging both the model&#8217;s internal knowledge and information retrieved from external sources.\nTo incorporate external information, the model needs to formulate a tool query <math alttext=\"Q^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><msup><mi>Q</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">Q^{T}</annotation></semantics></math> to retrieve relevant results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m6\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from an external tool <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m7\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "rag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our proposed formulation for integrating external tools into speech-in speech-out systems. We introduce a two-stage inference approach: <em class=\"ltx_emph ltx_font_italic\">Query Generation</em> and <em class=\"ltx_emph ltx_font_italic\">Response Generation</em>. In the Query Generation stage, the system processes an audio question and generates queries for each external tool to retrieve relevant information by maximizing the posterior distribution <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> (Examples of generated queries are provided in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T7\" title=\"Table 7 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> in the Appendix.). In the Response Generation stage, the retrieved results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from these tools are combined with the original audio question and input into the model to generate the final spoken response by maximizing the posterior distribution <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math>. By conditioning the final output generation on the input audio, this formulation not only provides a simple and effective mechanism for interacting with text-based APIs, but also preserves the key advantages of speech-in speech-out systems, mitigating error propagation and enabling the model to capture non-phonemic information (such as prosody and speaker intent) more effectively.</p>\n\n",
                "matched_terms": [
                    "model",
                    "output",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To minimize user-perceived latency, we introduce <em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>: the first framework to generate and issue tool queries in parallel as audio input is received. This novel approach is built on three key design components:\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> Trigger: When to initiate a new tool query;\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> Threads: The number of parallel tool query threads;\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic3\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">3</span></span></foreignobject></g></g></svg></span> Reflector: The module that determines whether intermediate tool results are sufficient for generating the final output. By exploring different design choices for each component, we introduce two complementary approaches for streaming tool query generation in the following subsections: a fixed-interval trigger method and a model-based trigger method.</p>\n\n",
                "matched_terms": [
                    "output",
                    "results",
                    "rag",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this approach, the <em class=\"ltx_emph ltx_font_italic\">trigger</em> is set to fire tool calls at fixed chunk intervals during audio input.\nThe input speech <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math> is divided into a sequence of <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> blocks, <math alttext=\"Q=\\{Q_{b}\\mid b=1,\\dots,B\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>Q</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>Q</mi><mi>b</mi></msub><mo fence=\"true\" lspace=\"0em\" rspace=\"0em\">&#8739;</mo><mrow><mi>b</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>B</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Q=\\{Q_{b}\\mid b=1,\\dots,B\\}</annotation></semantics></math>, with each block containing <math alttext=\"N_{\\text{block}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>N</mi><mtext>block</mtext></msub><annotation encoding=\"application/x-tex\">N_{\\text{block}}</annotation></semantics></math> frames.\nTo approximate <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> as described in S. 3.1, we follow a block-wise prediction strategy. In this approach, after processing each audio block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m6\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, the model predicts a tool query <math alttext=\"\\hat{Q}_{b}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m7\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}_{b}^{T}</annotation></semantics></math> by conditioning on the input speech accumulated up to block b, specifically <math alttext=\"Q_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">Q_{1:b}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "model",
                    "strategy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This strategy results in <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m9\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> parallel tool call threads running simultaneously (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F3.sf1\" title=\"Figure 3(a) &#8227; Figure 3 &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>), where each thread generates a tool query prediction <math alttext=\"\\hat{Q}_{b}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m10\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}_{b}^{T}</annotation></semantics></math> for its corresponding block <math alttext=\"b\\in[1,B]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m11\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mi>B</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b\\in[1,B]</annotation></semantics></math>.\nThe tool queries <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m12\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> generated after each block are then stored in cache. Given the high latency of tool calls (See T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), users typically complete their utterances before tool responses are ready. Upon utterance completion, an explicit <em class=\"ltx_emph ltx_font_italic\">reflector</em> module &#8220;<math alttext=\"\\text{reflect}()\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m13\" intent=\":literal\"><semantics><mrow><mtext>reflect</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{reflect}()</annotation></semantics></math>&#8221; evaluates the cached intermediate queries <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m14\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> against final query <math alttext=\"\\hat{Q}^{T}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m15\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{B}</annotation></semantics></math> to determine whether an early intermediate tool call provides sufficient information to answer the user&#8217;s question <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m16\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math>. The reflector module systematically evaluates all intermediate queries in the cache and identifies the <em class=\"ltx_emph ltx_font_italic\">earliest</em> sufficient tool call <math alttext=\"b^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m17\" intent=\":literal\"><semantics><msup><mi>b</mi><mo>&#8902;</mo></msup><annotation encoding=\"application/x-tex\">b^{\\star}</annotation></semantics></math> where the intermediate query <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m18\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> will give the same result as final query <math alttext=\"\\hat{Q}^{T}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m19\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{B}</annotation></semantics></math> as shown:</p>\n\n",
                "matched_terms": [
                    "results",
                    "strategy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All subsequent parallel tool calls after <math alttext=\"b^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m20\" intent=\":literal\"><semantics><msup><mi>b</mi><mo>&#8902;</mo></msup><annotation encoding=\"application/x-tex\">b^{\\star}</annotation></semantics></math> are promptly terminated, and the retrieved results <math alttext=\"R_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m21\" intent=\":literal\"><semantics><msub><mi>R</mi><msup><mi>b</mi><mo>&#8902;</mo></msup></msub><annotation encoding=\"application/x-tex\">R_{b^{\\star}}</annotation></semantics></math> from this intermediate call <math alttext=\"\\hat{Q}^{T}_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m22\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><msup><mi>b</mi><mo>&#8902;</mo></msup><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b^{\\star}}</annotation></semantics></math> are used to generate the final spoken response <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m23\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by maximizing the posterior distribution <math alttext=\"P(A|Q,R_{b^{\\star}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m24\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><msub><mi>R</mi><msup><mi>b</mi><mo>&#8902;</mo></msup></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R_{b^{\\star}})</annotation></semantics></math> (instead of <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m25\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math> in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>).\nWe employ a reflector module that uses simple yet effective heuristics: (a) if the top 5 web documents for an intermediate web query match those of the final web query, and (b) if the KG results for intermediate and final KG queries are identical.\nThese heuristics ensure that the information retrieved from an early tool call using <math alttext=\"\\hat{Q}^{T}_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m26\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><msup><mi>b</mi><mo>&#8902;</mo></msup><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b^{\\star}}</annotation></semantics></math> is consistent with what would have been obtained by waiting for the final tool call using <math alttext=\"\\hat{Q}^{T}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m27\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{B}</annotation></semantics></math>, thereby providing a strong quality guarantee. Since most tool call latency arises from chunking and reranking the chunks of web documents, these checks enable significant latency savings without any compromise in model performance.\nA key advantage of this strategy is its plug-and-play nature: it requires no additional post-training for speech-in, speech-out models and can be directly applied at inference time across a variety of architectures. However, there are important considerations. First, generating parallel tool calls at every fixed interval (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E1\" title=\"Equation 1 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) increases computational overhead, which may pose challenges for deployment on resource-constrained devices such as wearables devices. Second, reliance on an external reflector module (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E2\" title=\"Equation 2 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) to determine the sufficiency of intermediate tool calls may limit the extent of achievable latency improvements.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "strategy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the limitations of Fixed-Interval Streaming RAG and further optimize both efficiency and responsiveness, we propose a more adaptive approach: <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>.\nHere, the <em class=\"ltx_emph ltx_font_italic\">trigger</em> is learned: the model is trained to autonomously determine the optimal moments to initiate tool queries, issuing a query only when it encounters new or additional information as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F3.sf2\" title=\"Figure 3(b) &#8227; Figure 3 &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>. In this formulation, the model receives user input in fixed chunk intervals as before and intelligently determines whether a tool call is needed after each block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>. The model can either:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p1.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> Predict\nNO_QUERY if a new tool query is unnecessary, or\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p1.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> Generate a new tool query. To make this decision, the model conditions on both the accumulated input speech <math alttext=\"Q_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">Q_{1:b}</annotation></semantics></math> (see Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E1\" title=\"Equation 1 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) as well as most recent tool query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}=\\hat{Q}^{T}_{\\max\\{\\,i&lt;b:\\hat{Q}^{T}_{i}\\neq\\text{NO\\_QUERY}\\,\\}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><mo>=</mo><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">{</mo><mrow><mrow><mi>i</mi><mo>&lt;</mo><mi>b</mi></mrow><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>i</mi><mi>T</mi></msubsup><mo>&#8800;</mo><mtext>NO_QUERY</mtext></mrow></mrow><mo lspace=\"0.170em\" stretchy=\"false\">}</mo></mrow></mrow><mi>T</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}=\\hat{Q}^{T}_{\\max\\{\\,i&lt;b:\\hat{Q}^{T}_{i}\\neq\\text{NO\\_QUERY}\\,\\}}</annotation></semantics></math> as shown:</p>\n\n",
                "matched_terms": [
                    "model",
                    "modeltriggered",
                    "rag",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When a new query <math alttext=\"\\hat{Q}^{T}_{b}\\neq\\text{NO\\_QUERY}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m4\" intent=\":literal\"><semantics><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><mo>&#8800;</mo><mtext>NO_QUERY</mtext></mrow><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}\\neq\\text{NO\\_QUERY}</annotation></semantics></math>, the system immediately terminates any ongoing tool calls for the previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m5\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, ensuring that only a <em class=\"ltx_emph ltx_font_italic\">single tool call thread</em> runs at any given time. (Examples of generated <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m6\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> are provided in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T8\" title=\"Table 8 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> in the Appendix.)\nThis approach offers several key advantages. First, it effectively eliminating redundant parallel threads and significantly reducing computational overhead. This is especially important for deployment on resource-constrained devices.\nSecond, this formulation removes the need for an external <em class=\"ltx_emph ltx_font_italic\">reflector</em> (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E2\" title=\"Equation 2 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) module. The model confidently relies on the results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from the most recent tool call using <math alttext=\"\\hat{Q}^{\\text{prev}}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m8\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{B}</annotation></semantics></math> to generate the spoken response <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m9\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by maximizing <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math>, reducing system complexity.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-training</span>: To train the model, we transform text-based tool usage benchmarks into spoken format (See S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13\" title=\"13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>). Word-level timestamps are computed using a pre-trained ASR model. For each partial ASR transcript <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math> up to block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, we generate corresponding queries for each tool <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m3\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> using an LLM as pseudo ground truth (GT).\nTo create effective training labels that teach the model when to trigger new queries, we employ a similarity-based labeling strategy where we compare the current pseudo GT query <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m4\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> with the most recent non-empty tool query label <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m5\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math> (see Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E3\" title=\"Equation 3 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) before block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m6\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>.\nOur labeling function assigns the training label <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m7\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E3\" title=\"Equation 3 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) for the tool query after block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m8\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> as follows:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p2.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> when the current query is sufficiently similar to the previous query (as determined by manually defined heuristics <math alttext=\"f(\\cdots)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m9\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8943;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f(\\cdots)</annotation></semantics></math>), we assign the special label NO_QUERY to teach the model that no new tool call is needed.  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p2.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> When the queries are sufficiently different, we assign the actual pseudo ground truth query\n<math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m10\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> as the label to teach the model to trigger a new tool call:</p>\n\n",
                "matched_terms": [
                    "train",
                    "model",
                    "strategy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a multi-task fine-tuning strategy targeting two key capabilities.\nFirst, we train the model on Streaming Tool Query Generation by optimizing <math alttext=\"P(Q^{T}_{b}|Q_{1:b},\\hat{Q}^{\\text{prev}}_{b})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo fence=\"false\">|</mo><mrow><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><mo>,</mo><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}_{b}|Q_{1:b},\\hat{Q}^{\\text{prev}}_{b})</annotation></semantics></math> for <math alttext=\"b\\in[1,B]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m2\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mi>B</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b\\in[1,B]</annotation></semantics></math>, enabling intelligent decisions about when to trigger tool queries. Second, we fine-tune on Response Generation by optimizing <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m3\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) to improve the intelligibility of the speech output.</p>\n\n",
                "matched_terms": [
                    "model",
                    "strategy",
                    "train",
                    "output",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An important aspect of our post-training is enhancing the model&#8217;s ability to recover from errors in intermediate query predictions. For example, when presented with the audio question, &#8220;Who founded Rare Beauty in 2019?&#8221;, we observed that an initial misinterpretation of <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m1\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, such as &#8220;Red Bull founder&#8221;, can lead the model to subsequently generate NO_QUERY labels, effectively halting further attempts to retrieve the correct information. This issue arises because, during training, the model is always provided with correct previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m2\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, whereas during inference, it may make errors due to partial utterances <math alttext=\"{Q}_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">{Q}_{1:b}</annotation></semantics></math> being ambiguous. Thus the model lacks the ability to recover from such mistakes during inference.\nTo overcome this, we introduce a novel strategy in which we deliberately inject negative samples during post-training by substituting the previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m4\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math> in Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E4\" title=\"Equation 4 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> with incorrect ones <math alttext=\"Q^{\\text{neg}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m5\" intent=\":literal\"><semantics><msubsup><mi>Q</mi><mi>b</mi><mtext>neg</mtext></msubsup><annotation encoding=\"application/x-tex\">Q^{\\text{neg}}_{b}</annotation></semantics></math>. Crucially, when we perform negative sampling, we fall back to the pseudo ground truth query <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> as the training label:</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "model",
                    "strategy",
                    "negative"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To rigorously evaluate our proposed approach, we construct comprehensive benchmark datasets featuring spoken queries paired with simulated tool interactions. We begin with the CRAG dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>, which form the basis for our spoken version of CRAG, which we term <em class=\"ltx_emph ltx_font_italic\">AudioCRAG</em>. It consists of 2 distinct variants:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S4.SS1.p1.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> <span class=\"ltx_text ltx_font_bold\">Audio CRAG Synthetic</span>: To generate spoken queries, we use our in-house TTS system. We then apply a rigorous filtering procedure which results in a high-quality set of 1,862 spoken queries, which we refer to as the <em class=\"ltx_emph ltx_font_italic\">AudioCRAG-Synthetic</em> benchmark.  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S4.SS1.p1.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> <span class=\"ltx_text ltx_font_bold\">Audio CRAG Human</span>: To further enhance the realism and diversity of our evaluation, we introduce the AudioCRAG-Human benchmark, which consists of 618 human-recorded spoken queries. We will release\nAudio CRAG Human benchmark upon acceptance to supports the development of more natural and reliable voice assistants.\nFurther details on the construction of these benchmarks are provided in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S9\" title=\"9 Audio CRAG Benchmark &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. We follow the CRAG setup to incorporate both web and KG-based tools, and adopt its robust evaluation methodology, as described in Secs. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S10\" title=\"10 Tool Usage Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S11\" title=\"11 Evaluation Setting &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.\nAdditionally, we leverage a random subset of 16,000 questions from the text-based factual question answering dataset TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib15\" title=\"\">2017</a>)</cite> to post-train our speech-in, speech-out models, as detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13\" title=\"13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "posttrain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform an ablation study (referred to as &#8220;Tool Integration&#8221; in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) where we post-train the model on sequential query generation (i.e. <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) and output generation, to assess the impact of streaming RAG post-training versus standard post-training on final response generation.\nAdditionally, we conduct an ablation study on open book setting (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) using a self-cascade approach with a three-stage inference pipeline: (1) the audio question is used to generate a tool query <math alttext=\"Q^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><msup><mi>Q</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">Q^{T}</annotation></semantics></math> (corresponding to the &#8220;Query Generation&#8221; stage described in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>); (2) the audio question, and tool results are combined to produce the final text output <math alttext=\"X^{\\text{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>res</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{res}}</annotation></semantics></math> by maximizing <math alttext=\"P(X^{\\text{res}}|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mtext>res</mtext></msup><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(X^{\\text{res}}|Q,R)</annotation></semantics></math>; and (3) the audio question, and final text output are used to generate the final speech output <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by optimizing <math alttext=\"P(A|Q,X^{\\text{res}})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m6\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><msup><mi>X</mi><mtext>res</mtext></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,X^{\\text{res}})</annotation></semantics></math>. Since we teacher-force the text output to obtain the final speech output in stage (3), this self-cascade approach can only be applied to a &#8220;thinker-talker&#8221; architecture (eq. Qwen-OMNI) or Chain-of-Thought (CoT) style architectures (eg. OpusLM).\nThe motivation for this ablation is to investigate whether the inclusion of RAG references <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> during the &#8220;Response Generation&#8221; stage (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) affects the quality of the generated speech <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m8\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "rag",
                    "ablation",
                    "text",
                    "output",
                    "posttrain",
                    "book",
                    "open",
                    "results",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a comprehensive performance comparison of three models, Qwen2.5-7B, OpusLM, and Kimi Audio, evaluated across three settings: Closed Book, Open Book, and Streaming RAG. All models are assessed on both the AudioCRAG-Synthetic (Syn.) and AudioCRAG-Human (Hum.). In the Closed Book setting, where models rely solely on their internal knowledge without access to external tools (reference length = 0), all models achieve accuracy scores below 20%. These results highlight the inherent limitations of closed-book approaches in handling complex queries.\nThe Open Book setting, which provides models with access to external information, demonstrates the clear benefits of tool integration. Qwen2.5-7B and Kimi Audio&#8217;s accuracy rises substantially, underscoring the value of leveraging external context. As expected, latency increases due to the additional overhead of retrieving information from external tools (See T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> for detailed latency analysis).</p>\n\n",
                "matched_terms": [
                    "rag",
                    "open",
                    "book",
                    "qwen257b",
                    "length",
                    "results",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most notably, our post-training approach to build <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, as described in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>, delivers significant advancements in both accuracy and efficiency. Qwen2.5-7B and OpusLM achieve significant accuracy improvements across both benchmarks. While the absolute accuracy scores may appear low, they are consistent with the evaluation results observed in the CRAG benchmark. Notably, Qwen2.5-7B with <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> achieves accuracy comparable to the open book performance of similarly sized LLMs reported in the original CRAG paper (i.e., 34.2 for Qwen2.5-7B vs. 32.1 for LLAMA-3 8B Instruct in <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>).\nImportantly, although post-training is performed exclusively on the synthetic dataset, we observe consistent and even greater improvements on the human-spoken benchmark, demonstrating the robustness and strong generalization capabilities of our method. This setting also introduces substantial latency savings compared to the Open Book configuration, with Qwen2.5-7B and OpusLM achieving 20.7% and 14.8% reductions in first-token latency on the synthetic benchmark, and even greater savings on the human benchmark\n<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Our latency calculations on synthetic audio exclude end-point detection latency, which is required in all production systems. Since our streaming RAG approach enables processing without waiting for end-point detection, including this factor would further amplify the observed latency savings, as observed in higher latency savings for human-spoken audio, where endpoint detection errors often introduce trailing silence.</span></span></span>. These results demonstrate that our streaming RAG approach not only advances the accuracy of speech-in speech-out systems, but also optimizes response efficiency by enabling earlier and more effective prediction of tool queries.</p>\n\n",
                "matched_terms": [
                    "rag",
                    "book",
                    "qwen257b",
                    "open",
                    "modeltriggered",
                    "results",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Evaluated SOTA Speech-in Speech-out Models &#8227; 4 Experiment Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides a comparative evaluation of the three SOTA models in generating either text or speech outputs from speech inputs, both with and without the integration of external tool results. In the absence of tool results (Ref length = 0), all models achieve higher accuracy when generating text outputs compared to speech outputs. The incorporation of tool results generally leads to improved text generation performance, with Kimi Audio achieving the highest accuracy. In contrast, the accuracy for speech output remains consistently lower across all models and conditions. The self-cascade approach, in which the model first generates an intermediate text response before producing the final speech output, provides moderate improvements in speech output accuracy for both Qwen2.5-7B and OpusLM.\nHowever, it still underperforms compared to the text-out baseline, primarily due to errors in accurately generating answers involving uncommon entity nouns.\nOverall, these findings underscore a persistent challenge in direct speech generation, particularly when tool results are integrated, as this appears to negatively impact the quality of generated speech responses.\nOur <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> demonstrates clear advantages: it maintains comparable performance for text output and delivers substantial improvements for speech output, even outperforming the self-cascade approach. These results underscore the effectiveness of post-training in overcoming challenges in direct speech generation in tool-augmented scenarios.</p>\n\n",
                "matched_terms": [
                    "model",
                    "rag",
                    "text",
                    "output",
                    "modeltriggered",
                    "length",
                    "qwen257b",
                    "ref",
                    "results",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> provides a comprehensive breakdown of latency measurements for the Qwen2.5-7B speech-to-speech model, evaluated in both the Open Book setting and our proposed <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> setting on AudioCRAG-Synthetic. We report both median (P50) and 90th percentile (P90) values for each stage of the processing pipeline. The latency is decomposed into three main components: tool query generation, tool result generation, and speech response synthesis, with measurements provided for both the first token outputs (We also provide latency measurements for last token output in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T9\" title=\"Table 9 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>). For both tool query and tool result generation, it is assumed that all tools are accessed in parallel; thus, the reported latency corresponds to the maximum query or result generation time among all tools. The majority of this latency arises from leveraging external web pages, which introduces significant delays, most notably, increasing the first token latency by 2.3x in Open Book Setting.\nNotably, our <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> setting enables early generation of tool results, successfully reducing P50 first token latency by 9.8% and tool use latency by 20.7%.</p>\n\n",
                "matched_terms": [
                    "model",
                    "rag",
                    "output",
                    "book",
                    "qwen257b",
                    "open",
                    "modeltriggered",
                    "results",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Streaming RAG approaches</span>: Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> highlights the substantial latency savings enabled by the <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>), compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS1\" title=\"3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>). Three key metrics are evaluated: overall latency savings, the percentage of queries benefiting from reduced latency, and the number of parallel threads required.\nEven without any post-training, the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach already reduces tool usage latency (3.37s in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) by 10.7% for Open Book Qwen-OMNI, demonstrating its flexibility and plug-and-play compatibility with any existing speech-in speech-out model.\nThe <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> method, utilizing Qwen-OMNI, consistently delivers superior performance. It achieves greater average latency reductions and benefits a higher proportion of queries with improved response times. Notably, <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> require only a single parallel thread, representing a significant advancement in resource efficiency compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach, which demands multiple parallel threads.\nThese findings highlight the effectiveness of <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> in not only minimizing latency, validating our intuition from Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>, but also in optimizing system efficiency and resource utilization.</p>\n\n",
                "matched_terms": [
                    "model",
                    "rag",
                    "book",
                    "modeltriggered",
                    "open",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-training Strategies</span>: Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents performance comparison under different post-training conditions. Incorporating streaming tool query generation during post-training (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>) results in comparable performance for both models. These results suggest that <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> can be integrated into post-training process without negatively impacting model performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "rag",
                    "modeltriggered",
                    "results",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced the first comprehensive approach for integrating external tool usage directly into E2E speech-in, speech-out dialogue systems. By incorporating <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> pipeline, we further enhanced the model&#8217;s ability to leverage retrieved information and autonomously decide when to trigger new tool queries, resulting in improved accuracy and responsiveness.\nEmpirical evaluation on the newly introduced AudioCRAG benchmark demonstrated that tool integration can more than double factual question answering accuracy compared to closed-book models. Additionally, our streaming RAG approach achieved a 20% reduction in tool usage latency, thereby preserving natural conversational flow.\nOverall, our contributions advance the state of the art in spoken dialogue systems by enabling accurate, real-time, and tool-augmented voice assistants. In addition, we are committed to open science and will release our training code and Audio CRAG Human benchmark to support future research and development in this area.</p>\n\n",
                "matched_terms": [
                    "modeltriggered",
                    "streaming",
                    "rag",
                    "open"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We begin with the CRAG dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>, which contains 2,706 text queries. Since these queries are not directly suitable for speech-based evaluation, we first identify those requiring adaptation before TTS conversion. Through careful manual inspection, we determine that queries containing dates or brackets benefit from rewriting to ensure naturalness and clarity in spoken form. In total, we identify 569 such queries and rewrite them using a large language model (LLAMA-4 Maverick). The resulting 569 rewritten queries, combined with the remaining original queries, form the basis for our spoken version of CRAG, which we term <em class=\"ltx_emph ltx_font_italic\">AudioCRAG</em>. We follow the CRAG setup to incorporate web and KG-based tools and adopt its robust evaluation setup, as detailed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S10\" title=\"10 Tool Usage Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S11\" title=\"11 Evaluation Setting &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Similar to previous work <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>, we employ model-based automatic evaluation. We use a three-way scoring system, assigning scores of 1, -1, and 0 for accurate, incorrect, and missing answers, respectively. The evaluation is conducted using the Llama 4-maverick LLM evaluator. For speech outputs, we first transcribe the audio using Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib27\" title=\"\">2023</a>)</cite> before passing the transcriptions to the LLM evaluator. In this study, our primary focus is on enhancing system accuracy; therefore, we report average accuracy values in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Evaluated SOTA Speech-in Speech-out Models &#8227; 4 Experiment Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. For additional context, we also provide the average rates of accurate, hallucinated, and missing responses, as well as overall truthfulness scores for each system in the Open Book Setting (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S10.T6\" title=\"Table 6 &#8227; 10 Tool Usage Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>). Notably, our results indicate that Qwen-OMNI was less likely to generate hallucinated responses compared to OpusLM and Kimi Audio.</p>\n\n",
                "matched_terms": [
                    "results",
                    "book",
                    "open"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qwen-OMNI</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>)</cite> is a end-to-end multimodal model that seamlessly integrates diverse input modalities&#8212;including text, images, audio, and video&#8212;and generates both text and natural speech responses in a real-time streaming fashion. It leverages an innovative Thinker-Talker architecture, where the Thinker module performs high-level reasoning to produce a text response, which is then used by the Talker module, conditioning on both the text and the Thinker&#8217;s hidden representations, to generate streaming speech output.</p>\n\n",
                "matched_terms": [
                    "text",
                    "output",
                    "model",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Kimi Audio</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib8\" title=\"\">2025</a>)</cite> is a universal audio foundation model that unifies audio understanding, generation, and conversational abilities within a single framework. Pre-trained on over 13 million hours of diverse audio and text data, Kimi Audio achieves state-of-the-art performance across a wide range of audio benchmarks, including audio understanding and speech conversation tasks.</p>\n\n",
                "matched_terms": [
                    "text",
                    "data",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For tool-augmented scenarios, retrieval results are provided up to each model&#8217;s maximum token limit (&#8220;Ref length&#8221; in Tables), maintaining a 2:1 ratio of web page to KG results.\nSpecifically, we observe that Kimi-Audio is currently optimized for handling tool result references up to a certain length. When this limit is exceeded, an error arises during the audio detokenization process, specifically within the rotary embedding mechanism, highlighting an architectural constraint in processing longer input sequences or larger reference contexts. Addressing this limitation presents a valuable opportunity for future model enhancements.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation encompasses both speech-in-text-out and speech-in-speech-out scenarios. For the Fixed-Interval Streaming RAG setting (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS1\" title=\"3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>), intermediate tool queries are generated at consistent 1-second intervals. In the Model-Triggered Streaming RAG setting, the model dynamically determines the need for a tool call after processing each 500ms block. This approach allows us to utilize a smaller chunk size, as only a single tool call thread is required for Model-Triggered Streaming RAG, thereby enabling more efficient and responsive processing.</p>\n\n",
                "matched_terms": [
                    "model",
                    "modeltriggered",
                    "rag",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This subsection details the experimental setup for post-training the pretrained speech-in, speech-out model to significantly enhance its tool usage capabilities, as outlined in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>.\nWe leverage a random subset of 16,000 questions from the text-based factual question answering dataset TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib15\" title=\"\">2017</a>)</cite>, which contains 97,000 questions and 662,659 associated web documents.\nFor <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, we further compute word-level timestamps using a pre-trained ASR model, OWSM CTC v4 1B <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib26\" title=\"\">2025</a>)</cite>, enabling us to generate partial ASR transcripts <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m1\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math> at 500 ms intervals. Note that if word occurs at boundary of block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, it is excluded from <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math>. For each partial transcript <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math>, we generate corresponding psuedo ground truth queries <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m5\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> using LLAMA-4-Maverick to simulate incremental user input.\nTo simulate realistic tool usage, we concatenate all documents and employ a web query reranker to retrieve the top 50 most relevant documents for each query. The text questions from this 16k subset are converted into discrete speech tokens using text-to-speech synthesis with the corresponding pretrained speech-in, speech-out model. Recognizing that TriviaQA answers are typically single named entities, we further transform the queries into a conversational style using LLAMA-4-Maverick, making them more suitable for dialogue-based evaluation.</p>\n\n",
                "matched_terms": [
                    "16k",
                    "model",
                    "rag",
                    "text",
                    "modeltriggered",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT = \"\"\"\nYou are an agent that only outputs JSON. You are given a Query and Query Time. Do the following:\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>1) Determine the domain the query is about. The domain should be one of the following: \"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". If none of the domains apply, use \"other\". Use \"domain\" as the key in the result json.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>2) Extract structured information from the query. Include different keys into the result json depending on the domains, and put them DIRECTLY in the result json. Here are the rules:\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;encyclopedia&#8217; and &#8216;other&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;main_entity&#8217;: extract the main entity of the query.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;finance&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;market_identifier&#8217;: stock identifiers including individual company names, stock symbols.\n<br class=\"ltx_break\"/>- &#8216;metric&#8217;: financial metrics that the query is asking about. This must be one of the following: &#8216;price&#8217;, &#8216;dividend&#8217;, &#8216;P/E ratio&#8217;, &#8216;EPS&#8217;, &#8216;marketCap&#8217;, and &#8216;other&#8217;.\n<br class=\"ltx_break\"/>- &#8216;datetime&#8217;: time frame that the query asks about. When datetime is not explicitly mentioned, use &#8216;Query Time&#8217; as default.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;movie&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;movie_name&#8217;: name of the movie\n<br class=\"ltx_break\"/>- &#8216;movie_aspect&#8217;: if the query is about a movie, which movie aspect the query asks. This must be one of the following: &#8216;budget&#8217;, &#8216;genres&#8217;, &#8216;original_language&#8217;, &#8216;original_title&#8217;, &#8216;release_date&#8217;, &#8216;revenue&#8217;, &#8216;title&#8217;, &#8216;cast&#8217;, &#8216;crew&#8217;, &#8216;rating&#8217;, &#8216;length&#8217;.\n<br class=\"ltx_break\"/>- &#8216;person&#8217;: person name related to moves\n<br class=\"ltx_break\"/>- &#8216;person_aspect&#8217;: if the query is about a person, which person aspect the query asks. This must be one of the following: &#8216;acted_movies&#8217;, &#8216;directed_movies&#8217;, &#8216;oscar_awards&#8217;, &#8216;birthday&#8217;.\n<br class=\"ltx_break\"/>- &#8216;year&#8217;: if the query is about movies released in a specific year, extract the year\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;music&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;artist_name&#8217;: name of the artist\n<br class=\"ltx_break\"/>- &#8216;artist_aspect&#8217;: if the query is about an artist, extract the aspect of the artist. This must be one of the following: &#8216;member&#8217;, &#8216;birth place&#8217;, &#8216;birth date&#8217;, &#8216;lifespan&#8217;, &#8216;artist work&#8217;, &#8216;grammy award count&#8217;, &#8216;grammy award date&#8217;.\n<br class=\"ltx_break\"/>- &#8216;song_name&#8217;: name of the song\n<br class=\"ltx_break\"/>- &#8216;song_aspect&#8217;: if the query is about a song, extract the aspect of the song. This must be one of the following: &#8216;author&#8217;, &#8216;grammy award count&#8217;, &#8216;release country&#8217;, &#8216;release date&#8217;.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;sports&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;sport_type&#8217;: one of &#8216;basketball&#8216;, &#8216;soccer&#8216;, &#8216;other&#8216;\n<br class=\"ltx_break\"/>- &#8216;tournament&#8217;: NBA, World Cup, Olympic.\n<br class=\"ltx_break\"/>- &#8216;team&#8217;: teams that users are interested in.\n<br class=\"ltx_break\"/>- &#8216;datetime&#8217;: time frame that the user is interested in. When datetime is not explicitly mentioned, use &#8216;Query Time&#8217; as default.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Return the results in a FLAT json.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n<br class=\"ltx_break\"/>\"\"\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8216;sports&#8217; queries, these are possible keys:\n- &#8216;sport_type&#8217;: one of &#8216;basketball&#8216;, &#8216;soccer&#8216;, &#8216;other&#8216;\n- &#8216;tournament&#8217;: NBA, World Cup, Olympic.\n- &#8216;team&#8217;: teams that users are interested in.\n- &#8216;datetime&#8217;: time frame that the user is interested in. When datetime is not explicitly mentioned, use\n&#8216;Query Time&#8217; as default.\nReturn the results in a FLAT json.\n<br class=\"ltx_break\"/>*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*</p>\n\n",
                "matched_terms": [
                    "output",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">3) Compare your newly generated result to the &#8217;Previous Result&#8217;. **If your new result would be exactly the same as the &#8217;Previous Result&#8217;, output only NO_QUERY.**\nReturn the results in a FLAT json.</p>\n\n",
                "matched_terms": [
                    "output",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When rewriting the query, ensure that it captures all important information from the original question that could impact the retrieval results. Do not omit any crucial details, such as specific dates, locations, or relationships between entities. Also, do not invent any new details on your own. If necessary, use the Query Time to provide context for the query. The goal is to create a concise and accurate query that effectively conveys the user&#8217;s intent and retrieves relevant information.\n*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT THE REWRITTEN QUERY!!!*\n\"\"\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the length of amazon river?\"\nGround Truth: \"over 4000 miles\"\nPrediction: \"the length of amazon river is 4,000 miles\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not say Amazon River is longer than 4000 miles.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "length"
                ]
            }
        ]
    },
    "S10.T6": {
        "source_file": "Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage",
        "caption": "Table 6: Results on AudioCRAG-Synthetic for Qwen2.5-7B, OpusLM, and Kimi Audio comparing text vs. speech output across Open Book setting showing average rates of accurate, hallucinated, and missing responses, as well as overall\ntruthfulness scores for each system.",
        "body": "Ref length\nOutput\nModel\nScore\nAcc.\nHalluc\nMiss.\n\n\n\n\n0\nText\nQwen2.5-7B\n-13.1\n15.0\n28.1\n56.9\n\n\n0\nSpeech\nQwen2.5-7B\n-21.1\n11.1\n32.3\n56.6\n\n\n0\nText\nOpusLM\n-44.3\n20.1\n64.3\n15.6\n\n\n0\nSpeech\nOpusLM\n-47.9\n18.4\n66.2\n15.4\n\n\n0\nText\nKimi Audio\n-38.5\n24.2\n62.7\n13.1\n\n\n0\nSpeech\nKimi Audio\n-53.5\n16.7\n70.3\n12.9",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Ref length</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Output</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Model</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Score</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Acc.</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Halluc</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Miss.</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">0</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Text</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Qwen2.5-7B</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">-13.1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">15.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">28.1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">56.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">0</th>\n<td class=\"ltx_td ltx_align_left\">Speech</td>\n<td class=\"ltx_td ltx_align_left\">Qwen2.5-7B</td>\n<td class=\"ltx_td ltx_align_left\">-21.1</td>\n<td class=\"ltx_td ltx_align_left\">11.1</td>\n<td class=\"ltx_td ltx_align_left\">32.3</td>\n<td class=\"ltx_td ltx_align_left\">56.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">0</th>\n<td class=\"ltx_td ltx_align_left\">Text</td>\n<td class=\"ltx_td ltx_align_left\">OpusLM</td>\n<td class=\"ltx_td ltx_align_left\">-44.3</td>\n<td class=\"ltx_td ltx_align_left\">20.1</td>\n<td class=\"ltx_td ltx_align_left\">64.3</td>\n<td class=\"ltx_td ltx_align_left\">15.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">0</th>\n<td class=\"ltx_td ltx_align_left\">Speech</td>\n<td class=\"ltx_td ltx_align_left\">OpusLM</td>\n<td class=\"ltx_td ltx_align_left\">-47.9</td>\n<td class=\"ltx_td ltx_align_left\">18.4</td>\n<td class=\"ltx_td ltx_align_left\">66.2</td>\n<td class=\"ltx_td ltx_align_left\">15.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">0</th>\n<td class=\"ltx_td ltx_align_left\">Text</td>\n<td class=\"ltx_td ltx_align_left\">Kimi Audio</td>\n<td class=\"ltx_td ltx_align_left\">-38.5</td>\n<td class=\"ltx_td ltx_align_left\">24.2</td>\n<td class=\"ltx_td ltx_align_left\">62.7</td>\n<td class=\"ltx_td ltx_align_left\">13.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">0</th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Speech</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Kimi Audio</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">-53.5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">16.7</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">70.3</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">12.9</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "kimi",
            "hallucinated",
            "overall",
            "text",
            "output",
            "qwen257b",
            "length",
            "well",
            "score",
            "halluc",
            "truthfulness",
            "speech",
            "each",
            "across",
            "opuslm",
            "book",
            "open",
            "results",
            "system",
            "acc",
            "responses",
            "model",
            "accurate",
            "average",
            "ref",
            "comparing",
            "showing",
            "rates",
            "miss",
            "setting",
            "missing",
            "scores",
            "audiocragsynthetic",
            "audio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Similar to previous work <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>, we employ model-based automatic evaluation. We use a three-way scoring system, assigning scores of 1, -1, and 0 for accurate, incorrect, and missing answers, respectively. The evaluation is conducted using the Llama 4-maverick LLM evaluator. For speech outputs, we first transcribe the audio using Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib27\" title=\"\">2023</a>)</cite> before passing the transcriptions to the LLM evaluator. In this study, our primary focus is on enhancing system accuracy; therefore, we report average accuracy values in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Evaluated SOTA Speech-in Speech-out Models &#8227; 4 Experiment Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. For additional context, we also provide the average rates of accurate, hallucinated, and missing responses, as well as overall truthfulness scores for each system in the Open Book Setting (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S10.T6\" title=\"Table 6 &#8227; 10 Tool Usage Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>). Notably, our results indicate that Qwen-OMNI was less likely to generate hallucinated responses compared to OpusLM and Kimi Audio.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">End-to-end speech-in speech-out dialogue systems are emerging as a powerful alternative to traditional ASR&#8211;LLM&#8211;TTS pipelines, generating more natural, expressive responses with significantly lower latency. However, these systems remain prone to hallucinations due to limited factual grounding. While text-based dialogue systems address this challenge by integrating tools such as web search and knowledge graph APIs, we introduce the first approach to extend tool use directly into speech-in speech-out systems. A key challenge is that tool integration substantially increases response latency, disrupting conversational flow. To mitigate this, we propose Streaming Retrieval-Augmented Generation (<em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>), a novel framework that reduces user-perceived latency by predicting tool queries in parallel with user speech, even before the user finishes speaking. Specifically, we develop a post-training pipeline that teaches the model when to <em class=\"ltx_emph ltx_font_italic\">issue tool calls</em> during ongoing speech and how to generate spoken summaries that fuse audio queries with retrieved text results, thereby improving both <em class=\"ltx_emph ltx_font_italic\">accuracy and responsiveness</em>. To evaluate our approach, we construct AudioCRAG, a benchmark created by converting queries from the publicly available CRAG dataset into speech form. Experimental results demonstrate that our <em class=\"ltx_emph ltx_font_italic\">streaming RAG</em> approach increases QA accuracy by up to 200% relative (from 11.1% to 34.2% absolute) and further enhances user experience by reducing tool use latency by 20%. Importantly, our <em class=\"ltx_emph ltx_font_italic\">streaming RAG</em> approach is modality-agnostic and can be applied equally to typed input, paving the way for more agentic, real-time AI assistants.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "responses",
                    "text",
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spoken Dialogue Systems (SDS) are foundational to many everyday technologies, powering intelligent assistants such as Alexa and Siri, as well as interactive voice response systems in customer service. With the rapid expansion of SDS capabilities to mobile phones and wearable devices, the need for robust, scalable, and generalizable solutions has never been greater. Traditionally, SDS have relied on cascaded pipelines composed of multiple modules&#8212;including voice activity detection (VAD), automatic speech recognition (ASR), natural language understanding (NLU), natural language generation (NLG), and text-to-speech (TTS) synthesis&#8212;each introducing potential points of failure and latency <cite class=\"ltx_cite ltx_citemacro_citep\">(Glass, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib12\" title=\"\">1999</a>; Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib13\" title=\"\">2024</a>)</cite>.\nRecently, end-to-end (E2E) SDS <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib34\" title=\"\">2024</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib22\" title=\"\">2023</a>; Meng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib20\" title=\"\">2024</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib40\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib2\" title=\"\">2025b</a>)</cite> have been proposed, which directly generate spoken responses from speech input within a unified architecture. This E2E approach not only mitigates error propagation across modules but also captures non-phonemic information more effectively, resulting in significantly lower inference time and computational overhead, and paving the way for more natural and efficient conversational experiences.</p>\n\n",
                "matched_terms": [
                    "well",
                    "speech",
                    "responses",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, current E2E SDS are fundamentally constrained by their reliance on internalized knowledge from static training data, which often results in responses that lack factual grounding or fail to reflect the most up to date information. This shortcoming is particularly critical for action-oriented or knowledge-seeking tasks, such as booking hotels or answering questions about current events. In contrast, text-based conversational assistants have begun to overcome these limitations by integrating external tools through Retrieval-Augmented Generation (RAG)<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>; Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib3\" title=\"\">2023a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib5\" title=\"\">c</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib10\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib11\" title=\"\">b</a>)</cite>, dynamically retrieving relevant information from sources like web search, knowledge graphs (KG), and real-time APIs. Yet, the integration of such tool use into E2E SDS remains largely unexplored.\nA key challenge is that while external tools can substantially improve factual accuracy, invoking them often introduces additional latency, leading to awkward silences that disrupt the natural conversational flow. This raises a research question: <span class=\"ltx_text ltx_font_italic\">How can we trade-off between accuracy and responsiveness for developing SDS that feel both intelligent and natural?</span></p>\n\n",
                "matched_terms": [
                    "results",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present, to the best of our knowledge, the first <span class=\"ltx_text ltx_font_italic\">speech-in, speech-out language model that seamlessly integrates external tool invocation with low latency</span>. The key idea is a <span class=\"ltx_text ltx_font_bold\">Streaming RAG</span> strategy that generates tool queries in parallel with user speech, often times even before the user has finished speaking (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). A naive implementation of streaming queries, however, faces two challenges: (1) queries issued from partial speech may be suboptimal, yielding distracting tool outputs and inaccurate responses; and (2) unnecessary tool calls may be triggered, wasting computational resources. We introduce effective modeling techniques to address these challenges and make the following contributions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution 2</span>: To address this, we propose <em class=\"ltx_emph ltx_font_italic\">Streaming Retrieval-Augmented Generation</em> (<em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>), the <em class=\"ltx_emph ltx_font_italic\">first</em> framework that empowers the system to trigger tool queries in parallel with user speech, even before the user finishes speaking. Within this framework, we introduce two novel approaches: (1) <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em>, which issues tool queries at regular intervals during speech input and carefully examines quality of retrieval results on the full query to guarantee response quality, and can be incorporated into any speech-in, speech-out model without post-training; (2) <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, which post-trains the model to intelligently determine optimal query timing based on the evolving user utterance to save computation resources.\nOur results demonstrate that our proposed <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> delivers over 200% relative improvement in accuracy (from 11.1% to 34.2% absolute in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) compared to the no-tool baseline, while also reducing tool result generation latency by 20%. Though designed for speech-in speech-out systems, streaming RAG can also be adapted in cascaded SDS, or even chatbots as users type.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "system",
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several E2E spoken dialogue systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>; Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib34\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib1\" title=\"\">2025a</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib22\" title=\"\">2023</a>; Meng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib20\" title=\"\">2024</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib40\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib2\" title=\"\">2025b</a>)</cite> have recently been introduced, demonstrating impressive semantic understanding and high audio quality in their responses. However, these systems have not yet been trained or evaluated for their ability to <em class=\"ltx_emph ltx_font_italic\">use external tools</em>.\nAnother research direction <cite class=\"ltx_cite ltx_citemacro_citep\">(Feng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib9\" title=\"\">2025</a>)</cite> explores E2E RAG for direct speech-to-text retrieval, utilizing multimodal embeddings for enabling speech utterances to directly retrieve relevant text. While this method outperforms models without RAG, it is primarily limited to <em class=\"ltx_emph ltx_font_italic\">speech-to-text</em> scenarios. Its ability to access KGs and other APIs, critical for real-world applications, remains unproven. Furthermore, the retrieval scope is limited, as experiments are conducted with retrieval restricted to just 10 paragraphs, whereas modern RAG benchmarks require searching across thousands of web pages.\nAlthough recent studies <cite class=\"ltx_cite ltx_citemacro_citep\">(Maben et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib18\" title=\"\">2025</a>)</cite> have developed web interfaces that integrate tools into speech-in <em class=\"ltx_emph ltx_font_italic\">speech-out</em> scenarios using cascaded pipelines, comprehensive empirical investigations of E2E speech-in speech-out systems and, crucially, systematic analyses of user-perceived latency, remain largely unexplored. In this work, we address these gaps by developing a comprehensive framework for tool integration in E2E speech-in speech-out systems and designing benchmarks to quantitatively assess the tool usage capabilities of SOTA models. Furthermore, recognizing the importance of latency for natural conversational flow, we introduce novel streaming RAG methods that not only enhance tool usage performance but also reduce user-perceived latency.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "responses",
                    "across",
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A RAG spoken conversation system takes an audio question <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math> as input and outputs a spoken answer <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>. Let the ASR transcript of audio question be <math alttext=\"X^{\\text{asr}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>asr</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}</annotation></semantics></math> and the ASR transcript of the spoken answer be <math alttext=\"X^{\\text{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>res</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{res}}</annotation></semantics></math>.\nAnswers are generated by speech-in speech-out models, leveraging both the model&#8217;s internal knowledge and information retrieved from external sources.\nTo incorporate external information, the model needs to formulate a tool query <math alttext=\"Q^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><msup><mi>Q</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">Q^{T}</annotation></semantics></math> to retrieve relevant results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m6\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from an external tool <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m7\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "system",
                    "model",
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our proposed formulation for integrating external tools into speech-in speech-out systems. We introduce a two-stage inference approach: <em class=\"ltx_emph ltx_font_italic\">Query Generation</em> and <em class=\"ltx_emph ltx_font_italic\">Response Generation</em>. In the Query Generation stage, the system processes an audio question and generates queries for each external tool to retrieve relevant information by maximizing the posterior distribution <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> (Examples of generated queries are provided in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T7\" title=\"Table 7 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> in the Appendix.). In the Response Generation stage, the retrieved results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from these tools are combined with the original audio question and input into the model to generate the final spoken response by maximizing the posterior distribution <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math>. By conditioning the final output generation on the input audio, this formulation not only provides a simple and effective mechanism for interacting with text-based APIs, but also preserves the key advantages of speech-in speech-out systems, mitigating error propagation and enabling the model to capture non-phonemic information (such as prosody and speaker intent) more effectively.</p>\n\n",
                "matched_terms": [
                    "each",
                    "system",
                    "model",
                    "output",
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">RAG-based systems, as proposed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, can significantly improve factual accuracy by incorporating external tools. However, these tool calls often introduce substantial latency, which is particularly problematic in speech-in, speech-out applications where users expect rapid, conversational responses and even brief silences can disrupt the natural flow of dialogue.\nOne way to address this challenge lies in the nature of audio inputs, which arrive as a continuous stream. This streaming property enables tool calls to be initiated before the user has finished speaking, offering a unique opportunity to mitigate latency.</p>\n\n",
                "matched_terms": [
                    "responses",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To minimize user-perceived latency, we introduce <em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>: the first framework to generate and issue tool queries in parallel as audio input is received. This novel approach is built on three key design components:\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> Trigger: When to initiate a new tool query;\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> Threads: The number of parallel tool query threads;\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic3\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">3</span></span></foreignobject></g></g></svg></span> Reflector: The module that determines whether intermediate tool results are sufficient for generating the final output. By exploring different design choices for each component, we introduce two complementary approaches for streaming tool query generation in the following subsections: a fixed-interval trigger method and a model-based trigger method.</p>\n\n",
                "matched_terms": [
                    "each",
                    "output",
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this approach, the <em class=\"ltx_emph ltx_font_italic\">trigger</em> is set to fire tool calls at fixed chunk intervals during audio input.\nThe input speech <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math> is divided into a sequence of <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> blocks, <math alttext=\"Q=\\{Q_{b}\\mid b=1,\\dots,B\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>Q</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>Q</mi><mi>b</mi></msub><mo fence=\"true\" lspace=\"0em\" rspace=\"0em\">&#8739;</mo><mrow><mi>b</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>B</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Q=\\{Q_{b}\\mid b=1,\\dots,B\\}</annotation></semantics></math>, with each block containing <math alttext=\"N_{\\text{block}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>N</mi><mtext>block</mtext></msub><annotation encoding=\"application/x-tex\">N_{\\text{block}}</annotation></semantics></math> frames.\nTo approximate <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> as described in S. 3.1, we follow a block-wise prediction strategy. In this approach, after processing each audio block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m6\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, the model predicts a tool query <math alttext=\"\\hat{Q}_{b}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m7\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}_{b}^{T}</annotation></semantics></math> by conditioning on the input speech accumulated up to block b, specifically <math alttext=\"Q_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">Q_{1:b}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "each",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This strategy results in <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m9\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> parallel tool call threads running simultaneously (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F3.sf1\" title=\"Figure 3(a) &#8227; Figure 3 &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>), where each thread generates a tool query prediction <math alttext=\"\\hat{Q}_{b}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m10\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}_{b}^{T}</annotation></semantics></math> for its corresponding block <math alttext=\"b\\in[1,B]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m11\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mi>B</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b\\in[1,B]</annotation></semantics></math>.\nThe tool queries <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m12\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> generated after each block are then stored in cache. Given the high latency of tool calls (See T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), users typically complete their utterances before tool responses are ready. Upon utterance completion, an explicit <em class=\"ltx_emph ltx_font_italic\">reflector</em> module &#8220;<math alttext=\"\\text{reflect}()\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m13\" intent=\":literal\"><semantics><mrow><mtext>reflect</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{reflect}()</annotation></semantics></math>&#8221; evaluates the cached intermediate queries <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m14\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> against final query <math alttext=\"\\hat{Q}^{T}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m15\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{B}</annotation></semantics></math> to determine whether an early intermediate tool call provides sufficient information to answer the user&#8217;s question <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m16\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math>. The reflector module systematically evaluates all intermediate queries in the cache and identifies the <em class=\"ltx_emph ltx_font_italic\">earliest</em> sufficient tool call <math alttext=\"b^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m17\" intent=\":literal\"><semantics><msup><mi>b</mi><mo>&#8902;</mo></msup><annotation encoding=\"application/x-tex\">b^{\\star}</annotation></semantics></math> where the intermediate query <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m18\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> will give the same result as final query <math alttext=\"\\hat{Q}^{T}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m19\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{B}</annotation></semantics></math> as shown:</p>\n\n",
                "matched_terms": [
                    "each",
                    "results",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All subsequent parallel tool calls after <math alttext=\"b^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m20\" intent=\":literal\"><semantics><msup><mi>b</mi><mo>&#8902;</mo></msup><annotation encoding=\"application/x-tex\">b^{\\star}</annotation></semantics></math> are promptly terminated, and the retrieved results <math alttext=\"R_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m21\" intent=\":literal\"><semantics><msub><mi>R</mi><msup><mi>b</mi><mo>&#8902;</mo></msup></msub><annotation encoding=\"application/x-tex\">R_{b^{\\star}}</annotation></semantics></math> from this intermediate call <math alttext=\"\\hat{Q}^{T}_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m22\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><msup><mi>b</mi><mo>&#8902;</mo></msup><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b^{\\star}}</annotation></semantics></math> are used to generate the final spoken response <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m23\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by maximizing the posterior distribution <math alttext=\"P(A|Q,R_{b^{\\star}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m24\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><msub><mi>R</mi><msup><mi>b</mi><mo>&#8902;</mo></msup></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R_{b^{\\star}})</annotation></semantics></math> (instead of <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m25\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math> in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>).\nWe employ a reflector module that uses simple yet effective heuristics: (a) if the top 5 web documents for an intermediate web query match those of the final web query, and (b) if the KG results for intermediate and final KG queries are identical.\nThese heuristics ensure that the information retrieved from an early tool call using <math alttext=\"\\hat{Q}^{T}_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m26\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><msup><mi>b</mi><mo>&#8902;</mo></msup><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b^{\\star}}</annotation></semantics></math> is consistent with what would have been obtained by waiting for the final tool call using <math alttext=\"\\hat{Q}^{T}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m27\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{B}</annotation></semantics></math>, thereby providing a strong quality guarantee. Since most tool call latency arises from chunking and reranking the chunks of web documents, these checks enable significant latency savings without any compromise in model performance.\nA key advantage of this strategy is its plug-and-play nature: it requires no additional post-training for speech-in, speech-out models and can be directly applied at inference time across a variety of architectures. However, there are important considerations. First, generating parallel tool calls at every fixed interval (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E1\" title=\"Equation 1 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) increases computational overhead, which may pose challenges for deployment on resource-constrained devices such as wearables devices. Second, reliance on an external reflector module (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E2\" title=\"Equation 2 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) to determine the sufficiency of intermediate tool calls may limit the extent of achievable latency improvements.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the limitations of Fixed-Interval Streaming RAG and further optimize both efficiency and responsiveness, we propose a more adaptive approach: <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>.\nHere, the <em class=\"ltx_emph ltx_font_italic\">trigger</em> is learned: the model is trained to autonomously determine the optimal moments to initiate tool queries, issuing a query only when it encounters new or additional information as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F3.sf2\" title=\"Figure 3(b) &#8227; Figure 3 &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>. In this formulation, the model receives user input in fixed chunk intervals as before and intelligently determines whether a tool call is needed after each block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>. The model can either:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p1.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> Predict\nNO_QUERY if a new tool query is unnecessary, or\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p1.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> Generate a new tool query. To make this decision, the model conditions on both the accumulated input speech <math alttext=\"Q_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">Q_{1:b}</annotation></semantics></math> (see Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E1\" title=\"Equation 1 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) as well as most recent tool query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}=\\hat{Q}^{T}_{\\max\\{\\,i&lt;b:\\hat{Q}^{T}_{i}\\neq\\text{NO\\_QUERY}\\,\\}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><mo>=</mo><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">{</mo><mrow><mrow><mi>i</mi><mo>&lt;</mo><mi>b</mi></mrow><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>i</mi><mi>T</mi></msubsup><mo>&#8800;</mo><mtext>NO_QUERY</mtext></mrow></mrow><mo lspace=\"0.170em\" stretchy=\"false\">}</mo></mrow></mrow><mi>T</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}=\\hat{Q}^{T}_{\\max\\{\\,i&lt;b:\\hat{Q}^{T}_{i}\\neq\\text{NO\\_QUERY}\\,\\}}</annotation></semantics></math> as shown:</p>\n\n",
                "matched_terms": [
                    "well",
                    "each",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When a new query <math alttext=\"\\hat{Q}^{T}_{b}\\neq\\text{NO\\_QUERY}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m4\" intent=\":literal\"><semantics><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><mo>&#8800;</mo><mtext>NO_QUERY</mtext></mrow><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}\\neq\\text{NO\\_QUERY}</annotation></semantics></math>, the system immediately terminates any ongoing tool calls for the previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m5\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, ensuring that only a <em class=\"ltx_emph ltx_font_italic\">single tool call thread</em> runs at any given time. (Examples of generated <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m6\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> are provided in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T8\" title=\"Table 8 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> in the Appendix.)\nThis approach offers several key advantages. First, it effectively eliminating redundant parallel threads and significantly reducing computational overhead. This is especially important for deployment on resource-constrained devices.\nSecond, this formulation removes the need for an external <em class=\"ltx_emph ltx_font_italic\">reflector</em> (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E2\" title=\"Equation 2 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) module. The model confidently relies on the results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from the most recent tool call using <math alttext=\"\\hat{Q}^{\\text{prev}}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m8\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{B}</annotation></semantics></math> to generate the spoken response <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m9\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by maximizing <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math>, reducing system complexity.</p>\n\n",
                "matched_terms": [
                    "system",
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-training</span>: To train the model, we transform text-based tool usage benchmarks into spoken format (See S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13\" title=\"13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>). Word-level timestamps are computed using a pre-trained ASR model. For each partial ASR transcript <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math> up to block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, we generate corresponding queries for each tool <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m3\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> using an LLM as pseudo ground truth (GT).\nTo create effective training labels that teach the model when to trigger new queries, we employ a similarity-based labeling strategy where we compare the current pseudo GT query <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m4\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> with the most recent non-empty tool query label <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m5\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math> (see Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E3\" title=\"Equation 3 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) before block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m6\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>.\nOur labeling function assigns the training label <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m7\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E3\" title=\"Equation 3 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) for the tool query after block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m8\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> as follows:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p2.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> when the current query is sufficiently similar to the previous query (as determined by manually defined heuristics <math alttext=\"f(\\cdots)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m9\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8943;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f(\\cdots)</annotation></semantics></math>), we assign the special label NO_QUERY to teach the model that no new tool call is needed.  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p2.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> When the queries are sufficiently different, we assign the actual pseudo ground truth query\n<math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m10\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> as the label to teach the model to trigger a new tool call:</p>\n\n",
                "matched_terms": [
                    "each",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a multi-task fine-tuning strategy targeting two key capabilities.\nFirst, we train the model on Streaming Tool Query Generation by optimizing <math alttext=\"P(Q^{T}_{b}|Q_{1:b},\\hat{Q}^{\\text{prev}}_{b})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo fence=\"false\">|</mo><mrow><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><mo>,</mo><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}_{b}|Q_{1:b},\\hat{Q}^{\\text{prev}}_{b})</annotation></semantics></math> for <math alttext=\"b\\in[1,B]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m2\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mi>B</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b\\in[1,B]</annotation></semantics></math>, enabling intelligent decisions about when to trigger tool queries. Second, we fine-tune on Response Generation by optimizing <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m3\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) to improve the intelligibility of the speech output.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "output",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An important aspect of our post-training is enhancing the model&#8217;s ability to recover from errors in intermediate query predictions. For example, when presented with the audio question, &#8220;Who founded Rare Beauty in 2019?&#8221;, we observed that an initial misinterpretation of <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m1\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, such as &#8220;Red Bull founder&#8221;, can lead the model to subsequently generate NO_QUERY labels, effectively halting further attempts to retrieve the correct information. This issue arises because, during training, the model is always provided with correct previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m2\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, whereas during inference, it may make errors due to partial utterances <math alttext=\"{Q}_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">{Q}_{1:b}</annotation></semantics></math> being ambiguous. Thus the model lacks the ability to recover from such mistakes during inference.\nTo overcome this, we introduce a novel strategy in which we deliberately inject negative samples during post-training by substituting the previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m4\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math> in Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E4\" title=\"Equation 4 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> with incorrect ones <math alttext=\"Q^{\\text{neg}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m5\" intent=\":literal\"><semantics><msubsup><mi>Q</mi><mi>b</mi><mtext>neg</mtext></msubsup><annotation encoding=\"application/x-tex\">Q^{\\text{neg}}_{b}</annotation></semantics></math>. Crucially, when we perform negative sampling, we fall back to the pseudo ground truth query <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> as the training label:</p>\n\n",
                "matched_terms": [
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To rigorously evaluate our proposed approach, we construct comprehensive benchmark datasets featuring spoken queries paired with simulated tool interactions. We begin with the CRAG dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>, which form the basis for our spoken version of CRAG, which we term <em class=\"ltx_emph ltx_font_italic\">AudioCRAG</em>. It consists of 2 distinct variants:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S4.SS1.p1.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> <span class=\"ltx_text ltx_font_bold\">Audio CRAG Synthetic</span>: To generate spoken queries, we use our in-house TTS system. We then apply a rigorous filtering procedure which results in a high-quality set of 1,862 spoken queries, which we refer to as the <em class=\"ltx_emph ltx_font_italic\">AudioCRAG-Synthetic</em> benchmark.  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S4.SS1.p1.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> <span class=\"ltx_text ltx_font_bold\">Audio CRAG Human</span>: To further enhance the realism and diversity of our evaluation, we introduce the AudioCRAG-Human benchmark, which consists of 618 human-recorded spoken queries. We will release\nAudio CRAG Human benchmark upon acceptance to supports the development of more natural and reliable voice assistants.\nFurther details on the construction of these benchmarks are provided in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S9\" title=\"9 Audio CRAG Benchmark &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. We follow the CRAG setup to incorporate both web and KG-based tools, and adopt its robust evaluation methodology, as described in Secs. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S10\" title=\"10 Tool Usage Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S11\" title=\"11 Evaluation Setting &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.\nAdditionally, we leverage a random subset of 16,000 questions from the text-based factual question answering dataset TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib15\" title=\"\">2017</a>)</cite> to post-train our speech-in, speech-out models, as detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13\" title=\"13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "system",
                    "audiocragsynthetic",
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform an ablation study (referred to as &#8220;Tool Integration&#8221; in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) where we post-train the model on sequential query generation (i.e. <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) and output generation, to assess the impact of streaming RAG post-training versus standard post-training on final response generation.\nAdditionally, we conduct an ablation study on open book setting (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) using a self-cascade approach with a three-stage inference pipeline: (1) the audio question is used to generate a tool query <math alttext=\"Q^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><msup><mi>Q</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">Q^{T}</annotation></semantics></math> (corresponding to the &#8220;Query Generation&#8221; stage described in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>); (2) the audio question, and tool results are combined to produce the final text output <math alttext=\"X^{\\text{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>res</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{res}}</annotation></semantics></math> by maximizing <math alttext=\"P(X^{\\text{res}}|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mtext>res</mtext></msup><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(X^{\\text{res}}|Q,R)</annotation></semantics></math>; and (3) the audio question, and final text output are used to generate the final speech output <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by optimizing <math alttext=\"P(A|Q,X^{\\text{res}})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m6\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><msup><mi>X</mi><mtext>res</mtext></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,X^{\\text{res}})</annotation></semantics></math>. Since we teacher-force the text output to obtain the final speech output in stage (3), this self-cascade approach can only be applied to a &#8220;thinker-talker&#8221; architecture (eq. Qwen-OMNI) or Chain-of-Thought (CoT) style architectures (eg. OpusLM).\nThe motivation for this ablation is to investigate whether the inclusion of RAG references <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> during the &#8220;Response Generation&#8221; stage (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) affects the quality of the generated speech <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m8\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "setting",
                    "text",
                    "opuslm",
                    "output",
                    "book",
                    "open",
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a comprehensive performance comparison of three models, Qwen2.5-7B, OpusLM, and Kimi Audio, evaluated across three settings: Closed Book, Open Book, and Streaming RAG. All models are assessed on both the AudioCRAG-Synthetic (Syn.) and AudioCRAG-Human (Hum.). In the Closed Book setting, where models rely solely on their internal knowledge without access to external tools (reference length = 0), all models achieve accuracy scores below 20%. These results highlight the inherent limitations of closed-book approaches in handling complex queries.\nThe Open Book setting, which provides models with access to external information, demonstrates the clear benefits of tool integration. Qwen2.5-7B and Kimi Audio&#8217;s accuracy rises substantially, underscoring the value of leveraging external context. As expected, latency increases due to the additional overhead of retrieving information from external tools (See T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> for detailed latency analysis).</p>\n\n",
                "matched_terms": [
                    "kimi",
                    "across",
                    "setting",
                    "opuslm",
                    "open",
                    "scores",
                    "book",
                    "length",
                    "qwen257b",
                    "audiocragsynthetic",
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most notably, our post-training approach to build <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, as described in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>, delivers significant advancements in both accuracy and efficiency. Qwen2.5-7B and OpusLM achieve significant accuracy improvements across both benchmarks. While the absolute accuracy scores may appear low, they are consistent with the evaluation results observed in the CRAG benchmark. Notably, Qwen2.5-7B with <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> achieves accuracy comparable to the open book performance of similarly sized LLMs reported in the original CRAG paper (i.e., 34.2 for Qwen2.5-7B vs. 32.1 for LLAMA-3 8B Instruct in <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>).\nImportantly, although post-training is performed exclusively on the synthetic dataset, we observe consistent and even greater improvements on the human-spoken benchmark, demonstrating the robustness and strong generalization capabilities of our method. This setting also introduces substantial latency savings compared to the Open Book configuration, with Qwen2.5-7B and OpusLM achieving 20.7% and 14.8% reductions in first-token latency on the synthetic benchmark, and even greater savings on the human benchmark\n<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Our latency calculations on synthetic audio exclude end-point detection latency, which is required in all production systems. Since our streaming RAG approach enables processing without waiting for end-point detection, including this factor would further amplify the observed latency savings, as observed in higher latency savings for human-spoken audio, where endpoint detection errors often introduce trailing silence.</span></span></span>. These results demonstrate that our streaming RAG approach not only advances the accuracy of speech-in speech-out systems, but also optimizes response efficiency by enabling earlier and more effective prediction of tool queries.</p>\n\n",
                "matched_terms": [
                    "across",
                    "setting",
                    "opuslm",
                    "scores",
                    "book",
                    "qwen257b",
                    "open",
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Evaluated SOTA Speech-in Speech-out Models &#8227; 4 Experiment Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides a comparative evaluation of the three SOTA models in generating either text or speech outputs from speech inputs, both with and without the integration of external tool results. In the absence of tool results (Ref length = 0), all models achieve higher accuracy when generating text outputs compared to speech outputs. The incorporation of tool results generally leads to improved text generation performance, with Kimi Audio achieving the highest accuracy. In contrast, the accuracy for speech output remains consistently lower across all models and conditions. The self-cascade approach, in which the model first generates an intermediate text response before producing the final speech output, provides moderate improvements in speech output accuracy for both Qwen2.5-7B and OpusLM.\nHowever, it still underperforms compared to the text-out baseline, primarily due to errors in accurately generating answers involving uncommon entity nouns.\nOverall, these findings underscore a persistent challenge in direct speech generation, particularly when tool results are integrated, as this appears to negatively impact the quality of generated speech responses.\nOur <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> demonstrates clear advantages: it maintains comparable performance for text output and delivers substantial improvements for speech output, even outperforming the self-cascade approach. These results underscore the effectiveness of post-training in overcoming challenges in direct speech generation in tool-augmented scenarios.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "kimi",
                    "overall",
                    "responses",
                    "model",
                    "across",
                    "text",
                    "opuslm",
                    "output",
                    "qwen257b",
                    "length",
                    "ref",
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> provides a comprehensive breakdown of latency measurements for the Qwen2.5-7B speech-to-speech model, evaluated in both the Open Book setting and our proposed <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> setting on AudioCRAG-Synthetic. We report both median (P50) and 90th percentile (P90) values for each stage of the processing pipeline. The latency is decomposed into three main components: tool query generation, tool result generation, and speech response synthesis, with measurements provided for both the first token outputs (We also provide latency measurements for last token output in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T9\" title=\"Table 9 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>). For both tool query and tool result generation, it is assumed that all tools are accessed in parallel; thus, the reported latency corresponds to the maximum query or result generation time among all tools. The majority of this latency arises from leveraging external web pages, which introduces significant delays, most notably, increasing the first token latency by 2.3x in Open Book Setting.\nNotably, our <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> setting enables early generation of tool results, successfully reducing P50 first token latency by 9.8% and tool use latency by 20.7%.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "each",
                    "model",
                    "setting",
                    "output",
                    "book",
                    "qwen257b",
                    "open",
                    "audiocragsynthetic",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Streaming RAG approaches</span>: Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> highlights the substantial latency savings enabled by the <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>), compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS1\" title=\"3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>). Three key metrics are evaluated: overall latency savings, the percentage of queries benefiting from reduced latency, and the number of parallel threads required.\nEven without any post-training, the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach already reduces tool usage latency (3.37s in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) by 10.7% for Open Book Qwen-OMNI, demonstrating its flexibility and plug-and-play compatibility with any existing speech-in speech-out model.\nThe <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> method, utilizing Qwen-OMNI, consistently delivers superior performance. It achieves greater average latency reductions and benefits a higher proportion of queries with improved response times. Notably, <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> require only a single parallel thread, representing a significant advancement in resource efficiency compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach, which demands multiple parallel threads.\nThese findings highlight the effectiveness of <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> in not only minimizing latency, validating our intuition from Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>, but also in optimizing system efficiency and resource utilization.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "system",
                    "model",
                    "book",
                    "open",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-training Strategies</span>: Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents performance comparison under different post-training conditions. Incorporating streaming tool query generation during post-training (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>) results in comparable performance for both models. These results suggest that <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> can be integrated into post-training process without negatively impacting model performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation Results for Negative Sampling Strategy</span>: Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T5\" title=\"Table 5 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the ablation results evaluating the impact of deliberately injecting negative samples during post-training (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E5\" title=\"Equation 5 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). The findings highlight that, without negative sampling, streaming tool query generation can reduce final accuracy in text output settings, primarily due to errors in final query generation, as detailed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>. In contrast, our negative sampling approach significantly enhances the model&#8217;s robustness, enabling it to recover from intermediate prediction errors. This leads to consistently high accuracy while also achieving notable latency reductions (Tab. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "text",
                    "output",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced the first comprehensive approach for integrating external tool usage directly into E2E speech-in, speech-out dialogue systems. By incorporating <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> pipeline, we further enhanced the model&#8217;s ability to leverage retrieved information and autonomously decide when to trigger new tool queries, resulting in improved accuracy and responsiveness.\nEmpirical evaluation on the newly introduced AudioCRAG benchmark demonstrated that tool integration can more than double factual question answering accuracy compared to closed-book models. Additionally, our streaming RAG approach achieved a 20% reduction in tool usage latency, thereby preserving natural conversational flow.\nOverall, our contributions advance the state of the art in spoken dialogue systems by enabling accurate, real-time, and tool-augmented voice assistants. In addition, we are committed to open science and will release our training code and Audio CRAG Human benchmark to support future research and development in this area.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "accurate",
                    "audio",
                    "open"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We begin with the CRAG dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>, which contains 2,706 text queries. Since these queries are not directly suitable for speech-based evaluation, we first identify those requiring adaptation before TTS conversion. Through careful manual inspection, we determine that queries containing dates or brackets benefit from rewriting to ensure naturalness and clarity in spoken form. In total, we identify 569 such queries and rewrite them using a large language model (LLAMA-4 Maverick). The resulting 569 rewritten queries, combined with the remaining original queries, form the basis for our spoken version of CRAG, which we term <em class=\"ltx_emph ltx_font_italic\">AudioCRAG</em>. We follow the CRAG setup to incorporate web and KG-based tools and adopt its robust evaluation setup, as detailed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S10\" title=\"10 Tool Usage Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S11\" title=\"11 Evaluation Setting &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio CRAG Synthetic</span>:\nTo generate spoken queries, we process all 2,706 queries through our in-house TTS system. We then apply a rigorous filtering procedure to remove queries with intelligibility issues, specifically excluding any utterances for which Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib27\" title=\"\">2023</a>)</cite> hypotheses exhibit a non-zero word error rate. We also remove utterances with suboptimal audio quality, as determined by UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib28\" title=\"\">2022</a>)</cite> scores below 3.5. This results in a high-quality set of 1,862 spoken queries, which we refer to as the <em class=\"ltx_emph ltx_font_italic\">AudioCRAG-Synthetic</em> benchmark.</p>\n\n",
                "matched_terms": [
                    "system",
                    "scores",
                    "audiocragsynthetic",
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio CRAG Human</span>:\nTo further enhance the realism and diversity of our evaluation, we introduce the AudioCRAG-Human benchmark, which consists of 618 human-recorded spoken queries. These queries are recorded by a diverse pool of participants to capture natural variations in speech, accent, and prosody. The inclusion of human-recorded audio enables a more comprehensive assessment of speech-in speech-out systems under real-world conditions, providing valuable insights into model robustness and generalization beyond synthetic speech. This benchmark serves as a critical resource for evaluating the effectiveness of tool integration in conversational AI systems.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable effective tool usage, we build upon the CRAG framework by integrating two complementary information sources: web search, which provides access to fresh and dynamic content, and a knowledge graph, which offers structured and reliable information. For web search, we aggregate all 100,000 documents from the CRAG corpus and employ a BGE-based re-ranker<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/BAAI/bge-large-en-v1.5\" title=\"\">https://huggingface.co/BAAI/bge-large-en-v1.5</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Xiao et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib33\" title=\"\">2023</a>)</cite> to index and retrieve the top 50 most relevant documents for each query. These documents are then segmented into chunks and re-ranked using the same BGE model based on their similarity to the query, ensuring highly contextually relevant retrieval. Meanwhile, queries to the knowledge graph are performed via a simulated API, adhering to the methodology established in CRAG <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/facebookresearch/CRAG/tree/main/mock_api\" title=\"\">https://github.com/facebookresearch/CRAG/tree/main/mock_api</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qwen-OMNI</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>)</cite> is a end-to-end multimodal model that seamlessly integrates diverse input modalities&#8212;including text, images, audio, and video&#8212;and generates both text and natural speech responses in a real-time streaming fashion. It leverages an innovative Thinker-Talker architecture, where the Thinker module performs high-level reasoning to produce a text response, which is then used by the Talker module, conditioning on both the text and the Thinker&#8217;s hidden representations, to generate streaming speech output.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "responses",
                    "text",
                    "output",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OpusLM</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib30\" title=\"\">2025</a>)</cite> is an open-source speech-in, speech-out model post-trained to directly answer complex semantic and factual questions from raw audio inputs, through Chain-of-Thought reasoning.</p>\n\n",
                "matched_terms": [
                    "opuslm",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Kimi Audio</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib8\" title=\"\">2025</a>)</cite> is a universal audio foundation model that unifies audio understanding, generation, and conversational abilities within a single framework. Pre-trained on over 13 million hours of diverse audio and text data, Kimi Audio achieves state-of-the-art performance across a wide range of audio benchmarks, including audio understanding and speech conversation tasks.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "kimi",
                    "model",
                    "across",
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For tool-augmented scenarios, retrieval results are provided up to each model&#8217;s maximum token limit (&#8220;Ref length&#8221; in Tables), maintaining a 2:1 ratio of web page to KG results.\nSpecifically, we observe that Kimi-Audio is currently optimized for handling tool result references up to a certain length. When this limit is exceeded, an error arises during the audio detokenization process, specifically within the rotary embedding mechanism, highlighting an architectural constraint in processing longer input sequences or larger reference contexts. Addressing this limitation presents a valuable opportunity for future model enhancements.</p>\n\n",
                "matched_terms": [
                    "each",
                    "model",
                    "length",
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation encompasses both speech-in-text-out and speech-in-speech-out scenarios. For the Fixed-Interval Streaming RAG setting (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS1\" title=\"3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>), intermediate tool queries are generated at consistent 1-second intervals. In the Model-Triggered Streaming RAG setting, the model dynamically determines the need for a tool call after processing each 500ms block. This approach allows us to utilize a smaller chunk size, as only a single tool call thread is required for Model-Triggered Streaming RAG, thereby enabling more efficient and responsive processing.</p>\n\n",
                "matched_terms": [
                    "each",
                    "model",
                    "setting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This subsection details the experimental setup for post-training the pretrained speech-in, speech-out model to significantly enhance its tool usage capabilities, as outlined in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>.\nWe leverage a random subset of 16,000 questions from the text-based factual question answering dataset TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib15\" title=\"\">2017</a>)</cite>, which contains 97,000 questions and 662,659 associated web documents.\nFor <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, we further compute word-level timestamps using a pre-trained ASR model, OWSM CTC v4 1B <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib26\" title=\"\">2025</a>)</cite>, enabling us to generate partial ASR transcripts <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m1\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math> at 500 ms intervals. Note that if word occurs at boundary of block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, it is excluded from <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math>. For each partial transcript <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math>, we generate corresponding psuedo ground truth queries <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m5\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> using LLAMA-4-Maverick to simulate incremental user input.\nTo simulate realistic tool usage, we concatenate all documents and employ a web query reranker to retrieve the top 50 most relevant documents for each query. The text questions from this 16k subset are converted into discrete speech tokens using text-to-speech synthesis with the corresponding pretrained speech-in, speech-out model. Recognizing that TriviaQA answers are typically single named entities, we further transform the queries into a conversational style using LLAMA-4-Maverick, making them more suitable for dialogue-based evaluation.</p>\n\n",
                "matched_terms": [
                    "text",
                    "each",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT = \"\"\"\nYou are an agent that only outputs JSON. You are given a Query and Query Time. Do the following:\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>1) Determine the domain the query is about. The domain should be one of the following: \"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". If none of the domains apply, use \"other\". Use \"domain\" as the key in the result json.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>2) Extract structured information from the query. Include different keys into the result json depending on the domains, and put them DIRECTLY in the result json. Here are the rules:\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;encyclopedia&#8217; and &#8216;other&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;main_entity&#8217;: extract the main entity of the query.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;finance&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;market_identifier&#8217;: stock identifiers including individual company names, stock symbols.\n<br class=\"ltx_break\"/>- &#8216;metric&#8217;: financial metrics that the query is asking about. This must be one of the following: &#8216;price&#8217;, &#8216;dividend&#8217;, &#8216;P/E ratio&#8217;, &#8216;EPS&#8217;, &#8216;marketCap&#8217;, and &#8216;other&#8217;.\n<br class=\"ltx_break\"/>- &#8216;datetime&#8217;: time frame that the query asks about. When datetime is not explicitly mentioned, use &#8216;Query Time&#8217; as default.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;movie&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;movie_name&#8217;: name of the movie\n<br class=\"ltx_break\"/>- &#8216;movie_aspect&#8217;: if the query is about a movie, which movie aspect the query asks. This must be one of the following: &#8216;budget&#8217;, &#8216;genres&#8217;, &#8216;original_language&#8217;, &#8216;original_title&#8217;, &#8216;release_date&#8217;, &#8216;revenue&#8217;, &#8216;title&#8217;, &#8216;cast&#8217;, &#8216;crew&#8217;, &#8216;rating&#8217;, &#8216;length&#8217;.\n<br class=\"ltx_break\"/>- &#8216;person&#8217;: person name related to moves\n<br class=\"ltx_break\"/>- &#8216;person_aspect&#8217;: if the query is about a person, which person aspect the query asks. This must be one of the following: &#8216;acted_movies&#8217;, &#8216;directed_movies&#8217;, &#8216;oscar_awards&#8217;, &#8216;birthday&#8217;.\n<br class=\"ltx_break\"/>- &#8216;year&#8217;: if the query is about movies released in a specific year, extract the year\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;music&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;artist_name&#8217;: name of the artist\n<br class=\"ltx_break\"/>- &#8216;artist_aspect&#8217;: if the query is about an artist, extract the aspect of the artist. This must be one of the following: &#8216;member&#8217;, &#8216;birth place&#8217;, &#8216;birth date&#8217;, &#8216;lifespan&#8217;, &#8216;artist work&#8217;, &#8216;grammy award count&#8217;, &#8216;grammy award date&#8217;.\n<br class=\"ltx_break\"/>- &#8216;song_name&#8217;: name of the song\n<br class=\"ltx_break\"/>- &#8216;song_aspect&#8217;: if the query is about a song, extract the aspect of the song. This must be one of the following: &#8216;author&#8217;, &#8216;grammy award count&#8217;, &#8216;release country&#8217;, &#8216;release date&#8217;.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;sports&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;sport_type&#8217;: one of &#8216;basketball&#8216;, &#8216;soccer&#8216;, &#8216;other&#8216;\n<br class=\"ltx_break\"/>- &#8216;tournament&#8217;: NBA, World Cup, Olympic.\n<br class=\"ltx_break\"/>- &#8216;team&#8217;: teams that users are interested in.\n<br class=\"ltx_break\"/>- &#8216;datetime&#8217;: time frame that the user is interested in. When datetime is not explicitly mentioned, use &#8216;Query Time&#8217; as default.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Return the results in a FLAT json.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n<br class=\"ltx_break\"/>\"\"\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8216;sports&#8217; queries, these are possible keys:\n- &#8216;sport_type&#8217;: one of &#8216;basketball&#8216;, &#8216;soccer&#8216;, &#8216;other&#8216;\n- &#8216;tournament&#8217;: NBA, World Cup, Olympic.\n- &#8216;team&#8217;: teams that users are interested in.\n- &#8216;datetime&#8217;: time frame that the user is interested in. When datetime is not explicitly mentioned, use\n&#8216;Query Time&#8217; as default.\nReturn the results in a FLAT json.\n<br class=\"ltx_break\"/>*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*</p>\n\n",
                "matched_terms": [
                    "output",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">3) Compare your newly generated result to the &#8217;Previous Result&#8217;. **If your new result would be exactly the same as the &#8217;Previous Result&#8217;, output only NO_QUERY.**\nReturn the results in a FLAT json.</p>\n\n",
                "matched_terms": [
                    "output",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When rewriting the query, ensure that it captures all important information from the original question that could impact the retrieval results. Do not omit any crucial details, such as specific dates, locations, or relationships between entities. Also, do not invent any new details on your own. If necessary, use the Query Time to provide context for the query. The goal is to create a concise and accurate query that effectively conveys the user&#8217;s intent and retrieves relevant information.\n*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT THE REWRITTEN QUERY!!!*\n\"\"\"</p>\n\n",
                "matched_terms": [
                    "accurate",
                    "output",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When rewriting the query, ensure that it captures all important information from the original question that could impact the retrieval results. Do not omit any crucial details, such as specific dates, locations, or relationships between entities. Also, do not invent any new details on your own. If necessary, use the Query Time to provide context for the query. The goal is to create a concise and accurate query that effectively conveys the user&#8217;s intent and retrieves relevant information.\nNow, compare the new web query to the previously generated web query (&#8217;Previous Result&#8217;).</p>\n\n",
                "matched_terms": [
                    "accurate",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT=\"\"\"\n<br class=\"ltx_break\"/>Assume you are a human expert in grading predictions given by a model. You are given a question and a model prediction. Judge if the prediction matches the ground truth answer by following these steps:\n1: Take it as granted that the Ground Truth is always correct.\n2: If the Prediction indicates it is not sure about the answer, \"score\" should be \"0\"; otherwise, go the next step.\n3: If the Prediction exactly matches the Ground Truth, \"score\" is 1.\n4: If the Prediction does not exactly match the Ground Truth, go through the following steps and likely give a score as 0.\n5: If the Ground Truth is a number, \"score\" is 1 if and only if the Prediction gives a number that almost exactly matches the ground truth.\n6: If the Prediction is self-contradictory, \"score\" must be 0.\n7: If the prediction is not answering the question, \"score\" must be 0.\n8: If the prediction is a concise and correct summary of the ground truth, \"score\" is 1.\n9: If ground truth contains a set of items, prediction must contain exactly same items for the score to be 1.\n10: Otherwise, \"score\" is 0.</p>\n\n",
                "matched_terms": [
                    "model",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Output a JSON blob with an \"explanation\" field explaining your answer as short as possible and an \"score\" field with value 1 or 0.\n<br class=\"ltx_break\"/></p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">You should make the judgment based on provided examples.\nExamples:\nQuestion: \"which company has higher eps, btu or cma?\"\nGround Truth: \"cma\"\nPrediction: \"it is not possible to determine which company has a higher eps.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction is not sure about the answer.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who is taller, a or b?\"\nGround Truth: \"a\"\nPrediction: \"The answer is a. a is of height 1.75 m and b is of height 1.82 m. So b is taller.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction is self-contradictory.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who among patricia place and lana clarkson is younger?\"\nGround Truth: \"lana clarkson\"\nPrediction: \"lana clarkson (born april 5, 1962) is younger than patricia clarkson (born december 29, 1959).\"\nOutput: \"score\": 1, \"explanation\": \"The prediction gives the correct answer.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"did lakers win more games than warriors in 2024-4?\"\nGround Truth: \"yes\"\nPrediction: \"yes, lakers won more games than warriors in 2023-2024 season.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction is not answering the question.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the height of the building where citigroup is headquartered?\"\nGround Truth: \"151 m\"\nPrediction: \"the height is 915 feet (279 m).\"\nOutput: \"score\": 0, \"explanation\": \"The prediction, 151 m, does not match the ground truth, 279 m.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who were the last 3 prime ministers of uk?\"\nGround Truth: \"rishi sunak, liz truss, and boris johnson\"\nPrediction: \"1. liz truss, 2. boris johnson, 3.theresa may\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not contain item, rishi sunak, that is in the grount truth.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who authored the taming of the shrew?\"\nGround Truth: \"william shakespeare\"\nPrediction: \"william shakespeare\"\nOutput: \"score\": 1, \"explanation\": \"The prediction exactly matches the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who authored The Taming of the Shrew?\"\nGround Truth: \"william shakespeare\"\nPrediction: \"w shakespeare\"\nOutput: \"score\": 1, \"explanation\": \"The prediction matches the ground truth as w is the abbreviation of william.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the state bird of california?\"\nGround Truth: \"california quail\"\nPrediction: \"california valley quail\"\nOutput: \"score\": 1, \"explanation\": \"The prediction matches the ground truth as california quail is also known as california valley quail.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what country is maharashtra metro rail corporation limited located in?\"\nGround truth: \"india\"\nPrediction: \"indianapolis\"\nOutput: \"score\": 0, \"explanation\": \"The prediction doesn&#8217;t match the ground truth answers.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what was wolves&#8217;s record in their last game in eng-premier league?\"\nGround Truth: \"w\"\nPrediction: \"wolves&#8217;s record in their last game in the eng-premier League was a 2-1 win over fulham.\"\nOutput: \"score\": 1, \"explanation\": \"The prediction correctly answers the result is a win.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how many seconds is 3 minutes 15 seconds?\"\nGround Truth: \"195 seconds\"\nPrediction: \"3 minutes 15 seconds is 195 seconds.\"\nOutput: \"score\": 1, \"explanation\": \"The prediction matches ground truth as it correctly states the number of seconds asked in the question.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"on which days did am distribute dividends in the last year?\"\nGround Truth: \"2023-01-24, 2023-04-25, 2023-07-25, 2023-10-24\"\nPrediction: \"the company distributed dividends on october 24, 2023 and january 24, 2023.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction partially matches the ground truth, but it is incomplete.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"on which days did xxx distribute dividends in the last year?\"\nGround Truth: \"2023-01-13, 2023-03-25, 2023-11-21\"\nPrediction: \"xxx distributed dividends on 1. 2023-01-13, 2. 2023-03-25, 3. 2023-10-21.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction contains an item, 2023-10-21, which is not in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who are current band members of maroon 5?\"\nGround Truth: \"adam Levine, jesse carmichael, james valentine, matt flynn, pj morton, sam farrar\"\nPrediction: \"they are jesse carmichael, matt flynn, adam levine, pj morton, sam farrar, and james valentine.\"\nOutput: \"score\": 1, \"explanation\": \"The prediction exactly matches the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"which movies comprise the matrix franchise?\"\nGround Truth: \"the matrix, the matrix reloaded, the matrix revolutions, the matrix resurrections\"\nPrediction: \"the matrix, the matrix reloaded, the matrix revolutions, the animatrix, and the matrix resurrections.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction covers more items than what are given by the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how deep is the deepest lake of new york?\"\nGround Truth: \"618 ft\"\nPrediction: \"the deepest lake in new york is seneca lake, with a depth of 618.23 feet.\"\nOutput: \"score\": 1, \"explanation\": \"The prediction exactly matches the number in ground truth after rounding.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the closing price of meta yesterday?\"\nGround Truth: \"$310.17\"\nPrediction: \"310.2\"\nOutput: \"score\": 1, \"explanation\": \"The prediction exactly matches the number in ground truth after rounding.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the current market cap of appl?\"\nGround Truth: \"2.81 trillion\"\nPrediction: \"2.667 trillion\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the number in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the current pe ratio of appl?\"\nGround Truth: \"28.3\"\nPrediction: \"the current pe ratio of apple is 26.66\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the number in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how much is tesla&#8217;s stock price down from its all-time high?\"\nGround Truth: \"$221.83\"\nPrediction: \"209.52\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the number in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the length of amazon river?\"\nGround Truth: \"over 4000 miles\"\nPrediction: \"the length of amazon river is 4,000 miles\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not say Amazon River is longer than 4000 miles.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "length",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how many copies x were sold?\"\nGround Truth: \"2 million.\"\nPrediction: \"it is over 2 million\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the population of country x?\"\nGround Truth: \"3,576,873\"\nPrediction: \"the population of country x is 3.3 million.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction, 3.3 M, does not match the number, 3.6 M, in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the current market value of stock x?\"\nGround Truth: \"$2,237,578,268\"\nPrediction: \"$2.16 billion.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction, 2.16 B, does not match the number, 2.2 B, in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how old is michael jordan?\"\nGround Truth: \"61\"\nPrediction: \"mj was born on February 17, 1963.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not explicitly say the age.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how long did amy teach as a teacher?\"\nGround Truth: \"24 years and 255 days\"\nPrediction: \"25 years.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not exactly match the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"at what age did usher perform in the half time show of superbowl?\"\nGround Truth: \"45 years old\"\nPrediction: \"usher has not performed at a super bowl halftime show.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what year did olivia rodrigo win the grammys best new artist award?\"\nGround Truth: \"2022\"\nPrediction: \"sure! here&#8217;s an answer in three words or fewer: 2022 grammy awards.\"\nOutput: \"score\": 1, \"explanation\": \"The prediction answers the correct number of the year.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how many times have zendaya and tom holland starred in a film together?\"\nGround Truth: \"3\"\nPrediction: \"they have starred in 3 films together: spider-man: homecoming, spider-man: far from home, and spider-man: no way home.\"\nOutput: \"score\": 1, \"explanation\": \"The prediction gives the right number and it is reasonable.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who is younger, timothee chalamet or tom holland?\"\nGround Truth: \"tom holland\"\nPrediction: \"timoth&#233;e chalamet is younger than tom holland.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who had more number one hits on the us billboard, a or b?\"\nGround Truth: \"a had more number one hits on the us billboard than b, with 20 number one hits compared to b&#8217;s 15.\"\nPrediction: \"a\"\nOutput: \"score\": 1, \"explanation\": \"The prediction is a concise and correct summary of the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is xxx&#8217;s birthdate?\"\nGround Truth: \"1996-01-01.\"\nPrediction: \"02/01/1996\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what was the worldwide box office haul for movie x?\"\nGround Truth: \"101756123.\"\nPrediction: \"102 million\"\nOutput: \"score\": 1, \"explanation\": \"The prediction exactly matches the number in ground truth after rounding.\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how much has spotify&#8217;s user base increased by since 2020 in na?\"\nGround Truth: \"spotify&#8217;s user base increased by 34 million since 2020.\"\nPrediction: \"spotify&#8217;s north american user base increased from 36 million in 2020 to 85 million by 2021\"\nOutput: \"score\": 0, \"explanation\": \"The prediction is not answering the question as it only gives the increase from 2020 to 2021.\"\n<br class=\"ltx_break\"/>\"\"\"</p>\n\n",
                "matched_terms": [
                    "output",
                    "score"
                ]
            }
        ]
    },
    "S13.T7": {
        "source_file": "Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage",
        "caption": "Table 7: Example KG Queries, and Web Queries generated by Qwen-OMNI in Open Book setting",
        "body": "ASR Transcript of Question XasrX^{\\text{asr}}\n\n\n\n\nWeb Query Q^web\\hat{Q}^{\\text{web}}\n\n\n\n\nKG Query Q^KG\\hat{Q}^{\\text{KG}}\n\n\n\n\n\n\n\n\nwhich of nolan greenwald’s movies has achieved the highest level of box office success on a global scale?\n\n\n\n\nNolan Greenwald’s highest-grossing movie\n\n\n\n\n{’domain’: ’movie’, ’movie_name’: \"Nolan Greenwald’s movies\", ’movie_aspect’: ’revenue’}\n\n\n\n\n\n\nwho has played drums for the red hot chili peppers?\n\n\n\n\nRed Hot Chili Peppers drummers\n\n\n\n\n{’domain’: ’music’, ’artist_name’: ’Red Hot Chili Peppers’, ’artist_aspect’: ’member’}\n\n\n\n\n\n\nwhat’s the current stock price of tortoise midstream energy fund?\n\n\n\n\nTortoise Midstream Energy Fund stock price\n\n\n\n\n{’domain’: ’finance’, ’market_identifier’: ’Tortoise Midstream Energy Fund’, ’metric’: ’price’, ’datetime’: ’02/28/2024’}\n\n\n\n\n\n\nwhat was the volume of trading in cabot corporation’s stock on the most recent day that dividends were distributed?\n\n\n\n\nCABOT Corp stock trading volume on dividend distribution date\n\n\n\n\n{’domain’: ’finance’, ’market_identifier’: ’Cabot Corporation’, ’metric’: ’dividend’, ’datetime’: ’02/28/2024’}\n\n\n\n\n\n\nwhich movie won the academy award for best film in 2020?\n\n\n\n\n2020 Academy Award for Best Picture\n\n\n\n\n{’domain’: ’movie’, ’movie_aspect’: ’oscar_awards’, ’year’: 2020}\n\n\n\n\n\n\nwhich teams have won against phoenix suns during 2022-12?\n\n\n\n\nTeams that beat Phoenix Suns in December 2022\n\n\n\n\n{’domain’: ’sports’, ’sport_type’: ’basketball’, ’team’: ’Phoenix Suns’, ’datetime’: ’2022-12-15’}",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\"><span class=\"ltx_text ltx_font_bold\">ASR Transcript of Question <math alttext=\"X^{\\text{asr}}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.T7.m1\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext class=\"ltx_mathvariant_bold\">asr</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}</annotation></semantics></math></span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\"><span class=\"ltx_text ltx_font_bold\">Web Query <math alttext=\"\\hat{Q}^{\\text{web}}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.T7.m2\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mtext class=\"ltx_mathvariant_bold\">web</mtext></msup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{web}}</annotation></semantics></math></span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\"><span class=\"ltx_text ltx_font_bold\">KG Query <math alttext=\"\\hat{Q}^{\\text{KG}}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.T7.m3\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mtext class=\"ltx_mathvariant_bold\">KG</mtext></msup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{KG}}</annotation></semantics></math></span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">which of nolan greenwald&#8217;s movies has achieved the highest level of box office success on a global scale?</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">Nolan Greenwald&#8217;s highest-grossing movie</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">{&#8217;domain&#8217;: &#8217;movie&#8217;, &#8217;movie_name&#8217;: \"Nolan Greenwald&#8217;s movies\", &#8217;movie_aspect&#8217;: &#8217;revenue&#8217;}</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">who has played drums for the red hot chili peppers?</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">Red Hot Chili Peppers drummers</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">{&#8217;domain&#8217;: &#8217;music&#8217;, &#8217;artist_name&#8217;: &#8217;Red Hot Chili Peppers&#8217;, &#8217;artist_aspect&#8217;: &#8217;member&#8217;}</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">what&#8217;s the current stock price of tortoise midstream energy fund?</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">Tortoise Midstream Energy Fund stock price</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">{&#8217;domain&#8217;: &#8217;finance&#8217;, &#8217;market_identifier&#8217;: &#8217;Tortoise Midstream Energy Fund&#8217;, &#8217;metric&#8217;: &#8217;price&#8217;, &#8217;datetime&#8217;: &#8217;02/28/2024&#8217;}</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">what was the volume of trading in cabot corporation&#8217;s stock on the most recent day that dividends were distributed?</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">CABOT Corp stock trading volume on dividend distribution date</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">{&#8217;domain&#8217;: &#8217;finance&#8217;, &#8217;market_identifier&#8217;: &#8217;Cabot Corporation&#8217;, &#8217;metric&#8217;: &#8217;dividend&#8217;, &#8217;datetime&#8217;: &#8217;02/28/2024&#8217;}</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">which movie won the academy award for best film in 2020?</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">2020 Academy Award for Best Picture</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">{&#8217;domain&#8217;: &#8217;movie&#8217;, &#8217;movie_aspect&#8217;: &#8217;oscar_awards&#8217;, &#8217;year&#8217;: 2020}</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">which teams have won against phoenix suns during 2022-12?</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">Teams that beat Phoenix Suns in December 2022</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">{&#8217;domain&#8217;: &#8217;sports&#8217;, &#8217;sport_type&#8217;: &#8217;basketball&#8217;, &#8217;team&#8217;: &#8217;Phoenix Suns&#8217;, &#8217;datetime&#8217;: &#8217;2022-12-15&#8217;}</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "nolan",
            "global",
            "price",
            "corporation’",
            "corporation’s",
            "who",
            "success",
            "peppers’",
            "’tortoise",
            "queries",
            "teams",
            "query",
            "web",
            "movie",
            "distributed",
            "distribution",
            "what’s",
            "’metric’",
            "’basketball’",
            "’sports’",
            "level",
            "energy",
            "best",
            "phoenix",
            "cabot",
            "’revenue’",
            "hot",
            "chili",
            "played",
            "trading",
            "peppers",
            "has",
            "’02282024’",
            "midstream",
            "what",
            "corp",
            "day",
            "’oscarawards’",
            "book",
            "date",
            "drummers",
            "suns",
            "have",
            "scale",
            "suns’",
            "greenwald’s",
            "xasrxtextasr",
            "question",
            "asr",
            "qkghatqtextkg",
            "current",
            "dividends",
            "generated",
            "’member’",
            "highestgrossing",
            "box",
            "academy",
            "volume",
            "stock",
            "tortoise",
            "red",
            "’music’",
            "against",
            "’moviename’",
            "which",
            "most",
            "’dividend’",
            "’cabot",
            "won",
            "’team’",
            "open",
            "picture",
            "’year’",
            "award",
            "’20221215’",
            "’movieaspect’",
            "’artistaspect’",
            "recent",
            "transcript",
            "qwenomni",
            "achieved",
            "during",
            "’phoenix",
            "drums",
            "’marketidentifier’",
            "’datetime’",
            "beat",
            "’price’",
            "’red",
            "office",
            "example",
            "’finance’",
            "highest",
            "film",
            "’domain’",
            "qwebhatqtextweb",
            "fund’",
            "’movie’",
            "fund",
            "setting",
            "movies",
            "’sporttype’",
            "dividend",
            "december",
            "’artistname’"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our proposed formulation for integrating external tools into speech-in speech-out systems. We introduce a two-stage inference approach: <em class=\"ltx_emph ltx_font_italic\">Query Generation</em> and <em class=\"ltx_emph ltx_font_italic\">Response Generation</em>. In the Query Generation stage, the system processes an audio question and generates queries for each external tool to retrieve relevant information by maximizing the posterior distribution <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> (Examples of generated queries are provided in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T7\" title=\"Table 7 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> in the Appendix.). In the Response Generation stage, the retrieved results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from these tools are combined with the original audio question and input into the model to generate the final spoken response by maximizing the posterior distribution <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math>. By conditioning the final output generation on the input audio, this formulation not only provides a simple and effective mechanism for interacting with text-based APIs, but also preserves the key advantages of speech-in speech-out systems, mitigating error propagation and enabling the model to capture non-phonemic information (such as prosody and speaker intent) more effectively.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">End-to-end speech-in speech-out dialogue systems are emerging as a powerful alternative to traditional ASR&#8211;LLM&#8211;TTS pipelines, generating more natural, expressive responses with significantly lower latency. However, these systems remain prone to hallucinations due to limited factual grounding. While text-based dialogue systems address this challenge by integrating tools such as web search and knowledge graph APIs, we introduce the first approach to extend tool use directly into speech-in speech-out systems. A key challenge is that tool integration substantially increases response latency, disrupting conversational flow. To mitigate this, we propose Streaming Retrieval-Augmented Generation (<em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>), a novel framework that reduces user-perceived latency by predicting tool queries in parallel with user speech, even before the user finishes speaking. Specifically, we develop a post-training pipeline that teaches the model when to <em class=\"ltx_emph ltx_font_italic\">issue tool calls</em> during ongoing speech and how to generate spoken summaries that fuse audio queries with retrieved text results, thereby improving both <em class=\"ltx_emph ltx_font_italic\">accuracy and responsiveness</em>. To evaluate our approach, we construct AudioCRAG, a benchmark created by converting queries from the publicly available CRAG dataset into speech form. Experimental results demonstrate that our <em class=\"ltx_emph ltx_font_italic\">streaming RAG</em> approach increases QA accuracy by up to 200% relative (from 11.1% to 34.2% absolute) and further enhances user experience by reducing tool use latency by 20%. Importantly, our <em class=\"ltx_emph ltx_font_italic\">streaming RAG</em> approach is modality-agnostic and can be applied equally to typed input, paving the way for more agentic, real-time AI assistants.</p>\n\n",
                "matched_terms": [
                    "during",
                    "web",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spoken Dialogue Systems (SDS) are foundational to many everyday technologies, powering intelligent assistants such as Alexa and Siri, as well as interactive voice response systems in customer service. With the rapid expansion of SDS capabilities to mobile phones and wearable devices, the need for robust, scalable, and generalizable solutions has never been greater. Traditionally, SDS have relied on cascaded pipelines composed of multiple modules&#8212;including voice activity detection (VAD), automatic speech recognition (ASR), natural language understanding (NLU), natural language generation (NLG), and text-to-speech (TTS) synthesis&#8212;each introducing potential points of failure and latency <cite class=\"ltx_cite ltx_citemacro_citep\">(Glass, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib12\" title=\"\">1999</a>; Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib13\" title=\"\">2024</a>)</cite>.\nRecently, end-to-end (E2E) SDS <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib34\" title=\"\">2024</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib22\" title=\"\">2023</a>; Meng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib20\" title=\"\">2024</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib40\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib2\" title=\"\">2025b</a>)</cite> have been proposed, which directly generate spoken responses from speech input within a unified architecture. This E2E approach not only mitigates error propagation across modules but also captures non-phonemic information more effectively, resulting in significantly lower inference time and computational overhead, and paving the way for more natural and efficient conversational experiences.</p>\n\n",
                "matched_terms": [
                    "have",
                    "which",
                    "has",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, current E2E SDS are fundamentally constrained by their reliance on internalized knowledge from static training data, which often results in responses that lack factual grounding or fail to reflect the most up to date information. This shortcoming is particularly critical for action-oriented or knowledge-seeking tasks, such as booking hotels or answering questions about current events. In contrast, text-based conversational assistants have begun to overcome these limitations by integrating external tools through Retrieval-Augmented Generation (RAG)<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>; Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib3\" title=\"\">2023a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib5\" title=\"\">c</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib10\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib11\" title=\"\">b</a>)</cite>, dynamically retrieving relevant information from sources like web search, knowledge graphs (KG), and real-time APIs. Yet, the integration of such tool use into E2E SDS remains largely unexplored.\nA key challenge is that while external tools can substantially improve factual accuracy, invoking them often introduces additional latency, leading to awkward silences that disrupt the natural conversational flow. This raises a research question: <span class=\"ltx_text ltx_font_italic\">How can we trade-off between accuracy and responsiveness for developing SDS that feel both intelligent and natural?</span></p>\n\n",
                "matched_terms": [
                    "have",
                    "which",
                    "most",
                    "current",
                    "question",
                    "date",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present, to the best of our knowledge, the first <span class=\"ltx_text ltx_font_italic\">speech-in, speech-out language model that seamlessly integrates external tool invocation with low latency</span>. The key idea is a <span class=\"ltx_text ltx_font_bold\">Streaming RAG</span> strategy that generates tool queries in parallel with user speech, often times even before the user has finished speaking (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). A naive implementation of streaming queries, however, faces two challenges: (1) queries issued from partial speech may be suboptimal, yielding distracting tool outputs and inaccurate responses; and (2) unnecessary tool calls may be triggered, wasting computational resources. We introduce effective modeling techniques to address these challenges and make the following contributions.</p>\n\n",
                "matched_terms": [
                    "best",
                    "queries",
                    "has"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution 1</span>: We introduce a formal framework for tool integration in speech-in speech-out systems and empirically show that leveraging web search and KG APIs significantly enhances factual question answering. Evaluating three state-of-the-art (SOTA) models, Qwen-OMNI <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>)</cite>, OpusLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib30\" title=\"\">2025</a>)</cite>, and Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib8\" title=\"\">2025</a>)</cite>, we find that external tool integration delivers substantial performance gains, boosting accuracy by up to 140% relative (from 11.1% to 26.3% absolute). However, tool usage also introduces considerable latency, increasing first-token response time by 2.3x.</p>\n\n",
                "matched_terms": [
                    "question",
                    "qwenomni",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution 2</span>: To address this, we propose <em class=\"ltx_emph ltx_font_italic\">Streaming Retrieval-Augmented Generation</em> (<em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>), the <em class=\"ltx_emph ltx_font_italic\">first</em> framework that empowers the system to trigger tool queries in parallel with user speech, even before the user finishes speaking. Within this framework, we introduce two novel approaches: (1) <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em>, which issues tool queries at regular intervals during speech input and carefully examines quality of retrieval results on the full query to guarantee response quality, and can be incorporated into any speech-in, speech-out model without post-training; (2) <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, which post-trains the model to intelligently determine optimal query timing based on the evolving user utterance to save computation resources.\nOur results demonstrate that our proposed <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> delivers over 200% relative improvement in accuracy (from 11.1% to 34.2% absolute in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) compared to the no-tool baseline, while also reducing tool result generation latency by 20%. Though designed for speech-in speech-out systems, streaming RAG can also be adapted in cascaded SDS, or even chatbots as users type.</p>\n\n",
                "matched_terms": [
                    "query",
                    "during",
                    "which",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution 3</span>: Finally, we introduce AudioCRAG, a benchmark created by recording spoken queries from the CRAG <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite> dataset, enabling robust evaluation of tool usage capabilities in speech-in speech-out systems.\nTo support future research, we will open source our training code and AudioCRAG-Human benchmark, supporting future research in tool-integrated voice assistants.</p>\n\n",
                "matched_terms": [
                    "queries",
                    "open"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in benchmarking text-based dialogue systems for tool usage <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib4\" title=\"\">2023b</a>; Ouyang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib24\" title=\"\">2025</a>; Cheng and Dou, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib6\" title=\"\">2025</a>; Cohen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib7\" title=\"\">2025</a>; Xiong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib35\" title=\"\">2024a</a>)</cite> have primarily focused on evaluating factual question answering and task completion within simulated environments (detailed related work discussion in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S7\" title=\"7 Benchmarking Text Dialogue Systems for Tool Usage &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S8\" title=\"8 Multimodal Benchmarks for Tool Usage &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>). The CRAG benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite> is a leading example, featuring 4,409 question-answer pairs and providing mock APIs for both web and KG search. Recent benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Meta CRAG-MM Challenge Organizers, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib21\" title=\"\">2025</a>; Ma et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib17\" title=\"\">2024</a>; Jang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib14\" title=\"\">2025</a>)</cite> have extended tool-augmented dialogue evaluation to multimodal input and longer-context scenarios. While these benchmarks have significantly advanced the evaluation of tool-augmented dialogue systems, they remain largely limited to text-based outputs and do not fully address the unique challenges presented by speech-in speech-out systems.</p>\n\n",
                "matched_terms": [
                    "have",
                    "example",
                    "recent",
                    "question",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several E2E spoken dialogue systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>; Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib34\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib1\" title=\"\">2025a</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib22\" title=\"\">2023</a>; Meng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib20\" title=\"\">2024</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib40\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib2\" title=\"\">2025b</a>)</cite> have recently been introduced, demonstrating impressive semantic understanding and high audio quality in their responses. However, these systems have not yet been trained or evaluated for their ability to <em class=\"ltx_emph ltx_font_italic\">use external tools</em>.\nAnother research direction <cite class=\"ltx_cite ltx_citemacro_citep\">(Feng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib9\" title=\"\">2025</a>)</cite> explores E2E RAG for direct speech-to-text retrieval, utilizing multimodal embeddings for enabling speech utterances to directly retrieve relevant text. While this method outperforms models without RAG, it is primarily limited to <em class=\"ltx_emph ltx_font_italic\">speech-to-text</em> scenarios. Its ability to access KGs and other APIs, critical for real-world applications, remains unproven. Furthermore, the retrieval scope is limited, as experiments are conducted with retrieval restricted to just 10 paragraphs, whereas modern RAG benchmarks require searching across thousands of web pages.\nAlthough recent studies <cite class=\"ltx_cite ltx_citemacro_citep\">(Maben et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib18\" title=\"\">2025</a>)</cite> have developed web interfaces that integrate tools into speech-in <em class=\"ltx_emph ltx_font_italic\">speech-out</em> scenarios using cascaded pipelines, comprehensive empirical investigations of E2E speech-in speech-out systems and, crucially, systematic analyses of user-perceived latency, remain largely unexplored. In this work, we address these gaps by developing a comprehensive framework for tool integration in E2E speech-in speech-out systems and designing benchmarks to quantitatively assess the tool usage capabilities of SOTA models. Furthermore, recognizing the importance of latency for natural conversational flow, we introduce novel streaming RAG methods that not only enhance tool usage performance but also reduce user-perceived latency.</p>\n\n",
                "matched_terms": [
                    "have",
                    "web",
                    "recent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A RAG spoken conversation system takes an audio question <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math> as input and outputs a spoken answer <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>. Let the ASR transcript of audio question be <math alttext=\"X^{\\text{asr}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>asr</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}</annotation></semantics></math> and the ASR transcript of the spoken answer be <math alttext=\"X^{\\text{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>res</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{res}}</annotation></semantics></math>.\nAnswers are generated by speech-in speech-out models, leveraging both the model&#8217;s internal knowledge and information retrieved from external sources.\nTo incorporate external information, the model needs to formulate a tool query <math alttext=\"Q^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><msup><mi>Q</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">Q^{T}</annotation></semantics></math> to retrieve relevant results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m6\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from an external tool <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m7\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "transcript",
                    "question",
                    "xasrxtextasr",
                    "query",
                    "generated",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">RAG-based systems, as proposed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, can significantly improve factual accuracy by incorporating external tools. However, these tool calls often introduce substantial latency, which is particularly problematic in speech-in, speech-out applications where users expect rapid, conversational responses and even brief silences can disrupt the natural flow of dialogue.\nOne way to address this challenge lies in the nature of audio inputs, which arrive as a continuous stream. This streaming property enables tool calls to be initiated before the user has finished speaking, offering a unique opportunity to mitigate latency.</p>\n\n",
                "matched_terms": [
                    "which",
                    "has"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To minimize user-perceived latency, we introduce <em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>: the first framework to generate and issue tool queries in parallel as audio input is received. This novel approach is built on three key design components:\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> Trigger: When to initiate a new tool query;\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> Threads: The number of parallel tool query threads;\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic3\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">3</span></span></foreignobject></g></g></svg></span> Reflector: The module that determines whether intermediate tool results are sufficient for generating the final output. By exploring different design choices for each component, we introduce two complementary approaches for streaming tool query generation in the following subsections: a fixed-interval trigger method and a model-based trigger method.</p>\n\n",
                "matched_terms": [
                    "query",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this approach, the <em class=\"ltx_emph ltx_font_italic\">trigger</em> is set to fire tool calls at fixed chunk intervals during audio input.\nThe input speech <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math> is divided into a sequence of <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> blocks, <math alttext=\"Q=\\{Q_{b}\\mid b=1,\\dots,B\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>Q</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>Q</mi><mi>b</mi></msub><mo fence=\"true\" lspace=\"0em\" rspace=\"0em\">&#8739;</mo><mrow><mi>b</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>B</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Q=\\{Q_{b}\\mid b=1,\\dots,B\\}</annotation></semantics></math>, with each block containing <math alttext=\"N_{\\text{block}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>N</mi><mtext>block</mtext></msub><annotation encoding=\"application/x-tex\">N_{\\text{block}}</annotation></semantics></math> frames.\nTo approximate <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> as described in S. 3.1, we follow a block-wise prediction strategy. In this approach, after processing each audio block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m6\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, the model predicts a tool query <math alttext=\"\\hat{Q}_{b}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m7\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}_{b}^{T}</annotation></semantics></math> by conditioning on the input speech accumulated up to block b, specifically <math alttext=\"Q_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">Q_{1:b}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "query",
                    "during"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This strategy results in <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m9\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> parallel tool call threads running simultaneously (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F3.sf1\" title=\"Figure 3(a) &#8227; Figure 3 &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>), where each thread generates a tool query prediction <math alttext=\"\\hat{Q}_{b}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m10\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}_{b}^{T}</annotation></semantics></math> for its corresponding block <math alttext=\"b\\in[1,B]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m11\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mi>B</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b\\in[1,B]</annotation></semantics></math>.\nThe tool queries <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m12\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> generated after each block are then stored in cache. Given the high latency of tool calls (See T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), users typically complete their utterances before tool responses are ready. Upon utterance completion, an explicit <em class=\"ltx_emph ltx_font_italic\">reflector</em> module &#8220;<math alttext=\"\\text{reflect}()\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m13\" intent=\":literal\"><semantics><mrow><mtext>reflect</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{reflect}()</annotation></semantics></math>&#8221; evaluates the cached intermediate queries <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m14\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> against final query <math alttext=\"\\hat{Q}^{T}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m15\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{B}</annotation></semantics></math> to determine whether an early intermediate tool call provides sufficient information to answer the user&#8217;s question <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m16\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math>. The reflector module systematically evaluates all intermediate queries in the cache and identifies the <em class=\"ltx_emph ltx_font_italic\">earliest</em> sufficient tool call <math alttext=\"b^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m17\" intent=\":literal\"><semantics><msup><mi>b</mi><mo>&#8902;</mo></msup><annotation encoding=\"application/x-tex\">b^{\\star}</annotation></semantics></math> where the intermediate query <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m18\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> will give the same result as final query <math alttext=\"\\hat{Q}^{T}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m19\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{B}</annotation></semantics></math> as shown:</p>\n\n",
                "matched_terms": [
                    "queries",
                    "question",
                    "query",
                    "generated",
                    "against"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All subsequent parallel tool calls after <math alttext=\"b^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m20\" intent=\":literal\"><semantics><msup><mi>b</mi><mo>&#8902;</mo></msup><annotation encoding=\"application/x-tex\">b^{\\star}</annotation></semantics></math> are promptly terminated, and the retrieved results <math alttext=\"R_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m21\" intent=\":literal\"><semantics><msub><mi>R</mi><msup><mi>b</mi><mo>&#8902;</mo></msup></msub><annotation encoding=\"application/x-tex\">R_{b^{\\star}}</annotation></semantics></math> from this intermediate call <math alttext=\"\\hat{Q}^{T}_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m22\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><msup><mi>b</mi><mo>&#8902;</mo></msup><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b^{\\star}}</annotation></semantics></math> are used to generate the final spoken response <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m23\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by maximizing the posterior distribution <math alttext=\"P(A|Q,R_{b^{\\star}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m24\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><msub><mi>R</mi><msup><mi>b</mi><mo>&#8902;</mo></msup></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R_{b^{\\star}})</annotation></semantics></math> (instead of <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m25\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math> in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>).\nWe employ a reflector module that uses simple yet effective heuristics: (a) if the top 5 web documents for an intermediate web query match those of the final web query, and (b) if the KG results for intermediate and final KG queries are identical.\nThese heuristics ensure that the information retrieved from an early tool call using <math alttext=\"\\hat{Q}^{T}_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m26\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><msup><mi>b</mi><mo>&#8902;</mo></msup><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b^{\\star}}</annotation></semantics></math> is consistent with what would have been obtained by waiting for the final tool call using <math alttext=\"\\hat{Q}^{T}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m27\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{B}</annotation></semantics></math>, thereby providing a strong quality guarantee. Since most tool call latency arises from chunking and reranking the chunks of web documents, these checks enable significant latency savings without any compromise in model performance.\nA key advantage of this strategy is its plug-and-play nature: it requires no additional post-training for speech-in, speech-out models and can be directly applied at inference time across a variety of architectures. However, there are important considerations. First, generating parallel tool calls at every fixed interval (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E1\" title=\"Equation 1 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) increases computational overhead, which may pose challenges for deployment on resource-constrained devices such as wearables devices. Second, reliance on an external reflector module (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E2\" title=\"Equation 2 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) to determine the sufficiency of intermediate tool calls may limit the extent of achievable latency improvements.</p>\n\n",
                "matched_terms": [
                    "have",
                    "what",
                    "which",
                    "most",
                    "distribution",
                    "queries",
                    "query",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the limitations of Fixed-Interval Streaming RAG and further optimize both efficiency and responsiveness, we propose a more adaptive approach: <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>.\nHere, the <em class=\"ltx_emph ltx_font_italic\">trigger</em> is learned: the model is trained to autonomously determine the optimal moments to initiate tool queries, issuing a query only when it encounters new or additional information as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F3.sf2\" title=\"Figure 3(b) &#8227; Figure 3 &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>. In this formulation, the model receives user input in fixed chunk intervals as before and intelligently determines whether a tool call is needed after each block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>. The model can either:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p1.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> Predict\nNO_QUERY if a new tool query is unnecessary, or\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p1.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> Generate a new tool query. To make this decision, the model conditions on both the accumulated input speech <math alttext=\"Q_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">Q_{1:b}</annotation></semantics></math> (see Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E1\" title=\"Equation 1 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) as well as most recent tool query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}=\\hat{Q}^{T}_{\\max\\{\\,i&lt;b:\\hat{Q}^{T}_{i}\\neq\\text{NO\\_QUERY}\\,\\}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><mo>=</mo><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">{</mo><mrow><mrow><mi>i</mi><mo>&lt;</mo><mi>b</mi></mrow><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>i</mi><mi>T</mi></msubsup><mo>&#8800;</mo><mtext>NO_QUERY</mtext></mrow></mrow><mo lspace=\"0.170em\" stretchy=\"false\">}</mo></mrow></mrow><mi>T</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}=\\hat{Q}^{T}_{\\max\\{\\,i&lt;b:\\hat{Q}^{T}_{i}\\neq\\text{NO\\_QUERY}\\,\\}}</annotation></semantics></math> as shown:</p>\n\n",
                "matched_terms": [
                    "query",
                    "recent",
                    "queries",
                    "most"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When a new query <math alttext=\"\\hat{Q}^{T}_{b}\\neq\\text{NO\\_QUERY}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m4\" intent=\":literal\"><semantics><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><mo>&#8800;</mo><mtext>NO_QUERY</mtext></mrow><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}\\neq\\text{NO\\_QUERY}</annotation></semantics></math>, the system immediately terminates any ongoing tool calls for the previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m5\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, ensuring that only a <em class=\"ltx_emph ltx_font_italic\">single tool call thread</em> runs at any given time. (Examples of generated <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m6\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> are provided in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T8\" title=\"Table 8 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> in the Appendix.)\nThis approach offers several key advantages. First, it effectively eliminating redundant parallel threads and significantly reducing computational overhead. This is especially important for deployment on resource-constrained devices.\nSecond, this formulation removes the need for an external <em class=\"ltx_emph ltx_font_italic\">reflector</em> (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E2\" title=\"Equation 2 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) module. The model confidently relies on the results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from the most recent tool call using <math alttext=\"\\hat{Q}^{\\text{prev}}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m8\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{B}</annotation></semantics></math> to generate the spoken response <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m9\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by maximizing <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math>, reducing system complexity.</p>\n\n",
                "matched_terms": [
                    "query",
                    "generated",
                    "recent",
                    "most"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-training</span>: To train the model, we transform text-based tool usage benchmarks into spoken format (See S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13\" title=\"13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>). Word-level timestamps are computed using a pre-trained ASR model. For each partial ASR transcript <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math> up to block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, we generate corresponding queries for each tool <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m3\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> using an LLM as pseudo ground truth (GT).\nTo create effective training labels that teach the model when to trigger new queries, we employ a similarity-based labeling strategy where we compare the current pseudo GT query <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m4\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> with the most recent non-empty tool query label <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m5\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math> (see Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E3\" title=\"Equation 3 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) before block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m6\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>.\nOur labeling function assigns the training label <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m7\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E3\" title=\"Equation 3 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) for the tool query after block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m8\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> as follows:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p2.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> when the current query is sufficiently similar to the previous query (as determined by manually defined heuristics <math alttext=\"f(\\cdots)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m9\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8943;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f(\\cdots)</annotation></semantics></math>), we assign the special label NO_QUERY to teach the model that no new tool call is needed.  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p2.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> When the queries are sufficiently different, we assign the actual pseudo ground truth query\n<math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m10\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> as the label to teach the model to trigger a new tool call:</p>\n\n",
                "matched_terms": [
                    "recent",
                    "transcript",
                    "most",
                    "queries",
                    "current",
                    "query",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For KG queries, we assign a NO_QUERY label when the current query exactly matches the previous one. For web queries, we assign a NO_QUERY label if the top five retrieved documents for the current query remain unchanged from the previous query.</p>\n\n",
                "matched_terms": [
                    "query",
                    "current",
                    "web",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a multi-task fine-tuning strategy targeting two key capabilities.\nFirst, we train the model on Streaming Tool Query Generation by optimizing <math alttext=\"P(Q^{T}_{b}|Q_{1:b},\\hat{Q}^{\\text{prev}}_{b})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo fence=\"false\">|</mo><mrow><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><mo>,</mo><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}_{b}|Q_{1:b},\\hat{Q}^{\\text{prev}}_{b})</annotation></semantics></math> for <math alttext=\"b\\in[1,B]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m2\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mi>B</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b\\in[1,B]</annotation></semantics></math>, enabling intelligent decisions about when to trigger tool queries. Second, we fine-tune on Response Generation by optimizing <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m3\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) to improve the intelligibility of the speech output.</p>\n\n",
                "matched_terms": [
                    "query",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An important aspect of our post-training is enhancing the model&#8217;s ability to recover from errors in intermediate query predictions. For example, when presented with the audio question, &#8220;Who founded Rare Beauty in 2019?&#8221;, we observed that an initial misinterpretation of <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m1\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, such as &#8220;Red Bull founder&#8221;, can lead the model to subsequently generate NO_QUERY labels, effectively halting further attempts to retrieve the correct information. This issue arises because, during training, the model is always provided with correct previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m2\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, whereas during inference, it may make errors due to partial utterances <math alttext=\"{Q}_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">{Q}_{1:b}</annotation></semantics></math> being ambiguous. Thus the model lacks the ability to recover from such mistakes during inference.\nTo overcome this, we introduce a novel strategy in which we deliberately inject negative samples during post-training by substituting the previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m4\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math> in Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E4\" title=\"Equation 4 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> with incorrect ones <math alttext=\"Q^{\\text{neg}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m5\" intent=\":literal\"><semantics><msubsup><mi>Q</mi><mi>b</mi><mtext>neg</mtext></msubsup><annotation encoding=\"application/x-tex\">Q^{\\text{neg}}_{b}</annotation></semantics></math>. Crucially, when we perform negative sampling, we fall back to the pseudo ground truth query <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> as the training label:</p>\n\n",
                "matched_terms": [
                    "example",
                    "which",
                    "question",
                    "during",
                    "query"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To rigorously evaluate our proposed approach, we construct comprehensive benchmark datasets featuring spoken queries paired with simulated tool interactions. We begin with the CRAG dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>, which form the basis for our spoken version of CRAG, which we term <em class=\"ltx_emph ltx_font_italic\">AudioCRAG</em>. It consists of 2 distinct variants:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S4.SS1.p1.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> <span class=\"ltx_text ltx_font_bold\">Audio CRAG Synthetic</span>: To generate spoken queries, we use our in-house TTS system. We then apply a rigorous filtering procedure which results in a high-quality set of 1,862 spoken queries, which we refer to as the <em class=\"ltx_emph ltx_font_italic\">AudioCRAG-Synthetic</em> benchmark.  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S4.SS1.p1.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> <span class=\"ltx_text ltx_font_bold\">Audio CRAG Human</span>: To further enhance the realism and diversity of our evaluation, we introduce the AudioCRAG-Human benchmark, which consists of 618 human-recorded spoken queries. We will release\nAudio CRAG Human benchmark upon acceptance to supports the development of more natural and reliable voice assistants.\nFurther details on the construction of these benchmarks are provided in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S9\" title=\"9 Audio CRAG Benchmark &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. We follow the CRAG setup to incorporate both web and KG-based tools, and adopt its robust evaluation methodology, as described in Secs. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S10\" title=\"10 Tool Usage Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S11\" title=\"11 Evaluation Setting &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.\nAdditionally, we leverage a random subset of 16,000 questions from the text-based factual question answering dataset TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib15\" title=\"\">2017</a>)</cite> to post-train our speech-in, speech-out models, as detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13\" title=\"13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "question",
                    "web",
                    "queries",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform an ablation study (referred to as &#8220;Tool Integration&#8221; in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) where we post-train the model on sequential query generation (i.e. <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) and output generation, to assess the impact of streaming RAG post-training versus standard post-training on final response generation.\nAdditionally, we conduct an ablation study on open book setting (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) using a self-cascade approach with a three-stage inference pipeline: (1) the audio question is used to generate a tool query <math alttext=\"Q^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><msup><mi>Q</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">Q^{T}</annotation></semantics></math> (corresponding to the &#8220;Query Generation&#8221; stage described in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>); (2) the audio question, and tool results are combined to produce the final text output <math alttext=\"X^{\\text{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>res</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{res}}</annotation></semantics></math> by maximizing <math alttext=\"P(X^{\\text{res}}|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mtext>res</mtext></msup><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(X^{\\text{res}}|Q,R)</annotation></semantics></math>; and (3) the audio question, and final text output are used to generate the final speech output <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by optimizing <math alttext=\"P(A|Q,X^{\\text{res}})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m6\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><msup><mi>X</mi><mtext>res</mtext></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,X^{\\text{res}})</annotation></semantics></math>. Since we teacher-force the text output to obtain the final speech output in stage (3), this self-cascade approach can only be applied to a &#8220;thinker-talker&#8221; architecture (eq. Qwen-OMNI) or Chain-of-Thought (CoT) style architectures (eg. OpusLM).\nThe motivation for this ablation is to investigate whether the inclusion of RAG references <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> during the &#8220;Response Generation&#8221; stage (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) affects the quality of the generated speech <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m8\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "setting",
                    "qwenomni",
                    "question",
                    "during",
                    "book",
                    "open",
                    "query",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a comprehensive performance comparison of three models, Qwen2.5-7B, OpusLM, and Kimi Audio, evaluated across three settings: Closed Book, Open Book, and Streaming RAG. All models are assessed on both the AudioCRAG-Synthetic (Syn.) and AudioCRAG-Human (Hum.). In the Closed Book setting, where models rely solely on their internal knowledge without access to external tools (reference length = 0), all models achieve accuracy scores below 20%. These results highlight the inherent limitations of closed-book approaches in handling complex queries.\nThe Open Book setting, which provides models with access to external information, demonstrates the clear benefits of tool integration. Qwen2.5-7B and Kimi Audio&#8217;s accuracy rises substantially, underscoring the value of leveraging external context. As expected, latency increases due to the additional overhead of retrieving information from external tools (See T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> for detailed latency analysis).</p>\n\n",
                "matched_terms": [
                    "which",
                    "setting",
                    "queries",
                    "book",
                    "open"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most notably, our post-training approach to build <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, as described in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>, delivers significant advancements in both accuracy and efficiency. Qwen2.5-7B and OpusLM achieve significant accuracy improvements across both benchmarks. While the absolute accuracy scores may appear low, they are consistent with the evaluation results observed in the CRAG benchmark. Notably, Qwen2.5-7B with <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> achieves accuracy comparable to the open book performance of similarly sized LLMs reported in the original CRAG paper (i.e., 34.2 for Qwen2.5-7B vs. 32.1 for LLAMA-3 8B Instruct in <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>).\nImportantly, although post-training is performed exclusively on the synthetic dataset, we observe consistent and even greater improvements on the human-spoken benchmark, demonstrating the robustness and strong generalization capabilities of our method. This setting also introduces substantial latency savings compared to the Open Book configuration, with Qwen2.5-7B and OpusLM achieving 20.7% and 14.8% reductions in first-token latency on the synthetic benchmark, and even greater savings on the human benchmark\n<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Our latency calculations on synthetic audio exclude end-point detection latency, which is required in all production systems. Since our streaming RAG approach enables processing without waiting for end-point detection, including this factor would further amplify the observed latency savings, as observed in higher latency savings for human-spoken audio, where endpoint detection errors often introduce trailing silence.</span></span></span>. These results demonstrate that our streaming RAG approach not only advances the accuracy of speech-in speech-out systems, but also optimizes response efficiency by enabling earlier and more effective prediction of tool queries.</p>\n\n",
                "matched_terms": [
                    "which",
                    "setting",
                    "most",
                    "queries",
                    "book",
                    "open"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Evaluated SOTA Speech-in Speech-out Models &#8227; 4 Experiment Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides a comparative evaluation of the three SOTA models in generating either text or speech outputs from speech inputs, both with and without the integration of external tool results. In the absence of tool results (Ref length = 0), all models achieve higher accuracy when generating text outputs compared to speech outputs. The incorporation of tool results generally leads to improved text generation performance, with Kimi Audio achieving the highest accuracy. In contrast, the accuracy for speech output remains consistently lower across all models and conditions. The self-cascade approach, in which the model first generates an intermediate text response before producing the final speech output, provides moderate improvements in speech output accuracy for both Qwen2.5-7B and OpusLM.\nHowever, it still underperforms compared to the text-out baseline, primarily due to errors in accurately generating answers involving uncommon entity nouns.\nOverall, these findings underscore a persistent challenge in direct speech generation, particularly when tool results are integrated, as this appears to negatively impact the quality of generated speech responses.\nOur <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> demonstrates clear advantages: it maintains comparable performance for text output and delivers substantial improvements for speech output, even outperforming the self-cascade approach. These results underscore the effectiveness of post-training in overcoming challenges in direct speech generation in tool-augmented scenarios.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "which",
                    "highest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> provides a comprehensive breakdown of latency measurements for the Qwen2.5-7B speech-to-speech model, evaluated in both the Open Book setting and our proposed <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> setting on AudioCRAG-Synthetic. We report both median (P50) and 90th percentile (P90) values for each stage of the processing pipeline. The latency is decomposed into three main components: tool query generation, tool result generation, and speech response synthesis, with measurements provided for both the first token outputs (We also provide latency measurements for last token output in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T9\" title=\"Table 9 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>). For both tool query and tool result generation, it is assumed that all tools are accessed in parallel; thus, the reported latency corresponds to the maximum query or result generation time among all tools. The majority of this latency arises from leveraging external web pages, which introduces significant delays, most notably, increasing the first token latency by 2.3x in Open Book Setting.\nNotably, our <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> setting enables early generation of tool results, successfully reducing P50 first token latency by 9.8% and tool use latency by 20.7%.</p>\n\n",
                "matched_terms": [
                    "which",
                    "setting",
                    "most",
                    "book",
                    "open",
                    "query",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Streaming RAG approaches</span>: Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> highlights the substantial latency savings enabled by the <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>), compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS1\" title=\"3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>). Three key metrics are evaluated: overall latency savings, the percentage of queries benefiting from reduced latency, and the number of parallel threads required.\nEven without any post-training, the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach already reduces tool usage latency (3.37s in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) by 10.7% for Open Book Qwen-OMNI, demonstrating its flexibility and plug-and-play compatibility with any existing speech-in speech-out model.\nThe <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> method, utilizing Qwen-OMNI, consistently delivers superior performance. It achieves greater average latency reductions and benefits a higher proportion of queries with improved response times. Notably, <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> require only a single parallel thread, representing a significant advancement in resource efficiency compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach, which demands multiple parallel threads.\nThese findings highlight the effectiveness of <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> in not only minimizing latency, validating our intuition from Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>, but also in optimizing system efficiency and resource utilization.</p>\n\n",
                "matched_terms": [
                    "which",
                    "qwenomni",
                    "queries",
                    "book",
                    "open"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-training Strategies</span>: Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents performance comparison under different post-training conditions. Incorporating streaming tool query generation during post-training (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>) results in comparable performance for both models. These results suggest that <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> can be integrated into post-training process without negatively impacting model performance.</p>\n\n",
                "matched_terms": [
                    "query",
                    "during"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation Results for Negative Sampling Strategy</span>: Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T5\" title=\"Table 5 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the ablation results evaluating the impact of deliberately injecting negative samples during post-training (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E5\" title=\"Equation 5 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). The findings highlight that, without negative sampling, streaming tool query generation can reduce final accuracy in text output settings, primarily due to errors in final query generation, as detailed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>. In contrast, our negative sampling approach significantly enhances the model&#8217;s robustness, enabling it to recover from intermediate prediction errors. This leads to consistently high accuracy while also achieving notable latency reductions (Tab. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "query",
                    "during"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced the first comprehensive approach for integrating external tool usage directly into E2E speech-in, speech-out dialogue systems. By incorporating <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> pipeline, we further enhanced the model&#8217;s ability to leverage retrieved information and autonomously decide when to trigger new tool queries, resulting in improved accuracy and responsiveness.\nEmpirical evaluation on the newly introduced AudioCRAG benchmark demonstrated that tool integration can more than double factual question answering accuracy compared to closed-book models. Additionally, our streaming RAG approach achieved a 20% reduction in tool usage latency, thereby preserving natural conversational flow.\nOverall, our contributions advance the state of the art in spoken dialogue systems by enabling accurate, real-time, and tool-augmented voice assistants. In addition, we are committed to open science and will release our training code and Audio CRAG Human benchmark to support future research and development in this area.</p>\n\n",
                "matched_terms": [
                    "question",
                    "achieved",
                    "queries",
                    "open"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in benchmarking text-based dialogue systems for tool usage <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib4\" title=\"\">2023b</a>; Ouyang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib24\" title=\"\">2025</a>; Cheng and Dou, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib6\" title=\"\">2025</a>; Cohen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib7\" title=\"\">2025</a>; Xiong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib35\" title=\"\">2024a</a>; Vu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib31\" title=\"\">2023</a>; Xiong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib36\" title=\"\">2024b</a>; Peng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib25\" title=\"\">2024</a>; Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib29\" title=\"\">2025</a>; Ni et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib23\" title=\"\">2025</a>)</cite> have primarily focused on evaluating factual question answering and task completion within simulated environments. The CRAG benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite> is a leading example, featuring 4,409 question-answer pairs and providing mock APIs for both web and knowledge graph (KG) search. CRAG supports a range of KG and web retrieval tasks, and highlights key challenges such as hallucinations in retrieval-augmented generation (RAG) and the importance of leveraging KGs and search ranking to improve factual accuracy. Evaluation is conducted automatically using two LLM judges. SimpleQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib32\" title=\"\">2024</a>)</cite> is another widely adopted benchmark, designed to assess language models on short, fact-seeking questions. With 4,326 adversarially collected questions spanning diverse topics and a straightforward grading scheme based on single, indisputable answers, SimpleQA provides a robust testbed for factual accuracy. Moving beyond question answering, WebArena <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib41\" title=\"\">2024</a>)</cite> offers a simulated environment for evaluating dialogue agents on web-based tasks using fully functional websites, enabling assessment of more complex, action-oriented behaviors. While these benchmarks have significantly advanced the evaluation of tool-augmented dialogue systems, they remain largely limited to text-based interactions and do not fully address the unique challenges presented by speech-in speech-out systems.</p>\n\n",
                "matched_terms": [
                    "have",
                    "recent",
                    "example",
                    "question",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent benchmarks <cite class=\"ltx_cite ltx_citemacro_cite\">Mei et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib19\" title=\"\">2025</a>); Yu et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib39\" title=\"\">2025</a>); Luo et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib16\" title=\"\">2024</a>)</cite> have extended tool-augmented dialogue evaluation to multimodal and longer-context scenarios. The m&amp;m&#8217;s benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib17\" title=\"\">2024</a>)</cite> evaluates LLMs on multi-step, multi-modal tasks using a diverse set of 33 tools, including public APIs and multimodal models such as off-the-shelf automatic speech recognition (ASR) models, highlighting the potential for developing agents that leverage audio-based tools.\nCRAG_MM <cite class=\"ltx_cite ltx_citemacro_citep\">(Meta CRAG-MM Challenge Organizers, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib21\" title=\"\">2025</a>)</cite> builds on the original CRAG benchmark by introducing visual question answering (QA) tasks that combine images and text-based queries, utilizing mock APIs for both image descriptions and web search.\nFor video understanding and long-context reasoning, the Video Web Arena <cite class=\"ltx_cite ltx_citemacro_citep\">(Jang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib14\" title=\"\">2025</a>)</cite> benchmark evaluates multimodal agents on tasks involving 2,021 manually crafted tutorial videos.\nWhile these benchmarks advance the field by incorporating multimodal tools, they still do not evaluate systems in speech-in speech-out scenarios.</p>\n\n",
                "matched_terms": [
                    "have",
                    "recent",
                    "queries",
                    "question",
                    "web",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We begin with the CRAG dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>, which contains 2,706 text queries. Since these queries are not directly suitable for speech-based evaluation, we first identify those requiring adaptation before TTS conversion. Through careful manual inspection, we determine that queries containing dates or brackets benefit from rewriting to ensure naturalness and clarity in spoken form. In total, we identify 569 such queries and rewrite them using a large language model (LLAMA-4 Maverick). The resulting 569 rewritten queries, combined with the remaining original queries, form the basis for our spoken version of CRAG, which we term <em class=\"ltx_emph ltx_font_italic\">AudioCRAG</em>. We follow the CRAG setup to incorporate web and KG-based tools and adopt its robust evaluation setup, as detailed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S10\" title=\"10 Tool Usage Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S11\" title=\"11 Evaluation Setting &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "web",
                    "queries",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio CRAG Synthetic</span>:\nTo generate spoken queries, we process all 2,706 queries through our in-house TTS system. We then apply a rigorous filtering procedure to remove queries with intelligibility issues, specifically excluding any utterances for which Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib27\" title=\"\">2023</a>)</cite> hypotheses exhibit a non-zero word error rate. We also remove utterances with suboptimal audio quality, as determined by UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib28\" title=\"\">2022</a>)</cite> scores below 3.5. This results in a high-quality set of 1,862 spoken queries, which we refer to as the <em class=\"ltx_emph ltx_font_italic\">AudioCRAG-Synthetic</em> benchmark.</p>\n\n",
                "matched_terms": [
                    "queries",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio CRAG Human</span>:\nTo further enhance the realism and diversity of our evaluation, we introduce the AudioCRAG-Human benchmark, which consists of 618 human-recorded spoken queries. These queries are recorded by a diverse pool of participants to capture natural variations in speech, accent, and prosody. The inclusion of human-recorded audio enables a more comprehensive assessment of speech-in speech-out systems under real-world conditions, providing valuable insights into model robustness and generalization beyond synthetic speech. This benchmark serves as a critical resource for evaluating the effectiveness of tool integration in conversational AI systems.</p>\n\n",
                "matched_terms": [
                    "queries",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable effective tool usage, we build upon the CRAG framework by integrating two complementary information sources: web search, which provides access to fresh and dynamic content, and a knowledge graph, which offers structured and reliable information. For web search, we aggregate all 100,000 documents from the CRAG corpus and employ a BGE-based re-ranker<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/BAAI/bge-large-en-v1.5\" title=\"\">https://huggingface.co/BAAI/bge-large-en-v1.5</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Xiao et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib33\" title=\"\">2023</a>)</cite> to index and retrieve the top 50 most relevant documents for each query. These documents are then segmented into chunks and re-ranked using the same BGE model based on their similarity to the query, ensuring highly contextually relevant retrieval. Meanwhile, queries to the knowledge graph are performed via a simulated API, adhering to the methodology established in CRAG <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/facebookresearch/CRAG/tree/main/mock_api\" title=\"\">https://github.com/facebookresearch/CRAG/tree/main/mock_api</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "which",
                    "most",
                    "queries",
                    "query",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Similar to previous work <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>, we employ model-based automatic evaluation. We use a three-way scoring system, assigning scores of 1, -1, and 0 for accurate, incorrect, and missing answers, respectively. The evaluation is conducted using the Llama 4-maverick LLM evaluator. For speech outputs, we first transcribe the audio using Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib27\" title=\"\">2023</a>)</cite> before passing the transcriptions to the LLM evaluator. In this study, our primary focus is on enhancing system accuracy; therefore, we report average accuracy values in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Evaluated SOTA Speech-in Speech-out Models &#8227; 4 Experiment Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. For additional context, we also provide the average rates of accurate, hallucinated, and missing responses, as well as overall truthfulness scores for each system in the Open Book Setting (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S10.T6\" title=\"Table 6 &#8227; 10 Tool Usage Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>). Notably, our results indicate that Qwen-OMNI was less likely to generate hallucinated responses compared to OpusLM and Kimi Audio.</p>\n\n",
                "matched_terms": [
                    "qwenomni",
                    "book",
                    "setting",
                    "open"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qwen-OMNI</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>)</cite> is a end-to-end multimodal model that seamlessly integrates diverse input modalities&#8212;including text, images, audio, and video&#8212;and generates both text and natural speech responses in a real-time streaming fashion. It leverages an innovative Thinker-Talker architecture, where the Thinker module performs high-level reasoning to produce a text response, which is then used by the Talker module, conditioning on both the text and the Thinker&#8217;s hidden representations, to generate streaming speech output.</p>\n\n",
                "matched_terms": [
                    "qwenomni",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For tool-augmented scenarios, retrieval results are provided up to each model&#8217;s maximum token limit (&#8220;Ref length&#8221; in Tables), maintaining a 2:1 ratio of web page to KG results.\nSpecifically, we observe that Kimi-Audio is currently optimized for handling tool result references up to a certain length. When this limit is exceeded, an error arises during the audio detokenization process, specifically within the rotary embedding mechanism, highlighting an architectural constraint in processing longer input sequences or larger reference contexts. Addressing this limitation presents a valuable opportunity for future model enhancements.</p>\n\n",
                "matched_terms": [
                    "during",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation encompasses both speech-in-text-out and speech-in-speech-out scenarios. For the Fixed-Interval Streaming RAG setting (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS1\" title=\"3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>), intermediate tool queries are generated at consistent 1-second intervals. In the Model-Triggered Streaming RAG setting, the model dynamically determines the need for a tool call after processing each 500ms block. This approach allows us to utilize a smaller chunk size, as only a single tool call thread is required for Model-Triggered Streaming RAG, thereby enabling more efficient and responsive processing.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "queries",
                    "setting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This subsection details the experimental setup for post-training the pretrained speech-in, speech-out model to significantly enhance its tool usage capabilities, as outlined in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>.\nWe leverage a random subset of 16,000 questions from the text-based factual question answering dataset TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib15\" title=\"\">2017</a>)</cite>, which contains 97,000 questions and 662,659 associated web documents.\nFor <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, we further compute word-level timestamps using a pre-trained ASR model, OWSM CTC v4 1B <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib26\" title=\"\">2025</a>)</cite>, enabling us to generate partial ASR transcripts <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m1\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math> at 500 ms intervals. Note that if word occurs at boundary of block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, it is excluded from <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math>. For each partial transcript <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math>, we generate corresponding psuedo ground truth queries <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m5\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> using LLAMA-4-Maverick to simulate incremental user input.\nTo simulate realistic tool usage, we concatenate all documents and employ a web query reranker to retrieve the top 50 most relevant documents for each query. The text questions from this 16k subset are converted into discrete speech tokens using text-to-speech synthesis with the corresponding pretrained speech-in, speech-out model. Recognizing that TriviaQA answers are typically single named entities, we further transform the queries into a conversational style using LLAMA-4-Maverick, making them more suitable for dialogue-based evaluation.</p>\n\n",
                "matched_terms": [
                    "which",
                    "transcript",
                    "most",
                    "queries",
                    "question",
                    "query",
                    "web",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT = \"\"\"\nYou are given an Audio Question and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". Your task is to answer the question in as few words as possible.\n<br class=\"ltx_break\"/>Please follow these guidelines when formulating your answer:\n<br class=\"ltx_break\"/>1. If the question contains a false premise or assumption, answer &#8220;invalid question&#8221;.\n<br class=\"ltx_break\"/>2. If you are uncertain or don&#8217;t know the answer, respond with &#8220;I don&#8217;t know&#8221;.\n<br class=\"ltx_break\"/>### Question\n<br class=\"ltx_break\"/>{query}\n<br class=\"ltx_break\"/>### Query Time\n<br class=\"ltx_break\"/>{query_time}\n<br class=\"ltx_break\"/>### Answer\n<br class=\"ltx_break\"/>\"\"\"</p>\n\n",
                "matched_terms": [
                    "query",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT = \"\"\"\nYou are given an Audio Question, References and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". The references may or may not help answer the question. Your task is to answer the question in as few words as possible.\n<br class=\"ltx_break\"/>Please follow these guidelines when formulating your answer:\n<br class=\"ltx_break\"/>1. If the question contains a false premise or assumption, answer &#8220;invalid question&#8221;.\n<br class=\"ltx_break\"/>2. If you are uncertain or don&#8217;t know the answer, respond with &#8220;I don&#8217;t know&#8221;.\n<br class=\"ltx_break\"/>### Question\n<br class=\"ltx_break\"/>{query}\n<br class=\"ltx_break\"/>### Query Time\n<br class=\"ltx_break\"/>{query_time}\n<br class=\"ltx_break\"/>### References\n<br class=\"ltx_break\"/># web\n<br class=\"ltx_break\"/>{web_results}\n<br class=\"ltx_break\"/># knowledge graph\n<br class=\"ltx_break\"/>{kg_response}\n<br class=\"ltx_break\"/>### Answer\n<br class=\"ltx_break\"/>\"\"\"</p>\n\n",
                "matched_terms": [
                    "query",
                    "web",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT = \"\"\"\nYou are an agent that only outputs JSON. You are given a Query and Query Time. Do the following:\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>1) Determine the domain the query is about. The domain should be one of the following: \"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". If none of the domains apply, use \"other\". Use \"domain\" as the key in the result json.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>2) Extract structured information from the query. Include different keys into the result json depending on the domains, and put them DIRECTLY in the result json. Here are the rules:\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;encyclopedia&#8217; and &#8216;other&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;main_entity&#8217;: extract the main entity of the query.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;finance&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;market_identifier&#8217;: stock identifiers including individual company names, stock symbols.\n<br class=\"ltx_break\"/>- &#8216;metric&#8217;: financial metrics that the query is asking about. This must be one of the following: &#8216;price&#8217;, &#8216;dividend&#8217;, &#8216;P/E ratio&#8217;, &#8216;EPS&#8217;, &#8216;marketCap&#8217;, and &#8216;other&#8217;.\n<br class=\"ltx_break\"/>- &#8216;datetime&#8217;: time frame that the query asks about. When datetime is not explicitly mentioned, use &#8216;Query Time&#8217; as default.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;movie&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;movie_name&#8217;: name of the movie\n<br class=\"ltx_break\"/>- &#8216;movie_aspect&#8217;: if the query is about a movie, which movie aspect the query asks. This must be one of the following: &#8216;budget&#8217;, &#8216;genres&#8217;, &#8216;original_language&#8217;, &#8216;original_title&#8217;, &#8216;release_date&#8217;, &#8216;revenue&#8217;, &#8216;title&#8217;, &#8216;cast&#8217;, &#8216;crew&#8217;, &#8216;rating&#8217;, &#8216;length&#8217;.\n<br class=\"ltx_break\"/>- &#8216;person&#8217;: person name related to moves\n<br class=\"ltx_break\"/>- &#8216;person_aspect&#8217;: if the query is about a person, which person aspect the query asks. This must be one of the following: &#8216;acted_movies&#8217;, &#8216;directed_movies&#8217;, &#8216;oscar_awards&#8217;, &#8216;birthday&#8217;.\n<br class=\"ltx_break\"/>- &#8216;year&#8217;: if the query is about movies released in a specific year, extract the year\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;music&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;artist_name&#8217;: name of the artist\n<br class=\"ltx_break\"/>- &#8216;artist_aspect&#8217;: if the query is about an artist, extract the aspect of the artist. This must be one of the following: &#8216;member&#8217;, &#8216;birth place&#8217;, &#8216;birth date&#8217;, &#8216;lifespan&#8217;, &#8216;artist work&#8217;, &#8216;grammy award count&#8217;, &#8216;grammy award date&#8217;.\n<br class=\"ltx_break\"/>- &#8216;song_name&#8217;: name of the song\n<br class=\"ltx_break\"/>- &#8216;song_aspect&#8217;: if the query is about a song, extract the aspect of the song. This must be one of the following: &#8216;author&#8217;, &#8216;grammy award count&#8217;, &#8216;release country&#8217;, &#8216;release date&#8217;.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;sports&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;sport_type&#8217;: one of &#8216;basketball&#8216;, &#8216;soccer&#8216;, &#8216;other&#8216;\n<br class=\"ltx_break\"/>- &#8216;tournament&#8217;: NBA, World Cup, Olympic.\n<br class=\"ltx_break\"/>- &#8216;team&#8217;: teams that users are interested in.\n<br class=\"ltx_break\"/>- &#8216;datetime&#8217;: time frame that the user is interested in. When datetime is not explicitly mentioned, use &#8216;Query Time&#8217; as default.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Return the results in a FLAT json.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n<br class=\"ltx_break\"/>\"\"\"</p>\n\n",
                "matched_terms": [
                    "which",
                    "movie",
                    "movies",
                    "award",
                    "queries",
                    "teams",
                    "stock",
                    "query"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT = \"\"\"\nYou are an agent that only outputs JSON. You are given an Audio\nQuery, Previously generated JSON result (&#8217;Previous Result&#8217;) and Query Time. Do the following:</p>\n\n",
                "matched_terms": [
                    "query",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8216;encyclopedia&#8217; and &#8216;other&#8217; queries, these are possible keys:\n- &#8216;main_entity&#8217;: extract the main entity of the query.</p>\n\n",
                "matched_terms": [
                    "query",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8216;finance&#8217; queries, these are possible keys:\n- &#8216;market_identifier&#8217;: stock identifiers including individual company names, stock symbols.\n- &#8216;metric&#8217;: financial metrics that the query is asking about. This must be one of the following: &#8216;price&#8217;,\n&#8216;dividend&#8217;, &#8216;P/E ratio&#8217;, &#8216;EPS&#8217;, &#8216;marketCap&#8217;, and &#8216;other&#8217;.\n- &#8216;datetime&#8217;: time frame that the query asks about. When datetime is not explicitly mentioned, use\n&#8216;Query Time&#8217; as default.</p>\n\n",
                "matched_terms": [
                    "query",
                    "stock",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8216;movie&#8217; queries, these are possible keys:\n- &#8216;movie_name&#8217;: name of the movie\n- &#8216;movie_aspect&#8217;: if the query is about a movie, which movie aspect the query asks. This must be one\nof the following: &#8216;budget&#8217;, &#8216;genres&#8217;, &#8216;original_language&#8217;, &#8216;original_title&#8217;, &#8216;release_date&#8217;, &#8216;revenue&#8217;,\n&#8216;title&#8217;, &#8216;cast&#8217;, &#8216;crew&#8217;, &#8216;rating&#8217;, &#8216;length&#8217;.\n- &#8216;person&#8217;: person name related to moves\n- &#8216;person_aspect&#8217;: if the query is about a person, which person aspect the query asks. This must be\none of the following: &#8216;acted_movies&#8217;, &#8216;directed_movies&#8217;, &#8216;oscar_awards&#8217;, &#8216;birthday&#8217;.\n- &#8216;year&#8217;: if the query is about movies released in a specific year, extract the year</p>\n\n",
                "matched_terms": [
                    "which",
                    "movie",
                    "movies",
                    "queries",
                    "query"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8216;music&#8217; queries, these are possible keys:\n- &#8216;artist_name&#8217;: name of the artist\n- &#8216;artist_aspect&#8217;: if the query is about an artist, extract the aspect of the artist. This must be one of the\nfollowing: &#8216;member&#8217;, &#8216;birth place&#8217;, &#8216;birth date&#8217;, &#8216;lifespan&#8217;, &#8216;artist work&#8217;, &#8216;grammy award count&#8217;,\n&#8216;grammy award date&#8217;.\n- &#8216;song_name&#8217;: name of the song\n- &#8216;song_aspect&#8217;: if the query is about a song, extract the aspect of the song. This must be one of the\nfollowing: &#8216;author&#8217;, &#8216;grammy award count&#8217;, &#8216;release country&#8217;, &#8216;release date&#8217;.</p>\n\n",
                "matched_terms": [
                    "query",
                    "award",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8216;sports&#8217; queries, these are possible keys:\n- &#8216;sport_type&#8217;: one of &#8216;basketball&#8216;, &#8216;soccer&#8216;, &#8216;other&#8216;\n- &#8216;tournament&#8217;: NBA, World Cup, Olympic.\n- &#8216;team&#8217;: teams that users are interested in.\n- &#8216;datetime&#8217;: time frame that the user is interested in. When datetime is not explicitly mentioned, use\n&#8216;Query Time&#8217; as default.\nReturn the results in a FLAT json.\n<br class=\"ltx_break\"/>*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*</p>\n\n",
                "matched_terms": [
                    "queries",
                    "teams"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT = \"\"\" You are given an Audio Query and Query Time. Your task is to generate a web query that can be used to retrieve relevant web pages. Rewrite the following query into a short and succinct form, focusing on the main topic or domain (e.g. finance, sports, music, movie, encyclopedia), key entities mentioned (e.g. people, organizations, locations), and specific aspects of those entities (e.g. performance metrics, relationships, events). Ensure the rewritten query is clear, concise, and easy to understand.\nNote that simply outputting the original query is not acceptable. You must rephrase the query to make it more concise and focused on the key information that will help retrieve relevant web pages.</p>\n\n",
                "matched_terms": [
                    "query",
                    "web",
                    "movie"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8217;finance&#8217; queries, focus on:\n- Company names or stock symbols\n- Financial metrics (e.g. price, dividend, P/E ratio, EPS, marketCap)\n- Specific timeframes or events; if no timeframe is specified, use the Query Time as default</p>\n\n",
                "matched_terms": [
                    "’finance’",
                    "price",
                    "queries",
                    "dividend",
                    "stock",
                    "query"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8217;sports&#8217; queries, focus on:\n- Sports Type (eg. basketball, soccer)\n- Teams, players\n- Statistics or performance metrics (e.g. scores, wins, losses)\n- Specific events or tournaments (eg. NBA, World Cup, Olympic)\n- Time frame that the user is interested in; if no timeframe is specified, use the Query Time as default</p>\n\n",
                "matched_terms": [
                    "query",
                    "queries",
                    "teams",
                    "’sports’"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8217;music&#8217; queries, focus on:\n- Artist names or song titles\n- Specific aspects of artist (eg. band name, birth place, birth date, lifespan, artist work, grammy award count, grammy award date)\n- Specific aspects of song (eg. author, grammy award count, release country, release date)\n- Music genres or categories\n- Specific awards or recognition (e.g. Grammy Awards, Billboard)</p>\n\n",
                "matched_terms": [
                    "award",
                    "date",
                    "’music’",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8217;movie&#8217; queries, focus on:\n- Movie titles or celebrity names\n- Movie genres or other categories like budget, language, release_date, revenue, cast, crew, rating, length\n- Specific aspects of celebrity like acted_movies, directed_movies, oscar_awards, birthday\n- Specific awards or recognition (e.g. Oscars)\nFor &#8217;other&#8217; queries, focus on:\n- Main entity or topic\n- Specific aspects or attributes of the entity</p>\n\n",
                "matched_terms": [
                    "’movie’",
                    "queries",
                    "movie"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When rewriting the query, ensure that it captures all important information from the original question that could impact the retrieval results. Do not omit any crucial details, such as specific dates, locations, or relationships between entities. Also, do not invent any new details on your own. If necessary, use the Query Time to provide context for the query. The goal is to create a concise and accurate query that effectively conveys the user&#8217;s intent and retrieves relevant information.\n*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT THE REWRITTEN QUERY!!!*\n\"\"\"</p>\n\n",
                "matched_terms": [
                    "query",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT = \"\"\"You are given an Audio Query, previously generated Web query (&#8217;Previous Result&#8217;) and Query Time.</p>\n\n",
                "matched_terms": [
                    "query",
                    "generated",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Your task is to generate a web query that can be used to retrieve relevant web pages. Rewrite the following query into a short and succinct form, focusing on the main topic or domain (e.g. finance, sports, music, movie, encyclopedia), key entities mentioned (e.g. people, organizations, locations), and specific aspects of those entities (e.g. performance metrics, relationships, events). Ensure the rewritten query is clear, concise, and easy to understand.</p>\n\n",
                "matched_terms": [
                    "query",
                    "web",
                    "movie"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note that simply outputting the original query is not acceptable. You must rephrase the query to make it more concise and focused on the key information that will help retrieve relevant web pages.</p>\n\n",
                "matched_terms": [
                    "query",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8217;finance&#8217; queries, focus on:\n- Company names or stock symbols\n- Financial metrics (e.g. price, dividend, P/E ratio, EPS, marketCap)\n- Specific timeframes or events; if no timeframe is specified, use the Query Time as default</p>\n\n",
                "matched_terms": [
                    "’finance’",
                    "price",
                    "queries",
                    "dividend",
                    "stock",
                    "query"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8217;sports&#8217; queries, focus on:\n- Sports Type (eg. basketball, soccer)\n- Teams, players\n- Statistics or performance metrics (e.g. scores, wins, losses)\n- Specific events or tournaments (eg. NBA, World Cup, Olympic)\n- Time frame that the user is interested in; if no timeframe is specified, use the Query Time as default</p>\n\n",
                "matched_terms": [
                    "query",
                    "queries",
                    "teams",
                    "’sports’"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8217;music&#8217; queries, focus on:\n- Artist names or song titles\n- Specific aspects of artist (eg. band name, birth place, birth date, lifespan, artist work, grammy award count, grammy award date)\n- Specific aspects of song (eg. author, grammy award count, release country, release date)\n- Music genres or categories\n- Specific awards or recognition (e.g. Grammy Awards, Billboard)</p>\n\n",
                "matched_terms": [
                    "award",
                    "date",
                    "’music’",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8217;movie&#8217; queries, focus on:\n- Movie titles or celebrity names\n- Movie genres or other categories like budget, language, release_date, revenue, cast, crew, rating, length\n- Specific aspects of celebrity like acted_movies, directed_movies, oscar_awards, birthday\n- Specific awards or recognition (e.g. Oscars)</p>\n\n",
                "matched_terms": [
                    "’movie’",
                    "queries",
                    "movie"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When rewriting the query, ensure that it captures all important information from the original question that could impact the retrieval results. Do not omit any crucial details, such as specific dates, locations, or relationships between entities. Also, do not invent any new details on your own. If necessary, use the Query Time to provide context for the query. The goal is to create a concise and accurate query that effectively conveys the user&#8217;s intent and retrieves relevant information.\nNow, compare the new web query to the previously generated web query (&#8217;Previous Result&#8217;).</p>\n\n",
                "matched_terms": [
                    "query",
                    "generated",
                    "web",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the new query is similar enough to the previous web query (i.e., it effectively conveys the same user intent and would retrieve similar relevant information), output only *NO_QUERY*.</p>\n\n",
                "matched_terms": [
                    "query",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">You should make the judgment based on provided examples.\nExamples:\nQuestion: \"which company has higher eps, btu or cma?\"\nGround Truth: \"cma\"\nPrediction: \"it is not possible to determine which company has a higher eps.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction is not sure about the answer.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "which",
                    "has"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who is taller, a or b?\"\nGround Truth: \"a\"\nPrediction: \"The answer is a. a is of height 1.75 m and b is of height 1.82 m. So b is taller.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction is self-contradictory.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "who"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who among patricia place and lana clarkson is younger?\"\nGround Truth: \"lana clarkson\"\nPrediction: \"lana clarkson (born april 5, 1962) is younger than patricia clarkson (born december 29, 1959).\"\nOutput: \"score\": 1, \"explanation\": \"The prediction gives the correct answer.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "who",
                    "december"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"did lakers win more games than warriors in 2024-4?\"\nGround Truth: \"yes\"\nPrediction: \"yes, lakers won more games than warriors in 2023-2024 season.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction is not answering the question.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "won"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the height of the building where citigroup is headquartered?\"\nGround Truth: \"151 m\"\nPrediction: \"the height is 915 feet (279 m).\"\nOutput: \"score\": 0, \"explanation\": \"The prediction, 151 m, does not match the ground truth, 279 m.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "what"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who were the last 3 prime ministers of uk?\"\nGround Truth: \"rishi sunak, liz truss, and boris johnson\"\nPrediction: \"1. liz truss, 2. boris johnson, 3.theresa may\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not contain item, rishi sunak, that is in the grount truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "who"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who authored the taming of the shrew?\"\nGround Truth: \"william shakespeare\"\nPrediction: \"william shakespeare\"\nOutput: \"score\": 1, \"explanation\": \"The prediction exactly matches the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "who"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who authored The Taming of the Shrew?\"\nGround Truth: \"william shakespeare\"\nPrediction: \"w shakespeare\"\nOutput: \"score\": 1, \"explanation\": \"The prediction matches the ground truth as w is the abbreviation of william.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "who"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the state bird of california?\"\nGround Truth: \"california quail\"\nPrediction: \"california valley quail\"\nOutput: \"score\": 1, \"explanation\": \"The prediction matches the ground truth as california quail is also known as california valley quail.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "what"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what country is maharashtra metro rail corporation limited located in?\"\nGround truth: \"india\"\nPrediction: \"indianapolis\"\nOutput: \"score\": 0, \"explanation\": \"The prediction doesn&#8217;t match the ground truth answers.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "what"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what was wolves&#8217;s record in their last game in eng-premier league?\"\nGround Truth: \"w\"\nPrediction: \"wolves&#8217;s record in their last game in the eng-premier League was a 2-1 win over fulham.\"\nOutput: \"score\": 1, \"explanation\": \"The prediction correctly answers the result is a win.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "what"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"on which days did am distribute dividends in the last year?\"\nGround Truth: \"2023-01-24, 2023-04-25, 2023-07-25, 2023-10-24\"\nPrediction: \"the company distributed dividends on october 24, 2023 and january 24, 2023.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction partially matches the ground truth, but it is incomplete.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "distributed",
                    "dividends",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"on which days did xxx distribute dividends in the last year?\"\nGround Truth: \"2023-01-13, 2023-03-25, 2023-11-21\"\nPrediction: \"xxx distributed dividends on 1. 2023-01-13, 2. 2023-03-25, 3. 2023-10-21.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction contains an item, 2023-10-21, which is not in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "distributed",
                    "dividends",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who are current band members of maroon 5?\"\nGround Truth: \"adam Levine, jesse carmichael, james valentine, matt flynn, pj morton, sam farrar\"\nPrediction: \"they are jesse carmichael, matt flynn, adam levine, pj morton, sam farrar, and james valentine.\"\nOutput: \"score\": 1, \"explanation\": \"The prediction exactly matches the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "who",
                    "current"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"which movies comprise the matrix franchise?\"\nGround Truth: \"the matrix, the matrix reloaded, the matrix revolutions, the matrix resurrections\"\nPrediction: \"the matrix, the matrix reloaded, the matrix revolutions, the animatrix, and the matrix resurrections.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction covers more items than what are given by the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "what",
                    "which",
                    "movies"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the closing price of meta yesterday?\"\nGround Truth: \"$310.17\"\nPrediction: \"310.2\"\nOutput: \"score\": 1, \"explanation\": \"The prediction exactly matches the number in ground truth after rounding.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "what",
                    "price"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the current market cap of appl?\"\nGround Truth: \"2.81 trillion\"\nPrediction: \"2.667 trillion\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the number in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "what",
                    "current"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the current pe ratio of appl?\"\nGround Truth: \"28.3\"\nPrediction: \"the current pe ratio of apple is 26.66\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the number in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "what",
                    "current"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how much is tesla&#8217;s stock price down from its all-time high?\"\nGround Truth: \"$221.83\"\nPrediction: \"209.52\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the number in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "stock",
                    "price"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the length of amazon river?\"\nGround Truth: \"over 4000 miles\"\nPrediction: \"the length of amazon river is 4,000 miles\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not say Amazon River is longer than 4000 miles.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "what"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the population of country x?\"\nGround Truth: \"3,576,873\"\nPrediction: \"the population of country x is 3.3 million.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction, 3.3 M, does not match the number, 3.6 M, in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "what"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the current market value of stock x?\"\nGround Truth: \"$2,237,578,268\"\nPrediction: \"$2.16 billion.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction, 2.16 B, does not match the number, 2.2 B, in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "stock",
                    "what",
                    "current"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"at what age did usher perform in the half time show of superbowl?\"\nGround Truth: \"45 years old\"\nPrediction: \"usher has not performed at a super bowl halftime show.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "what",
                    "has"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what year did olivia rodrigo win the grammys best new artist award?\"\nGround Truth: \"2022\"\nPrediction: \"sure! here&#8217;s an answer in three words or fewer: 2022 grammy awards.\"\nOutput: \"score\": 1, \"explanation\": \"The prediction answers the correct number of the year.\"</p>\n\n",
                "matched_terms": [
                    "award",
                    "best",
                    "what",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how many times have zendaya and tom holland starred in a film together?\"\nGround Truth: \"3\"\nPrediction: \"they have starred in 3 films together: spider-man: homecoming, spider-man: far from home, and spider-man: no way home.\"\nOutput: \"score\": 1, \"explanation\": \"The prediction gives the right number and it is reasonable.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "have",
                    "film"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who is younger, timothee chalamet or tom holland?\"\nGround Truth: \"tom holland\"\nPrediction: \"timoth&#233;e chalamet is younger than tom holland.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "who"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who had more number one hits on the us billboard, a or b?\"\nGround Truth: \"a had more number one hits on the us billboard than b, with 20 number one hits compared to b&#8217;s 15.\"\nPrediction: \"a\"\nOutput: \"score\": 1, \"explanation\": \"The prediction is a concise and correct summary of the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "who"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is xxx&#8217;s birthdate?\"\nGround Truth: \"1996-01-01.\"\nPrediction: \"02/01/1996\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "what"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what was the worldwide box office haul for movie x?\"\nGround Truth: \"101756123.\"\nPrediction: \"102 million\"\nOutput: \"score\": 1, \"explanation\": \"The prediction exactly matches the number in ground truth after rounding.\"</p>\n\n",
                "matched_terms": [
                    "office",
                    "what",
                    "box",
                    "movie",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how much has spotify&#8217;s user base increased by since 2020 in na?\"\nGround Truth: \"spotify&#8217;s user base increased by 34 million since 2020.\"\nPrediction: \"spotify&#8217;s north american user base increased from 36 million in 2020 to 85 million by 2021\"\nOutput: \"score\": 0, \"explanation\": \"The prediction is not answering the question as it only gives the increase from 2020 to 2021.\"\n<br class=\"ltx_break\"/>\"\"\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "has"
                ]
            }
        ]
    },
    "S13.T8": {
        "source_file": "Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage",
        "caption": "Table 8: Streaming Tool Queries generated by Qwen-OMNI after Model-Triggered Streaming RAG post-training",
        "body": "ASR Transcript of Question XasrX^{\\text{asr}}\n\n\n\n\nPartial ASR Transcript XbasrX^{\\text{asr}}_{b}\n\n\n\n\nWeb Query Q^bweb\\hat{Q}^{\\text{web}}_{b}\n\n\n\n\nKG Query Q^bKG\\hat{Q}^{\\text{KG}}_{b}\n\n\n\n\n\n\nWho founded Rare Beauty in 2019?\n\n\n\n\nWho founded\n\n\n\n\nWho founded what\n\n\n\n\n{\"domain\": \"other\", \"main_entity\": \"Who\"}\n\n\n\n\n\n\nWho founded rare\n\n\n\n\nRed Bull founder\n\n\n\n\n{\"domain\": \"other\", \"main_entity\": \"Red\"}\n\n\n\n\n\n\nWho founded rare beauty\n\n\n\n\nWho founded Rare Beauty\n\n\n\n\n{\"domain\": \"other\", \"main_entity\": \"Rare Beauty\"}\n\n\n\n\n\n\nWho founded rare beauty in\n\n\n\n\nNO_QUERY\n\n\n\n\nNO_QUERY\n\n\n\n\n\n\nWho founded rare beauty in 2019?\n\n\n\n\nNO_QUERY\n\n\n\n\nNO_QUERY\n\n\n\n\n\n\nHow many jump shots did Darius Miles score with in the game on 2000-11-08?\n\n\n\n\nhow many\n\n\n\n\nNumber of queries on 03/27/2024\n\n\n\n\n{\"domain\": \"other\", \"main_entity\": \"How many\"}\n\n\n\n\n\n\nhow many jump\n\n\n\n\nNumber of jump shots in basketball\n\n\n\n\n{\"domain\": \"other\", \"main_entity\": \"jump shot\"}\n\n\n\n\n\n\nhow many jump shots did\n\n\n\n\nDerek Jeter jump shots count\n\n\n\n\n{\"domain\": \"sports\", \"sport_type\": \"other\", \"person\": \"Dairy\", \"datetime\": \"03/27/2024, 19:52:43 PT\"}\n\n\n\n\n\n\nhow many jump shots did Darius miles\n\n\n\n\nDarius jump shots count\n\n\n\n\n{\"domain\": \"sports\", \"sport_type\": \"other\", \"person\": \"Darius\", \"datetime\": \"03/27/2024, 19:52:43 PT\"}\n\n\n\n\n\n\nhow many jump shots did Darius miles score\n\n\n\n\nDarius Miles jump shots count\n\n\n\n\n{\"domain\": \"sports\", \"sport_type\": \"other\", \"person\": \"Darius Miles\", \"datetime\": \"03/27/2024, 19:52:43 PT\"}\n\n\n\n\n\n\nhow many jump shots did Darius miles score with in\n\n\n\n\nDarius Miles jump shots scored\n\n\n\n\nNO_QUERY\n\n\n\n\n\n\nhow many jump shots did Darius miles score with in the game on\n\n\n\n\nDarius Miles jump shots scored in game on 03/27/2024\n\n\n\n\nNO_QUERY\n\n\n\n\n\n\nhow many jump shots did Darius miles score with in the game on November\n\n\n\n\nDarius Miles jump shots scored in game on November 2024\n\n\n\n\n{\"domain\": \"sports\", \"sport_type\": \"other\", \"person\": \"Darius Miles\", \"datetime\": \"November\"}\n\n\n\n\n\n\nhow many jump shots did Darius miles score with in the game on November 8\n\n\n\n\nDarius Miles jump shots scored on November 8\n\n\n\n\n{\"domain\": \"sports\", \"sport_type\": \"other\", \"person\": \"Darius Miles\", \"datetime\": \"November 8\"}\n\n\n\n\n\n\nhow many jump shots did Darius miles score with in the game on November 8\n\n\n\n\nNO_QUERY\n\n\n\n\nNO_QUERY\n\n\n\n\n\n\nhow many jump shots did Darius miles score with in the game on November 8, 2000\n\n\n\n\nDarius Miles jump shots scored on November 8, 2000\n\n\n\n\n{\"domain\": \"sports\", \"sport_type\": \"other\", \"person\": \"Darius Miles\", \"datetime\": \"November 8, 2000\"}",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:56.9pt;\"><span class=\"ltx_text ltx_font_bold\">ASR Transcript of Question <math alttext=\"X^{\\text{asr}}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.T8.m1\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext class=\"ltx_mathvariant_bold\">asr</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}</annotation></semantics></math></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\"><span class=\"ltx_text ltx_font_bold\">Partial ASR Transcript <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.T8.m2\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext class=\"ltx_mathvariant_bold\">asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\"><span class=\"ltx_text ltx_font_bold\">Web Query <math alttext=\"\\hat{Q}^{\\text{web}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.T8.m3\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext class=\"ltx_mathvariant_bold\">web</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{web}}_{b}</annotation></semantics></math></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\"><span class=\"ltx_text ltx_font_bold\">KG Query <math alttext=\"\\hat{Q}^{\\text{KG}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.T8.m4\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext class=\"ltx_mathvariant_bold\">KG</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{KG}}_{b}</annotation></semantics></math></span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"5\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:56.9pt;\">Who founded Rare Beauty in 2019?</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">Who founded</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">Who founded what</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">{\"domain\": \"other\", \"main_entity\": \"Who\"}</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">Who founded rare</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">Red Bull founder</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">{\"domain\": \"other\", \"main_entity\": \"Red\"}</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">Who founded rare beauty</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">Who founded Rare Beauty</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">{\"domain\": \"other\", \"main_entity\": \"Rare Beauty\"}</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">Who founded rare beauty in</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">NO_QUERY</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">NO_QUERY</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">Who founded rare beauty in 2019?</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">NO_QUERY</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">NO_QUERY</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"12\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:56.9pt;\">How many jump shots did Darius Miles score with in the game on 2000-11-08?</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">how many</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">Number of queries on 03/27/2024</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">{\"domain\": \"other\", \"main_entity\": \"How many\"}</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">how many jump</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">Number of jump shots in basketball</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">{\"domain\": \"other\", \"main_entity\": \"jump shot\"}</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">how many jump shots did</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">Derek Jeter jump shots count</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">{\"domain\": \"sports\", \"sport_type\": \"other\", \"person\": \"Dairy\", \"datetime\": \"03/27/2024, 19:52:43 PT\"}</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">how many jump shots did Darius miles</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">Darius jump shots count</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">{\"domain\": \"sports\", \"sport_type\": \"other\", \"person\": \"Darius\", \"datetime\": \"03/27/2024, 19:52:43 PT\"}</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">how many jump shots did Darius miles score</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">Darius Miles jump shots count</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">{\"domain\": \"sports\", \"sport_type\": \"other\", \"person\": \"Darius Miles\", \"datetime\": \"03/27/2024, 19:52:43 PT\"}</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">how many jump shots did Darius miles score with in</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">Darius Miles jump shots scored</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">NO_QUERY</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">how many jump shots did Darius miles score with in the game on</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">Darius Miles jump shots scored in game on 03/27/2024</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">NO_QUERY</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">how many jump shots did Darius miles score with in the game on November</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">Darius Miles jump shots scored in game on November 2024</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">{\"domain\": \"sports\", \"sport_type\": \"other\", \"person\": \"Darius Miles\", \"datetime\": \"November\"}</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">how many jump shots did Darius miles score with in the game on November 8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">Darius Miles jump shots scored on November 8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">{\"domain\": \"sports\", \"sport_type\": \"other\", \"person\": \"Darius Miles\", \"datetime\": \"November 8\"}</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">how many jump shots did Darius miles score with in the game on November 8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">NO_QUERY</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">NO_QUERY</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">how many jump shots did Darius miles score with in the game on November 8, 2000</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">Darius Miles jump shots scored on November 8, 2000</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">{\"domain\": \"sports\", \"sport_type\": \"other\", \"person\": \"Darius Miles\", \"datetime\": \"November 8, 2000\"}</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "other",
            "tool",
            "november",
            "rare",
            "bull",
            "score",
            "red",
            "who",
            "mainentity",
            "datetime",
            "what",
            "sporttype",
            "person",
            "scored",
            "qbkghatqtextkgb",
            "queries",
            "derek",
            "partial",
            "qbwebhatqtextwebb",
            "sports",
            "basketball",
            "dairy",
            "query",
            "jump",
            "web",
            "streaming",
            "darius",
            "did",
            "shots",
            "rag",
            "noquery",
            "founder",
            "xbasrxtextasrb",
            "beauty",
            "xasrxtextasr",
            "question",
            "count",
            "number",
            "founded",
            "asr",
            "posttraining",
            "domain",
            "how",
            "miles",
            "shot",
            "transcript",
            "qwenomni",
            "many",
            "game",
            "modeltriggered",
            "jeter",
            "generated",
            "after"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">When a new query <math alttext=\"\\hat{Q}^{T}_{b}\\neq\\text{NO\\_QUERY}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m4\" intent=\":literal\"><semantics><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><mo>&#8800;</mo><mtext>NO_QUERY</mtext></mrow><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}\\neq\\text{NO\\_QUERY}</annotation></semantics></math>, the system immediately terminates any ongoing tool calls for the previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m5\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, ensuring that only a <em class=\"ltx_emph ltx_font_italic\">single tool call thread</em> runs at any given time. (Examples of generated <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m6\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> are provided in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T8\" title=\"Table 8 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> in the Appendix.)\nThis approach offers several key advantages. First, it effectively eliminating redundant parallel threads and significantly reducing computational overhead. This is especially important for deployment on resource-constrained devices.\nSecond, this formulation removes the need for an external <em class=\"ltx_emph ltx_font_italic\">reflector</em> (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E2\" title=\"Equation 2 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) module. The model confidently relies on the results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from the most recent tool call using <math alttext=\"\\hat{Q}^{\\text{prev}}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m8\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{B}</annotation></semantics></math> to generate the spoken response <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m9\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by maximizing <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math>, reducing system complexity.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">End-to-end speech-in speech-out dialogue systems are emerging as a powerful alternative to traditional ASR&#8211;LLM&#8211;TTS pipelines, generating more natural, expressive responses with significantly lower latency. However, these systems remain prone to hallucinations due to limited factual grounding. While text-based dialogue systems address this challenge by integrating tools such as web search and knowledge graph APIs, we introduce the first approach to extend tool use directly into speech-in speech-out systems. A key challenge is that tool integration substantially increases response latency, disrupting conversational flow. To mitigate this, we propose Streaming Retrieval-Augmented Generation (<em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>), a novel framework that reduces user-perceived latency by predicting tool queries in parallel with user speech, even before the user finishes speaking. Specifically, we develop a post-training pipeline that teaches the model when to <em class=\"ltx_emph ltx_font_italic\">issue tool calls</em> during ongoing speech and how to generate spoken summaries that fuse audio queries with retrieved text results, thereby improving both <em class=\"ltx_emph ltx_font_italic\">accuracy and responsiveness</em>. To evaluate our approach, we construct AudioCRAG, a benchmark created by converting queries from the publicly available CRAG dataset into speech form. Experimental results demonstrate that our <em class=\"ltx_emph ltx_font_italic\">streaming RAG</em> approach increases QA accuracy by up to 200% relative (from 11.1% to 34.2% absolute) and further enhances user experience by reducing tool use latency by 20%. Importantly, our <em class=\"ltx_emph ltx_font_italic\">streaming RAG</em> approach is modality-agnostic and can be applied equally to typed input, paving the way for more agentic, real-time AI assistants.</p>\n\n",
                "matched_terms": [
                    "how",
                    "tool",
                    "rag",
                    "queries",
                    "web",
                    "posttraining",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spoken Dialogue Systems (SDS) are foundational to many everyday technologies, powering intelligent assistants such as Alexa and Siri, as well as interactive voice response systems in customer service. With the rapid expansion of SDS capabilities to mobile phones and wearable devices, the need for robust, scalable, and generalizable solutions has never been greater. Traditionally, SDS have relied on cascaded pipelines composed of multiple modules&#8212;including voice activity detection (VAD), automatic speech recognition (ASR), natural language understanding (NLU), natural language generation (NLG), and text-to-speech (TTS) synthesis&#8212;each introducing potential points of failure and latency <cite class=\"ltx_cite ltx_citemacro_citep\">(Glass, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib12\" title=\"\">1999</a>; Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib13\" title=\"\">2024</a>)</cite>.\nRecently, end-to-end (E2E) SDS <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib34\" title=\"\">2024</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib22\" title=\"\">2023</a>; Meng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib20\" title=\"\">2024</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib40\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib2\" title=\"\">2025b</a>)</cite> have been proposed, which directly generate spoken responses from speech input within a unified architecture. This E2E approach not only mitigates error propagation across modules but also captures non-phonemic information more effectively, resulting in significantly lower inference time and computational overhead, and paving the way for more natural and efficient conversational experiences.</p>\n\n",
                "matched_terms": [
                    "many",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, current E2E SDS are fundamentally constrained by their reliance on internalized knowledge from static training data, which often results in responses that lack factual grounding or fail to reflect the most up to date information. This shortcoming is particularly critical for action-oriented or knowledge-seeking tasks, such as booking hotels or answering questions about current events. In contrast, text-based conversational assistants have begun to overcome these limitations by integrating external tools through Retrieval-Augmented Generation (RAG)<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>; Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib3\" title=\"\">2023a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib5\" title=\"\">c</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib10\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib11\" title=\"\">b</a>)</cite>, dynamically retrieving relevant information from sources like web search, knowledge graphs (KG), and real-time APIs. Yet, the integration of such tool use into E2E SDS remains largely unexplored.\nA key challenge is that while external tools can substantially improve factual accuracy, invoking them often introduces additional latency, leading to awkward silences that disrupt the natural conversational flow. This raises a research question: <span class=\"ltx_text ltx_font_italic\">How can we trade-off between accuracy and responsiveness for developing SDS that feel both intelligent and natural?</span></p>\n\n",
                "matched_terms": [
                    "question",
                    "web",
                    "tool",
                    "how"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present, to the best of our knowledge, the first <span class=\"ltx_text ltx_font_italic\">speech-in, speech-out language model that seamlessly integrates external tool invocation with low latency</span>. The key idea is a <span class=\"ltx_text ltx_font_bold\">Streaming RAG</span> strategy that generates tool queries in parallel with user speech, often times even before the user has finished speaking (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). A naive implementation of streaming queries, however, faces two challenges: (1) queries issued from partial speech may be suboptimal, yielding distracting tool outputs and inaccurate responses; and (2) unnecessary tool calls may be triggered, wasting computational resources. We introduce effective modeling techniques to address these challenges and make the following contributions.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "rag",
                    "queries",
                    "partial",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution 1</span>: We introduce a formal framework for tool integration in speech-in speech-out systems and empirically show that leveraging web search and KG APIs significantly enhances factual question answering. Evaluating three state-of-the-art (SOTA) models, Qwen-OMNI <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>)</cite>, OpusLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib30\" title=\"\">2025</a>)</cite>, and Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib8\" title=\"\">2025</a>)</cite>, we find that external tool integration delivers substantial performance gains, boosting accuracy by up to 140% relative (from 11.1% to 26.3% absolute). However, tool usage also introduces considerable latency, increasing first-token response time by 2.3x.</p>\n\n",
                "matched_terms": [
                    "question",
                    "web",
                    "tool",
                    "qwenomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution 2</span>: To address this, we propose <em class=\"ltx_emph ltx_font_italic\">Streaming Retrieval-Augmented Generation</em> (<em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>), the <em class=\"ltx_emph ltx_font_italic\">first</em> framework that empowers the system to trigger tool queries in parallel with user speech, even before the user finishes speaking. Within this framework, we introduce two novel approaches: (1) <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em>, which issues tool queries at regular intervals during speech input and carefully examines quality of retrieval results on the full query to guarantee response quality, and can be incorporated into any speech-in, speech-out model without post-training; (2) <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, which post-trains the model to intelligently determine optimal query timing based on the evolving user utterance to save computation resources.\nOur results demonstrate that our proposed <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> delivers over 200% relative improvement in accuracy (from 11.1% to 34.2% absolute in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) compared to the no-tool baseline, while also reducing tool result generation latency by 20%. Though designed for speech-in speech-out systems, streaming RAG can also be adapted in cascaded SDS, or even chatbots as users type.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "rag",
                    "queries",
                    "modeltriggered",
                    "query",
                    "posttraining",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution 3</span>: Finally, we introduce AudioCRAG, a benchmark created by recording spoken queries from the CRAG <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite> dataset, enabling robust evaluation of tool usage capabilities in speech-in speech-out systems.\nTo support future research, we will open source our training code and AudioCRAG-Human benchmark, supporting future research in tool-integrated voice assistants.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in benchmarking text-based dialogue systems for tool usage <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib4\" title=\"\">2023b</a>; Ouyang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib24\" title=\"\">2025</a>; Cheng and Dou, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib6\" title=\"\">2025</a>; Cohen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib7\" title=\"\">2025</a>; Xiong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib35\" title=\"\">2024a</a>)</cite> have primarily focused on evaluating factual question answering and task completion within simulated environments (detailed related work discussion in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S7\" title=\"7 Benchmarking Text Dialogue Systems for Tool Usage &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S8\" title=\"8 Multimodal Benchmarks for Tool Usage &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>). The CRAG benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite> is a leading example, featuring 4,409 question-answer pairs and providing mock APIs for both web and KG search. Recent benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Meta CRAG-MM Challenge Organizers, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib21\" title=\"\">2025</a>; Ma et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib17\" title=\"\">2024</a>; Jang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib14\" title=\"\">2025</a>)</cite> have extended tool-augmented dialogue evaluation to multimodal input and longer-context scenarios. While these benchmarks have significantly advanced the evaluation of tool-augmented dialogue systems, they remain largely limited to text-based outputs and do not fully address the unique challenges presented by speech-in speech-out systems.</p>\n\n",
                "matched_terms": [
                    "question",
                    "web",
                    "tool"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several E2E spoken dialogue systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>; Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib34\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib1\" title=\"\">2025a</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib22\" title=\"\">2023</a>; Meng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib20\" title=\"\">2024</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib40\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib2\" title=\"\">2025b</a>)</cite> have recently been introduced, demonstrating impressive semantic understanding and high audio quality in their responses. However, these systems have not yet been trained or evaluated for their ability to <em class=\"ltx_emph ltx_font_italic\">use external tools</em>.\nAnother research direction <cite class=\"ltx_cite ltx_citemacro_citep\">(Feng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib9\" title=\"\">2025</a>)</cite> explores E2E RAG for direct speech-to-text retrieval, utilizing multimodal embeddings for enabling speech utterances to directly retrieve relevant text. While this method outperforms models without RAG, it is primarily limited to <em class=\"ltx_emph ltx_font_italic\">speech-to-text</em> scenarios. Its ability to access KGs and other APIs, critical for real-world applications, remains unproven. Furthermore, the retrieval scope is limited, as experiments are conducted with retrieval restricted to just 10 paragraphs, whereas modern RAG benchmarks require searching across thousands of web pages.\nAlthough recent studies <cite class=\"ltx_cite ltx_citemacro_citep\">(Maben et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib18\" title=\"\">2025</a>)</cite> have developed web interfaces that integrate tools into speech-in <em class=\"ltx_emph ltx_font_italic\">speech-out</em> scenarios using cascaded pipelines, comprehensive empirical investigations of E2E speech-in speech-out systems and, crucially, systematic analyses of user-perceived latency, remain largely unexplored. In this work, we address these gaps by developing a comprehensive framework for tool integration in E2E speech-in speech-out systems and designing benchmarks to quantitatively assess the tool usage capabilities of SOTA models. Furthermore, recognizing the importance of latency for natural conversational flow, we introduce novel streaming RAG methods that not only enhance tool usage performance but also reduce user-perceived latency.</p>\n\n",
                "matched_terms": [
                    "other",
                    "tool",
                    "rag",
                    "web",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A RAG spoken conversation system takes an audio question <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math> as input and outputs a spoken answer <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>. Let the ASR transcript of audio question be <math alttext=\"X^{\\text{asr}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>asr</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}</annotation></semantics></math> and the ASR transcript of the spoken answer be <math alttext=\"X^{\\text{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>res</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{res}}</annotation></semantics></math>.\nAnswers are generated by speech-in speech-out models, leveraging both the model&#8217;s internal knowledge and information retrieved from external sources.\nTo incorporate external information, the model needs to formulate a tool query <math alttext=\"Q^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><msup><mi>Q</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">Q^{T}</annotation></semantics></math> to retrieve relevant results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m6\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from an external tool <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m7\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "transcript",
                    "rag",
                    "question",
                    "xasrxtextasr",
                    "query",
                    "generated",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our proposed formulation for integrating external tools into speech-in speech-out systems. We introduce a two-stage inference approach: <em class=\"ltx_emph ltx_font_italic\">Query Generation</em> and <em class=\"ltx_emph ltx_font_italic\">Response Generation</em>. In the Query Generation stage, the system processes an audio question and generates queries for each external tool to retrieve relevant information by maximizing the posterior distribution <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> (Examples of generated queries are provided in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T7\" title=\"Table 7 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> in the Appendix.). In the Response Generation stage, the retrieved results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from these tools are combined with the original audio question and input into the model to generate the final spoken response by maximizing the posterior distribution <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math>. By conditioning the final output generation on the input audio, this formulation not only provides a simple and effective mechanism for interacting with text-based APIs, but also preserves the key advantages of speech-in speech-out systems, mitigating error propagation and enabling the model to capture non-phonemic information (such as prosody and speaker intent) more effectively.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "queries",
                    "question",
                    "query",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">RAG-based systems, as proposed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, can significantly improve factual accuracy by incorporating external tools. However, these tool calls often introduce substantial latency, which is particularly problematic in speech-in, speech-out applications where users expect rapid, conversational responses and even brief silences can disrupt the natural flow of dialogue.\nOne way to address this challenge lies in the nature of audio inputs, which arrive as a continuous stream. This streaming property enables tool calls to be initiated before the user has finished speaking, offering a unique opportunity to mitigate latency.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To minimize user-perceived latency, we introduce <em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>: the first framework to generate and issue tool queries in parallel as audio input is received. This novel approach is built on three key design components:\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> Trigger: When to initiate a new tool query;\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> Threads: The number of parallel tool query threads;\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic3\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">3</span></span></foreignobject></g></g></svg></span> Reflector: The module that determines whether intermediate tool results are sufficient for generating the final output. By exploring different design choices for each component, we introduce two complementary approaches for streaming tool query generation in the following subsections: a fixed-interval trigger method and a model-based trigger method.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "rag",
                    "queries",
                    "query",
                    "number",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this approach, the <em class=\"ltx_emph ltx_font_italic\">trigger</em> is set to fire tool calls at fixed chunk intervals during audio input.\nThe input speech <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math> is divided into a sequence of <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> blocks, <math alttext=\"Q=\\{Q_{b}\\mid b=1,\\dots,B\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>Q</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>Q</mi><mi>b</mi></msub><mo fence=\"true\" lspace=\"0em\" rspace=\"0em\">&#8739;</mo><mrow><mi>b</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>B</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Q=\\{Q_{b}\\mid b=1,\\dots,B\\}</annotation></semantics></math>, with each block containing <math alttext=\"N_{\\text{block}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>N</mi><mtext>block</mtext></msub><annotation encoding=\"application/x-tex\">N_{\\text{block}}</annotation></semantics></math> frames.\nTo approximate <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> as described in S. 3.1, we follow a block-wise prediction strategy. In this approach, after processing each audio block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m6\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, the model predicts a tool query <math alttext=\"\\hat{Q}_{b}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m7\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}_{b}^{T}</annotation></semantics></math> by conditioning on the input speech accumulated up to block b, specifically <math alttext=\"Q_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">Q_{1:b}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "query",
                    "after",
                    "tool"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This strategy results in <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m9\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> parallel tool call threads running simultaneously (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F3.sf1\" title=\"Figure 3(a) &#8227; Figure 3 &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>), where each thread generates a tool query prediction <math alttext=\"\\hat{Q}_{b}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m10\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}_{b}^{T}</annotation></semantics></math> for its corresponding block <math alttext=\"b\\in[1,B]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m11\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mi>B</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b\\in[1,B]</annotation></semantics></math>.\nThe tool queries <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m12\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> generated after each block are then stored in cache. Given the high latency of tool calls (See T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), users typically complete their utterances before tool responses are ready. Upon utterance completion, an explicit <em class=\"ltx_emph ltx_font_italic\">reflector</em> module &#8220;<math alttext=\"\\text{reflect}()\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m13\" intent=\":literal\"><semantics><mrow><mtext>reflect</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{reflect}()</annotation></semantics></math>&#8221; evaluates the cached intermediate queries <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m14\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> against final query <math alttext=\"\\hat{Q}^{T}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m15\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{B}</annotation></semantics></math> to determine whether an early intermediate tool call provides sufficient information to answer the user&#8217;s question <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m16\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math>. The reflector module systematically evaluates all intermediate queries in the cache and identifies the <em class=\"ltx_emph ltx_font_italic\">earliest</em> sufficient tool call <math alttext=\"b^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m17\" intent=\":literal\"><semantics><msup><mi>b</mi><mo>&#8902;</mo></msup><annotation encoding=\"application/x-tex\">b^{\\star}</annotation></semantics></math> where the intermediate query <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m18\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> will give the same result as final query <math alttext=\"\\hat{Q}^{T}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m19\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{B}</annotation></semantics></math> as shown:</p>\n\n",
                "matched_terms": [
                    "tool",
                    "queries",
                    "question",
                    "query",
                    "generated",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All subsequent parallel tool calls after <math alttext=\"b^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m20\" intent=\":literal\"><semantics><msup><mi>b</mi><mo>&#8902;</mo></msup><annotation encoding=\"application/x-tex\">b^{\\star}</annotation></semantics></math> are promptly terminated, and the retrieved results <math alttext=\"R_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m21\" intent=\":literal\"><semantics><msub><mi>R</mi><msup><mi>b</mi><mo>&#8902;</mo></msup></msub><annotation encoding=\"application/x-tex\">R_{b^{\\star}}</annotation></semantics></math> from this intermediate call <math alttext=\"\\hat{Q}^{T}_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m22\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><msup><mi>b</mi><mo>&#8902;</mo></msup><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b^{\\star}}</annotation></semantics></math> are used to generate the final spoken response <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m23\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by maximizing the posterior distribution <math alttext=\"P(A|Q,R_{b^{\\star}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m24\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><msub><mi>R</mi><msup><mi>b</mi><mo>&#8902;</mo></msup></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R_{b^{\\star}})</annotation></semantics></math> (instead of <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m25\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math> in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>).\nWe employ a reflector module that uses simple yet effective heuristics: (a) if the top 5 web documents for an intermediate web query match those of the final web query, and (b) if the KG results for intermediate and final KG queries are identical.\nThese heuristics ensure that the information retrieved from an early tool call using <math alttext=\"\\hat{Q}^{T}_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m26\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><msup><mi>b</mi><mo>&#8902;</mo></msup><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b^{\\star}}</annotation></semantics></math> is consistent with what would have been obtained by waiting for the final tool call using <math alttext=\"\\hat{Q}^{T}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m27\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{B}</annotation></semantics></math>, thereby providing a strong quality guarantee. Since most tool call latency arises from chunking and reranking the chunks of web documents, these checks enable significant latency savings without any compromise in model performance.\nA key advantage of this strategy is its plug-and-play nature: it requires no additional post-training for speech-in, speech-out models and can be directly applied at inference time across a variety of architectures. However, there are important considerations. First, generating parallel tool calls at every fixed interval (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E1\" title=\"Equation 1 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) increases computational overhead, which may pose challenges for deployment on resource-constrained devices such as wearables devices. Second, reliance on an external reflector module (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E2\" title=\"Equation 2 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) to determine the sufficiency of intermediate tool calls may limit the extent of achievable latency improvements.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "what",
                    "queries",
                    "query",
                    "after",
                    "web",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the limitations of Fixed-Interval Streaming RAG and further optimize both efficiency and responsiveness, we propose a more adaptive approach: <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>.\nHere, the <em class=\"ltx_emph ltx_font_italic\">trigger</em> is learned: the model is trained to autonomously determine the optimal moments to initiate tool queries, issuing a query only when it encounters new or additional information as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F3.sf2\" title=\"Figure 3(b) &#8227; Figure 3 &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>. In this formulation, the model receives user input in fixed chunk intervals as before and intelligently determines whether a tool call is needed after each block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>. The model can either:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p1.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> Predict\nNO_QUERY if a new tool query is unnecessary, or\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p1.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> Generate a new tool query. To make this decision, the model conditions on both the accumulated input speech <math alttext=\"Q_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">Q_{1:b}</annotation></semantics></math> (see Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E1\" title=\"Equation 1 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) as well as most recent tool query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}=\\hat{Q}^{T}_{\\max\\{\\,i&lt;b:\\hat{Q}^{T}_{i}\\neq\\text{NO\\_QUERY}\\,\\}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><mo>=</mo><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">{</mo><mrow><mrow><mi>i</mi><mo>&lt;</mo><mi>b</mi></mrow><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>i</mi><mi>T</mi></msubsup><mo>&#8800;</mo><mtext>NO_QUERY</mtext></mrow></mrow><mo lspace=\"0.170em\" stretchy=\"false\">}</mo></mrow></mrow><mi>T</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}=\\hat{Q}^{T}_{\\max\\{\\,i&lt;b:\\hat{Q}^{T}_{i}\\neq\\text{NO\\_QUERY}\\,\\}}</annotation></semantics></math> as shown:</p>\n\n",
                "matched_terms": [
                    "tool",
                    "rag",
                    "noquery",
                    "queries",
                    "modeltriggered",
                    "query",
                    "after",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-training</span>: To train the model, we transform text-based tool usage benchmarks into spoken format (See S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13\" title=\"13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>). Word-level timestamps are computed using a pre-trained ASR model. For each partial ASR transcript <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math> up to block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, we generate corresponding queries for each tool <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m3\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> using an LLM as pseudo ground truth (GT).\nTo create effective training labels that teach the model when to trigger new queries, we employ a similarity-based labeling strategy where we compare the current pseudo GT query <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m4\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> with the most recent non-empty tool query label <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m5\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math> (see Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E3\" title=\"Equation 3 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) before block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m6\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>.\nOur labeling function assigns the training label <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m7\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E3\" title=\"Equation 3 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) for the tool query after block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m8\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> as follows:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p2.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> when the current query is sufficiently similar to the previous query (as determined by manually defined heuristics <math alttext=\"f(\\cdots)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m9\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8943;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f(\\cdots)</annotation></semantics></math>), we assign the special label NO_QUERY to teach the model that no new tool call is needed.  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p2.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> When the queries are sufficiently different, we assign the actual pseudo ground truth query\n<math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m10\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> as the label to teach the model to trigger a new tool call:</p>\n\n",
                "matched_terms": [
                    "tool",
                    "transcript",
                    "noquery",
                    "queries",
                    "partial",
                    "xbasrxtextasrb",
                    "query",
                    "after",
                    "posttraining",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For KG queries, we assign a NO_QUERY label when the current query exactly matches the previous one. For web queries, we assign a NO_QUERY label if the top five retrieved documents for the current query remain unchanged from the previous query.</p>\n\n",
                "matched_terms": [
                    "query",
                    "web",
                    "queries",
                    "noquery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a multi-task fine-tuning strategy targeting two key capabilities.\nFirst, we train the model on Streaming Tool Query Generation by optimizing <math alttext=\"P(Q^{T}_{b}|Q_{1:b},\\hat{Q}^{\\text{prev}}_{b})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo fence=\"false\">|</mo><mrow><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><mo>,</mo><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}_{b}|Q_{1:b},\\hat{Q}^{\\text{prev}}_{b})</annotation></semantics></math> for <math alttext=\"b\\in[1,B]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m2\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mi>B</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b\\in[1,B]</annotation></semantics></math>, enabling intelligent decisions about when to trigger tool queries. Second, we fine-tune on Response Generation by optimizing <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m3\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) to improve the intelligibility of the speech output.</p>\n\n",
                "matched_terms": [
                    "query",
                    "tool",
                    "queries",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An important aspect of our post-training is enhancing the model&#8217;s ability to recover from errors in intermediate query predictions. For example, when presented with the audio question, &#8220;Who founded Rare Beauty in 2019?&#8221;, we observed that an initial misinterpretation of <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m1\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, such as &#8220;Red Bull founder&#8221;, can lead the model to subsequently generate NO_QUERY labels, effectively halting further attempts to retrieve the correct information. This issue arises because, during training, the model is always provided with correct previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m2\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, whereas during inference, it may make errors due to partial utterances <math alttext=\"{Q}_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">{Q}_{1:b}</annotation></semantics></math> being ambiguous. Thus the model lacks the ability to recover from such mistakes during inference.\nTo overcome this, we introduce a novel strategy in which we deliberately inject negative samples during post-training by substituting the previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m4\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math> in Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E4\" title=\"Equation 4 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> with incorrect ones <math alttext=\"Q^{\\text{neg}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m5\" intent=\":literal\"><semantics><msubsup><mi>Q</mi><mi>b</mi><mtext>neg</mtext></msubsup><annotation encoding=\"application/x-tex\">Q^{\\text{neg}}_{b}</annotation></semantics></math>. Crucially, when we perform negative sampling, we fall back to the pseudo ground truth query <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> as the training label:</p>\n\n",
                "matched_terms": [
                    "noquery",
                    "partial",
                    "rare",
                    "beauty",
                    "question",
                    "bull",
                    "query",
                    "founded",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To rigorously evaluate our proposed approach, we construct comprehensive benchmark datasets featuring spoken queries paired with simulated tool interactions. We begin with the CRAG dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>, which form the basis for our spoken version of CRAG, which we term <em class=\"ltx_emph ltx_font_italic\">AudioCRAG</em>. It consists of 2 distinct variants:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S4.SS1.p1.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> <span class=\"ltx_text ltx_font_bold\">Audio CRAG Synthetic</span>: To generate spoken queries, we use our in-house TTS system. We then apply a rigorous filtering procedure which results in a high-quality set of 1,862 spoken queries, which we refer to as the <em class=\"ltx_emph ltx_font_italic\">AudioCRAG-Synthetic</em> benchmark.  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S4.SS1.p1.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> <span class=\"ltx_text ltx_font_bold\">Audio CRAG Human</span>: To further enhance the realism and diversity of our evaluation, we introduce the AudioCRAG-Human benchmark, which consists of 618 human-recorded spoken queries. We will release\nAudio CRAG Human benchmark upon acceptance to supports the development of more natural and reliable voice assistants.\nFurther details on the construction of these benchmarks are provided in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S9\" title=\"9 Audio CRAG Benchmark &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. We follow the CRAG setup to incorporate both web and KG-based tools, and adopt its robust evaluation methodology, as described in Secs. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S10\" title=\"10 Tool Usage Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S11\" title=\"11 Evaluation Setting &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.\nAdditionally, we leverage a random subset of 16,000 questions from the text-based factual question answering dataset TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib15\" title=\"\">2017</a>)</cite> to post-train our speech-in, speech-out models, as detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13\" title=\"13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "question",
                    "web",
                    "tool",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform an ablation study (referred to as &#8220;Tool Integration&#8221; in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) where we post-train the model on sequential query generation (i.e. <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) and output generation, to assess the impact of streaming RAG post-training versus standard post-training on final response generation.\nAdditionally, we conduct an ablation study on open book setting (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) using a self-cascade approach with a three-stage inference pipeline: (1) the audio question is used to generate a tool query <math alttext=\"Q^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><msup><mi>Q</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">Q^{T}</annotation></semantics></math> (corresponding to the &#8220;Query Generation&#8221; stage described in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>); (2) the audio question, and tool results are combined to produce the final text output <math alttext=\"X^{\\text{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>res</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{res}}</annotation></semantics></math> by maximizing <math alttext=\"P(X^{\\text{res}}|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mtext>res</mtext></msup><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(X^{\\text{res}}|Q,R)</annotation></semantics></math>; and (3) the audio question, and final text output are used to generate the final speech output <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by optimizing <math alttext=\"P(A|Q,X^{\\text{res}})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m6\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><msup><mi>X</mi><mtext>res</mtext></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,X^{\\text{res}})</annotation></semantics></math>. Since we teacher-force the text output to obtain the final speech output in stage (3), this self-cascade approach can only be applied to a &#8220;thinker-talker&#8221; architecture (eq. Qwen-OMNI) or Chain-of-Thought (CoT) style architectures (eg. OpusLM).\nThe motivation for this ablation is to investigate whether the inclusion of RAG references <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> during the &#8220;Response Generation&#8221; stage (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) affects the quality of the generated speech <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m8\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "rag",
                    "qwenomni",
                    "question",
                    "query",
                    "generated",
                    "posttraining",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a comprehensive performance comparison of three models, Qwen2.5-7B, OpusLM, and Kimi Audio, evaluated across three settings: Closed Book, Open Book, and Streaming RAG. All models are assessed on both the AudioCRAG-Synthetic (Syn.) and AudioCRAG-Human (Hum.). In the Closed Book setting, where models rely solely on their internal knowledge without access to external tools (reference length = 0), all models achieve accuracy scores below 20%. These results highlight the inherent limitations of closed-book approaches in handling complex queries.\nThe Open Book setting, which provides models with access to external information, demonstrates the clear benefits of tool integration. Qwen2.5-7B and Kimi Audio&#8217;s accuracy rises substantially, underscoring the value of leveraging external context. As expected, latency increases due to the additional overhead of retrieving information from external tools (See T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> for detailed latency analysis).</p>\n\n",
                "matched_terms": [
                    "tool",
                    "queries",
                    "rag",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most notably, our post-training approach to build <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, as described in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>, delivers significant advancements in both accuracy and efficiency. Qwen2.5-7B and OpusLM achieve significant accuracy improvements across both benchmarks. While the absolute accuracy scores may appear low, they are consistent with the evaluation results observed in the CRAG benchmark. Notably, Qwen2.5-7B with <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> achieves accuracy comparable to the open book performance of similarly sized LLMs reported in the original CRAG paper (i.e., 34.2 for Qwen2.5-7B vs. 32.1 for LLAMA-3 8B Instruct in <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>).\nImportantly, although post-training is performed exclusively on the synthetic dataset, we observe consistent and even greater improvements on the human-spoken benchmark, demonstrating the robustness and strong generalization capabilities of our method. This setting also introduces substantial latency savings compared to the Open Book configuration, with Qwen2.5-7B and OpusLM achieving 20.7% and 14.8% reductions in first-token latency on the synthetic benchmark, and even greater savings on the human benchmark\n<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Our latency calculations on synthetic audio exclude end-point detection latency, which is required in all production systems. Since our streaming RAG approach enables processing without waiting for end-point detection, including this factor would further amplify the observed latency savings, as observed in higher latency savings for human-spoken audio, where endpoint detection errors often introduce trailing silence.</span></span></span>. These results demonstrate that our streaming RAG approach not only advances the accuracy of speech-in speech-out systems, but also optimizes response efficiency by enabling earlier and more effective prediction of tool queries.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "rag",
                    "queries",
                    "streaming",
                    "modeltriggered",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Evaluated SOTA Speech-in Speech-out Models &#8227; 4 Experiment Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides a comparative evaluation of the three SOTA models in generating either text or speech outputs from speech inputs, both with and without the integration of external tool results. In the absence of tool results (Ref length = 0), all models achieve higher accuracy when generating text outputs compared to speech outputs. The incorporation of tool results generally leads to improved text generation performance, with Kimi Audio achieving the highest accuracy. In contrast, the accuracy for speech output remains consistently lower across all models and conditions. The self-cascade approach, in which the model first generates an intermediate text response before producing the final speech output, provides moderate improvements in speech output accuracy for both Qwen2.5-7B and OpusLM.\nHowever, it still underperforms compared to the text-out baseline, primarily due to errors in accurately generating answers involving uncommon entity nouns.\nOverall, these findings underscore a persistent challenge in direct speech generation, particularly when tool results are integrated, as this appears to negatively impact the quality of generated speech responses.\nOur <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> demonstrates clear advantages: it maintains comparable performance for text output and delivers substantial improvements for speech output, even outperforming the self-cascade approach. These results underscore the effectiveness of post-training in overcoming challenges in direct speech generation in tool-augmented scenarios.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "rag",
                    "streaming",
                    "modeltriggered",
                    "generated",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> provides a comprehensive breakdown of latency measurements for the Qwen2.5-7B speech-to-speech model, evaluated in both the Open Book setting and our proposed <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> setting on AudioCRAG-Synthetic. We report both median (P50) and 90th percentile (P90) values for each stage of the processing pipeline. The latency is decomposed into three main components: tool query generation, tool result generation, and speech response synthesis, with measurements provided for both the first token outputs (We also provide latency measurements for last token output in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T9\" title=\"Table 9 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>). For both tool query and tool result generation, it is assumed that all tools are accessed in parallel; thus, the reported latency corresponds to the maximum query or result generation time among all tools. The majority of this latency arises from leveraging external web pages, which introduces significant delays, most notably, increasing the first token latency by 2.3x in Open Book Setting.\nNotably, our <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> setting enables early generation of tool results, successfully reducing P50 first token latency by 9.8% and tool use latency by 20.7%.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "rag",
                    "modeltriggered",
                    "query",
                    "web",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Streaming RAG approaches</span>: Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> highlights the substantial latency savings enabled by the <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>), compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS1\" title=\"3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>). Three key metrics are evaluated: overall latency savings, the percentage of queries benefiting from reduced latency, and the number of parallel threads required.\nEven without any post-training, the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach already reduces tool usage latency (3.37s in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) by 10.7% for Open Book Qwen-OMNI, demonstrating its flexibility and plug-and-play compatibility with any existing speech-in speech-out model.\nThe <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> method, utilizing Qwen-OMNI, consistently delivers superior performance. It achieves greater average latency reductions and benefits a higher proportion of queries with improved response times. Notably, <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> require only a single parallel thread, representing a significant advancement in resource efficiency compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach, which demands multiple parallel threads.\nThese findings highlight the effectiveness of <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> in not only minimizing latency, validating our intuition from Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>, but also in optimizing system efficiency and resource utilization.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "rag",
                    "qwenomni",
                    "queries",
                    "modeltriggered",
                    "number",
                    "posttraining",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-training Strategies</span>: Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents performance comparison under different post-training conditions. Incorporating streaming tool query generation during post-training (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>) results in comparable performance for both models. These results suggest that <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> can be integrated into post-training process without negatively impacting model performance.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "rag",
                    "streaming",
                    "modeltriggered",
                    "query",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation Results for Negative Sampling Strategy</span>: Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T5\" title=\"Table 5 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the ablation results evaluating the impact of deliberately injecting negative samples during post-training (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E5\" title=\"Equation 5 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). The findings highlight that, without negative sampling, streaming tool query generation can reduce final accuracy in text output settings, primarily due to errors in final query generation, as detailed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>. In contrast, our negative sampling approach significantly enhances the model&#8217;s robustness, enabling it to recover from intermediate prediction errors. This leads to consistently high accuracy while also achieving notable latency reductions (Tab. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "query",
                    "tool",
                    "posttraining",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced the first comprehensive approach for integrating external tool usage directly into E2E speech-in, speech-out dialogue systems. By incorporating <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> pipeline, we further enhanced the model&#8217;s ability to leverage retrieved information and autonomously decide when to trigger new tool queries, resulting in improved accuracy and responsiveness.\nEmpirical evaluation on the newly introduced AudioCRAG benchmark demonstrated that tool integration can more than double factual question answering accuracy compared to closed-book models. Additionally, our streaming RAG approach achieved a 20% reduction in tool usage latency, thereby preserving natural conversational flow.\nOverall, our contributions advance the state of the art in spoken dialogue systems by enabling accurate, real-time, and tool-augmented voice assistants. In addition, we are committed to open science and will release our training code and Audio CRAG Human benchmark to support future research and development in this area.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "rag",
                    "queries",
                    "modeltriggered",
                    "question",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in benchmarking text-based dialogue systems for tool usage <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib4\" title=\"\">2023b</a>; Ouyang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib24\" title=\"\">2025</a>; Cheng and Dou, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib6\" title=\"\">2025</a>; Cohen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib7\" title=\"\">2025</a>; Xiong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib35\" title=\"\">2024a</a>; Vu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib31\" title=\"\">2023</a>; Xiong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib36\" title=\"\">2024b</a>; Peng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib25\" title=\"\">2024</a>; Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib29\" title=\"\">2025</a>; Ni et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib23\" title=\"\">2025</a>)</cite> have primarily focused on evaluating factual question answering and task completion within simulated environments. The CRAG benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite> is a leading example, featuring 4,409 question-answer pairs and providing mock APIs for both web and knowledge graph (KG) search. CRAG supports a range of KG and web retrieval tasks, and highlights key challenges such as hallucinations in retrieval-augmented generation (RAG) and the importance of leveraging KGs and search ranking to improve factual accuracy. Evaluation is conducted automatically using two LLM judges. SimpleQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib32\" title=\"\">2024</a>)</cite> is another widely adopted benchmark, designed to assess language models on short, fact-seeking questions. With 4,326 adversarially collected questions spanning diverse topics and a straightforward grading scheme based on single, indisputable answers, SimpleQA provides a robust testbed for factual accuracy. Moving beyond question answering, WebArena <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib41\" title=\"\">2024</a>)</cite> offers a simulated environment for evaluating dialogue agents on web-based tasks using fully functional websites, enabling assessment of more complex, action-oriented behaviors. While these benchmarks have significantly advanced the evaluation of tool-augmented dialogue systems, they remain largely limited to text-based interactions and do not fully address the unique challenges presented by speech-in speech-out systems.</p>\n\n",
                "matched_terms": [
                    "question",
                    "web",
                    "tool",
                    "rag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent benchmarks <cite class=\"ltx_cite ltx_citemacro_cite\">Mei et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib19\" title=\"\">2025</a>); Yu et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib39\" title=\"\">2025</a>); Luo et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib16\" title=\"\">2024</a>)</cite> have extended tool-augmented dialogue evaluation to multimodal and longer-context scenarios. The m&amp;m&#8217;s benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib17\" title=\"\">2024</a>)</cite> evaluates LLMs on multi-step, multi-modal tasks using a diverse set of 33 tools, including public APIs and multimodal models such as off-the-shelf automatic speech recognition (ASR) models, highlighting the potential for developing agents that leverage audio-based tools.\nCRAG_MM <cite class=\"ltx_cite ltx_citemacro_citep\">(Meta CRAG-MM Challenge Organizers, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib21\" title=\"\">2025</a>)</cite> builds on the original CRAG benchmark by introducing visual question answering (QA) tasks that combine images and text-based queries, utilizing mock APIs for both image descriptions and web search.\nFor video understanding and long-context reasoning, the Video Web Arena <cite class=\"ltx_cite ltx_citemacro_citep\">(Jang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib14\" title=\"\">2025</a>)</cite> benchmark evaluates multimodal agents on tasks involving 2,021 manually crafted tutorial videos.\nWhile these benchmarks advance the field by incorporating multimodal tools, they still do not evaluate systems in speech-in speech-out scenarios.</p>\n\n",
                "matched_terms": [
                    "question",
                    "web",
                    "queries",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We begin with the CRAG dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>, which contains 2,706 text queries. Since these queries are not directly suitable for speech-based evaluation, we first identify those requiring adaptation before TTS conversion. Through careful manual inspection, we determine that queries containing dates or brackets benefit from rewriting to ensure naturalness and clarity in spoken form. In total, we identify 569 such queries and rewrite them using a large language model (LLAMA-4 Maverick). The resulting 569 rewritten queries, combined with the remaining original queries, form the basis for our spoken version of CRAG, which we term <em class=\"ltx_emph ltx_font_italic\">AudioCRAG</em>. We follow the CRAG setup to incorporate web and KG-based tools and adopt its robust evaluation setup, as detailed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S10\" title=\"10 Tool Usage Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S11\" title=\"11 Evaluation Setting &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "web",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio CRAG Human</span>:\nTo further enhance the realism and diversity of our evaluation, we introduce the AudioCRAG-Human benchmark, which consists of 618 human-recorded spoken queries. These queries are recorded by a diverse pool of participants to capture natural variations in speech, accent, and prosody. The inclusion of human-recorded audio enables a more comprehensive assessment of speech-in speech-out systems under real-world conditions, providing valuable insights into model robustness and generalization beyond synthetic speech. This benchmark serves as a critical resource for evaluating the effectiveness of tool integration in conversational AI systems.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable effective tool usage, we build upon the CRAG framework by integrating two complementary information sources: web search, which provides access to fresh and dynamic content, and a knowledge graph, which offers structured and reliable information. For web search, we aggregate all 100,000 documents from the CRAG corpus and employ a BGE-based re-ranker<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/BAAI/bge-large-en-v1.5\" title=\"\">https://huggingface.co/BAAI/bge-large-en-v1.5</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Xiao et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib33\" title=\"\">2023</a>)</cite> to index and retrieve the top 50 most relevant documents for each query. These documents are then segmented into chunks and re-ranked using the same BGE model based on their similarity to the query, ensuring highly contextually relevant retrieval. Meanwhile, queries to the knowledge graph are performed via a simulated API, adhering to the methodology established in CRAG <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/facebookresearch/CRAG/tree/main/mock_api\" title=\"\">https://github.com/facebookresearch/CRAG/tree/main/mock_api</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "query",
                    "web",
                    "tool",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qwen-OMNI</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>)</cite> is a end-to-end multimodal model that seamlessly integrates diverse input modalities&#8212;including text, images, audio, and video&#8212;and generates both text and natural speech responses in a real-time streaming fashion. It leverages an innovative Thinker-Talker architecture, where the Thinker module performs high-level reasoning to produce a text response, which is then used by the Talker module, conditioning on both the text and the Thinker&#8217;s hidden representations, to generate streaming speech output.</p>\n\n",
                "matched_terms": [
                    "qwenomni",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For tool-augmented scenarios, retrieval results are provided up to each model&#8217;s maximum token limit (&#8220;Ref length&#8221; in Tables), maintaining a 2:1 ratio of web page to KG results.\nSpecifically, we observe that Kimi-Audio is currently optimized for handling tool result references up to a certain length. When this limit is exceeded, an error arises during the audio detokenization process, specifically within the rotary embedding mechanism, highlighting an architectural constraint in processing longer input sequences or larger reference contexts. Addressing this limitation presents a valuable opportunity for future model enhancements.</p>\n\n",
                "matched_terms": [
                    "web",
                    "tool"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation encompasses both speech-in-text-out and speech-in-speech-out scenarios. For the Fixed-Interval Streaming RAG setting (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS1\" title=\"3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>), intermediate tool queries are generated at consistent 1-second intervals. In the Model-Triggered Streaming RAG setting, the model dynamically determines the need for a tool call after processing each 500ms block. This approach allows us to utilize a smaller chunk size, as only a single tool call thread is required for Model-Triggered Streaming RAG, thereby enabling more efficient and responsive processing.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "rag",
                    "queries",
                    "modeltriggered",
                    "generated",
                    "after",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This subsection details the experimental setup for post-training the pretrained speech-in, speech-out model to significantly enhance its tool usage capabilities, as outlined in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>.\nWe leverage a random subset of 16,000 questions from the text-based factual question answering dataset TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib15\" title=\"\">2017</a>)</cite>, which contains 97,000 questions and 662,659 associated web documents.\nFor <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, we further compute word-level timestamps using a pre-trained ASR model, OWSM CTC v4 1B <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib26\" title=\"\">2025</a>)</cite>, enabling us to generate partial ASR transcripts <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m1\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math> at 500 ms intervals. Note that if word occurs at boundary of block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, it is excluded from <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math>. For each partial transcript <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math>, we generate corresponding psuedo ground truth queries <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m5\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> using LLAMA-4-Maverick to simulate incremental user input.\nTo simulate realistic tool usage, we concatenate all documents and employ a web query reranker to retrieve the top 50 most relevant documents for each query. The text questions from this 16k subset are converted into discrete speech tokens using text-to-speech synthesis with the corresponding pretrained speech-in, speech-out model. Recognizing that TriviaQA answers are typically single named entities, we further transform the queries into a conversational style using LLAMA-4-Maverick, making them more suitable for dialogue-based evaluation.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "transcript",
                    "rag",
                    "queries",
                    "partial",
                    "xbasrxtextasrb",
                    "question",
                    "asr",
                    "modeltriggered",
                    "query",
                    "web",
                    "posttraining",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT = \"\"\"\nYou are given an Audio Question and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". Your task is to answer the question in as few words as possible.\n<br class=\"ltx_break\"/>Please follow these guidelines when formulating your answer:\n<br class=\"ltx_break\"/>1. If the question contains a false premise or assumption, answer &#8220;invalid question&#8221;.\n<br class=\"ltx_break\"/>2. If you are uncertain or don&#8217;t know the answer, respond with &#8220;I don&#8217;t know&#8221;.\n<br class=\"ltx_break\"/>### Question\n<br class=\"ltx_break\"/>{query}\n<br class=\"ltx_break\"/>### Query Time\n<br class=\"ltx_break\"/>{query_time}\n<br class=\"ltx_break\"/>### Answer\n<br class=\"ltx_break\"/>\"\"\"</p>\n\n",
                "matched_terms": [
                    "query",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT = \"\"\"\nYou are given an Audio Question, References and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". The references may or may not help answer the question. Your task is to answer the question in as few words as possible.\n<br class=\"ltx_break\"/>Please follow these guidelines when formulating your answer:\n<br class=\"ltx_break\"/>1. If the question contains a false premise or assumption, answer &#8220;invalid question&#8221;.\n<br class=\"ltx_break\"/>2. If you are uncertain or don&#8217;t know the answer, respond with &#8220;I don&#8217;t know&#8221;.\n<br class=\"ltx_break\"/>### Question\n<br class=\"ltx_break\"/>{query}\n<br class=\"ltx_break\"/>### Query Time\n<br class=\"ltx_break\"/>{query_time}\n<br class=\"ltx_break\"/>### References\n<br class=\"ltx_break\"/># web\n<br class=\"ltx_break\"/>{web_results}\n<br class=\"ltx_break\"/># knowledge graph\n<br class=\"ltx_break\"/>{kg_response}\n<br class=\"ltx_break\"/>### Answer\n<br class=\"ltx_break\"/>\"\"\"</p>\n\n",
                "matched_terms": [
                    "query",
                    "web",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT = \"\"\"\nYou are an agent that only outputs JSON. You are given a Query and Query Time. Do the following:\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>1) Determine the domain the query is about. The domain should be one of the following: \"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". If none of the domains apply, use \"other\". Use \"domain\" as the key in the result json.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>2) Extract structured information from the query. Include different keys into the result json depending on the domains, and put them DIRECTLY in the result json. Here are the rules:\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;encyclopedia&#8217; and &#8216;other&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;main_entity&#8217;: extract the main entity of the query.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;finance&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;market_identifier&#8217;: stock identifiers including individual company names, stock symbols.\n<br class=\"ltx_break\"/>- &#8216;metric&#8217;: financial metrics that the query is asking about. This must be one of the following: &#8216;price&#8217;, &#8216;dividend&#8217;, &#8216;P/E ratio&#8217;, &#8216;EPS&#8217;, &#8216;marketCap&#8217;, and &#8216;other&#8217;.\n<br class=\"ltx_break\"/>- &#8216;datetime&#8217;: time frame that the query asks about. When datetime is not explicitly mentioned, use &#8216;Query Time&#8217; as default.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;movie&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;movie_name&#8217;: name of the movie\n<br class=\"ltx_break\"/>- &#8216;movie_aspect&#8217;: if the query is about a movie, which movie aspect the query asks. This must be one of the following: &#8216;budget&#8217;, &#8216;genres&#8217;, &#8216;original_language&#8217;, &#8216;original_title&#8217;, &#8216;release_date&#8217;, &#8216;revenue&#8217;, &#8216;title&#8217;, &#8216;cast&#8217;, &#8216;crew&#8217;, &#8216;rating&#8217;, &#8216;length&#8217;.\n<br class=\"ltx_break\"/>- &#8216;person&#8217;: person name related to moves\n<br class=\"ltx_break\"/>- &#8216;person_aspect&#8217;: if the query is about a person, which person aspect the query asks. This must be one of the following: &#8216;acted_movies&#8217;, &#8216;directed_movies&#8217;, &#8216;oscar_awards&#8217;, &#8216;birthday&#8217;.\n<br class=\"ltx_break\"/>- &#8216;year&#8217;: if the query is about movies released in a specific year, extract the year\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;music&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;artist_name&#8217;: name of the artist\n<br class=\"ltx_break\"/>- &#8216;artist_aspect&#8217;: if the query is about an artist, extract the aspect of the artist. This must be one of the following: &#8216;member&#8217;, &#8216;birth place&#8217;, &#8216;birth date&#8217;, &#8216;lifespan&#8217;, &#8216;artist work&#8217;, &#8216;grammy award count&#8217;, &#8216;grammy award date&#8217;.\n<br class=\"ltx_break\"/>- &#8216;song_name&#8217;: name of the song\n<br class=\"ltx_break\"/>- &#8216;song_aspect&#8217;: if the query is about a song, extract the aspect of the song. This must be one of the following: &#8216;author&#8217;, &#8216;grammy award count&#8217;, &#8216;release country&#8217;, &#8216;release date&#8217;.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;sports&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;sport_type&#8217;: one of &#8216;basketball&#8216;, &#8216;soccer&#8216;, &#8216;other&#8216;\n<br class=\"ltx_break\"/>- &#8216;tournament&#8217;: NBA, World Cup, Olympic.\n<br class=\"ltx_break\"/>- &#8216;team&#8217;: teams that users are interested in.\n<br class=\"ltx_break\"/>- &#8216;datetime&#8217;: time frame that the user is interested in. When datetime is not explicitly mentioned, use &#8216;Query Time&#8217; as default.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Return the results in a FLAT json.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n<br class=\"ltx_break\"/>\"\"\"</p>\n\n",
                "matched_terms": [
                    "other",
                    "domain",
                    "person",
                    "queries",
                    "sports",
                    "query",
                    "datetime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT = \"\"\"\nYou are an agent that only outputs JSON. You are given an Audio\nQuery, Previously generated JSON result (&#8217;Previous Result&#8217;) and Query Time. Do the following:</p>\n\n",
                "matched_terms": [
                    "query",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">1) Determine the domain the query is about. The domain should be one of the following:\nf&#776;inance,&#776; s&#776;ports,&#776; m&#776;usic,&#776; m&#776;ovie,&#776; &#235;ncyclopedia.&#776; If none of the domains apply, use &#246;ther.&#776; Use\nd&#776;omain&#228;s the key in the result json.</p>\n\n",
                "matched_terms": [
                    "query",
                    "domain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8216;encyclopedia&#8217; and &#8216;other&#8217; queries, these are possible keys:\n- &#8216;main_entity&#8217;: extract the main entity of the query.</p>\n\n",
                "matched_terms": [
                    "query",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8216;finance&#8217; queries, these are possible keys:\n- &#8216;market_identifier&#8217;: stock identifiers including individual company names, stock symbols.\n- &#8216;metric&#8217;: financial metrics that the query is asking about. This must be one of the following: &#8216;price&#8217;,\n&#8216;dividend&#8217;, &#8216;P/E ratio&#8217;, &#8216;EPS&#8217;, &#8216;marketCap&#8217;, and &#8216;other&#8217;.\n- &#8216;datetime&#8217;: time frame that the query asks about. When datetime is not explicitly mentioned, use\n&#8216;Query Time&#8217; as default.</p>\n\n",
                "matched_terms": [
                    "query",
                    "datetime",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8216;movie&#8217; queries, these are possible keys:\n- &#8216;movie_name&#8217;: name of the movie\n- &#8216;movie_aspect&#8217;: if the query is about a movie, which movie aspect the query asks. This must be one\nof the following: &#8216;budget&#8217;, &#8216;genres&#8217;, &#8216;original_language&#8217;, &#8216;original_title&#8217;, &#8216;release_date&#8217;, &#8216;revenue&#8217;,\n&#8216;title&#8217;, &#8216;cast&#8217;, &#8216;crew&#8217;, &#8216;rating&#8217;, &#8216;length&#8217;.\n- &#8216;person&#8217;: person name related to moves\n- &#8216;person_aspect&#8217;: if the query is about a person, which person aspect the query asks. This must be\none of the following: &#8216;acted_movies&#8217;, &#8216;directed_movies&#8217;, &#8216;oscar_awards&#8217;, &#8216;birthday&#8217;.\n- &#8216;year&#8217;: if the query is about movies released in a specific year, extract the year</p>\n\n",
                "matched_terms": [
                    "query",
                    "queries",
                    "person"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8216;music&#8217; queries, these are possible keys:\n- &#8216;artist_name&#8217;: name of the artist\n- &#8216;artist_aspect&#8217;: if the query is about an artist, extract the aspect of the artist. This must be one of the\nfollowing: &#8216;member&#8217;, &#8216;birth place&#8217;, &#8216;birth date&#8217;, &#8216;lifespan&#8217;, &#8216;artist work&#8217;, &#8216;grammy award count&#8217;,\n&#8216;grammy award date&#8217;.\n- &#8216;song_name&#8217;: name of the song\n- &#8216;song_aspect&#8217;: if the query is about a song, extract the aspect of the song. This must be one of the\nfollowing: &#8216;author&#8217;, &#8216;grammy award count&#8217;, &#8216;release country&#8217;, &#8216;release date&#8217;.</p>\n\n",
                "matched_terms": [
                    "query",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8216;sports&#8217; queries, these are possible keys:\n- &#8216;sport_type&#8217;: one of &#8216;basketball&#8216;, &#8216;soccer&#8216;, &#8216;other&#8216;\n- &#8216;tournament&#8217;: NBA, World Cup, Olympic.\n- &#8216;team&#8217;: teams that users are interested in.\n- &#8216;datetime&#8217;: time frame that the user is interested in. When datetime is not explicitly mentioned, use\n&#8216;Query Time&#8217; as default.\nReturn the results in a FLAT json.\n<br class=\"ltx_break\"/>*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*</p>\n\n",
                "matched_terms": [
                    "datetime",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">3) Compare your newly generated result to the &#8217;Previous Result&#8217;. **If your new result would be exactly the same as the &#8217;Previous Result&#8217;, output only NO_QUERY.**\nReturn the results in a FLAT json.</p>\n\n",
                "matched_terms": [
                    "noquery",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT = \"\"\" You are given an Audio Query and Query Time. Your task is to generate a web query that can be used to retrieve relevant web pages. Rewrite the following query into a short and succinct form, focusing on the main topic or domain (e.g. finance, sports, music, movie, encyclopedia), key entities mentioned (e.g. people, organizations, locations), and specific aspects of those entities (e.g. performance metrics, relationships, events). Ensure the rewritten query is clear, concise, and easy to understand.\nNote that simply outputting the original query is not acceptable. You must rephrase the query to make it more concise and focused on the key information that will help retrieve relevant web pages.</p>\n\n",
                "matched_terms": [
                    "query",
                    "sports",
                    "web",
                    "domain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8217;finance&#8217; queries, focus on:\n- Company names or stock symbols\n- Financial metrics (e.g. price, dividend, P/E ratio, EPS, marketCap)\n- Specific timeframes or events; if no timeframe is specified, use the Query Time as default</p>\n\n",
                "matched_terms": [
                    "query",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8217;sports&#8217; queries, focus on:\n- Sports Type (eg. basketball, soccer)\n- Teams, players\n- Statistics or performance metrics (e.g. scores, wins, losses)\n- Specific events or tournaments (eg. NBA, World Cup, Olympic)\n- Time frame that the user is interested in; if no timeframe is specified, use the Query Time as default</p>\n\n",
                "matched_terms": [
                    "query",
                    "sports",
                    "basketball",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8217;music&#8217; queries, focus on:\n- Artist names or song titles\n- Specific aspects of artist (eg. band name, birth place, birth date, lifespan, artist work, grammy award count, grammy award date)\n- Specific aspects of song (eg. author, grammy award count, release country, release date)\n- Music genres or categories\n- Specific awards or recognition (e.g. Grammy Awards, Billboard)</p>\n\n",
                "matched_terms": [
                    "count",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8217;movie&#8217; queries, focus on:\n- Movie titles or celebrity names\n- Movie genres or other categories like budget, language, release_date, revenue, cast, crew, rating, length\n- Specific aspects of celebrity like acted_movies, directed_movies, oscar_awards, birthday\n- Specific awards or recognition (e.g. Oscars)\nFor &#8217;other&#8217; queries, focus on:\n- Main entity or topic\n- Specific aspects or attributes of the entity</p>\n\n",
                "matched_terms": [
                    "other",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When rewriting the query, ensure that it captures all important information from the original question that could impact the retrieval results. Do not omit any crucial details, such as specific dates, locations, or relationships between entities. Also, do not invent any new details on your own. If necessary, use the Query Time to provide context for the query. The goal is to create a concise and accurate query that effectively conveys the user&#8217;s intent and retrieves relevant information.\n*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT THE REWRITTEN QUERY!!!*\n\"\"\"</p>\n\n",
                "matched_terms": [
                    "query",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT = \"\"\"You are given an Audio Query, previously generated Web query (&#8217;Previous Result&#8217;) and Query Time.</p>\n\n",
                "matched_terms": [
                    "query",
                    "generated",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Your task is to generate a web query that can be used to retrieve relevant web pages. Rewrite the following query into a short and succinct form, focusing on the main topic or domain (e.g. finance, sports, music, movie, encyclopedia), key entities mentioned (e.g. people, organizations, locations), and specific aspects of those entities (e.g. performance metrics, relationships, events). Ensure the rewritten query is clear, concise, and easy to understand.</p>\n\n",
                "matched_terms": [
                    "query",
                    "sports",
                    "web",
                    "domain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note that simply outputting the original query is not acceptable. You must rephrase the query to make it more concise and focused on the key information that will help retrieve relevant web pages.</p>\n\n",
                "matched_terms": [
                    "query",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8217;finance&#8217; queries, focus on:\n- Company names or stock symbols\n- Financial metrics (e.g. price, dividend, P/E ratio, EPS, marketCap)\n- Specific timeframes or events; if no timeframe is specified, use the Query Time as default</p>\n\n",
                "matched_terms": [
                    "query",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8217;sports&#8217; queries, focus on:\n- Sports Type (eg. basketball, soccer)\n- Teams, players\n- Statistics or performance metrics (e.g. scores, wins, losses)\n- Specific events or tournaments (eg. NBA, World Cup, Olympic)\n- Time frame that the user is interested in; if no timeframe is specified, use the Query Time as default</p>\n\n",
                "matched_terms": [
                    "query",
                    "sports",
                    "basketball",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8217;music&#8217; queries, focus on:\n- Artist names or song titles\n- Specific aspects of artist (eg. band name, birth place, birth date, lifespan, artist work, grammy award count, grammy award date)\n- Specific aspects of song (eg. author, grammy award count, release country, release date)\n- Music genres or categories\n- Specific awards or recognition (e.g. Grammy Awards, Billboard)</p>\n\n",
                "matched_terms": [
                    "count",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For &#8217;movie&#8217; queries, focus on:\n- Movie titles or celebrity names\n- Movie genres or other categories like budget, language, release_date, revenue, cast, crew, rating, length\n- Specific aspects of celebrity like acted_movies, directed_movies, oscar_awards, birthday\n- Specific awards or recognition (e.g. Oscars)</p>\n\n",
                "matched_terms": [
                    "other",
                    "queries"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When rewriting the query, ensure that it captures all important information from the original question that could impact the retrieval results. Do not omit any crucial details, such as specific dates, locations, or relationships between entities. Also, do not invent any new details on your own. If necessary, use the Query Time to provide context for the query. The goal is to create a concise and accurate query that effectively conveys the user&#8217;s intent and retrieves relevant information.\nNow, compare the new web query to the previously generated web query (&#8217;Previous Result&#8217;).</p>\n\n",
                "matched_terms": [
                    "query",
                    "generated",
                    "web",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the new query is similar enough to the previous web query (i.e., it effectively conveys the same user intent and would retrieve similar relevant information), output only *NO_QUERY*.</p>\n\n",
                "matched_terms": [
                    "noquery",
                    "query",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT=\"\"\"\n<br class=\"ltx_break\"/>Assume you are a human expert in grading predictions given by a model. You are given a question and a model prediction. Judge if the prediction matches the ground truth answer by following these steps:\n1: Take it as granted that the Ground Truth is always correct.\n2: If the Prediction indicates it is not sure about the answer, \"score\" should be \"0\"; otherwise, go the next step.\n3: If the Prediction exactly matches the Ground Truth, \"score\" is 1.\n4: If the Prediction does not exactly match the Ground Truth, go through the following steps and likely give a score as 0.\n5: If the Ground Truth is a number, \"score\" is 1 if and only if the Prediction gives a number that almost exactly matches the ground truth.\n6: If the Prediction is self-contradictory, \"score\" must be 0.\n7: If the prediction is not answering the question, \"score\" must be 0.\n8: If the prediction is a concise and correct summary of the ground truth, \"score\" is 1.\n9: If ground truth contains a set of items, prediction must contain exactly same items for the score to be 1.\n10: Otherwise, \"score\" is 0.</p>\n\n",
                "matched_terms": [
                    "question",
                    "number",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">You should make the judgment based on provided examples.\nExamples:\nQuestion: \"which company has higher eps, btu or cma?\"\nGround Truth: \"cma\"\nPrediction: \"it is not possible to determine which company has a higher eps.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction is not sure about the answer.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who is taller, a or b?\"\nGround Truth: \"a\"\nPrediction: \"The answer is a. a is of height 1.75 m and b is of height 1.82 m. So b is taller.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction is self-contradictory.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "who",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who among patricia place and lana clarkson is younger?\"\nGround Truth: \"lana clarkson\"\nPrediction: \"lana clarkson (born april 5, 1962) is younger than patricia clarkson (born december 29, 1959).\"\nOutput: \"score\": 1, \"explanation\": \"The prediction gives the correct answer.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "who",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"did lakers win more games than warriors in 2024-4?\"\nGround Truth: \"yes\"\nPrediction: \"yes, lakers won more games than warriors in 2023-2024 season.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction is not answering the question.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "did",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the height of the building where citigroup is headquartered?\"\nGround Truth: \"151 m\"\nPrediction: \"the height is 915 feet (279 m).\"\nOutput: \"score\": 0, \"explanation\": \"The prediction, 151 m, does not match the ground truth, 279 m.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "what",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who were the last 3 prime ministers of uk?\"\nGround Truth: \"rishi sunak, liz truss, and boris johnson\"\nPrediction: \"1. liz truss, 2. boris johnson, 3.theresa may\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not contain item, rishi sunak, that is in the grount truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "who",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who authored the taming of the shrew?\"\nGround Truth: \"william shakespeare\"\nPrediction: \"william shakespeare\"\nOutput: \"score\": 1, \"explanation\": \"The prediction exactly matches the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "who",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who authored The Taming of the Shrew?\"\nGround Truth: \"william shakespeare\"\nPrediction: \"w shakespeare\"\nOutput: \"score\": 1, \"explanation\": \"The prediction matches the ground truth as w is the abbreviation of william.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "who",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the state bird of california?\"\nGround Truth: \"california quail\"\nPrediction: \"california valley quail\"\nOutput: \"score\": 1, \"explanation\": \"The prediction matches the ground truth as california quail is also known as california valley quail.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "what",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what country is maharashtra metro rail corporation limited located in?\"\nGround truth: \"india\"\nPrediction: \"indianapolis\"\nOutput: \"score\": 0, \"explanation\": \"The prediction doesn&#8217;t match the ground truth answers.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "what",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what was wolves&#8217;s record in their last game in eng-premier league?\"\nGround Truth: \"w\"\nPrediction: \"wolves&#8217;s record in their last game in the eng-premier League was a 2-1 win over fulham.\"\nOutput: \"score\": 1, \"explanation\": \"The prediction correctly answers the result is a win.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "game",
                    "what",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how many seconds is 3 minutes 15 seconds?\"\nGround Truth: \"195 seconds\"\nPrediction: \"3 minutes 15 seconds is 195 seconds.\"\nOutput: \"score\": 1, \"explanation\": \"The prediction matches ground truth as it correctly states the number of seconds asked in the question.\"</p>\n\n",
                "matched_terms": [
                    "how",
                    "many",
                    "score",
                    "question",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"on which days did am distribute dividends in the last year?\"\nGround Truth: \"2023-01-24, 2023-04-25, 2023-07-25, 2023-10-24\"\nPrediction: \"the company distributed dividends on october 24, 2023 and january 24, 2023.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction partially matches the ground truth, but it is incomplete.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "did",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"on which days did xxx distribute dividends in the last year?\"\nGround Truth: \"2023-01-13, 2023-03-25, 2023-11-21\"\nPrediction: \"xxx distributed dividends on 1. 2023-01-13, 2. 2023-03-25, 3. 2023-10-21.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction contains an item, 2023-10-21, which is not in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "did",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who are current band members of maroon 5?\"\nGround Truth: \"adam Levine, jesse carmichael, james valentine, matt flynn, pj morton, sam farrar\"\nPrediction: \"they are jesse carmichael, matt flynn, adam levine, pj morton, sam farrar, and james valentine.\"\nOutput: \"score\": 1, \"explanation\": \"The prediction exactly matches the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "who",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"which movies comprise the matrix franchise?\"\nGround Truth: \"the matrix, the matrix reloaded, the matrix revolutions, the matrix resurrections\"\nPrediction: \"the matrix, the matrix reloaded, the matrix revolutions, the animatrix, and the matrix resurrections.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction covers more items than what are given by the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "what",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how deep is the deepest lake of new york?\"\nGround Truth: \"618 ft\"\nPrediction: \"the deepest lake in new york is seneca lake, with a depth of 618.23 feet.\"\nOutput: \"score\": 1, \"explanation\": \"The prediction exactly matches the number in ground truth after rounding.\"</p>\n\n",
                "matched_terms": [
                    "how",
                    "after",
                    "score",
                    "question",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the closing price of meta yesterday?\"\nGround Truth: \"$310.17\"\nPrediction: \"310.2\"\nOutput: \"score\": 1, \"explanation\": \"The prediction exactly matches the number in ground truth after rounding.\"</p>\n\n",
                "matched_terms": [
                    "what",
                    "number",
                    "score",
                    "question",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the current market cap of appl?\"\nGround Truth: \"2.81 trillion\"\nPrediction: \"2.667 trillion\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the number in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "number",
                    "what",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the current pe ratio of appl?\"\nGround Truth: \"28.3\"\nPrediction: \"the current pe ratio of apple is 26.66\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the number in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "number",
                    "what",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how much is tesla&#8217;s stock price down from its all-time high?\"\nGround Truth: \"$221.83\"\nPrediction: \"209.52\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the number in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "number",
                    "how",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the length of amazon river?\"\nGround Truth: \"over 4000 miles\"\nPrediction: \"the length of amazon river is 4,000 miles\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not say Amazon River is longer than 4000 miles.\"</p>\n\n",
                "matched_terms": [
                    "miles",
                    "question",
                    "what",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how many copies x were sold?\"\nGround Truth: \"2 million.\"\nPrediction: \"it is over 2 million\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "how",
                    "question",
                    "many",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the population of country x?\"\nGround Truth: \"3,576,873\"\nPrediction: \"the population of country x is 3.3 million.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction, 3.3 M, does not match the number, 3.6 M, in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "number",
                    "what",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is the current market value of stock x?\"\nGround Truth: \"$2,237,578,268\"\nPrediction: \"$2.16 billion.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction, 2.16 B, does not match the number, 2.2 B, in ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "number",
                    "what",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how old is michael jordan?\"\nGround Truth: \"61\"\nPrediction: \"mj was born on February 17, 1963.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not explicitly say the age.\"</p>\n\n",
                "matched_terms": [
                    "how",
                    "question",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how long did amy teach as a teacher?\"\nGround Truth: \"24 years and 255 days\"\nPrediction: \"25 years.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not exactly match the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "did",
                    "how",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"at what age did usher perform in the half time show of superbowl?\"\nGround Truth: \"45 years old\"\nPrediction: \"usher has not performed at a super bowl halftime show.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "did",
                    "what",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what year did olivia rodrigo win the grammys best new artist award?\"\nGround Truth: \"2022\"\nPrediction: \"sure! here&#8217;s an answer in three words or fewer: 2022 grammy awards.\"\nOutput: \"score\": 1, \"explanation\": \"The prediction answers the correct number of the year.\"</p>\n\n",
                "matched_terms": [
                    "did",
                    "what",
                    "score",
                    "question",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how many times have zendaya and tom holland starred in a film together?\"\nGround Truth: \"3\"\nPrediction: \"they have starred in 3 films together: spider-man: homecoming, spider-man: far from home, and spider-man: no way home.\"\nOutput: \"score\": 1, \"explanation\": \"The prediction gives the right number and it is reasonable.\"</p>\n\n",
                "matched_terms": [
                    "how",
                    "many",
                    "score",
                    "question",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who is younger, timothee chalamet or tom holland?\"\nGround Truth: \"tom holland\"\nPrediction: \"timoth&#233;e chalamet is younger than tom holland.\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "who",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"who had more number one hits on the us billboard, a or b?\"\nGround Truth: \"a had more number one hits on the us billboard than b, with 20 number one hits compared to b&#8217;s 15.\"\nPrediction: \"a\"\nOutput: \"score\": 1, \"explanation\": \"The prediction is a concise and correct summary of the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "number",
                    "who",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what is xxx&#8217;s birthdate?\"\nGround Truth: \"1996-01-01.\"\nPrediction: \"02/01/1996\"\nOutput: \"score\": 0, \"explanation\": \"The prediction does not match the ground truth.\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "what",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"what was the worldwide box office haul for movie x?\"\nGround Truth: \"101756123.\"\nPrediction: \"102 million\"\nOutput: \"score\": 1, \"explanation\": \"The prediction exactly matches the number in ground truth after rounding.\"</p>\n\n",
                "matched_terms": [
                    "what",
                    "after",
                    "score",
                    "question",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: \"how much has spotify&#8217;s user base increased by since 2020 in na?\"\nGround Truth: \"spotify&#8217;s user base increased by 34 million since 2020.\"\nPrediction: \"spotify&#8217;s north american user base increased from 36 million in 2020 to 85 million by 2021\"\nOutput: \"score\": 0, \"explanation\": \"The prediction is not answering the question as it only gives the increase from 2020 to 2021.\"\n<br class=\"ltx_break\"/>\"\"\"</p>\n\n",
                "matched_terms": [
                    "question",
                    "how",
                    "score"
                ]
            }
        ]
    },
    "S13.T9": {
        "source_file": "Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage",
        "caption": "Table 9: Last-token Latency breakdown, showing median (P50) and 90th percentile (P90) timings, for the Qwen2.5-7B in Open Book Setting on AudioCRAG-Synthetic (First Token Latency=5.9 sec in T. 1).",
        "body": "Model\nToken\nP\nLatency (sec)\n\n\nTool Latency\nResponse Gen\nTotal\n\n\nQuery Gen\nTool Results Gen\n\n\nQwen2.5-7B\nLast Token\nP50\n0.59\n2.78\n16.70\n20.07\n\n\nP90\n0.85\n4.90\n42.41\n48.16",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\">Model</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"3\">Token</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"3\">P</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"4\">Latency (sec)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">Tool Latency</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\">Response Gen</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\">Total</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Query Gen</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Tool Results Gen</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"2\">Qwen2.5-7B</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\" rowspan=\"2\">Last Token</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">P50</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">16.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">20.07</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">P90</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">4.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">42.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">48.16</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "lasttoken",
            "tool",
            "90th",
            "qwen257b",
            "response",
            "first",
            "book",
            "median",
            "open",
            "latency",
            "percentile",
            "query",
            "results",
            "last",
            "p90",
            "model",
            "gen",
            "token",
            "p50",
            "showing",
            "breakdown",
            "latency59",
            "setting",
            "timings",
            "total",
            "sec",
            "audiocragsynthetic"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> provides a comprehensive breakdown of latency measurements for the Qwen2.5-7B speech-to-speech model, evaluated in both the Open Book setting and our proposed <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> setting on AudioCRAG-Synthetic. We report both median (P50) and 90th percentile (P90) values for each stage of the processing pipeline. The latency is decomposed into three main components: tool query generation, tool result generation, and speech response synthesis, with measurements provided for both the first token outputs (We also provide latency measurements for last token output in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T9\" title=\"Table 9 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>). For both tool query and tool result generation, it is assumed that all tools are accessed in parallel; thus, the reported latency corresponds to the maximum query or result generation time among all tools. The majority of this latency arises from leveraging external web pages, which introduces significant delays, most notably, increasing the first token latency by 2.3x in Open Book Setting.\nNotably, our <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> setting enables early generation of tool results, successfully reducing P50 first token latency by 9.8% and tool use latency by 20.7%.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">End-to-end speech-in speech-out dialogue systems are emerging as a powerful alternative to traditional ASR&#8211;LLM&#8211;TTS pipelines, generating more natural, expressive responses with significantly lower latency. However, these systems remain prone to hallucinations due to limited factual grounding. While text-based dialogue systems address this challenge by integrating tools such as web search and knowledge graph APIs, we introduce the first approach to extend tool use directly into speech-in speech-out systems. A key challenge is that tool integration substantially increases response latency, disrupting conversational flow. To mitigate this, we propose Streaming Retrieval-Augmented Generation (<em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>), a novel framework that reduces user-perceived latency by predicting tool queries in parallel with user speech, even before the user finishes speaking. Specifically, we develop a post-training pipeline that teaches the model when to <em class=\"ltx_emph ltx_font_italic\">issue tool calls</em> during ongoing speech and how to generate spoken summaries that fuse audio queries with retrieved text results, thereby improving both <em class=\"ltx_emph ltx_font_italic\">accuracy and responsiveness</em>. To evaluate our approach, we construct AudioCRAG, a benchmark created by converting queries from the publicly available CRAG dataset into speech form. Experimental results demonstrate that our <em class=\"ltx_emph ltx_font_italic\">streaming RAG</em> approach increases QA accuracy by up to 200% relative (from 11.1% to 34.2% absolute) and further enhances user experience by reducing tool use latency by 20%. Importantly, our <em class=\"ltx_emph ltx_font_italic\">streaming RAG</em> approach is modality-agnostic and can be applied equally to typed input, paving the way for more agentic, real-time AI assistants.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "latency",
                    "response",
                    "results",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spoken Dialogue Systems (SDS) are foundational to many everyday technologies, powering intelligent assistants such as Alexa and Siri, as well as interactive voice response systems in customer service. With the rapid expansion of SDS capabilities to mobile phones and wearable devices, the need for robust, scalable, and generalizable solutions has never been greater. Traditionally, SDS have relied on cascaded pipelines composed of multiple modules&#8212;including voice activity detection (VAD), automatic speech recognition (ASR), natural language understanding (NLU), natural language generation (NLG), and text-to-speech (TTS) synthesis&#8212;each introducing potential points of failure and latency <cite class=\"ltx_cite ltx_citemacro_citep\">(Glass, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib12\" title=\"\">1999</a>; Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib13\" title=\"\">2024</a>)</cite>.\nRecently, end-to-end (E2E) SDS <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib34\" title=\"\">2024</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib22\" title=\"\">2023</a>; Meng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib20\" title=\"\">2024</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib40\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib2\" title=\"\">2025b</a>)</cite> have been proposed, which directly generate spoken responses from speech input within a unified architecture. This E2E approach not only mitigates error propagation across modules but also captures non-phonemic information more effectively, resulting in significantly lower inference time and computational overhead, and paving the way for more natural and efficient conversational experiences.</p>\n\n",
                "matched_terms": [
                    "response",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, current E2E SDS are fundamentally constrained by their reliance on internalized knowledge from static training data, which often results in responses that lack factual grounding or fail to reflect the most up to date information. This shortcoming is particularly critical for action-oriented or knowledge-seeking tasks, such as booking hotels or answering questions about current events. In contrast, text-based conversational assistants have begun to overcome these limitations by integrating external tools through Retrieval-Augmented Generation (RAG)<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>; Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib3\" title=\"\">2023a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib5\" title=\"\">c</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib10\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib11\" title=\"\">b</a>)</cite>, dynamically retrieving relevant information from sources like web search, knowledge graphs (KG), and real-time APIs. Yet, the integration of such tool use into E2E SDS remains largely unexplored.\nA key challenge is that while external tools can substantially improve factual accuracy, invoking them often introduces additional latency, leading to awkward silences that disrupt the natural conversational flow. This raises a research question: <span class=\"ltx_text ltx_font_italic\">How can we trade-off between accuracy and responsiveness for developing SDS that feel both intelligent and natural?</span></p>\n\n",
                "matched_terms": [
                    "tool",
                    "results",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present, to the best of our knowledge, the first <span class=\"ltx_text ltx_font_italic\">speech-in, speech-out language model that seamlessly integrates external tool invocation with low latency</span>. The key idea is a <span class=\"ltx_text ltx_font_bold\">Streaming RAG</span> strategy that generates tool queries in parallel with user speech, often times even before the user has finished speaking (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). A naive implementation of streaming queries, however, faces two challenges: (1) queries issued from partial speech may be suboptimal, yielding distracting tool outputs and inaccurate responses; and (2) unnecessary tool calls may be triggered, wasting computational resources. We introduce effective modeling techniques to address these challenges and make the following contributions.</p>\n\n",
                "matched_terms": [
                    "latency",
                    "tool",
                    "model",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution 1</span>: We introduce a formal framework for tool integration in speech-in speech-out systems and empirically show that leveraging web search and KG APIs significantly enhances factual question answering. Evaluating three state-of-the-art (SOTA) models, Qwen-OMNI <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>)</cite>, OpusLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib30\" title=\"\">2025</a>)</cite>, and Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib8\" title=\"\">2025</a>)</cite>, we find that external tool integration delivers substantial performance gains, boosting accuracy by up to 140% relative (from 11.1% to 26.3% absolute). However, tool usage also introduces considerable latency, increasing first-token response time by 2.3x.</p>\n\n",
                "matched_terms": [
                    "response",
                    "tool",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution 2</span>: To address this, we propose <em class=\"ltx_emph ltx_font_italic\">Streaming Retrieval-Augmented Generation</em> (<em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>), the <em class=\"ltx_emph ltx_font_italic\">first</em> framework that empowers the system to trigger tool queries in parallel with user speech, even before the user finishes speaking. Within this framework, we introduce two novel approaches: (1) <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em>, which issues tool queries at regular intervals during speech input and carefully examines quality of retrieval results on the full query to guarantee response quality, and can be incorporated into any speech-in, speech-out model without post-training; (2) <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, which post-trains the model to intelligently determine optimal query timing based on the evolving user utterance to save computation resources.\nOur results demonstrate that our proposed <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> delivers over 200% relative improvement in accuracy (from 11.1% to 34.2% absolute in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) compared to the no-tool baseline, while also reducing tool result generation latency by 20%. Though designed for speech-in speech-out systems, streaming RAG can also be adapted in cascaded SDS, or even chatbots as users type.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "latency",
                    "query",
                    "response",
                    "results",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution 3</span>: Finally, we introduce AudioCRAG, a benchmark created by recording spoken queries from the CRAG <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite> dataset, enabling robust evaluation of tool usage capabilities in speech-in speech-out systems.\nTo support future research, we will open source our training code and AudioCRAG-Human benchmark, supporting future research in tool-integrated voice assistants.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "open"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several E2E spoken dialogue systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>; Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib34\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib1\" title=\"\">2025a</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib22\" title=\"\">2023</a>; Meng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib20\" title=\"\">2024</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib40\" title=\"\">2024</a>; Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib2\" title=\"\">2025b</a>)</cite> have recently been introduced, demonstrating impressive semantic understanding and high audio quality in their responses. However, these systems have not yet been trained or evaluated for their ability to <em class=\"ltx_emph ltx_font_italic\">use external tools</em>.\nAnother research direction <cite class=\"ltx_cite ltx_citemacro_citep\">(Feng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib9\" title=\"\">2025</a>)</cite> explores E2E RAG for direct speech-to-text retrieval, utilizing multimodal embeddings for enabling speech utterances to directly retrieve relevant text. While this method outperforms models without RAG, it is primarily limited to <em class=\"ltx_emph ltx_font_italic\">speech-to-text</em> scenarios. Its ability to access KGs and other APIs, critical for real-world applications, remains unproven. Furthermore, the retrieval scope is limited, as experiments are conducted with retrieval restricted to just 10 paragraphs, whereas modern RAG benchmarks require searching across thousands of web pages.\nAlthough recent studies <cite class=\"ltx_cite ltx_citemacro_citep\">(Maben et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib18\" title=\"\">2025</a>)</cite> have developed web interfaces that integrate tools into speech-in <em class=\"ltx_emph ltx_font_italic\">speech-out</em> scenarios using cascaded pipelines, comprehensive empirical investigations of E2E speech-in speech-out systems and, crucially, systematic analyses of user-perceived latency, remain largely unexplored. In this work, we address these gaps by developing a comprehensive framework for tool integration in E2E speech-in speech-out systems and designing benchmarks to quantitatively assess the tool usage capabilities of SOTA models. Furthermore, recognizing the importance of latency for natural conversational flow, we introduce novel streaming RAG methods that not only enhance tool usage performance but also reduce user-perceived latency.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A RAG spoken conversation system takes an audio question <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math> as input and outputs a spoken answer <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>. Let the ASR transcript of audio question be <math alttext=\"X^{\\text{asr}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>asr</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}</annotation></semantics></math> and the ASR transcript of the spoken answer be <math alttext=\"X^{\\text{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>res</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{res}}</annotation></semantics></math>.\nAnswers are generated by speech-in speech-out models, leveraging both the model&#8217;s internal knowledge and information retrieved from external sources.\nTo incorporate external information, the model needs to formulate a tool query <math alttext=\"Q^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><msup><mi>Q</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">Q^{T}</annotation></semantics></math> to retrieve relevant results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m6\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from an external tool <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m7\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "query",
                    "model",
                    "tool",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our proposed formulation for integrating external tools into speech-in speech-out systems. We introduce a two-stage inference approach: <em class=\"ltx_emph ltx_font_italic\">Query Generation</em> and <em class=\"ltx_emph ltx_font_italic\">Response Generation</em>. In the Query Generation stage, the system processes an audio question and generates queries for each external tool to retrieve relevant information by maximizing the posterior distribution <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> (Examples of generated queries are provided in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T7\" title=\"Table 7 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> in the Appendix.). In the Response Generation stage, the retrieved results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from these tools are combined with the original audio question and input into the model to generate the final spoken response by maximizing the posterior distribution <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math>. By conditioning the final output generation on the input audio, this formulation not only provides a simple and effective mechanism for interacting with text-based APIs, but also preserves the key advantages of speech-in speech-out systems, mitigating error propagation and enabling the model to capture non-phonemic information (such as prosody and speaker intent) more effectively.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "query",
                    "response",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">RAG-based systems, as proposed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, can significantly improve factual accuracy by incorporating external tools. However, these tool calls often introduce substantial latency, which is particularly problematic in speech-in, speech-out applications where users expect rapid, conversational responses and even brief silences can disrupt the natural flow of dialogue.\nOne way to address this challenge lies in the nature of audio inputs, which arrive as a continuous stream. This streaming property enables tool calls to be initiated before the user has finished speaking, offering a unique opportunity to mitigate latency.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To minimize user-perceived latency, we introduce <em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>: the first framework to generate and issue tool queries in parallel as audio input is received. This novel approach is built on three key design components:\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> Trigger: When to initiate a new tool query;\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> Threads: The number of parallel tool query threads;\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.p2.pic3\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">3</span></span></foreignobject></g></g></svg></span> Reflector: The module that determines whether intermediate tool results are sufficient for generating the final output. By exploring different design choices for each component, we introduce two complementary approaches for streaming tool query generation in the following subsections: a fixed-interval trigger method and a model-based trigger method.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "latency",
                    "query",
                    "results",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this approach, the <em class=\"ltx_emph ltx_font_italic\">trigger</em> is set to fire tool calls at fixed chunk intervals during audio input.\nThe input speech <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math> is divided into a sequence of <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> blocks, <math alttext=\"Q=\\{Q_{b}\\mid b=1,\\dots,B\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>Q</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>Q</mi><mi>b</mi></msub><mo fence=\"true\" lspace=\"0em\" rspace=\"0em\">&#8739;</mo><mrow><mi>b</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>B</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Q=\\{Q_{b}\\mid b=1,\\dots,B\\}</annotation></semantics></math>, with each block containing <math alttext=\"N_{\\text{block}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>N</mi><mtext>block</mtext></msub><annotation encoding=\"application/x-tex\">N_{\\text{block}}</annotation></semantics></math> frames.\nTo approximate <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> as described in S. 3.1, we follow a block-wise prediction strategy. In this approach, after processing each audio block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m6\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, the model predicts a tool query <math alttext=\"\\hat{Q}_{b}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m7\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}_{b}^{T}</annotation></semantics></math> by conditioning on the input speech accumulated up to block b, specifically <math alttext=\"Q_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">Q_{1:b}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "query",
                    "tool",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This strategy results in <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m9\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> parallel tool call threads running simultaneously (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F3.sf1\" title=\"Figure 3(a) &#8227; Figure 3 &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>), where each thread generates a tool query prediction <math alttext=\"\\hat{Q}_{b}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m10\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}_{b}^{T}</annotation></semantics></math> for its corresponding block <math alttext=\"b\\in[1,B]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m11\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mi>B</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b\\in[1,B]</annotation></semantics></math>.\nThe tool queries <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m12\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> generated after each block are then stored in cache. Given the high latency of tool calls (See T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), users typically complete their utterances before tool responses are ready. Upon utterance completion, an explicit <em class=\"ltx_emph ltx_font_italic\">reflector</em> module &#8220;<math alttext=\"\\text{reflect}()\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m13\" intent=\":literal\"><semantics><mrow><mtext>reflect</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{reflect}()</annotation></semantics></math>&#8221; evaluates the cached intermediate queries <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m14\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> against final query <math alttext=\"\\hat{Q}^{T}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m15\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{B}</annotation></semantics></math> to determine whether an early intermediate tool call provides sufficient information to answer the user&#8217;s question <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m16\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math>. The reflector module systematically evaluates all intermediate queries in the cache and identifies the <em class=\"ltx_emph ltx_font_italic\">earliest</em> sufficient tool call <math alttext=\"b^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m17\" intent=\":literal\"><semantics><msup><mi>b</mi><mo>&#8902;</mo></msup><annotation encoding=\"application/x-tex\">b^{\\star}</annotation></semantics></math> where the intermediate query <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m18\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> will give the same result as final query <math alttext=\"\\hat{Q}^{T}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m19\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{B}</annotation></semantics></math> as shown:</p>\n\n",
                "matched_terms": [
                    "query",
                    "tool",
                    "results",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All subsequent parallel tool calls after <math alttext=\"b^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m20\" intent=\":literal\"><semantics><msup><mi>b</mi><mo>&#8902;</mo></msup><annotation encoding=\"application/x-tex\">b^{\\star}</annotation></semantics></math> are promptly terminated, and the retrieved results <math alttext=\"R_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m21\" intent=\":literal\"><semantics><msub><mi>R</mi><msup><mi>b</mi><mo>&#8902;</mo></msup></msub><annotation encoding=\"application/x-tex\">R_{b^{\\star}}</annotation></semantics></math> from this intermediate call <math alttext=\"\\hat{Q}^{T}_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m22\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><msup><mi>b</mi><mo>&#8902;</mo></msup><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b^{\\star}}</annotation></semantics></math> are used to generate the final spoken response <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m23\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by maximizing the posterior distribution <math alttext=\"P(A|Q,R_{b^{\\star}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m24\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><msub><mi>R</mi><msup><mi>b</mi><mo>&#8902;</mo></msup></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R_{b^{\\star}})</annotation></semantics></math> (instead of <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m25\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math> in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>).\nWe employ a reflector module that uses simple yet effective heuristics: (a) if the top 5 web documents for an intermediate web query match those of the final web query, and (b) if the KG results for intermediate and final KG queries are identical.\nThese heuristics ensure that the information retrieved from an early tool call using <math alttext=\"\\hat{Q}^{T}_{b^{\\star}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m26\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><msup><mi>b</mi><mo>&#8902;</mo></msup><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b^{\\star}}</annotation></semantics></math> is consistent with what would have been obtained by waiting for the final tool call using <math alttext=\"\\hat{Q}^{T}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m27\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{B}</annotation></semantics></math>, thereby providing a strong quality guarantee. Since most tool call latency arises from chunking and reranking the chunks of web documents, these checks enable significant latency savings without any compromise in model performance.\nA key advantage of this strategy is its plug-and-play nature: it requires no additional post-training for speech-in, speech-out models and can be directly applied at inference time across a variety of architectures. However, there are important considerations. First, generating parallel tool calls at every fixed interval (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E1\" title=\"Equation 1 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) increases computational overhead, which may pose challenges for deployment on resource-constrained devices such as wearables devices. Second, reliance on an external reflector module (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E2\" title=\"Equation 2 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) to determine the sufficiency of intermediate tool calls may limit the extent of achievable latency improvements.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "latency",
                    "query",
                    "response",
                    "results",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the limitations of Fixed-Interval Streaming RAG and further optimize both efficiency and responsiveness, we propose a more adaptive approach: <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>.\nHere, the <em class=\"ltx_emph ltx_font_italic\">trigger</em> is learned: the model is trained to autonomously determine the optimal moments to initiate tool queries, issuing a query only when it encounters new or additional information as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.F3.sf2\" title=\"Figure 3(b) &#8227; Figure 3 &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>. In this formulation, the model receives user input in fixed chunk intervals as before and intelligently determines whether a tool call is needed after each block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>. The model can either:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p1.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> Predict\nNO_QUERY if a new tool query is unnecessary, or\n <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p1.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> Generate a new tool query. To make this decision, the model conditions on both the accumulated input speech <math alttext=\"Q_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">Q_{1:b}</annotation></semantics></math> (see Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E1\" title=\"Equation 1 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) as well as most recent tool query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}=\\hat{Q}^{T}_{\\max\\{\\,i&lt;b:\\hat{Q}^{T}_{i}\\neq\\text{NO\\_QUERY}\\,\\}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><mo>=</mo><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">{</mo><mrow><mrow><mi>i</mi><mo>&lt;</mo><mi>b</mi></mrow><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>i</mi><mi>T</mi></msubsup><mo>&#8800;</mo><mtext>NO_QUERY</mtext></mrow></mrow><mo lspace=\"0.170em\" stretchy=\"false\">}</mo></mrow></mrow><mi>T</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}=\\hat{Q}^{T}_{\\max\\{\\,i&lt;b:\\hat{Q}^{T}_{i}\\neq\\text{NO\\_QUERY}\\,\\}}</annotation></semantics></math> as shown:</p>\n\n",
                "matched_terms": [
                    "query",
                    "tool",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When a new query <math alttext=\"\\hat{Q}^{T}_{b}\\neq\\text{NO\\_QUERY}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m4\" intent=\":literal\"><semantics><mrow><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><mo>&#8800;</mo><mtext>NO_QUERY</mtext></mrow><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}\\neq\\text{NO\\_QUERY}</annotation></semantics></math>, the system immediately terminates any ongoing tool calls for the previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m5\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, ensuring that only a <em class=\"ltx_emph ltx_font_italic\">single tool call thread</em> runs at any given time. (Examples of generated <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m6\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> are provided in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13.T8\" title=\"Table 8 &#8227; 13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> in the Appendix.)\nThis approach offers several key advantages. First, it effectively eliminating redundant parallel threads and significantly reducing computational overhead. This is especially important for deployment on resource-constrained devices.\nSecond, this formulation removes the need for an external <em class=\"ltx_emph ltx_font_italic\">reflector</em> (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E2\" title=\"Equation 2 &#8227; 3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) module. The model confidently relies on the results <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> from the most recent tool call using <math alttext=\"\\hat{Q}^{\\text{prev}}_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m8\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>B</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{B}</annotation></semantics></math> to generate the spoken response <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m9\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by maximizing <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math>, reducing system complexity.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "query",
                    "response",
                    "results",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-training</span>: To train the model, we transform text-based tool usage benchmarks into spoken format (See S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13\" title=\"13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>). Word-level timestamps are computed using a pre-trained ASR model. For each partial ASR transcript <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math> up to block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, we generate corresponding queries for each tool <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m3\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> using an LLM as pseudo ground truth (GT).\nTo create effective training labels that teach the model when to trigger new queries, we employ a similarity-based labeling strategy where we compare the current pseudo GT query <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m4\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> with the most recent non-empty tool query label <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m5\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math> (see Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E3\" title=\"Equation 3 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) before block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m6\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>.\nOur labeling function assigns the training label <math alttext=\"\\hat{Q}^{T}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m7\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{T}_{b}</annotation></semantics></math> (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E3\" title=\"Equation 3 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) for the tool query after block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m8\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> as follows:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p2.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> when the current query is sufficiently similar to the previous query (as determined by manually defined heuristics <math alttext=\"f(\\cdots)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m9\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8943;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f(\\cdots)</annotation></semantics></math>), we assign the special label NO_QUERY to teach the model that no new tool call is needed.  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S3.SS2.SSS2.p2.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> When the queries are sufficiently different, we assign the actual pseudo ground truth query\n<math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m10\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> as the label to teach the model to trigger a new tool call:</p>\n\n",
                "matched_terms": [
                    "query",
                    "tool",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a multi-task fine-tuning strategy targeting two key capabilities.\nFirst, we train the model on Streaming Tool Query Generation by optimizing <math alttext=\"P(Q^{T}_{b}|Q_{1:b},\\hat{Q}^{\\text{prev}}_{b})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo fence=\"false\">|</mo><mrow><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><mo>,</mo><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}_{b}|Q_{1:b},\\hat{Q}^{\\text{prev}}_{b})</annotation></semantics></math> for <math alttext=\"b\\in[1,B]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m2\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mi>B</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b\\in[1,B]</annotation></semantics></math>, enabling intelligent decisions about when to trigger tool queries. Second, we fine-tune on Response Generation by optimizing <math alttext=\"P(A|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m3\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,R)</annotation></semantics></math> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) to improve the intelligibility of the speech output.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "query",
                    "response",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An important aspect of our post-training is enhancing the model&#8217;s ability to recover from errors in intermediate query predictions. For example, when presented with the audio question, &#8220;Who founded Rare Beauty in 2019?&#8221;, we observed that an initial misinterpretation of <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m1\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, such as &#8220;Red Bull founder&#8221;, can lead the model to subsequently generate NO_QUERY labels, effectively halting further attempts to retrieve the correct information. This issue arises because, during training, the model is always provided with correct previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m2\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math>, whereas during inference, it may make errors due to partial utterances <math alttext=\"{Q}_{1:b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>Q</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><annotation encoding=\"application/x-tex\">{Q}_{1:b}</annotation></semantics></math> being ambiguous. Thus the model lacks the ability to recover from such mistakes during inference.\nTo overcome this, we introduce a novel strategy in which we deliberately inject negative samples during post-training by substituting the previous query <math alttext=\"\\hat{Q}^{\\text{prev}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m4\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>Q</mi><mo>^</mo></mover><mi>b</mi><mtext>prev</mtext></msubsup><annotation encoding=\"application/x-tex\">\\hat{Q}^{\\text{prev}}_{b}</annotation></semantics></math> in Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E4\" title=\"Equation 4 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> with incorrect ones <math alttext=\"Q^{\\text{neg}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m5\" intent=\":literal\"><semantics><msubsup><mi>Q</mi><mi>b</mi><mtext>neg</mtext></msubsup><annotation encoding=\"application/x-tex\">Q^{\\text{neg}}_{b}</annotation></semantics></math>. Crucially, when we perform negative sampling, we fall back to the pseudo ground truth query <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> as the training label:</p>\n\n",
                "matched_terms": [
                    "query",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This approach explicitly teaches the model to recover from errors in intermediate query prediction, thereby maintaining accuracy (see T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T5\" title=\"Table 5 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> for ablation) while achieving latency savings.</p>\n\n",
                "matched_terms": [
                    "query",
                    "model",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To rigorously evaluate our proposed approach, we construct comprehensive benchmark datasets featuring spoken queries paired with simulated tool interactions. We begin with the CRAG dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>, which form the basis for our spoken version of CRAG, which we term <em class=\"ltx_emph ltx_font_italic\">AudioCRAG</em>. It consists of 2 distinct variants:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S4.SS1.p1.pic1\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> <span class=\"ltx_text ltx_font_bold\">Audio CRAG Synthetic</span>: To generate spoken queries, we use our in-house TTS system. We then apply a rigorous filtering procedure which results in a high-quality set of 1,862 spoken queries, which we refer to as the <em class=\"ltx_emph ltx_font_italic\">AudioCRAG-Synthetic</em> benchmark.  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"13.38\" id=\"S4.SS1.p1.pic2\" overflow=\"visible\" style=\"vertical-align:-2.23px\" version=\"1.1\" viewbox=\"0 0 13.38 13.38\" width=\"13.38\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)\"><path d=\"M 6.42 0 C 6.42 3.54 3.54 6.42 0 6.42 C -3.54 6.42 -6.42 3.54 -6.42 0 C -6.42 -3.54 -3.54 -6.42 0 -6.42 C 3.54 -6.42 6.42 -3.54 6.42 0 Z M 0 0\" style=\"fill:none\"/><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject height=\"8.92\" overflow=\"visible\" style=\"--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> <span class=\"ltx_text ltx_font_bold\">Audio CRAG Human</span>: To further enhance the realism and diversity of our evaluation, we introduce the AudioCRAG-Human benchmark, which consists of 618 human-recorded spoken queries. We will release\nAudio CRAG Human benchmark upon acceptance to supports the development of more natural and reliable voice assistants.\nFurther details on the construction of these benchmarks are provided in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S9\" title=\"9 Audio CRAG Benchmark &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. We follow the CRAG setup to incorporate both web and KG-based tools, and adopt its robust evaluation methodology, as described in Secs. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S10\" title=\"10 Tool Usage Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S11\" title=\"11 Evaluation Setting &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.\nAdditionally, we leverage a random subset of 16,000 questions from the text-based factual question answering dataset TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib15\" title=\"\">2017</a>)</cite> to post-train our speech-in, speech-out models, as detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S13\" title=\"13 Post Training Data Preparation &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "sec",
                    "tool",
                    "results",
                    "audiocragsynthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform an ablation study (referred to as &#8220;Tool Integration&#8221; in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) where we post-train the model on sequential query generation (i.e. <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) and output generation, to assess the impact of streaming RAG post-training versus standard post-training on final response generation.\nAdditionally, we conduct an ablation study on open book setting (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) using a self-cascade approach with a three-stage inference pipeline: (1) the audio question is used to generate a tool query <math alttext=\"Q^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><msup><mi>Q</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">Q^{T}</annotation></semantics></math> (corresponding to the &#8220;Query Generation&#8221; stage described in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>); (2) the audio question, and tool results are combined to produce the final text output <math alttext=\"X^{\\text{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>res</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{res}}</annotation></semantics></math> by maximizing <math alttext=\"P(X^{\\text{res}}|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mtext>res</mtext></msup><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(X^{\\text{res}}|Q,R)</annotation></semantics></math>; and (3) the audio question, and final text output are used to generate the final speech output <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by optimizing <math alttext=\"P(A|Q,X^{\\text{res}})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m6\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><msup><mi>X</mi><mtext>res</mtext></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,X^{\\text{res}})</annotation></semantics></math>. Since we teacher-force the text output to obtain the final speech output in stage (3), this self-cascade approach can only be applied to a &#8220;thinker-talker&#8221; architecture (eq. Qwen-OMNI) or Chain-of-Thought (CoT) style architectures (eg. OpusLM).\nThe motivation for this ablation is to investigate whether the inclusion of RAG references <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> during the &#8220;Response Generation&#8221; stage (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) affects the quality of the generated speech <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m8\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "setting",
                    "book",
                    "open",
                    "query",
                    "response",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a comprehensive performance comparison of three models, Qwen2.5-7B, OpusLM, and Kimi Audio, evaluated across three settings: Closed Book, Open Book, and Streaming RAG. All models are assessed on both the AudioCRAG-Synthetic (Syn.) and AudioCRAG-Human (Hum.). In the Closed Book setting, where models rely solely on their internal knowledge without access to external tools (reference length = 0), all models achieve accuracy scores below 20%. These results highlight the inherent limitations of closed-book approaches in handling complex queries.\nThe Open Book setting, which provides models with access to external information, demonstrates the clear benefits of tool integration. Qwen2.5-7B and Kimi Audio&#8217;s accuracy rises substantially, underscoring the value of leveraging external context. As expected, latency increases due to the additional overhead of retrieving information from external tools (See T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> for detailed latency analysis).</p>\n\n",
                "matched_terms": [
                    "tool",
                    "setting",
                    "open",
                    "book",
                    "latency",
                    "qwen257b",
                    "audiocragsynthetic",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most notably, our post-training approach to build <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, as described in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>, delivers significant advancements in both accuracy and efficiency. Qwen2.5-7B and OpusLM achieve significant accuracy improvements across both benchmarks. While the absolute accuracy scores may appear low, they are consistent with the evaluation results observed in the CRAG benchmark. Notably, Qwen2.5-7B with <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> achieves accuracy comparable to the open book performance of similarly sized LLMs reported in the original CRAG paper (i.e., 34.2 for Qwen2.5-7B vs. 32.1 for LLAMA-3 8B Instruct in <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>).\nImportantly, although post-training is performed exclusively on the synthetic dataset, we observe consistent and even greater improvements on the human-spoken benchmark, demonstrating the robustness and strong generalization capabilities of our method. This setting also introduces substantial latency savings compared to the Open Book configuration, with Qwen2.5-7B and OpusLM achieving 20.7% and 14.8% reductions in first-token latency on the synthetic benchmark, and even greater savings on the human benchmark\n<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Our latency calculations on synthetic audio exclude end-point detection latency, which is required in all production systems. Since our streaming RAG approach enables processing without waiting for end-point detection, including this factor would further amplify the observed latency savings, as observed in higher latency savings for human-spoken audio, where endpoint detection errors often introduce trailing silence.</span></span></span>. These results demonstrate that our streaming RAG approach not only advances the accuracy of speech-in speech-out systems, but also optimizes response efficiency by enabling earlier and more effective prediction of tool queries.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "setting",
                    "book",
                    "qwen257b",
                    "open",
                    "latency",
                    "response",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Evaluated SOTA Speech-in Speech-out Models &#8227; 4 Experiment Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides a comparative evaluation of the three SOTA models in generating either text or speech outputs from speech inputs, both with and without the integration of external tool results. In the absence of tool results (Ref length = 0), all models achieve higher accuracy when generating text outputs compared to speech outputs. The incorporation of tool results generally leads to improved text generation performance, with Kimi Audio achieving the highest accuracy. In contrast, the accuracy for speech output remains consistently lower across all models and conditions. The self-cascade approach, in which the model first generates an intermediate text response before producing the final speech output, provides moderate improvements in speech output accuracy for both Qwen2.5-7B and OpusLM.\nHowever, it still underperforms compared to the text-out baseline, primarily due to errors in accurately generating answers involving uncommon entity nouns.\nOverall, these findings underscore a persistent challenge in direct speech generation, particularly when tool results are integrated, as this appears to negatively impact the quality of generated speech responses.\nOur <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> demonstrates clear advantages: it maintains comparable performance for text output and delivers substantial improvements for speech output, even outperforming the self-cascade approach. These results underscore the effectiveness of post-training in overcoming challenges in direct speech generation in tool-augmented scenarios.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "qwen257b",
                    "response",
                    "results",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Streaming RAG approaches</span>: Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> highlights the substantial latency savings enabled by the <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>), compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS1\" title=\"3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>). Three key metrics are evaluated: overall latency savings, the percentage of queries benefiting from reduced latency, and the number of parallel threads required.\nEven without any post-training, the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach already reduces tool usage latency (3.37s in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) by 10.7% for Open Book Qwen-OMNI, demonstrating its flexibility and plug-and-play compatibility with any existing speech-in speech-out model.\nThe <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> method, utilizing Qwen-OMNI, consistently delivers superior performance. It achieves greater average latency reductions and benefits a higher proportion of queries with improved response times. Notably, <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> require only a single parallel thread, representing a significant advancement in resource efficiency compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach, which demands multiple parallel threads.\nThese findings highlight the effectiveness of <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> in not only minimizing latency, validating our intuition from Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>, but also in optimizing system efficiency and resource utilization.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "book",
                    "latency",
                    "open",
                    "response"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-training Strategies</span>: Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents performance comparison under different post-training conditions. Incorporating streaming tool query generation during post-training (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>) results in comparable performance for both models. These results suggest that <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> can be integrated into post-training process without negatively impacting model performance.</p>\n\n",
                "matched_terms": [
                    "query",
                    "model",
                    "tool",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation Results for Negative Sampling Strategy</span>: Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T5\" title=\"Table 5 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the ablation results evaluating the impact of deliberately injecting negative samples during post-training (Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.E5\" title=\"Equation 5 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). The findings highlight that, without negative sampling, streaming tool query generation can reduce final accuracy in text output settings, primarily due to errors in final query generation, as detailed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>. In contrast, our negative sampling approach significantly enhances the model&#8217;s robustness, enabling it to recover from intermediate prediction errors. This leads to consistently high accuracy while also achieving notable latency reductions (Tab. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "query",
                    "tool",
                    "results",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced the first comprehensive approach for integrating external tool usage directly into E2E speech-in, speech-out dialogue systems. By incorporating <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> pipeline, we further enhanced the model&#8217;s ability to leverage retrieved information and autonomously decide when to trigger new tool queries, resulting in improved accuracy and responsiveness.\nEmpirical evaluation on the newly introduced AudioCRAG benchmark demonstrated that tool integration can more than double factual question answering accuracy compared to closed-book models. Additionally, our streaming RAG approach achieved a 20% reduction in tool usage latency, thereby preserving natural conversational flow.\nOverall, our contributions advance the state of the art in spoken dialogue systems by enabling accurate, real-time, and tool-augmented voice assistants. In addition, we are committed to open science and will release our training code and Audio CRAG Human benchmark to support future research and development in this area.</p>\n\n",
                "matched_terms": [
                    "latency",
                    "tool",
                    "first",
                    "open"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We begin with the CRAG dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>, which contains 2,706 text queries. Since these queries are not directly suitable for speech-based evaluation, we first identify those requiring adaptation before TTS conversion. Through careful manual inspection, we determine that queries containing dates or brackets benefit from rewriting to ensure naturalness and clarity in spoken form. In total, we identify 569 such queries and rewrite them using a large language model (LLAMA-4 Maverick). The resulting 569 rewritten queries, combined with the remaining original queries, form the basis for our spoken version of CRAG, which we term <em class=\"ltx_emph ltx_font_italic\">AudioCRAG</em>. We follow the CRAG setup to incorporate web and KG-based tools and adopt its robust evaluation setup, as detailed in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S10\" title=\"10 Tool Usage Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S11\" title=\"11 Evaluation Setting &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "total",
                    "model",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio CRAG Synthetic</span>:\nTo generate spoken queries, we process all 2,706 queries through our in-house TTS system. We then apply a rigorous filtering procedure to remove queries with intelligibility issues, specifically excluding any utterances for which Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib27\" title=\"\">2023</a>)</cite> hypotheses exhibit a non-zero word error rate. We also remove utterances with suboptimal audio quality, as determined by UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib28\" title=\"\">2022</a>)</cite> scores below 3.5. This results in a high-quality set of 1,862 spoken queries, which we refer to as the <em class=\"ltx_emph ltx_font_italic\">AudioCRAG-Synthetic</em> benchmark.</p>\n\n",
                "matched_terms": [
                    "audiocragsynthetic",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio CRAG Human</span>:\nTo further enhance the realism and diversity of our evaluation, we introduce the AudioCRAG-Human benchmark, which consists of 618 human-recorded spoken queries. These queries are recorded by a diverse pool of participants to capture natural variations in speech, accent, and prosody. The inclusion of human-recorded audio enables a more comprehensive assessment of speech-in speech-out systems under real-world conditions, providing valuable insights into model robustness and generalization beyond synthetic speech. This benchmark serves as a critical resource for evaluating the effectiveness of tool integration in conversational AI systems.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable effective tool usage, we build upon the CRAG framework by integrating two complementary information sources: web search, which provides access to fresh and dynamic content, and a knowledge graph, which offers structured and reliable information. For web search, we aggregate all 100,000 documents from the CRAG corpus and employ a BGE-based re-ranker<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/BAAI/bge-large-en-v1.5\" title=\"\">https://huggingface.co/BAAI/bge-large-en-v1.5</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Xiao et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib33\" title=\"\">2023</a>)</cite> to index and retrieve the top 50 most relevant documents for each query. These documents are then segmented into chunks and re-ranked using the same BGE model based on their similarity to the query, ensuring highly contextually relevant retrieval. Meanwhile, queries to the knowledge graph are performed via a simulated API, adhering to the methodology established in CRAG <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/facebookresearch/CRAG/tree/main/mock_api\" title=\"\">https://github.com/facebookresearch/CRAG/tree/main/mock_api</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "query",
                    "tool",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Similar to previous work <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>, we employ model-based automatic evaluation. We use a three-way scoring system, assigning scores of 1, -1, and 0 for accurate, incorrect, and missing answers, respectively. The evaluation is conducted using the Llama 4-maverick LLM evaluator. For speech outputs, we first transcribe the audio using Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib27\" title=\"\">2023</a>)</cite> before passing the transcriptions to the LLM evaluator. In this study, our primary focus is on enhancing system accuracy; therefore, we report average accuracy values in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Evaluated SOTA Speech-in Speech-out Models &#8227; 4 Experiment Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. For additional context, we also provide the average rates of accurate, hallucinated, and missing responses, as well as overall truthfulness scores for each system in the Open Book Setting (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S10.T6\" title=\"Table 6 &#8227; 10 Tool Usage Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>). Notably, our results indicate that Qwen-OMNI was less likely to generate hallucinated responses compared to OpusLM and Kimi Audio.</p>\n\n",
                "matched_terms": [
                    "setting",
                    "book",
                    "open",
                    "results",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qwen-OMNI</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>)</cite> is a end-to-end multimodal model that seamlessly integrates diverse input modalities&#8212;including text, images, audio, and video&#8212;and generates both text and natural speech responses in a real-time streaming fashion. It leverages an innovative Thinker-Talker architecture, where the Thinker module performs high-level reasoning to produce a text response, which is then used by the Talker module, conditioning on both the text and the Thinker&#8217;s hidden representations, to generate streaming speech output.</p>\n\n",
                "matched_terms": [
                    "response",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For tool-augmented scenarios, retrieval results are provided up to each model&#8217;s maximum token limit (&#8220;Ref length&#8221; in Tables), maintaining a 2:1 ratio of web page to KG results.\nSpecifically, we observe that Kimi-Audio is currently optimized for handling tool result references up to a certain length. When this limit is exceeded, an error arises during the audio detokenization process, specifically within the rotary embedding mechanism, highlighting an architectural constraint in processing longer input sequences or larger reference contexts. Addressing this limitation presents a valuable opportunity for future model enhancements.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tool",
                    "results",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation encompasses both speech-in-text-out and speech-in-speech-out scenarios. For the Fixed-Interval Streaming RAG setting (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS1\" title=\"3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>), intermediate tool queries are generated at consistent 1-second intervals. In the Model-Triggered Streaming RAG setting, the model dynamically determines the need for a tool call after processing each 500ms block. This approach allows us to utilize a smaller chunk size, as only a single tool call thread is required for Model-Triggered Streaming RAG, thereby enabling more efficient and responsive processing.</p>\n\n",
                "matched_terms": [
                    "tool",
                    "model",
                    "setting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This subsection details the experimental setup for post-training the pretrained speech-in, speech-out model to significantly enhance its tool usage capabilities, as outlined in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>.\nWe leverage a random subset of 16,000 questions from the text-based factual question answering dataset TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib15\" title=\"\">2017</a>)</cite>, which contains 97,000 questions and 662,659 associated web documents.\nFor <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, we further compute word-level timestamps using a pre-trained ASR model, OWSM CTC v4 1B <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib26\" title=\"\">2025</a>)</cite>, enabling us to generate partial ASR transcripts <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m1\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math> at 500 ms intervals. Note that if word occurs at boundary of block <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, it is excluded from <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math>. For each partial transcript <math alttext=\"X^{\\text{asr}}_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>b</mi><mtext>asr</mtext></msubsup><annotation encoding=\"application/x-tex\">X^{\\text{asr}}_{b}</annotation></semantics></math>, we generate corresponding psuedo ground truth queries <math alttext=\"\\overline{Q^{T}_{b}}\" class=\"ltx_Math\" display=\"inline\" id=\"S13.p1.m5\" intent=\":literal\"><semantics><mover accent=\"true\"><msubsup><mi>Q</mi><mi>b</mi><mi>T</mi></msubsup><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{Q^{T}_{b}}</annotation></semantics></math> using LLAMA-4-Maverick to simulate incremental user input.\nTo simulate realistic tool usage, we concatenate all documents and employ a web query reranker to retrieve the top 50 most relevant documents for each query. The text questions from this 16k subset are converted into discrete speech tokens using text-to-speech synthesis with the corresponding pretrained speech-in, speech-out model. Recognizing that TriviaQA answers are typically single named entities, we further transform the queries into a conversational style using LLAMA-4-Maverick, making them more suitable for dialogue-based evaluation.</p>\n\n",
                "matched_terms": [
                    "query",
                    "tool",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PROMPT = \"\"\"\nYou are an agent that only outputs JSON. You are given a Query and Query Time. Do the following:\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>1) Determine the domain the query is about. The domain should be one of the following: \"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". If none of the domains apply, use \"other\". Use \"domain\" as the key in the result json.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>2) Extract structured information from the query. Include different keys into the result json depending on the domains, and put them DIRECTLY in the result json. Here are the rules:\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;encyclopedia&#8217; and &#8216;other&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;main_entity&#8217;: extract the main entity of the query.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;finance&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;market_identifier&#8217;: stock identifiers including individual company names, stock symbols.\n<br class=\"ltx_break\"/>- &#8216;metric&#8217;: financial metrics that the query is asking about. This must be one of the following: &#8216;price&#8217;, &#8216;dividend&#8217;, &#8216;P/E ratio&#8217;, &#8216;EPS&#8217;, &#8216;marketCap&#8217;, and &#8216;other&#8217;.\n<br class=\"ltx_break\"/>- &#8216;datetime&#8217;: time frame that the query asks about. When datetime is not explicitly mentioned, use &#8216;Query Time&#8217; as default.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;movie&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;movie_name&#8217;: name of the movie\n<br class=\"ltx_break\"/>- &#8216;movie_aspect&#8217;: if the query is about a movie, which movie aspect the query asks. This must be one of the following: &#8216;budget&#8217;, &#8216;genres&#8217;, &#8216;original_language&#8217;, &#8216;original_title&#8217;, &#8216;release_date&#8217;, &#8216;revenue&#8217;, &#8216;title&#8217;, &#8216;cast&#8217;, &#8216;crew&#8217;, &#8216;rating&#8217;, &#8216;length&#8217;.\n<br class=\"ltx_break\"/>- &#8216;person&#8217;: person name related to moves\n<br class=\"ltx_break\"/>- &#8216;person_aspect&#8217;: if the query is about a person, which person aspect the query asks. This must be one of the following: &#8216;acted_movies&#8217;, &#8216;directed_movies&#8217;, &#8216;oscar_awards&#8217;, &#8216;birthday&#8217;.\n<br class=\"ltx_break\"/>- &#8216;year&#8217;: if the query is about movies released in a specific year, extract the year\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;music&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;artist_name&#8217;: name of the artist\n<br class=\"ltx_break\"/>- &#8216;artist_aspect&#8217;: if the query is about an artist, extract the aspect of the artist. This must be one of the following: &#8216;member&#8217;, &#8216;birth place&#8217;, &#8216;birth date&#8217;, &#8216;lifespan&#8217;, &#8216;artist work&#8217;, &#8216;grammy award count&#8217;, &#8216;grammy award date&#8217;.\n<br class=\"ltx_break\"/>- &#8216;song_name&#8217;: name of the song\n<br class=\"ltx_break\"/>- &#8216;song_aspect&#8217;: if the query is about a song, extract the aspect of the song. This must be one of the following: &#8216;author&#8217;, &#8216;grammy award count&#8217;, &#8216;release country&#8217;, &#8216;release date&#8217;.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>For &#8216;sports&#8217; queries, these are possible keys:\n<br class=\"ltx_break\"/>- &#8216;sport_type&#8217;: one of &#8216;basketball&#8216;, &#8216;soccer&#8216;, &#8216;other&#8216;\n<br class=\"ltx_break\"/>- &#8216;tournament&#8217;: NBA, World Cup, Olympic.\n<br class=\"ltx_break\"/>- &#8216;team&#8217;: teams that users are interested in.\n<br class=\"ltx_break\"/>- &#8216;datetime&#8217;: time frame that the user is interested in. When datetime is not explicitly mentioned, use &#8216;Query Time&#8217; as default.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Return the results in a FLAT json.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n<br class=\"ltx_break\"/>\"\"\"</p>\n\n",
                "matched_terms": [
                    "query",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When rewriting the query, ensure that it captures all important information from the original question that could impact the retrieval results. Do not omit any crucial details, such as specific dates, locations, or relationships between entities. Also, do not invent any new details on your own. If necessary, use the Query Time to provide context for the query. The goal is to create a concise and accurate query that effectively conveys the user&#8217;s intent and retrieves relevant information.\n*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT THE REWRITTEN QUERY!!!*\n\"\"\"</p>\n\n",
                "matched_terms": [
                    "query",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When rewriting the query, ensure that it captures all important information from the original question that could impact the retrieval results. Do not omit any crucial details, such as specific dates, locations, or relationships between entities. Also, do not invent any new details on your own. If necessary, use the Query Time to provide context for the query. The goal is to create a concise and accurate query that effectively conveys the user&#8217;s intent and retrieves relevant information.\nNow, compare the new web query to the previously generated web query (&#8217;Previous Result&#8217;).</p>\n\n",
                "matched_terms": [
                    "query",
                    "results"
                ]
            }
        ]
    },
    "S15.T10": {
        "source_file": "Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage",
        "caption": "Table 10: Post-training Parameters of OpusLM",
        "body": "Parameter\n\n\n\n\nValue\n\n\n\n\n\n\n\n\ntrain_micro_batch_size_per_gpu\n\n\n\n\n1\n\n\n\n\n\n\ngradient_accumulation_steps\n\n\n\n\n2\n\n\n\n\n\n\nepochs\n\n\n\n\n2\n\n\n\n\n\n\ngradient_clipping\n\n\n\n\n1.0\n\n\n\n\n\n\nbf16 enabled\n\n\n\n\ntrue\n\n\n\n\n\n\noptimizer type\n\n\n\n\nAdam\n\n\n\n\n\n\noptimizer lr\n\n\n\n\n0.00001\n\n\n\n\n\n\noptimizer betas\n\n\n\n\n[0.9, 0.95]\n\n\n\n\n\n\noptimizer eps\n\n\n\n\n1e-8\n\n\n\n\n\n\noptimizer weight_decay\n\n\n\n\n3e-7\n\n\n\n\n\n\noptimizer adam_w_mode\n\n\n\n\ntrue\n\n\n\n\n\n\nscheduler type\n\n\n\n\nWarmupDecayLR\n\n\n\n\n\n\nscheduler warmup_type\n\n\n\n\nlinear\n\n\n\n\n\n\nscheduler total_num_steps\n\n\n\n\n21534\n\n\n\n\n\n\nscheduler warmup_num_steps\n\n\n\n\n1077\n\n\n\n\n\n\nscheduler warmup_min_lr\n\n\n\n\n0\n\n\n\n\n\n\nscheduler warmup_max_lr\n\n\n\n\n0.00001",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\"><span class=\"ltx_text ltx_font_bold\">Parameter</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\"><span class=\"ltx_text ltx_font_bold\">Value</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">train_micro_batch_size_per_gpu</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">1</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">gradient_accumulation_steps</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">2</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">epochs</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">2</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">gradient_clipping</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">1.0</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">bf16 enabled</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">true</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">optimizer type</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">Adam</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">optimizer lr</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">0.00001</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">optimizer betas</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">[0.9, 0.95]</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">optimizer eps</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">1e-8</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">optimizer weight_decay</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">3e-7</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">optimizer adam_w_mode</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">true</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">scheduler type</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">WarmupDecayLR</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">scheduler warmup_type</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">linear</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">scheduler total_num_steps</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">21534</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">scheduler warmup_num_steps</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">1077</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">scheduler warmup_min_lr</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">0</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">scheduler warmup_max_lr</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">0.00001</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "gradientclipping",
            "type",
            "parameter",
            "trainmicrobatchsizepergpu",
            "betas",
            "weightdecay",
            "warmuptype",
            "warmupminlr",
            "adam",
            "opuslm",
            "true",
            "3e7",
            "scheduler",
            "1e8",
            "bf16",
            "eps",
            "enabled",
            "totalnumsteps",
            "warmupnumsteps",
            "epochs",
            "adamwmode",
            "posttraining",
            "gradientaccumulationsteps",
            "warmupmaxlr",
            "value",
            "optimizer",
            "linear",
            "parameters",
            "warmupdecaylr"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution 2</span>: To address this, we propose <em class=\"ltx_emph ltx_font_italic\">Streaming Retrieval-Augmented Generation</em> (<em class=\"ltx_emph ltx_font_italic\">Streaming RAG</em>), the <em class=\"ltx_emph ltx_font_italic\">first</em> framework that empowers the system to trigger tool queries in parallel with user speech, even before the user finishes speaking. Within this framework, we introduce two novel approaches: (1) <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em>, which issues tool queries at regular intervals during speech input and carefully examines quality of retrieval results on the full query to guarantee response quality, and can be incorporated into any speech-in, speech-out model without post-training; (2) <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, which post-trains the model to intelligently determine optimal query timing based on the evolving user utterance to save computation resources.\nOur results demonstrate that our proposed <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> delivers over 200% relative improvement in accuracy (from 11.1% to 34.2% absolute in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) compared to the no-tool baseline, while also reducing tool result generation latency by 20%. Though designed for speech-in speech-out systems, streaming RAG can also be adapted in cascaded SDS, or even chatbots as users type.</p>\n\n",
                "matched_terms": [
                    "type",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform an ablation study (referred to as &#8220;Tool Integration&#8221; in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) where we post-train the model on sequential query generation (i.e. <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) and output generation, to assess the impact of streaming RAG post-training versus standard post-training on final response generation.\nAdditionally, we conduct an ablation study on open book setting (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) using a self-cascade approach with a three-stage inference pipeline: (1) the audio question is used to generate a tool query <math alttext=\"Q^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><msup><mi>Q</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">Q^{T}</annotation></semantics></math> (corresponding to the &#8220;Query Generation&#8221; stage described in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>); (2) the audio question, and tool results are combined to produce the final text output <math alttext=\"X^{\\text{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>res</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{res}}</annotation></semantics></math> by maximizing <math alttext=\"P(X^{\\text{res}}|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mtext>res</mtext></msup><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(X^{\\text{res}}|Q,R)</annotation></semantics></math>; and (3) the audio question, and final text output are used to generate the final speech output <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by optimizing <math alttext=\"P(A|Q,X^{\\text{res}})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m6\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><msup><mi>X</mi><mtext>res</mtext></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,X^{\\text{res}})</annotation></semantics></math>. Since we teacher-force the text output to obtain the final speech output in stage (3), this self-cascade approach can only be applied to a &#8220;thinker-talker&#8221; architecture (eq. Qwen-OMNI) or Chain-of-Thought (CoT) style architectures (eg. OpusLM).\nThe motivation for this ablation is to investigate whether the inclusion of RAG references <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> during the &#8220;Response Generation&#8221; stage (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) affects the quality of the generated speech <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m8\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "opuslm",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a comprehensive performance comparison of three models, Qwen2.5-7B, OpusLM, and Kimi Audio, evaluated across three settings: Closed Book, Open Book, and Streaming RAG. All models are assessed on both the AudioCRAG-Synthetic (Syn.) and AudioCRAG-Human (Hum.). In the Closed Book setting, where models rely solely on their internal knowledge without access to external tools (reference length = 0), all models achieve accuracy scores below 20%. These results highlight the inherent limitations of closed-book approaches in handling complex queries.\nThe Open Book setting, which provides models with access to external information, demonstrates the clear benefits of tool integration. Qwen2.5-7B and Kimi Audio&#8217;s accuracy rises substantially, underscoring the value of leveraging external context. As expected, latency increases due to the additional overhead of retrieving information from external tools (See T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> for detailed latency analysis).</p>\n\n",
                "matched_terms": [
                    "opuslm",
                    "value"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most notably, our post-training approach to build <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em>, as described in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>, delivers significant advancements in both accuracy and efficiency. Qwen2.5-7B and OpusLM achieve significant accuracy improvements across both benchmarks. While the absolute accuracy scores may appear low, they are consistent with the evaluation results observed in the CRAG benchmark. Notably, Qwen2.5-7B with <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> achieves accuracy comparable to the open book performance of similarly sized LLMs reported in the original CRAG paper (i.e., 34.2 for Qwen2.5-7B vs. 32.1 for LLAMA-3 8B Instruct in <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib38\" title=\"\">2024</a>)</cite>).\nImportantly, although post-training is performed exclusively on the synthetic dataset, we observe consistent and even greater improvements on the human-spoken benchmark, demonstrating the robustness and strong generalization capabilities of our method. This setting also introduces substantial latency savings compared to the Open Book configuration, with Qwen2.5-7B and OpusLM achieving 20.7% and 14.8% reductions in first-token latency on the synthetic benchmark, and even greater savings on the human benchmark\n<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Our latency calculations on synthetic audio exclude end-point detection latency, which is required in all production systems. Since our streaming RAG approach enables processing without waiting for end-point detection, including this factor would further amplify the observed latency savings, as observed in higher latency savings for human-spoken audio, where endpoint detection errors often introduce trailing silence.</span></span></span>. These results demonstrate that our streaming RAG approach not only advances the accuracy of speech-in speech-out systems, but also optimizes response efficiency by enabling earlier and more effective prediction of tool queries.</p>\n\n",
                "matched_terms": [
                    "opuslm",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Evaluated SOTA Speech-in Speech-out Models &#8227; 4 Experiment Setup &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides a comparative evaluation of the three SOTA models in generating either text or speech outputs from speech inputs, both with and without the integration of external tool results. In the absence of tool results (Ref length = 0), all models achieve higher accuracy when generating text outputs compared to speech outputs. The incorporation of tool results generally leads to improved text generation performance, with Kimi Audio achieving the highest accuracy. In contrast, the accuracy for speech output remains consistently lower across all models and conditions. The self-cascade approach, in which the model first generates an intermediate text response before producing the final speech output, provides moderate improvements in speech output accuracy for both Qwen2.5-7B and OpusLM.\nHowever, it still underperforms compared to the text-out baseline, primarily due to errors in accurately generating answers involving uncommon entity nouns.\nOverall, these findings underscore a persistent challenge in direct speech generation, particularly when tool results are integrated, as this appears to negatively impact the quality of generated speech responses.\nOur <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> demonstrates clear advantages: it maintains comparable performance for text output and delivers substantial improvements for speech output, even outperforming the self-cascade approach. These results underscore the effectiveness of post-training in overcoming challenges in direct speech generation in tool-augmented scenarios.</p>\n\n",
                "matched_terms": [
                    "opuslm",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Streaming RAG approaches</span>: Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> highlights the substantial latency savings enabled by the <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>), compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS1\" title=\"3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>). Three key metrics are evaluated: overall latency savings, the percentage of queries benefiting from reduced latency, and the number of parallel threads required.\nEven without any post-training, the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach already reduces tool usage latency (3.37s in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) by 10.7% for Open Book Qwen-OMNI, demonstrating its flexibility and plug-and-play compatibility with any existing speech-in speech-out model.\nThe <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> method, utilizing Qwen-OMNI, consistently delivers superior performance. It achieves greater average latency reductions and benefits a higher proportion of queries with improved response times. Notably, <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> require only a single parallel thread, representing a significant advancement in resource efficiency compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach, which demands multiple parallel threads.\nThese findings highlight the effectiveness of <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> in not only minimizing latency, validating our intuition from Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>, but also in optimizing system efficiency and resource utilization.</p>\n\n",
                "matched_terms": [
                    "enabled",
                    "posttraining"
                ]
            }
        ]
    },
    "S15.T11": {
        "source_file": "Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage",
        "caption": "Table 11: Post-training Parameters of Qwen-OMNI Thinker",
        "body": "Parameter\n\n\n\n\nValue\n\n\n\n\n\n\n\n\nbf16\n\n\n\n\nTrue\n\n\n\n\n\n\ngradient_accumulation_steps\n\n\n\n\n4\n\n\n\n\n\n\nepochs\n\n\n\n\n1\n\n\n\n\n\n\ngradient_clipping\n\n\n\n\n1.0\n\n\n\n\n\n\nlearning_rate\n\n\n\n\n7e-6\n\n\n\n\n\n\nlr_scheduler_type\n\n\n\n\ncosine\n\n\n\n\n\n\nwarmup_ratio\n\n\n\n\n0.05\n\n\n\n\n\n\nper_device_train_batch_size\n\n\n\n\n1\n\n\n\n\n\n\nweight_decay\n\n\n\n\n0.01",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\"><span class=\"ltx_text ltx_font_bold\">Parameter</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\"><span class=\"ltx_text ltx_font_bold\">Value</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">bf16</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">True</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">gradient_accumulation_steps</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">4</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">epochs</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">1</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">gradient_clipping</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">1.0</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">learning_rate</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">7e-6</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">lr_scheduler_type</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">cosine</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">warmup_ratio</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">0.05</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">per_device_train_batch_size</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">1</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">weight_decay</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">0.01</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "gradientaccumulationsteps",
            "7e6",
            "gradientclipping",
            "learningrate",
            "lrschedulertype",
            "qwenomni",
            "bf16",
            "epochs",
            "value",
            "parameter",
            "true",
            "perdevicetrainbatchsize",
            "thinker",
            "weightdecay",
            "warmupratio",
            "cosine",
            "parameters",
            "posttraining"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We perform an ablation study (referred to as &#8220;Tool Integration&#8221; in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) where we post-train the model on sequential query generation (i.e. <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) and output generation, to assess the impact of streaming RAG post-training versus standard post-training on final response generation.\nAdditionally, we conduct an ablation study on open book setting (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) using a self-cascade approach with a three-stage inference pipeline: (1) the audio question is used to generate a tool query <math alttext=\"Q^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><msup><mi>Q</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">Q^{T}</annotation></semantics></math> (corresponding to the &#8220;Query Generation&#8221; stage described in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>); (2) the audio question, and tool results are combined to produce the final text output <math alttext=\"X^{\\text{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>res</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{res}}</annotation></semantics></math> by maximizing <math alttext=\"P(X^{\\text{res}}|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mtext>res</mtext></msup><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(X^{\\text{res}}|Q,R)</annotation></semantics></math>; and (3) the audio question, and final text output are used to generate the final speech output <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by optimizing <math alttext=\"P(A|Q,X^{\\text{res}})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m6\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><msup><mi>X</mi><mtext>res</mtext></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,X^{\\text{res}})</annotation></semantics></math>. Since we teacher-force the text output to obtain the final speech output in stage (3), this self-cascade approach can only be applied to a &#8220;thinker-talker&#8221; architecture (eq. Qwen-OMNI) or Chain-of-Thought (CoT) style architectures (eg. OpusLM).\nThe motivation for this ablation is to investigate whether the inclusion of RAG references <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> during the &#8220;Response Generation&#8221; stage (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) affects the quality of the generated speech <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m8\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "qwenomni",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Streaming RAG approaches</span>: Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> highlights the substantial latency savings enabled by the <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>), compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS1\" title=\"3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>). Three key metrics are evaluated: overall latency savings, the percentage of queries benefiting from reduced latency, and the number of parallel threads required.\nEven without any post-training, the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach already reduces tool usage latency (3.37s in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) by 10.7% for Open Book Qwen-OMNI, demonstrating its flexibility and plug-and-play compatibility with any existing speech-in speech-out model.\nThe <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> method, utilizing Qwen-OMNI, consistently delivers superior performance. It achieves greater average latency reductions and benefits a higher proportion of queries with improved response times. Notably, <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> require only a single parallel thread, representing a significant advancement in resource efficiency compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach, which demands multiple parallel threads.\nThese findings highlight the effectiveness of <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> in not only minimizing latency, validating our intuition from Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>, but also in optimizing system efficiency and resource utilization.</p>\n\n",
                "matched_terms": [
                    "qwenomni",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qwen-OMNI</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>)</cite> is a end-to-end multimodal model that seamlessly integrates diverse input modalities&#8212;including text, images, audio, and video&#8212;and generates both text and natural speech responses in a real-time streaming fashion. It leverages an innovative Thinker-Talker architecture, where the Thinker module performs high-level reasoning to produce a text response, which is then used by the Talker module, conditioning on both the text and the Thinker&#8217;s hidden representations, to generate streaming speech output.</p>\n\n",
                "matched_terms": [
                    "thinker",
                    "qwenomni"
                ]
            }
        ]
    },
    "S15.T12": {
        "source_file": "Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage",
        "caption": "Table 12: Post-training Parameters of Qwen-OMNI Talker",
        "body": "Parameter\n\n\n\n\nValue\n\n\n\n\n\n\n\n\nbf16\n\n\n\n\nTrue\n\n\n\n\n\n\ngradient_accumulation_steps\n\n\n\n\n4\n\n\n\n\n\n\ngradient_clipping\n\n\n\n\n1.0\n\n\n\n\n\n\nepochs\n\n\n\n\n2\n\n\n\n\n\n\nlearning_rate\n\n\n\n\n5e-5\n\n\n\n\n\n\nper_device_train_batch_size\n\n\n\n\n1\n\n\n\n\n\n\nlr_scheduler_type\n\n\n\n\nlinear\n\n\n\n\n\n\nwarmup_ratio\n\n\n\n\n0.0\n\n\n\n\n\n\nweight_decay\n\n\n\n\n0.01",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\"><span class=\"ltx_text ltx_font_bold\">Parameter</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\"><span class=\"ltx_text ltx_font_bold\">Value</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">bf16</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">True</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">gradient_accumulation_steps</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">4</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">gradient_clipping</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">1.0</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">epochs</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">2</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">learning_rate</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">5e-5</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">per_device_train_batch_size</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">1</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">lr_scheduler_type</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">linear</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">warmup_ratio</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">0.0</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\">weight_decay</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">0.01</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "gradientaccumulationsteps",
            "talker",
            "gradientclipping",
            "learningrate",
            "lrschedulertype",
            "qwenomni",
            "bf16",
            "epochs",
            "value",
            "parameter",
            "true",
            "perdevicetrainbatchsize",
            "weightdecay",
            "linear",
            "warmupratio",
            "5e5",
            "parameters",
            "posttraining"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We perform an ablation study (referred to as &#8220;Tool Integration&#8221; in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) where we post-train the model on sequential query generation (i.e. <math alttext=\"P(Q^{T}|Q)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo fence=\"false\">|</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Q^{T}|Q)</annotation></semantics></math> in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) and output generation, to assess the impact of streaming RAG post-training versus standard post-training on final response generation.\nAdditionally, we conduct an ablation study on open book setting (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) using a self-cascade approach with a three-stage inference pipeline: (1) the audio question is used to generate a tool query <math alttext=\"Q^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><msup><mi>Q</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">Q^{T}</annotation></semantics></math> (corresponding to the &#8220;Query Generation&#8221; stage described in S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>); (2) the audio question, and tool results are combined to produce the final text output <math alttext=\"X^{\\text{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><msup><mi>X</mi><mtext>res</mtext></msup><annotation encoding=\"application/x-tex\">X^{\\text{res}}</annotation></semantics></math> by maximizing <math alttext=\"P(X^{\\text{res}}|Q,R)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mtext>res</mtext></msup><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(X^{\\text{res}}|Q,R)</annotation></semantics></math>; and (3) the audio question, and final text output are used to generate the final speech output <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> by optimizing <math alttext=\"P(A|Q,X^{\\text{res}})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m6\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo fence=\"false\">|</mo><mrow><mi>Q</mi><mo>,</mo><msup><mi>X</mi><mtext>res</mtext></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(A|Q,X^{\\text{res}})</annotation></semantics></math>. Since we teacher-force the text output to obtain the final speech output in stage (3), this self-cascade approach can only be applied to a &#8220;thinker-talker&#8221; architecture (eq. Qwen-OMNI) or Chain-of-Thought (CoT) style architectures (eg. OpusLM).\nThe motivation for this ablation is to investigate whether the inclusion of RAG references <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> during the &#8220;Response Generation&#8221; stage (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS1\" title=\"3.1 Tool Integration for Speech-in Speech-Out LLMs &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) affects the quality of the generated speech <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m8\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "qwenomni",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Streaming RAG approaches</span>: Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Ablation Study &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> highlights the substantial latency savings enabled by the <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>), compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> (S. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS1\" title=\"3.2.1 Fixed-Interval Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>). Three key metrics are evaluated: overall latency savings, the percentage of queries benefiting from reduced latency, and the number of parallel threads required.\nEven without any post-training, the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach already reduces tool usage latency (3.37s in T. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Analysis: Modality Gap Between Speech and Text Output &#8227; 5 Results &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) by 10.7% for Open Book Qwen-OMNI, demonstrating its flexibility and plug-and-play compatibility with any existing speech-in speech-out model.\nThe <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> method, utilizing Qwen-OMNI, consistently delivers superior performance. It achieves greater average latency reductions and benefits a higher proportion of queries with improved response times. Notably, <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> require only a single parallel thread, representing a significant advancement in resource efficiency compared to the <em class=\"ltx_emph ltx_font_italic\">Fixed-Interval Streaming RAG</em> approach, which demands multiple parallel threads.\nThese findings highlight the effectiveness of <em class=\"ltx_emph ltx_font_italic\">Model-Triggered Streaming RAG</em> in not only minimizing latency, validating our intuition from Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#S3.SS2.SSS2\" title=\"3.2.2 Model-Triggered Streaming RAG &#8227; 3.2 Streaming RAG &#8227; 3 Methodology &#8227; Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage\"><span class=\"ltx_text ltx_ref_tag\">3.2.2</span></a>, but also in optimizing system efficiency and resource utilization.</p>\n\n",
                "matched_terms": [
                    "qwenomni",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qwen-OMNI</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.02044v1#bib.bib37\" title=\"\">2025</a>)</cite> is a end-to-end multimodal model that seamlessly integrates diverse input modalities&#8212;including text, images, audio, and video&#8212;and generates both text and natural speech responses in a real-time streaming fashion. It leverages an innovative Thinker-Talker architecture, where the Thinker module performs high-level reasoning to produce a text response, which is then used by the Talker module, conditioning on both the text and the Thinker&#8217;s hidden representations, to generate streaming speech output.</p>\n\n",
                "matched_terms": [
                    "qwenomni",
                    "talker"
                ]
            }
        ]
    }
}