{
    "S3.T1": {
        "source_file": "Advancing speech summarization in Multi-modal LLMs with Reinforcement Learning",
        "caption": "Table 1: Summarization results",
        "body": "Golden3↑\\uparrow\nAMI↑\\uparrow\nFloras↑\\uparrow\n\n\n\n\n\nGPT-4o[8] Audio\n\n6.26\n5.83\n5.77\n\n\n\nGPT-4o[8] Text\n\n6.57\n6.75\n6.82\n\n\n\nPhi-4MM[11] Text\n\n5.50\n5.28\n5.17\n\n\n\nPhi-4MM[11] Audio\n\n5.02\n4.55\n4.69\n\n\nPhi-4MM replicated\n4.84\n4.13\n4.16\n\n\nPhi-4MM SFT\n4.97\n5.14\n5.14\n\n\nPhi-4MM SFT+KD\n6.05\n5.75\n4.93\n\n\nPhi-4MM SFT+KD+DPO\n6.36\n6.26\n5.74",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Golden3<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AMI<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Floras<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">GPT-4o</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\"> Audio</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.26</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.83</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.77</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">GPT-4o</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\"> Text</span>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.57</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.75</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.82</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Phi-4MM</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib11\" title=\"\">11</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\"> Text</span>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.50</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.28</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.17</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Phi-4MM</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib11\" title=\"\">11</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\"> Audio</span>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.02</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.55</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.69</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Phi-4MM replicated</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.13</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.16</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Phi-4MM SFT</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.14</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.14</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Phi-4MM SFT+KD</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.05</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.75</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.93</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">Phi-4MM SFT+KD+DPO</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.36</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.26</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.74</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "gpt4o8",
            "text",
            "floras↑uparrow",
            "sftkd",
            "golden3↑uparrow",
            "sft",
            "summarization",
            "sftkddpo",
            "ami↑uparrow",
            "phi4mm",
            "results",
            "audio",
            "replicated",
            "phi4mm11"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.4 Evaluation Setting &#8227; 3 Experimental Settings &#8227; Advancing speech summarization in Multi-modal LLMs with Reinforcement Learning\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> presents results across the three benchmarks, comparing our proposed PhiSSum models with both open-source baselines and state-of-the-art text-based and multi-modal systems. To establish a fair baseline, we replicated the Phi-4MM pre-training stage using 500k summarization samples restricted to summarization-only data, whereas the released Phi-4MM was trained on additional spoken QA data. As expected, this replication underperforms the released model.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As audio and audiovisual content increasingly dominate modern communication and media consumption, speech summarization (SSum)&#8212;the task of generating concise and coherent textual summaries directly from spoken input&#8212;has become a critical capability for improving accessibility, productivity, and information retrieval. By enabling faster access to information, supporting academic and business workflows, and facilitating personal consumption, SSum plays a pivotal role in bridging the gap between the vast amount of spoken data and the convenience of text-based interaction. Its importance continues to grow as society produces large amounts of spoken and audiovisual data across meetings, lectures, podcasts, and social media platforms.</span>\n</p>\n\n",
                "matched_terms": [
                    "summarization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Traditional SSum has been approached through a cascaded pipeline, where automatic speech recognition (ASR) is followed by text summarization</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. While effective to some extent, this approach introduces error propagation from ASR and often struggles to preserve nuances such as discourse structure, prosody, and emphasis. To address these shortcomings, end-to-end speech summarization methods have been proposed </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which generate summaries directly from speech without an intermediate transcription step. These approaches typically combine a speech encoder with an independently trained LLM or a task-specific summarization module, thereby aiming to reduce the error propagation introduced by ASR. However, because the components are independently trained and their finetuning is often constrained by limited training data, these systems generally lack strong instruction following ability, zero-shot generalization, and controllability</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "summarization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Building on the success of Large Language Models (LLMs), Multi-modal Large Language Models (MLLMs) extend LLMs to handle multi-modal inputs, integrating modalities beyond text. This extension is particularly critical for speech summarization, where the input is inherently multi-modal: acoustic signals encode not only linguistic content but also paralinguistic information such as speaker emphasis, prosody, and emotion. Leveraging such information has the potential to produce summaries that are more accurate and contextually faithful compared with text-only models. Commercial models such as GPT-4o-Audio </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Gemini-2.5 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have demonstrated promising SSum capabilities, but their large size, closed-source nature, and limited accessibility make them challenging to deploy at scale for general users. In contrast, open-source models such as Qwen2-Audio </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have been applied to zero-shot SSum without task-specific training. Similarly, Phi-4MM </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the first open-source model with general-purpose speech summarization capability that supports long-form audio inputs up to 2 hours. However, these open-source models still exhibit a substantial performance gap compared with the state-of-the-art commercial models like GPT-4o </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Moreover, all current MLLMs consistently underperform in the audio modality relative to the text modality, highlighting a persistent modality gap that limits their effectiveness for speech summarization.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "summarization",
                    "phi4mm",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose a three-stage training framework designed to enhance the speech summarization capabilities of multi-modal large language models (MLLMs). As illustrated in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Advancing speech summarization in Multi-modal LLMs with Reinforcement Learning\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the framework consists of: (1) Supervised finetuning (SFT) on synthetic data. (2) On-policy knowledge distillation (KD) to transfer summarization ability from strong text-based LLMs. (3) Direct Preference Optimization (DPO) to mitigate hallucinations.</span>\n</p>\n\n",
                "matched_terms": [
                    "sft",
                    "summarization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Supervised fine-tuning (SFT) serves as the foundation of our multi-stage training framework because it directly shapes the model&#8217;s ability to follow instructions and generate useful summaries. To further enhance the instruction-following capabilities of MLLMs, we construct a large and diverse synthetic dataset oriented toward summarization.</span>\n</p>\n\n",
                "matched_terms": [
                    "sft",
                    "summarization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our data collection process builds on the approach introduced in Phi-4MM </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where anonymized audio recordings are paired with their transcripts. The audio spans a broad range of topics, ensuring representation of both everyday and domain-specific speech. While the original Phi-4MM pipeline generated ten query&#8211;summary pairs per audio sample in a single pass, we extend and refine this process to further improve diversity and quality. Specifically, for each transcript, we first use GPT-4.1 to generate multiple candidate queries, each accompanied by an importance score reflecting its suitability for summarization. Queries with low scores are discarded to retain only high-quality instructions. From the filtered set, we then randomly select one query to associate with the corresponding audio, emphasizing diversity while avoiding redundancy. The resulting queries vary in granularity and style, ranging from concise requests for short summaries to more elaborate instructions requiring structured outputs such as bullet points, JSON objects, or email-style narratives. Given the selected query and transcript, GPT-4.1 is then prompted to generate a reference summary. This two-step design&#8212;importance scoring followed by query selection&#8212;ensures that the resulting dataset balances diversity with consistency and relevance. Compared with the baseline pipeline in Phi-4MM </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where GPT-4 was instructed to generate ten query&#8211;summary pairs for each audio in a single run, our method produces summaries that are significantly longer, richer in content, and more varied in format.</span>\n</p>\n\n",
                "matched_terms": [
                    "phi4mm",
                    "summarization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Knowledge distillation (KD) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib17\" title=\"\">17</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a widely used technique for transferring the behavior of a powerful teacher model to a smaller student model. However, in the case of speech summarization, direct distillation is limited by a strong distributional mismatch: the large teacher model produces outputs in the text modality, often with more expressive linguistic patterns than the audio-conditioned student can reproduce, and the student is trained only to imitate teacher-generated sequences rather than to correct its own errors. This mismatch leads to instability and weaker generalization, especially when the student encounters inputs that deviate from the teacher&#8217;s training distribution. This phenomenon is commonly referred to as mode collapse in the literature </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "summarization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To overcome these issues, we adopt an on-policy distillation strategy, where the student learns from its own generated sequences rather than from teacher outputs alone. During training, the student generates rollouts conditioned on audio input, and the teacher provides token-level probability supervision on these sequences. In this way, the supervision remains grounded in the student&#8217;s trajectory while still benefiting from the teacher&#8217;s richer linguistic knowledge. This reduces exposure bias and enables the student to improve on the kinds of errors it is likely to make at inference time, leading to more stable optimization and closer alignment between capacity and supervision. Importantly, this design also facilitates cross-modality transfer: although the teacher operates in the text modality, the student effectively absorbs the teacher&#8217;s summarization ability while grounding it in audio, thereby narrowing the performance gap between modalities. Moreover, when using on-policy data during distillation, the student receives token-specific feedback from the teacher&#8217;s logits on the erroneous tokens in its self-generated output sequences. This enables a form of feedback loop akin to what is observed in RL, which helps minimize the train-inference distribution mismatch.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "summarization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The baseline model builds on Phi-4MM </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which was trained on 500k summarization samples consisting of 50k audio recordings, each paired with 10 query&#8211;summary pairs. For clearer analysis and comparison, we do not use the final publicly released model. Instead, we build our model on the pre-training stage model, where it was trained on large-scale automatic speech recognition (ASR) data to align the audio encoder with text-based Phi-4Mini </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in the semantic space.</span>\n</p>\n\n",
                "matched_terms": [
                    "phi4mm",
                    "summarization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the SFT stage, we expand the audio data from 50k to 1M samples compared to the baseline. For each audio, we generate only a single query&#8211;response pair, as described in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#S2.SS1\" style=\"font-size:90%;\" title=\"2.1 Synthetic data for SFT &#8227; 2 Method &#8227; Advancing speech summarization in Multi-modal LLMs with Reinforcement Learning\">\n    <span class=\"ltx_text ltx_ref_tag\">2.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Compared with the baseline, our synthetic pipeline produces queries that are more diverse and summaries that are substantially richer in content. On average, summaries are three times longer than those in the baseline dataset, capturing finer details.</span>\n</p>\n\n",
                "matched_terms": [
                    "sft",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In all training stages, the audio encoder is kept frozen; only the audio projector and LoRA modules are updated. The LoRA configuration uses </span>\n  <math alttext=\"\\alpha=32\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#945;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">32</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\alpha=32</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"rank=16\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">a</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">n</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">k</mi>\n        </mrow>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">16</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">rank=16</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. During the SFT stage, the model is trained for 2 epochs on 32 A100 GPUs. For the knowledge distillation stage, we use the GPT-4o text only mode </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the teacher model. Training is conducted with the verl framework </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> where vLLM </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is used for rollout. The student model is trained on 8 A100 GPUs not including the GPUs allocated to the teacher model. In the DPO stage, the model is trained for 1 epoch on 32 A100 GPUs. Across all stages, training is performed on audio inputs up to 30 minutes in length (corresponding to approximately 22.5k tokens). Given the 128k context length of the language decoder, our model supports up to 2.8 hours of audio during inference.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">AMI</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">): A public English-only meeting corpus (&#160;100 hours) with multimodal streams. We evaluate on the test split (20 meetings, &#160;32 minutes each) using close-talking audio. Each meeting includes 3 summarization instructions (60 total).</span>\n</p>\n\n",
                "matched_terms": [
                    "summarization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">During inference, our model processes long-form audio in one shot without segmentation. For evaluation metrics, we follow the same setting as in Phi-4MM</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with small modifications. The outputs are scored by GPT-4.1 against the transcription for overall quality. The overall quality score, ranging from 1 to 7, measures accuracy in capturing details, coherence, writing style, degree of hallucination, and adherence to instruction-specific requirements regarding format, content, and length.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "phi4mm11"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our first-stage model, Phi-4MM-SFT, already delivers substantial improvements over the replicated Phi-4MM, demonstrating the effectiveness of the enhanced synthetic data pipeline. Building upon this, we apply on-policy knowledge distillation (KD) with GPT-4o (text mode) as the teacher. This stage yields the most substantial single boost in performance, with notable improvements on Golden3 and AMI. The results confirm that transferring summarization ability from a powerful text-only teacher into an audio-conditioned student is highly effective. However, this stage also introduces more frequent hallucinations as mentioned in section </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#S2.SS3\" style=\"font-size:90%;\" title=\"2.3 DPO &#8227; 2 Method &#8227; Advancing speech summarization in Multi-modal LLMs with Reinforcement Learning\">\n    <span class=\"ltx_text ltx_ref_tag\">2.3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For instance, given the query &#8221;List all the TV shows specifically named in this conversation&#8221;, the model outputs &#8221;The TV shows specifically mentioned in the conversation are: - How to Get Away - How to Get Away &#8230;.&#8221; repeating &#8221;How to Get Away&#8221; multiple times in succession.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "summarization",
                    "phi4mm",
                    "results",
                    "replicated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Another notable finding is the model&#8217;s strong generalization across languages, even though training used only English data. On the multilingual Floras benchmark, our model maintains performance close to GPT-4o-Audio, demonstrating robust zero-shot cross-lingual transfer. These results highlight that, with carefully staged training&#8212;including synthetic data, on-policy knowledge distillation, and preference alignment&#8212;smaller open-source models can rival or even surpass much larger commercial systems in speech summarization.</span>\n</p>\n\n",
                "matched_terms": [
                    "summarization",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.1 Main Results &#8227; 4 Experimental Results &#8227; Advancing speech summarization in Multi-modal LLMs with Reinforcement Learning\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows that larger SFT datasets consistently improve performance. Even 200k examples surpass the 500k data in the baseline, underscoring the importance of data quality. The 1M-sample set yields the strongest results.</span>\n</p>\n\n",
                "matched_terms": [
                    "sft",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.2 Ablation Study &#8227; 4 Experimental Results &#8227; Advancing speech summarization in Multi-modal LLMs with Reinforcement Learning\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares different teacher models for on-policy knowledge distillation. When using Phi-4MM text mode as the teacher, one might expect results similar to Phi-4MM since they share the same backbone. However, the teacher provides modest gains on Golden3 but leads to degradation on AMI and FLORAS, which we attribute to reward hacking behaviors more prevalent in smaller LLMs. In contrast, using GPT-4o (text mode) as the teacher yields consistent improvements across all datasets, validating the importance of leveraging a stronger teacher to mitigate hallucinations and achieve improvements across all datasets.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "phi4mm",
                    "results"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "Advancing speech summarization in Multi-modal LLMs with Reinforcement Learning",
        "caption": "Table 2: Summarization results on different size of SFT datasets",
        "body": "Golden3↑\\uparrow\nAMI↑\\uparrow\nFloras↑\\uparrow\n\n\n\n\nPhi-4MM replicated\n4.84\n4.13\n4.16\n\n\nPhi-4MM SFT 100k\n4.82\n4.83\n4.63\n\n\nPhi-4MM SFT 200k\n4.97\n4.98\n4.76\n\n\nPhi-4MM SFT 400k\n5.03\n5.00\n4.97\n\n\nPhi-4MM SFT 600k\n4.97\n5.05\n5.04\n\n\nPhi-4MM SFT 1M\n4.97\n5.14\n5.14",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Golden3<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AMI<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Floras<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Phi-4MM replicated</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.13</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.16</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Phi-4MM SFT 100k</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.82</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.83</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.63</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Phi-4MM SFT 200k</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.76</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Phi-4MM SFT 400k</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.03</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.97</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Phi-4MM SFT 600k</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.05</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.04</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">Phi-4MM SFT 1M</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.14</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.14</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "200k",
            "size",
            "floras↑uparrow",
            "400k",
            "different",
            "golden3↑uparrow",
            "100k",
            "600k",
            "sft",
            "summarization",
            "datasets",
            "ami↑uparrow",
            "phi4mm",
            "results",
            "replicated"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.1 Main Results &#8227; 4 Experimental Results &#8227; Advancing speech summarization in Multi-modal LLMs with Reinforcement Learning\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows that larger SFT datasets consistently improve performance. Even 200k examples surpass the 500k data in the baseline, underscoring the importance of data quality. The 1M-sample set yields the strongest results.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Building on the success of Large Language Models (LLMs), Multi-modal Large Language Models (MLLMs) extend LLMs to handle multi-modal inputs, integrating modalities beyond text. This extension is particularly critical for speech summarization, where the input is inherently multi-modal: acoustic signals encode not only linguistic content but also paralinguistic information such as speaker emphasis, prosody, and emotion. Leveraging such information has the potential to produce summaries that are more accurate and contextually faithful compared with text-only models. Commercial models such as GPT-4o-Audio </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Gemini-2.5 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have demonstrated promising SSum capabilities, but their large size, closed-source nature, and limited accessibility make them challenging to deploy at scale for general users. In contrast, open-source models such as Qwen2-Audio </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have been applied to zero-shot SSum without task-specific training. Similarly, Phi-4MM </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the first open-source model with general-purpose speech summarization capability that supports long-form audio inputs up to 2 hours. However, these open-source models still exhibit a substantial performance gap compared with the state-of-the-art commercial models like GPT-4o </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Moreover, all current MLLMs consistently underperform in the audio modality relative to the text modality, highlighting a persistent modality gap that limits their effectiveness for speech summarization.</span>\n</p>\n\n",
                "matched_terms": [
                    "phi4mm",
                    "summarization",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose a three-stage training framework designed to enhance the speech summarization capabilities of multi-modal large language models (MLLMs). As illustrated in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Advancing speech summarization in Multi-modal LLMs with Reinforcement Learning\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the framework consists of: (1) Supervised finetuning (SFT) on synthetic data. (2) On-policy knowledge distillation (KD) to transfer summarization ability from strong text-based LLMs. (3) Direct Preference Optimization (DPO) to mitigate hallucinations.</span>\n</p>\n\n",
                "matched_terms": [
                    "sft",
                    "summarization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Supervised fine-tuning (SFT) serves as the foundation of our multi-stage training framework because it directly shapes the model&#8217;s ability to follow instructions and generate useful summaries. To further enhance the instruction-following capabilities of MLLMs, we construct a large and diverse synthetic dataset oriented toward summarization.</span>\n</p>\n\n",
                "matched_terms": [
                    "sft",
                    "summarization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our data collection process builds on the approach introduced in Phi-4MM </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where anonymized audio recordings are paired with their transcripts. The audio spans a broad range of topics, ensuring representation of both everyday and domain-specific speech. While the original Phi-4MM pipeline generated ten query&#8211;summary pairs per audio sample in a single pass, we extend and refine this process to further improve diversity and quality. Specifically, for each transcript, we first use GPT-4.1 to generate multiple candidate queries, each accompanied by an importance score reflecting its suitability for summarization. Queries with low scores are discarded to retain only high-quality instructions. From the filtered set, we then randomly select one query to associate with the corresponding audio, emphasizing diversity while avoiding redundancy. The resulting queries vary in granularity and style, ranging from concise requests for short summaries to more elaborate instructions requiring structured outputs such as bullet points, JSON objects, or email-style narratives. Given the selected query and transcript, GPT-4.1 is then prompted to generate a reference summary. This two-step design&#8212;importance scoring followed by query selection&#8212;ensures that the resulting dataset balances diversity with consistency and relevance. Compared with the baseline pipeline in Phi-4MM </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where GPT-4 was instructed to generate ten query&#8211;summary pairs for each audio in a single run, our method produces summaries that are significantly longer, richer in content, and more varied in format.</span>\n</p>\n\n",
                "matched_terms": [
                    "phi4mm",
                    "summarization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The baseline model builds on Phi-4MM </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which was trained on 500k summarization samples consisting of 50k audio recordings, each paired with 10 query&#8211;summary pairs. For clearer analysis and comparison, we do not use the final publicly released model. Instead, we build our model on the pre-training stage model, where it was trained on large-scale automatic speech recognition (ASR) data to align the audio encoder with text-based Phi-4Mini </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in the semantic space.</span>\n</p>\n\n",
                "matched_terms": [
                    "phi4mm",
                    "summarization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.4 Evaluation Setting &#8227; 3 Experimental Settings &#8227; Advancing speech summarization in Multi-modal LLMs with Reinforcement Learning\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> presents results across the three benchmarks, comparing our proposed PhiSSum models with both open-source baselines and state-of-the-art text-based and multi-modal systems. To establish a fair baseline, we replicated the Phi-4MM pre-training stage using 500k summarization samples restricted to summarization-only data, whereas the released Phi-4MM was trained on additional spoken QA data. As expected, this replication underperforms the released model.</span>\n</p>\n\n",
                "matched_terms": [
                    "phi4mm",
                    "summarization",
                    "results",
                    "replicated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our first-stage model, Phi-4MM-SFT, already delivers substantial improvements over the replicated Phi-4MM, demonstrating the effectiveness of the enhanced synthetic data pipeline. Building upon this, we apply on-policy knowledge distillation (KD) with GPT-4o (text mode) as the teacher. This stage yields the most substantial single boost in performance, with notable improvements on Golden3 and AMI. The results confirm that transferring summarization ability from a powerful text-only teacher into an audio-conditioned student is highly effective. However, this stage also introduces more frequent hallucinations as mentioned in section </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#S2.SS3\" style=\"font-size:90%;\" title=\"2.3 DPO &#8227; 2 Method &#8227; Advancing speech summarization in Multi-modal LLMs with Reinforcement Learning\">\n    <span class=\"ltx_text ltx_ref_tag\">2.3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For instance, given the query &#8221;List all the TV shows specifically named in this conversation&#8221;, the model outputs &#8221;The TV shows specifically mentioned in the conversation are: - How to Get Away - How to Get Away &#8230;.&#8221; repeating &#8221;How to Get Away&#8221; multiple times in succession.</span>\n</p>\n\n",
                "matched_terms": [
                    "phi4mm",
                    "summarization",
                    "results",
                    "replicated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Another notable finding is the model&#8217;s strong generalization across languages, even though training used only English data. On the multilingual Floras benchmark, our model maintains performance close to GPT-4o-Audio, demonstrating robust zero-shot cross-lingual transfer. These results highlight that, with carefully staged training&#8212;including synthetic data, on-policy knowledge distillation, and preference alignment&#8212;smaller open-source models can rival or even surpass much larger commercial systems in speech summarization.</span>\n</p>\n\n",
                "matched_terms": [
                    "summarization",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.2 Ablation Study &#8227; 4 Experimental Results &#8227; Advancing speech summarization in Multi-modal LLMs with Reinforcement Learning\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares different teacher models for on-policy knowledge distillation. When using Phi-4MM text mode as the teacher, one might expect results similar to Phi-4MM since they share the same backbone. However, the teacher provides modest gains on Golden3 but leads to degradation on AMI and FLORAS, which we attribute to reward hacking behaviors more prevalent in smaller LLMs. In contrast, using GPT-4o (text mode) as the teacher yields consistent improvements across all datasets, validating the importance of leveraging a stronger teacher to mitigate hallucinations and achieve improvements across all datasets.</span>\n</p>\n\n",
                "matched_terms": [
                    "phi4mm",
                    "different",
                    "datasets",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We also attempted to apply DPO directly to the model in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.2 Ablation Study &#8227; 4 Experimental Results &#8227; Advancing speech summarization in Multi-modal LLMs with Reinforcement Learning\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Both DPO and on-policy KD are recognized as state-of-the-art RL methods, but their effectiveness varies across datasets. On Golden3, KD substantially outperforms DPO, while on AMI and FLORAS, DPO yields stronger results. Our error analysis suggests that KD produces higher-quality summaries overall, but the hallucinations lower evaluation scores. This complementary behavior underscores the necessity of combining KD with DPO in our framework.</span>\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "datasets"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "Advancing speech summarization in Multi-modal LLMs with Reinforcement Learning",
        "caption": "Table 3: Comparison of different teachers in on-policy KD",
        "body": "Golden3↑\\uparrow\nAMI↑\\uparrow\nFloras↑\\uparrow\n\n\nPhi-4MM\n5.02\n4.55\n4.69\n\n\nPhi-4MM + DPO\n5.53\n5.2\n5.36\n\n\nPhi-4MM + KD(Phi-4MM text)\n5.36\n4.46\n4.13\n\n\nPhi-4MM + KD(GPT-4o)\n6.03\n5.32\n4.84",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Golden3<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AMI<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Floras<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Phi-4MM</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.02</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.55</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.69</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Phi-4MM + DPO</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.36</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Phi-4MM + KD(Phi-4MM text)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.36</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.46</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.13</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">Phi-4MM + KD(GPT-4o)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.84</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "teachers",
            "kdphi4mm",
            "text",
            "floras↑uparrow",
            "different",
            "golden3↑uparrow",
            "onpolicy",
            "dpo",
            "kdgpt4o",
            "ami↑uparrow",
            "phi4mm",
            "comparison"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.2 Ablation Study &#8227; 4 Experimental Results &#8227; Advancing speech summarization in Multi-modal LLMs with Reinforcement Learning\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares different teacher models for on-policy knowledge distillation. When using Phi-4MM text mode as the teacher, one might expect results similar to Phi-4MM since they share the same backbone. However, the teacher provides modest gains on Golden3 but leads to degradation on AMI and FLORAS, which we attribute to reward hacking behaviors more prevalent in smaller LLMs. In contrast, using GPT-4o (text mode) as the teacher yields consistent improvements across all datasets, validating the importance of leveraging a stronger teacher to mitigate hallucinations and achieve improvements across all datasets.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We also attempted to apply DPO directly to the model in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.2 Ablation Study &#8227; 4 Experimental Results &#8227; Advancing speech summarization in Multi-modal LLMs with Reinforcement Learning\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Both DPO and on-policy KD are recognized as state-of-the-art RL methods, but their effectiveness varies across datasets. On Golden3, KD substantially outperforms DPO, while on AMI and FLORAS, DPO yields stronger results. Our error analysis suggests that KD produces higher-quality summaries overall, but the hallucinations lower evaluation scores. This complementary behavior underscores the necessity of combining KD with DPO in our framework.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Building on the success of Large Language Models (LLMs), Multi-modal Large Language Models (MLLMs) extend LLMs to handle multi-modal inputs, integrating modalities beyond text. This extension is particularly critical for speech summarization, where the input is inherently multi-modal: acoustic signals encode not only linguistic content but also paralinguistic information such as speaker emphasis, prosody, and emotion. Leveraging such information has the potential to produce summaries that are more accurate and contextually faithful compared with text-only models. Commercial models such as GPT-4o-Audio </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Gemini-2.5 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have demonstrated promising SSum capabilities, but their large size, closed-source nature, and limited accessibility make them challenging to deploy at scale for general users. In contrast, open-source models such as Qwen2-Audio </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have been applied to zero-shot SSum without task-specific training. Similarly, Phi-4MM </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the first open-source model with general-purpose speech summarization capability that supports long-form audio inputs up to 2 hours. However, these open-source models still exhibit a substantial performance gap compared with the state-of-the-art commercial models like GPT-4o </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Moreover, all current MLLMs consistently underperform in the audio modality relative to the text modality, highlighting a persistent modality gap that limits their effectiveness for speech summarization.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "phi4mm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To this end, we propose the following approach. We first construct large-scale synthetic datasets to improve the model&#8217;s ability to follow a wide range of instructions, thus strengthening its general instruction following capability. Next, we employ large-scale reinforcement learning (RL) of on-policy knowledge distillation </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib14\" title=\"\">14</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to transfer knowledge from powerful text-based LLMs into student MLLM, thereby reducing the modality gap. Finally, we bootstrap the model with another RL method Direct Preference Optimization (DPO) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which mitigates issues such as hallucinations and improves robustness.</span>\n</p>\n\n",
                "matched_terms": [
                    "onpolicy",
                    "dpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose a three-stage training framework designed to enhance the speech summarization capabilities of multi-modal large language models (MLLMs). As illustrated in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Advancing speech summarization in Multi-modal LLMs with Reinforcement Learning\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the framework consists of: (1) Supervised finetuning (SFT) on synthetic data. (2) On-policy knowledge distillation (KD) to transfer summarization ability from strong text-based LLMs. (3) Direct Preference Optimization (DPO) to mitigate hallucinations.</span>\n</p>\n\n",
                "matched_terms": [
                    "onpolicy",
                    "dpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To overcome these issues, we adopt an on-policy distillation strategy, where the student learns from its own generated sequences rather than from teacher outputs alone. During training, the student generates rollouts conditioned on audio input, and the teacher provides token-level probability supervision on these sequences. In this way, the supervision remains grounded in the student&#8217;s trajectory while still benefiting from the teacher&#8217;s richer linguistic knowledge. This reduces exposure bias and enables the student to improve on the kinds of errors it is likely to make at inference time, leading to more stable optimization and closer alignment between capacity and supervision. Importantly, this design also facilitates cross-modality transfer: although the teacher operates in the text modality, the student effectively absorbs the teacher&#8217;s summarization ability while grounding it in audio, thereby narrowing the performance gap between modalities. Moreover, when using on-policy data during distillation, the student receives token-specific feedback from the teacher&#8217;s logits on the erroneous tokens in its self-generated output sequences. This enables a form of feedback loop akin to what is observed in RL, which helps minimize the train-inference distribution mismatch.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "onpolicy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"\\pi\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#960;</mi>\n      <annotation encoding=\"application/x-tex\">\\pi</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the current model which is initialized from the reference model, </span>\n  <math alttext=\"\\pi_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#960;</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">e</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">f</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\pi_{ref}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is the checkpoint obtained from on-policy knowledge distillation in section </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#S2.SS2\" style=\"font-size:90%;\" title=\"2.2 On-policy knowledge distillation &#8227; 2 Method &#8227; Advancing speech summarization in Multi-modal LLMs with Reinforcement Learning\">\n    <span class=\"ltx_text ltx_ref_tag\">2.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. By applying DPO as the final step, we effectively reduce hallucinations and improve the overall consistency of generated summaries.</span>\n</p>\n\n",
                "matched_terms": [
                    "onpolicy",
                    "dpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The baseline model builds on Phi-4MM </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which was trained on 500k summarization samples consisting of 50k audio recordings, each paired with 10 query&#8211;summary pairs. For clearer analysis and comparison, we do not use the final publicly released model. Instead, we build our model on the pre-training stage model, where it was trained on large-scale automatic speech recognition (ASR) data to align the audio encoder with text-based Phi-4Mini </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in the semantic space.</span>\n</p>\n\n",
                "matched_terms": [
                    "phi4mm",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In all training stages, the audio encoder is kept frozen; only the audio projector and LoRA modules are updated. The LoRA configuration uses </span>\n  <math alttext=\"\\alpha=32\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#945;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">32</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\alpha=32</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"rank=16\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">a</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">n</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">k</mi>\n        </mrow>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">16</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">rank=16</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. During the SFT stage, the model is trained for 2 epochs on 32 A100 GPUs. For the knowledge distillation stage, we use the GPT-4o text only mode </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the teacher model. Training is conducted with the verl framework </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> where vLLM </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is used for rollout. The student model is trained on 8 A100 GPUs not including the GPUs allocated to the teacher model. In the DPO stage, the model is trained for 1 epoch on 32 A100 GPUs. Across all stages, training is performed on audio inputs up to 30 minutes in length (corresponding to approximately 22.5k tokens). Given the 128k context length of the language decoder, our model supports up to 2.8 hours of audio during inference.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "dpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our first-stage model, Phi-4MM-SFT, already delivers substantial improvements over the replicated Phi-4MM, demonstrating the effectiveness of the enhanced synthetic data pipeline. Building upon this, we apply on-policy knowledge distillation (KD) with GPT-4o (text mode) as the teacher. This stage yields the most substantial single boost in performance, with notable improvements on Golden3 and AMI. The results confirm that transferring summarization ability from a powerful text-only teacher into an audio-conditioned student is highly effective. However, this stage also introduces more frequent hallucinations as mentioned in section </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19631v1#S2.SS3\" style=\"font-size:90%;\" title=\"2.3 DPO &#8227; 2 Method &#8227; Advancing speech summarization in Multi-modal LLMs with Reinforcement Learning\">\n    <span class=\"ltx_text ltx_ref_tag\">2.3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For instance, given the query &#8221;List all the TV shows specifically named in this conversation&#8221;, the model outputs &#8221;The TV shows specifically mentioned in the conversation are: - How to Get Away - How to Get Away &#8230;.&#8221; repeating &#8221;How to Get Away&#8221; multiple times in succession.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "phi4mm",
                    "onpolicy"
                ]
            }
        ]
    }
}