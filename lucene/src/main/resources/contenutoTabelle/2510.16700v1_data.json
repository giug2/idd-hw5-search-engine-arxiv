{
    "S3.T1": {
        "source_file": "Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios",
        "caption": "Table 1: WER (%) on Torgo. ‘M’, ‘L’, and ‘VL’ denote moderate, low, and very low severity, respectively. ‘FT’ indicates fine-tuning. ‘AVG’ is the speaker-level average WER. See Section 4.2 for definitions of V, F1, and F2.",
        "body": "Settings\n\nSeverity\nLevel\n\nAVG\n\n\n\n\nM\nL\nVL\n\n\n\n\n[36]\n54.46\n33.89\n10.35\n40.86\n\n\n\n[37]\n48.40\n31.07\n8.66\n36.30\n\n\nWithout\n[11]\n56.34\n46.60\n13.85\n44.50\n\n\nDDA\n[12]\n85.35\n28.54\n3.44\n57.77\n\n\n\nLOSO\n39.56\n24.60\n6.30\n29.38\n\n\nZero-Shot\n\nV FT LOSO\n34.38\n24.40\n6.20\n26.09 (Δ\\Delta 3.29%)\n\n\nOne-Shot\n\nF1 FT LOSO\n33.36\n21.50\n5.85\n25.00\n\n\n\n\nF2 FT LOSO\n31.06\n20.90\n5.50\n\n23.40 (Δ\\Delta 5.98%)\n\n\nAll-Test-Data\n[11]\n50.14\n36.80\n12.60\n39.09\n\n\n\n[12]\n26.37\n26.14\n3.82\n20.70",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Settings</span></th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Severity</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Level</span></td>\n<td class=\"ltx_td ltx_border_r ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">AVG</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"/>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">M</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">L</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">VL</span></td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib36\" title=\"\">36</a>]</cite></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">54.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">10.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">40.86</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib37\" title=\"\">37</a>]</cite></th>\n<td class=\"ltx_td ltx_align_center\">48.40</td>\n<td class=\"ltx_td ltx_align_center\">31.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">8.66</td>\n<td class=\"ltx_td ltx_align_center\">36.30</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">Without</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib11\" title=\"\">11</a>]</cite></th>\n<td class=\"ltx_td ltx_align_center\">56.34</td>\n<td class=\"ltx_td ltx_align_center\">46.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">13.85</td>\n<td class=\"ltx_td ltx_align_center\">44.50</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">DDA</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib12\" title=\"\">12</a>]</cite></th>\n<td class=\"ltx_td ltx_align_center\">85.35</td>\n<td class=\"ltx_td ltx_align_center\">28.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">3.44</span></td>\n<td class=\"ltx_td ltx_align_center\">57.77</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">LOSO</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">39.56</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">24.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">6.30</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">29.38</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Zero-Shot</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">V</span> FT LOSO</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">34.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">24.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">6.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26.09 (<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">3.29%</span>)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">One-Shot</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">F1</span> FT LOSO</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">21.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">5.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span class=\"ltx_text ltx_font_bold\">F2</span> FT LOSO</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">31.06</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">20.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">5.50</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">23.40</span> (<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">5.98%</span>)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">All-Test-Data</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib11\" title=\"\">11</a>]</cite></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">50.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">12.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39.09</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_r\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib12\" title=\"\">12</a>]</cite></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">26.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">26.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">3.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">20.70</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "respectively",
            "wer",
            "see",
            "severity",
            "torgo",
            "avg",
            "without",
            "dda",
            "oneshot",
            "‘avg’",
            "moderate",
            "loso",
            "indicates",
            "speakerlevel",
            "‘ft’",
            "very",
            "δdelta",
            "level",
            "average",
            "zeroshot",
            "definitions",
            "‘m’",
            "settings",
            "finetuning",
            "‘vl’",
            "denote",
            "alltestdata",
            "low",
            "‘l’"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The experimental results on the Torgo dataset are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#S3.T1\" title=\"Table 1 &#8227; 3 Strategy design and model selection &#8227; Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Espana Bonnet et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib36\" title=\"\">36</a>]</cite> compared the performance of various DNN architectures on Torgo, but none of the models achieved optimal results for all speakers. For our comparison, we selected the best performance for each speaker from <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib36\" title=\"\">36</a>]</cite>. Joy et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib37\" title=\"\">37</a>]</cite> improved DSR performance by adjusting DNN architecture parameters. Soleymanpour et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib11\" title=\"\">11</a>]</cite> employed a DNN-HMM architecture for DSR, while Leung et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib12\" title=\"\">12</a>]</cite> utilized the large speech model Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib41\" title=\"\">41</a>]</cite> for DSR. We include the results from <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib12\" title=\"\">12</a>]</cite> both before and after DDA. DDA methods used in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib12\" title=\"\">12</a>]</cite> are described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#S2\" title=\"2 Previous methods &#8227; Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Our LOSO models demonstrate strong performance without DDA. The Zero-Shot setting, which employs the DDA method that combines the VITS model with the text-coverage strategy, achieves a significant 3.29% absolute decrease in average WER. Further improvements are observed in the One-Shot setting, where Fish-Speech utilizes a single speech sample from the target dysarthric speaker to generate data that is more aligned with the target domain, leading to enhanced performance across all severity levels. Compared to the results of <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib12\" title=\"\">12</a>]</cite> enhanced under the All-Test-Data setting, our One-Shot results significantly outperform <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib11\" title=\"\">11</a>]</cite> (p-value &lt;0.05) and achieve comparable performance to <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib12\" title=\"\">12</a>]</cite>. Notably, we achieve these results using only one sample from the unseen target speaker, indicating that our method of leveraging Fish-Speech to learn the speaking style while aligning with the target text domain is an effective DDA approach.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Dysarthric speech recognition (DSR) research has witnessed remarkable progress in recent years, evolving from the basic understanding of individual words to the intricate comprehension of sentence-level expressions, all driven by the pressing communication needs of individuals with dysarthria. Nevertheless, the scarcity of available data remains a substantial hurdle, posing a significant challenge to the development of effective sentence-level DSR systems. In response to this issue, dysarthric data augmentation (DDA) has emerged as a highly promising approach. Generative models are frequently employed to generate training data for automatic speech recognition tasks. However, their effectiveness hinges on the ability of the synthesized data to accurately represent the target domain. The wide-ranging variability in pronunciation among dysarthric speakers makes it extremely difficult for models trained on data from existing speakers to produce useful augmented data, especially in zero-shot or one-shot learning settings. To address this limitation, we put forward a novel text-coverage strategy specifically designed for text-matching data synthesis. This innovative strategy allows for efficient zero/one-shot DDA, leading to substantial enhancements in the performance of DSR when dealing with unseen dysarthric speakers. Such improvements are of great significance in practical applications, including dysarthria rehabilitation programs and day-to-day common-sentence communication scenarios.</p>\n\n",
                "matched_terms": [
                    "settings",
                    "dda",
                    "zeroshot",
                    "oneshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Acquiring data from dysarthric speakers presents challenges due to participant limitations and recording difficulties. The unique characteristics of dysarthric speech further complicate the annotation process, requiring specialized expertise. Consequently, few open-source dysarthric datasets exist, primarily at the word or command level <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib8\" title=\"\">8</a>]</cite>. While some sentence-level dysarthric datasets are available <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib10\" title=\"\">10</a>]</cite>, their quantity remains limited. To address this data scarcity, dysarthric data augmentation (DDA) is essential. Current sentence-level DDA methods employ either Zero-Shot (simulating general dysarthric characteristics without using target speaker speech data) or All-Test-Data <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib12\" title=\"\">12</a>]</cite> (utilizing all available test data from the target speaker to train speech generative models and then using these models to synthesize training data) settings. The All-Test-Data setting exhibits the best performance but represents an idealized scenario. Our work focuses on the more practical Zero-Shot and One-Shot settings with speech generative models.</p>\n\n",
                "matched_terms": [
                    "settings",
                    "without",
                    "level",
                    "dda",
                    "oneshot",
                    "alltestdata",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Data augmentation in ASR, which typically involves synthesizing speech from texts in the training set, encounters challenges when applied to DSR. The limited size of dysarthric datasets often results in content mismatches between the training and target domains. Additionally, pronunciation variations between unseen target speakers and those in the training set exacerbate this mismatch. Consequently, synthetic data generated from training set texts using speech generative models trained on either typical speech or existing dysarthric speakers may not accurately capture the characteristics of the target domain. Rosenberg et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib13\" title=\"\">13</a>]</cite> established a performance benchmark by synthesizing speech from test set texts, suggesting that while this data augmentation setting may be impractical for general ASR, it holds value for specific domain applications. We consider sentence-level DSR, a newly developed and challenging task, to be one such application. Chen et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib14\" title=\"\">14</a>]</cite> emphasized the necessity for the texts of synthesized speech to closely align with the target domain to enhance ASR performance. Yang et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib15\" title=\"\">15</a>]</cite> highlighted that text content is more crucial than speaking style in speaker adaptation, positing that performance improvements arise from synthesized speech standardizing the ASR model&#8217;s language understanding. Based on these insights, we propose a text-coverage strategy (see Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>), evaluating its effectiveness in Zero-Shot and One-Shot DDA settings. This strategy utilizes speech generative models to synthesize speech data directly from pre-defined texts. Although limiting the target domain content may not be ideal for general ASR, it offers advantages for applications such as dysarthria rehabilitation or scenarios where dysarthric speakers use common sentences for communication, particularly given the prior focus of DSR on word recognition. Considering the limited dysarthric data and the prosody variability of dysarthric speech, we selected VITS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib16\" title=\"\">16</a>]</cite> for Zero-Shot and Fish-Speech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib17\" title=\"\">17</a>]</cite> for One-Shot as our DDA models (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#S3\" title=\"3 Strategy design and model selection &#8227; Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> for selection analysis). To our knowledge, this is the first application of these models in the context of DDA.</p>\n\n",
                "matched_terms": [
                    "settings",
                    "see",
                    "dda",
                    "oneshot",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated our text-coverage strategy on two sentence-level dysarthric speech datasets: the English Torgo dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib9\" title=\"\">9</a>]</cite> and the Chinese CDSD dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib10\" title=\"\">10</a>]</cite>. On the Torgo dataset, the Zero-Shot text-coverage strategy achieved a 3.29% absolute reduction in average WER, while the One-Shot text-coverage strategy resulted in a 5.98% absolute WER reduction. For the CDSD dataset, we implemented a stepwise filtering mechanism, ceasing further DDA for speakers with high intelligibility (CER &lt;25%; <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib10\" title=\"\">10</a>]</cite> test set CER: 26.46%). Finally, 20 out of 24 speakers to achieve a CER below 25%. By utilizing <span class=\"ltx_text ltx_font_bold\">a maximum of one sample</span> per speaker, the average final CER was 19.525% (calculated by averaging the boldfaced values in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#S4.T2\" title=\"Table 2 &#8227; 4.3 Results and discussions &#8227; 4 Experiments &#8227; Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
                "matched_terms": [
                    "wer",
                    "oneshot",
                    "average",
                    "dda",
                    "torgo",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a novel and efficient text-coverage strategy that enables the first sentence-level zero-shot and one-shot DDA based on speech generative models.</p>\n\n",
                "matched_terms": [
                    "dda",
                    "oneshot",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the effectiveness of our proposed DDA strategy and selected models using the Torgo and CDSD dysarthric datasets.</p>\n\n",
                "matched_terms": [
                    "dda",
                    "torgo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Reinforcement learning</span> Jin et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib28\" title=\"\">28</a>]</cite> proposed a dynamic DDA method that utilizes reinforcement learning to optimize SpecAugment parameters during DSR training. While this approach demonstrates good performance through a speech chain, it is dependent on existing data and necessitates sufficient training data, along with further exploration of diverse settings.</p>\n\n",
                "matched_terms": [
                    "dda",
                    "settings"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Reusing speech generative models</span> Recent DDA research has adapted Voice Conversion (VC) and Text-to-Speech (TTS) models, originally designed for typical speech, by training or fine-tuning them using dysarthric data. Models that have been adopted include: (1) attention-based VC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib29\" title=\"\">29</a>]</cite>, (2) sparse-structured spectral envelope transformation-based VC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib30\" title=\"\">30</a>]</cite>, (3) autoencoder-based VC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib31\" title=\"\">31</a>]</cite>, (4) diffusion-based VC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib32\" title=\"\">32</a>]</cite> and TTS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib12\" title=\"\">12</a>]</cite>. Beyond directly reusing models, Soleymanpour et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib11\" title=\"\">11</a>]</cite> modified a multi-speaker TTS system by incorporating a dysarthria severity level coefficient and a pause insertion model, enabling the synthesis of dysarthric speech across varying severity levels. While these methods show promise, Hu et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib33\" title=\"\">33</a>]</cite> demonstrated that some TTS models struggle to generate natural-sounding dysarthric speech, requiring larger datasets or longer training times. Furthermore, with the exception of <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib12\" title=\"\">12</a>]</cite>, these works primarily focus on enhancing word recognition, and this approach has not fully kept pace with the progress of typical speech TTS models, nor has it explored Voice Cloning technology.</p>\n\n",
                "matched_terms": [
                    "dda",
                    "finetuning",
                    "severity",
                    "level"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-coverage strategy</span> To enhance the performance of Dysarthria Speech Recognition (DSR) models for unseen dysarthric speakers, improving the generalization ability of the DSR model or adapting the model is crucial. However, constructing a speaker-independent and text-independent DSR model requires large and diverse training data. To our knowledge, the number of speakers and texts covered by the widely-used Torgo and CDSD datasets remains limited. Constructing a speaker-dependent DSR model requires a large amount of comprehensive speech data from the target speakers to capture their unique pronunciation patterns and prosodic variations, but collecting and annotating dysarthric data are very difficult. Building on the findings discussed in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib15\" title=\"\">15</a>]</cite> in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#S1\" title=\"1 Introduction &#8227; Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we propose the text-coverage strategy (see Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). This strategy involves synthesizing speech data based on the pre-defined texts content of the target scenario, allowing the DSR model to adapt proactively and enhance its performance.</p>\n\n",
                "matched_terms": [
                    "very",
                    "see",
                    "torgo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fish-Speech</span> We selected Fish-Speech, a free and open-source voice cloning model. Its inference framework is illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#S2.F2\" title=\"Figure 2 &#8227; 2 Previous methods &#8227; Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, which we created based on the code from <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib17\" title=\"\">17</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The code used is from August 2024.</span></span></span>. Our decision was influenced by several factors. First, Fish-Speech&#8217;s UTF-8 text encoding and byte pair encoding tokenizer bypasses the phoneme system, eliminating the need for language-specific preprocessing and training, an advantage for our experiments involving two languages. Second, its architecture, a recently evolving neural codec-based architecture <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib34\" title=\"\">34</a>]</cite>, can efficiently generate speech with diverse and realistic prosody. Finally, preliminary testing on the Torgo dataset indicated that its cloning performance for dysarthric speech, evaluated through sampling and manual assessment, is acceptable and could be further improved by fine-tuning LLaMA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib35\" title=\"\">35</a>]</cite> of Fish-Speech.</p>\n\n",
                "matched_terms": [
                    "torgo",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Torgo dataset</span> The Torgo dataset comprises English speech from 8 dysarthric speakers and 7 control speakers. After filtering out samples with transcription errors and unrestricted content, a total of 16,582 data segments remained, with an approximate duration of 13.67 hours. Using the data from the 8 dysarthric speakers, we fine-tuned the pre-trained model using the leave-one-speaker-out (LOSO) method to construct the LOSO model. For the DDA models, all data except that of the target dysarthric speaker were used during training or fine-tuning, including data from the control speakers.</p>\n\n",
                "matched_terms": [
                    "dda",
                    "loso",
                    "torgo",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CDSD dataset</span> We utilize the initial release of the CDSD dataset, comprising 29,466 Chinese recordings from 24 dysarthric speakers. Using the data from these 24 speakers, we also fine-tune the pre-trained model with the LOSO method to construct a LOSO model for DSR. For the DDA models, we incorporate 12,885 recordings (14.38 hours) from 31 dysarthric speakers with intelligibility annotations from the AISHELL-6B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><span class=\"ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self\">https://www.aishelltech.com/AISHELL_6B</span></span></span></span> dataset during training or fine-tuning</p>\n\n",
                "matched_terms": [
                    "loso",
                    "dda",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DDA models</span> We implemented the VITS model using the open-source framework<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><span class=\"ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self\">https://github.com/jaywalnut310/VITS</span></span></span></span>, which has 39.68M parameters. We followed the original VITS codebase for English text processing, while performing word segmentation and Pinyin parsing for Chinese text. Hyperparameters were adapted from the vctk_base.json: for Torgo, batch size 16, n_speakers 14, 64k training steps; for AISHELL-6B, batch size 64, n_speakers 31, 36k training steps. We used the pre-trained Fish-Speech model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><span class=\"ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self\">huggingface.co/fishaudio/fish-speech-1.2-sft</span></span></span></span> (532.07M parameters, 150k hours of pre-training data: 50k hours each of English, Chinese, and Japanese, and 1.5k hours of mixed data for supervised fine-tuning). Pre-trained Fish-Speech produced good timbre similarity but poor prosody. Since VQGAN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib38\" title=\"\">38</a>]</cite> in Fish-Speech affects timbre and LLaMA affects prosody, we fine-tuned the LLaMA component using LoRA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib39\" title=\"\">39</a>]</cite> with a learning rate of 1e-10 for 2000 steps.</p>\n\n",
                "matched_terms": [
                    "dda",
                    "torgo",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DSR models</span> We trained models using the ESPnet framework<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><span class=\"ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self\">https://github.com/espnet/espnet</span></span></span></span>. Training was set for a maximum of 200 epochs, with early stopping based on validation loss. For Torgo, an ASR model (70.47M parameters) pre-trained on the 960-hour LibriSpeech dataset<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><span class=\"ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self\">https://www.openslr.org/12/</span></span></span></span> was fine-tuned using the LOSO approach. We synthesized speech for the test set texts using VITS, pre-trained Fish-Speech, and fine-tuned Fish-Speech, creating settings <span class=\"ltx_text ltx_font_bold\">V</span>, <span class=\"ltx_text ltx_font_bold\">F1</span>, and <span class=\"ltx_text ltx_font_bold\">F2</span>. Synthesized data was used to further fine-tune the LOSO models. For CDSD, an ASR model (116.91M parameters) pre-trained on the 10k-hour WenetSpeech dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib40\" title=\"\">40</a>]</cite> was fine-tuned with LOSO, followed by additional fine-tuning using the DDA data. To mitigate potential negative impacts of DDA on high-intelligibility speakers (CER &lt;25%), we implemented a stepwise filtering mechanism. This involved progressing through LOSO models with Zero-Shot (<span class=\"ltx_text ltx_font_bold\">V</span>), One-Shot (<span class=\"ltx_text ltx_font_bold\">F2</span>), and All-Test-Data (<span class=\"ltx_text ltx_font_bold\">F3</span>) settings. Note that <span class=\"ltx_text ltx_font_bold\">F3</span> is identical to <span class=\"ltx_text ltx_font_bold\">F2</span> except that it includes all test data from the target speaker during the Fish-Speech fine-tuning stage. When a model&#8217;s CER on a target speaker&#8217;s speech is less than 25%, the LOSO model for that speaker is excluded from further DDA testing.</p>\n\n",
                "matched_terms": [
                    "settings",
                    "finetuning",
                    "dda",
                    "oneshot",
                    "alltestdata",
                    "loso",
                    "torgo",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> represents the speech features, <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m2\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math> is the label sequence, <math alttext=\"p(y|x)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo fence=\"false\">|</mo><mi>x</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p(y|x)</annotation></semantics></math> is the ASR model probability, <math alttext=\"p_{LM}(y)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m4\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>M</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{LM}(y)</annotation></semantics></math> is the LM probability, and <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m5\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> weights the LM&#8217;s contribution. Due to the limited amount of sentence data in Torgo (25.5%) and space constraints, we only evaluated text-only domain adaptation on CDSD to assess the impact of text adaptation. We trained ESPnet LMs (54.06M parameters) for 100 epochs using the training and test set texts (representing source and target domains) in the LOSO setup. We explored the effects of <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m6\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> values of 0.3, 0.6, and 0.8.</p>\n\n",
                "matched_terms": [
                    "loso",
                    "torgo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the CDSD dataset, lacking prior work using the LOSO setup for comparison, we first validated our model settings by building a DSR model. This was achieved by fine-tuning the pre-trained model using the data split described in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib10\" title=\"\">10</a>]</cite>. Our model achieved a CER of 20.4% on the test split, a 6.06% absolute reduction compared to the results reported in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib10\" title=\"\">10</a>]</cite>. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#S4.T2\" title=\"Table 2 &#8227; 4.3 Results and discussions &#8227; 4 Experiments &#8227; Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, our LOSO models reduced the CER for 13 speakers to below 21%. The effectiveness of our DDA methods is evident from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#S4.T2\" title=\"Table 2 &#8227; 4.3 Results and discussions &#8227; 4 Experiments &#8227; Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>: (1) The Zero-Shot setting further reduced the CER of 4 additional speakers to below 18% (p &lt;0.05), demonstrating the benefit of combining a multi-speaker dysarthric TTS model with our text-coverage strategy. (2) The One-Shot setting, which combines voice cloning and the text-coverage strategy, further improved DSR performance for the remaining 7 speakers (p &lt;0.05). Overall, we reduced the CER to below 25% for 20 speakers. However, speakers 06 and 18 (both children) and speakers 11 and 21 (both elderly men) presented greater challenges due to higher severity levels of dysarthria. This highlights the need for exploring new solutions specifically tailored to these more severely affected speakers.</p>\n\n",
                "matched_terms": [
                    "settings",
                    "finetuning",
                    "severity",
                    "dda",
                    "oneshot",
                    "loso",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We applied text-only domain adaptation to enhance LOSO models on the CDSD dataset (results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#S4.T2\" title=\"Table 2 &#8227; 4.3 Results and discussions &#8227; 4 Experiments &#8227; Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). By tuning the LM weight, <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math>, we consistently achieved performance improvements over the baseline LOSO models. Although the ESPnet framework defaults to a <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> of 0.3, increasing <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> generally improved performance for most speakers, highlighting the difficulty LOSO models face with unseen dysarthric speech. Furthermore, for most speakers, LMs trained on the training set texts (source domain) outperformed those trained on test set texts (target domain), likely due to the limited size of the test sets in the LOSO setup. However, our proposed method, which leverages only test set texts and acoustic information from the DDA model, yielded significantly greater improvements. We attribute this to our method&#8217;s focus on directly improving the LOSO model itself.</p>\n\n",
                "matched_terms": [
                    "loso",
                    "dda"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios",
        "caption": "Table 2: CER (%) on CDSD. ‘LM*’ denotes enhancing the LOSO models via text-only domain adaptation.",
        "body": "Speaker ID\n04\n05\n07\n09\n12\n13\n14\n15\n19\n20\n23\n25\n26\n\n\nPre-train ASR\n38.8\n34.9\n33.3\n63.2\n21.5\n7.0\n22.1\n57.3\n19.0\n46.3\n47.5\n54.5\n58.2\n\n\nLOSO models\n3.0\n7.1\n7.7\n18.8\n2.8\n1.5\n3.0\n15.3\n14.5\n11.8\n9.6\n18.2\n20.9\n\n\nSpeaker ID\n01\n02\n10\n16\n03\n08\n17\n06\n11\n18\n21\n\n\n\n\nPre-train ASR\n30.3\n73.8\n76.6\n54.1\n63.8\n67.1\n48.1\n96.0\n73.8\n96.0\n59.8\n\n\n\n\nLOSO models\n26.7\n40.3\n43.0\n36.5\n40.1\n49.3\n37.5\n87.1\n56.8\n94.6\n43.5\n\n\n\n\nZero-Shot\n8.7\n13.3\n17.6\n14.6\n29.4\n28.1\n29.1\n71.3\n40.9\n92.7\n36.0\n\n\n\n\nOne-Shot\n—\n—\n—\n—\n17.8\n22.2\n15.5\n63.5\n38.2\n90.9\n32.1\n\n\n\n\nAll-Test-Data\n—\n—\n—\n—\n—\n—\n—\n58.0\n36.4\n90.7\n31.1\n\n\n\n\n\nL​Ms​o​u​r​c​eLM_{source} λ=0.3\\lambda=0.3\n\n20.4\n32.5\n32.1\n38.6\n36.5\n36.4\n40.7\n86.9\n48.5\n94.4\n37.5\n\n\n\n\n\nL​Ms​o​u​r​c​eLM_{source} λ=0.6\\lambda=0.6\n\n19.1\n28.3\n25.5\n42.8\n36.5\n27.9\n45.8\n83.9\n45.9\n94.2\n35.0\n\n\n\n\n\nL​Ms​o​u​r​c​eLM_{source} λ=0.8\\lambda=0.8\n\n19.8\n28.7\n24.1\n47.5\n37.6\n26.0\n49.7\n83.0\n46.8\n94.2\n35.7\n\n\n\n\n\nL​Mt​a​r​g​e​tLM_{target} λ=0.3\\lambda=0.3\n\n24.8\n37.3\n40.6\n34.7\n39.1\n45.0\n36.4\n87.5\n52.7\n93.4\n41.3\n\n\n\n\n\nL​Mt​a​r​g​e​tLM_{target} λ=0.6\\lambda=0.6\n\n23.9\n35.2\n38.4\n33.9\n38.8\n42.6\n37.0\n84.5\n51.6\n92.2\n40.8\n\n\n\n\n\nL​Mt​a​r​g​e​tLM_{target} λ=0.8\\lambda=0.8\n\n24.1\n35.4\n38.7\n34.1\n39.4\n42.5\n38.5\n83.9\n52.8\n92.1\n41.4",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">Speaker ID</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">04</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">05</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">07</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">09</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">12</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">13</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">14</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">15</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">19</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">20</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">23</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">25</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">26</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">Pre-train ASR</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">38.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">34.9</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">33.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">63.2</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">21.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">7.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">22.1</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">57.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">19.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">46.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">47.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">54.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">58.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">LOSO models</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">3.0</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">7.1</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">7.7</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">18.8</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">2.8</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">1.5</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">3.0</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">15.3</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">14.5</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">11.8</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">9.6</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">18.2</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">20.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">Speaker ID</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">01</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">02</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">10</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">16</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">03</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">08</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">17</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">06</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">11</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">18</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">21</span></td>\n<td class=\"ltx_td ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n<td class=\"ltx_td ltx_border_tt\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">Pre-train ASR</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">30.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">73.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">76.6</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">54.1</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">63.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">67.1</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">48.1</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">96.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">73.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">96.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">59.8</td>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">LOSO models</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">26.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">40.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">43.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">36.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">40.1</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">49.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">37.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">87.1</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">56.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">94.6</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">43.5</td>\n<td class=\"ltx_td\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">Zero-Shot</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">8.7</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">13.3</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">17.6</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">14.6</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">29.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">28.1</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">29.1</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">71.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">40.9</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">92.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">36.0</td>\n<td class=\"ltx_td\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">One-Shot</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">&#8212;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">&#8212;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">&#8212;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">&#8212;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">17.8</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">22.2</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">15.5</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">63.5</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">38.2</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">90.9</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">32.1</span></td>\n<td class=\"ltx_td\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"><span class=\"ltx_text ltx_font_bold\">All-Test-Data</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">&#8212;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">&#8212;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">&#8212;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">&#8212;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">&#8212;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">&#8212;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">&#8212;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">58.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">36.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">90.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">31.1</td>\n<td class=\"ltx_td\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">\n<math alttext=\"LM_{source}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LM_{source}</annotation></semantics></math> <math alttext=\"\\lambda=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=0.3</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">20.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">32.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">32.1</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">38.6</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">36.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">36.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">40.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">86.9</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">48.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">94.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">37.5</td>\n<td class=\"ltx_td\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">\n<math alttext=\"LM_{source}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LM_{source}</annotation></semantics></math> <math alttext=\"\\lambda=0.6\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>0.6</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=0.6</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">19.1</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">28.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">25.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">42.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">36.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">27.9</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">45.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">83.9</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">45.9</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">94.2</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">35.0</td>\n<td class=\"ltx_td\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">\n<math alttext=\"LM_{source}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LM_{source}</annotation></semantics></math> <math alttext=\"\\lambda=0.8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>0.8</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=0.8</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">19.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">28.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">24.1</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">47.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">37.6</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">26.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">49.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">83.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">46.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">94.2</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">35.7</td>\n<td class=\"ltx_td\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">\n<math alttext=\"LM_{target}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>M</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LM_{target}</annotation></semantics></math> <math alttext=\"\\lambda=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=0.3</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">24.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">37.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">40.6</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">34.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">39.1</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">45.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">36.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">87.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">52.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">93.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">41.3</td>\n<td class=\"ltx_td\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">\n<math alttext=\"LM_{target}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m9\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>M</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LM_{target}</annotation></semantics></math> <math alttext=\"\\lambda=0.6\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m10\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>0.6</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=0.6</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">23.9</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">35.2</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">38.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">33.9</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">38.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">42.6</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">37.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">84.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">51.6</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">92.2</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">40.8</td>\n<td class=\"ltx_td\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">\n<math alttext=\"LM_{target}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m11\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>M</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LM_{target}</annotation></semantics></math> <math alttext=\"\\lambda=0.8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m12\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>0.8</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=0.8</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">24.1</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">35.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">38.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">34.1</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">39.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">42.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">38.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">83.9</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">52.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">92.1</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.3pt;padding-right:1.3pt;\">41.4</td>\n<td class=\"ltx_td ltx_border_bb\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n<td class=\"ltx_td ltx_border_bb\" style=\"padding-left:1.3pt;padding-right:1.3pt;\"/>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "‘lm’",
            "cer",
            "λ06lambda06",
            "cdsd",
            "via",
            "denotes",
            "asr",
            "domain",
            "adaptation",
            "oneshot",
            "loso",
            "l​ms​o​u​r​c​elmsource",
            "λ03lambda03",
            "zeroshot",
            "pretrain",
            "λ08lambda08",
            "models",
            "speaker",
            "l​mt​a​r​g​e​tlmtarget",
            "enhancing",
            "alltestdata",
            "textonly"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluated our text-coverage strategy on two sentence-level dysarthric speech datasets: the English Torgo dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib9\" title=\"\">9</a>]</cite> and the Chinese CDSD dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib10\" title=\"\">10</a>]</cite>. On the Torgo dataset, the Zero-Shot text-coverage strategy achieved a 3.29% absolute reduction in average WER, while the One-Shot text-coverage strategy resulted in a 5.98% absolute WER reduction. For the CDSD dataset, we implemented a stepwise filtering mechanism, ceasing further DDA for speakers with high intelligibility (CER &lt;25%; <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib10\" title=\"\">10</a>]</cite> test set CER: 26.46%). Finally, 20 out of 24 speakers to achieve a CER below 25%. By utilizing <span class=\"ltx_text ltx_font_bold\">a maximum of one sample</span> per speaker, the average final CER was 19.525% (calculated by averaging the boldfaced values in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#S4.T2\" title=\"Table 2 &#8227; 4.3 Results and discussions &#8227; 4 Experiments &#8227; Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
            "<p class=\"ltx_p\">For the CDSD dataset, lacking prior work using the LOSO setup for comparison, we first validated our model settings by building a DSR model. This was achieved by fine-tuning the pre-trained model using the data split described in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib10\" title=\"\">10</a>]</cite>. Our model achieved a CER of 20.4% on the test split, a 6.06% absolute reduction compared to the results reported in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib10\" title=\"\">10</a>]</cite>. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#S4.T2\" title=\"Table 2 &#8227; 4.3 Results and discussions &#8227; 4 Experiments &#8227; Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, our LOSO models reduced the CER for 13 speakers to below 21%. The effectiveness of our DDA methods is evident from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#S4.T2\" title=\"Table 2 &#8227; 4.3 Results and discussions &#8227; 4 Experiments &#8227; Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>: (1) The Zero-Shot setting further reduced the CER of 4 additional speakers to below 18% (p &lt;0.05), demonstrating the benefit of combining a multi-speaker dysarthric TTS model with our text-coverage strategy. (2) The One-Shot setting, which combines voice cloning and the text-coverage strategy, further improved DSR performance for the remaining 7 speakers (p &lt;0.05). Overall, we reduced the CER to below 25% for 20 speakers. However, speakers 06 and 18 (both children) and speakers 11 and 21 (both elderly men) presented greater challenges due to higher severity levels of dysarthria. This highlights the need for exploring new solutions specifically tailored to these more severely affected speakers.</p>\n\n",
            "<p class=\"ltx_p\">We applied text-only domain adaptation to enhance LOSO models on the CDSD dataset (results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#S4.T2\" title=\"Table 2 &#8227; 4.3 Results and discussions &#8227; 4 Experiments &#8227; Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). By tuning the LM weight, <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math>, we consistently achieved performance improvements over the baseline LOSO models. Although the ESPnet framework defaults to a <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> of 0.3, increasing <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> generally improved performance for most speakers, highlighting the difficulty LOSO models face with unseen dysarthric speech. Furthermore, for most speakers, LMs trained on the training set texts (source domain) outperformed those trained on test set texts (target domain), likely due to the limited size of the test sets in the LOSO setup. However, our proposed method, which leverages only test set texts and acoustic information from the DDA model, yielded significantly greater improvements. We attribute this to our method&#8217;s focus on directly improving the LOSO model itself.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Dysarthric speech recognition (DSR) research has witnessed remarkable progress in recent years, evolving from the basic understanding of individual words to the intricate comprehension of sentence-level expressions, all driven by the pressing communication needs of individuals with dysarthria. Nevertheless, the scarcity of available data remains a substantial hurdle, posing a significant challenge to the development of effective sentence-level DSR systems. In response to this issue, dysarthric data augmentation (DDA) has emerged as a highly promising approach. Generative models are frequently employed to generate training data for automatic speech recognition tasks. However, their effectiveness hinges on the ability of the synthesized data to accurately represent the target domain. The wide-ranging variability in pronunciation among dysarthric speakers makes it extremely difficult for models trained on data from existing speakers to produce useful augmented data, especially in zero-shot or one-shot learning settings. To address this limitation, we put forward a novel text-coverage strategy specifically designed for text-matching data synthesis. This innovative strategy allows for efficient zero/one-shot DDA, leading to substantial enhancements in the performance of DSR when dealing with unseen dysarthric speakers. Such improvements are of great significance in practical applications, including dysarthria rehabilitation programs and day-to-day common-sentence communication scenarios.</p>\n\n",
                "matched_terms": [
                    "oneshot",
                    "zeroshot",
                    "domain",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Dysarthria, a speech disorder characterized by impaired vocal control, is commonly associated with neurological conditions such as cerebral palsy and Parkinson&#8217;s disease. It leads to speech that exhibits pronunciation errors and irregular prosody, significantly hindering automatic speech recognition (ASR) systems trained on typical speech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib1\" title=\"\">1</a>]</cite>. Research in dysarthric speech recognition (DSR) primarily focuses on word recognition <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib3\" title=\"\">3</a>]</cite>. Current methodologies include speaker-independent models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib5\" title=\"\">5</a>]</cite>, which often struggle to generalize to unseen dysarthric speakers due to pronunciation variability, and speaker-dependent models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib6\" title=\"\">6</a>]</cite>, which necessitate continuous data collection and costly fine-tuning to maintain optimal performance. Wang et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib7\" title=\"\">7</a>]</cite> proposed a prototype-based, fine-tuning-free classification method for word-level DSR that utilizes speaker-specific pronunciation patterns for rapid and effective adaptation. However, DSR now faces the challenge of achieving sentence-level understanding. Sentence-level DSR is particularly sensitive to pronunciation patterns, including breathing sounds, abnormal pauses (which can lead to insertion errors), and mispronunciations (resulting in replacement errors). Additionally, sentence-level DSR encounters challenges related to larger vocabularies and coarticulation effects, requiring more targeted data for effective adaptation.</p>\n\n",
                "matched_terms": [
                    "adaptation",
                    "models",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Acquiring data from dysarthric speakers presents challenges due to participant limitations and recording difficulties. The unique characteristics of dysarthric speech further complicate the annotation process, requiring specialized expertise. Consequently, few open-source dysarthric datasets exist, primarily at the word or command level <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib8\" title=\"\">8</a>]</cite>. While some sentence-level dysarthric datasets are available <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib10\" title=\"\">10</a>]</cite>, their quantity remains limited. To address this data scarcity, dysarthric data augmentation (DDA) is essential. Current sentence-level DDA methods employ either Zero-Shot (simulating general dysarthric characteristics without using target speaker speech data) or All-Test-Data <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib12\" title=\"\">12</a>]</cite> (utilizing all available test data from the target speaker to train speech generative models and then using these models to synthesize training data) settings. The All-Test-Data setting exhibits the best performance but represents an idealized scenario. Our work focuses on the more practical Zero-Shot and One-Shot settings with speech generative models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speaker",
                    "oneshot",
                    "alltestdata",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Data augmentation in ASR, which typically involves synthesizing speech from texts in the training set, encounters challenges when applied to DSR. The limited size of dysarthric datasets often results in content mismatches between the training and target domains. Additionally, pronunciation variations between unseen target speakers and those in the training set exacerbate this mismatch. Consequently, synthetic data generated from training set texts using speech generative models trained on either typical speech or existing dysarthric speakers may not accurately capture the characteristics of the target domain. Rosenberg et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib13\" title=\"\">13</a>]</cite> established a performance benchmark by synthesizing speech from test set texts, suggesting that while this data augmentation setting may be impractical for general ASR, it holds value for specific domain applications. We consider sentence-level DSR, a newly developed and challenging task, to be one such application. Chen et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib14\" title=\"\">14</a>]</cite> emphasized the necessity for the texts of synthesized speech to closely align with the target domain to enhance ASR performance. Yang et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib15\" title=\"\">15</a>]</cite> highlighted that text content is more crucial than speaking style in speaker adaptation, positing that performance improvements arise from synthesized speech standardizing the ASR model&#8217;s language understanding. Based on these insights, we propose a text-coverage strategy (see Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>), evaluating its effectiveness in Zero-Shot and One-Shot DDA settings. This strategy utilizes speech generative models to synthesize speech data directly from pre-defined texts. Although limiting the target domain content may not be ideal for general ASR, it offers advantages for applications such as dysarthria rehabilitation or scenarios where dysarthric speakers use common sentences for communication, particularly given the prior focus of DSR on word recognition. Considering the limited dysarthric data and the prosody variability of dysarthric speech, we selected VITS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib16\" title=\"\">16</a>]</cite> for Zero-Shot and Fish-Speech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib17\" title=\"\">17</a>]</cite> for One-Shot as our DDA models (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#S3\" title=\"3 Strategy design and model selection &#8227; Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> for selection analysis). To our knowledge, this is the first application of these models in the context of DDA.</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr",
                    "domain",
                    "speaker",
                    "adaptation",
                    "oneshot",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our text-coverage strategy leverages prior knowledge of the target domain&#8217;s content and incorporates dysarthric acoustic knowledge from the speech generative model. Related work in text-only domain adaptation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib19\" title=\"\">19</a>]</cite> typically employs language models (LMs) trained on target domain text data to improve ASR performance via shallow fusion of ASR and LM outputs during inference. This approach enhances the fluency of predicted text and adapts to the target domain&#8217;s linguistic characteristics. We compared this approach with ours on the CDSD dataset and found that while it can improve performance, our method yields more significant gains.</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr",
                    "domain",
                    "adaptation",
                    "cdsd",
                    "via",
                    "textonly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a novel and efficient text-coverage strategy that enables the first sentence-level zero-shot and one-shot DDA based on speech generative models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "zeroshot",
                    "oneshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the effectiveness of our proposed DDA strategy and selected models using the Torgo and CDSD dysarthric datasets.</p>\n\n",
                "matched_terms": [
                    "models",
                    "cdsd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Reusing speech generative models</span> Recent DDA research has adapted Voice Conversion (VC) and Text-to-Speech (TTS) models, originally designed for typical speech, by training or fine-tuning them using dysarthric data. Models that have been adopted include: (1) attention-based VC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib29\" title=\"\">29</a>]</cite>, (2) sparse-structured spectral envelope transformation-based VC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib30\" title=\"\">30</a>]</cite>, (3) autoencoder-based VC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib31\" title=\"\">31</a>]</cite>, (4) diffusion-based VC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib32\" title=\"\">32</a>]</cite> and TTS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib12\" title=\"\">12</a>]</cite>. Beyond directly reusing models, Soleymanpour et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib11\" title=\"\">11</a>]</cite> modified a multi-speaker TTS system by incorporating a dysarthria severity level coefficient and a pause insertion model, enabling the synthesis of dysarthric speech across varying severity levels. While these methods show promise, Hu et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib33\" title=\"\">33</a>]</cite> demonstrated that some TTS models struggle to generate natural-sounding dysarthric speech, requiring larger datasets or longer training times. Furthermore, with the exception of <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib12\" title=\"\">12</a>]</cite>, these works primarily focus on enhancing word recognition, and this approach has not fully kept pace with the progress of typical speech TTS models, nor has it explored Voice Cloning technology.</p>\n\n",
                "matched_terms": [
                    "models",
                    "enhancing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-coverage strategy</span> To enhance the performance of Dysarthria Speech Recognition (DSR) models for unseen dysarthric speakers, improving the generalization ability of the DSR model or adapting the model is crucial. However, constructing a speaker-independent and text-independent DSR model requires large and diverse training data. To our knowledge, the number of speakers and texts covered by the widely-used Torgo and CDSD datasets remains limited. Constructing a speaker-dependent DSR model requires a large amount of comprehensive speech data from the target speakers to capture their unique pronunciation patterns and prosodic variations, but collecting and annotating dysarthric data are very difficult. Building on the findings discussed in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib15\" title=\"\">15</a>]</cite> in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#S1\" title=\"1 Introduction &#8227; Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we propose the text-coverage strategy (see Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). This strategy involves synthesizing speech data based on the pre-defined texts content of the target scenario, allowing the DSR model to adapt proactively and enhance its performance.</p>\n\n",
                "matched_terms": [
                    "models",
                    "cdsd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Torgo dataset</span> The Torgo dataset comprises English speech from 8 dysarthric speakers and 7 control speakers. After filtering out samples with transcription errors and unrestricted content, a total of 16,582 data segments remained, with an approximate duration of 13.67 hours. Using the data from the 8 dysarthric speakers, we fine-tuned the pre-trained model using the leave-one-speaker-out (LOSO) method to construct the LOSO model. For the DDA models, all data except that of the target dysarthric speaker were used during training or fine-tuning, including data from the control speakers.</p>\n\n",
                "matched_terms": [
                    "loso",
                    "models",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CDSD dataset</span> We utilize the initial release of the CDSD dataset, comprising 29,466 Chinese recordings from 24 dysarthric speakers. Using the data from these 24 speakers, we also fine-tune the pre-trained model with the LOSO method to construct a LOSO model for DSR. For the DDA models, we incorporate 12,885 recordings (14.38 hours) from 31 dysarthric speakers with intelligibility annotations from the AISHELL-6B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><span class=\"ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self\">https://www.aishelltech.com/AISHELL_6B</span></span></span></span> dataset during training or fine-tuning</p>\n\n",
                "matched_terms": [
                    "loso",
                    "models",
                    "cdsd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DSR models</span> We trained models using the ESPnet framework<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><span class=\"ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self\">https://github.com/espnet/espnet</span></span></span></span>. Training was set for a maximum of 200 epochs, with early stopping based on validation loss. For Torgo, an ASR model (70.47M parameters) pre-trained on the 960-hour LibriSpeech dataset<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><span class=\"ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self\">https://www.openslr.org/12/</span></span></span></span> was fine-tuned using the LOSO approach. We synthesized speech for the test set texts using VITS, pre-trained Fish-Speech, and fine-tuned Fish-Speech, creating settings <span class=\"ltx_text ltx_font_bold\">V</span>, <span class=\"ltx_text ltx_font_bold\">F1</span>, and <span class=\"ltx_text ltx_font_bold\">F2</span>. Synthesized data was used to further fine-tune the LOSO models. For CDSD, an ASR model (116.91M parameters) pre-trained on the 10k-hour WenetSpeech dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib40\" title=\"\">40</a>]</cite> was fine-tuned with LOSO, followed by additional fine-tuning using the DDA data. To mitigate potential negative impacts of DDA on high-intelligibility speakers (CER &lt;25%), we implemented a stepwise filtering mechanism. This involved progressing through LOSO models with Zero-Shot (<span class=\"ltx_text ltx_font_bold\">V</span>), One-Shot (<span class=\"ltx_text ltx_font_bold\">F2</span>), and All-Test-Data (<span class=\"ltx_text ltx_font_bold\">F3</span>) settings. Note that <span class=\"ltx_text ltx_font_bold\">F3</span> is identical to <span class=\"ltx_text ltx_font_bold\">F2</span> except that it includes all test data from the target speaker during the Fish-Speech fine-tuning stage. When a model&#8217;s CER on a target speaker&#8217;s speech is less than 25%, the LOSO model for that speaker is excluded from further DDA testing.</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr",
                    "speaker",
                    "oneshot",
                    "cdsd",
                    "alltestdata",
                    "loso",
                    "cer",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-only domain adaptation</span> uses shallow fusion of ASR and language model (LM) predictions during inference:</p>\n\n",
                "matched_terms": [
                    "adaptation",
                    "textonly",
                    "asr",
                    "domain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> represents the speech features, <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m2\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math> is the label sequence, <math alttext=\"p(y|x)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo fence=\"false\">|</mo><mi>x</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p(y|x)</annotation></semantics></math> is the ASR model probability, <math alttext=\"p_{LM}(y)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m4\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>M</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{LM}(y)</annotation></semantics></math> is the LM probability, and <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m5\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> weights the LM&#8217;s contribution. Due to the limited amount of sentence data in Torgo (25.5%) and space constraints, we only evaluated text-only domain adaptation on CDSD to assess the impact of text adaptation. We trained ESPnet LMs (54.06M parameters) for 100 epochs using the training and test set texts (representing source and target domains) in the LOSO setup. We explored the effects of <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m6\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> values of 0.3, 0.6, and 0.8.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "domain",
                    "adaptation",
                    "cdsd",
                    "loso",
                    "textonly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The experimental results on the Torgo dataset are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#S3.T1\" title=\"Table 1 &#8227; 3 Strategy design and model selection &#8227; Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Espana Bonnet et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib36\" title=\"\">36</a>]</cite> compared the performance of various DNN architectures on Torgo, but none of the models achieved optimal results for all speakers. For our comparison, we selected the best performance for each speaker from <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib36\" title=\"\">36</a>]</cite>. Joy et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib37\" title=\"\">37</a>]</cite> improved DSR performance by adjusting DNN architecture parameters. Soleymanpour et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib11\" title=\"\">11</a>]</cite> employed a DNN-HMM architecture for DSR, while Leung et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib12\" title=\"\">12</a>]</cite> utilized the large speech model Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib41\" title=\"\">41</a>]</cite> for DSR. We include the results from <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib12\" title=\"\">12</a>]</cite> both before and after DDA. DDA methods used in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib12\" title=\"\">12</a>]</cite> are described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#S2\" title=\"2 Previous methods &#8227; Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Our LOSO models demonstrate strong performance without DDA. The Zero-Shot setting, which employs the DDA method that combines the VITS model with the text-coverage strategy, achieves a significant 3.29% absolute decrease in average WER. Further improvements are observed in the One-Shot setting, where Fish-Speech utilizes a single speech sample from the target dysarthric speaker to generate data that is more aligned with the target domain, leading to enhanced performance across all severity levels. Compared to the results of <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib12\" title=\"\">12</a>]</cite> enhanced under the All-Test-Data setting, our One-Shot results significantly outperform <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib11\" title=\"\">11</a>]</cite> (p-value &lt;0.05) and achieve comparable performance to <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16700v1#bib.bib12\" title=\"\">12</a>]</cite>. Notably, we achieve these results using only one sample from the unseen target speaker, indicating that our method of leveraging Fish-Speech to learn the speaking style while aligning with the target text domain is an effective DDA approach.</p>\n\n",
                "matched_terms": [
                    "models",
                    "domain",
                    "speaker",
                    "oneshot",
                    "alltestdata",
                    "loso",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a novel text-coverage strategy to improve sentence-level dysarthric speech recognition for unseen speakers in constrained scenarios.\nThis strategy, combined with VITS and Fish-Speech, enables the first sentence-level zero-/one-shot dysarthric data augmentation using speech generative models. We have validated our strategy on the English dysarthric dataset Torgo and the Chinese dysarthric dataset CDSD.</p>\n\n",
                "matched_terms": [
                    "models",
                    "cdsd"
                ]
            }
        ]
    }
}