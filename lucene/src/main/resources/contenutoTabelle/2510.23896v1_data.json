{
    "S2.T1": {
        "source_file": "AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages",
        "caption": "Table 1: Tasks, task families and datasets included in AfriMTEB (Full). The task families are Bitext Mining (Btxt), Pair Classification (Pr Clf), Classification (Clf), Semantic Text Similarity (STS), Multi-label Classification (Multi. Clf), Retrieval (Rtrvl), Clustering (Clust) and Reranking (Rrnk). We introduce additional the five datasets highlighted in bold.",
        "body": "Task\n\nNumber of Languages\n\n\nTask\nFamily\nDatasets (AfriMTEB Full)\nin MMTEB\nin AfriMTEB\n\n\nBitext Mining\nBtxt\nFlores, NTREX, BibleNLP, NollySenti, Tatoeba\n59\n59\n\n\nNLI\nPr Clf\nXNLI, AfriXNLI\n1\n15\n\n\nTopic class.\nClf\nSIB200Classification, SIB200_14Classes\n0\n56\n\n\nNews Topic class.\nClf\nMasakhaNEWS, TswanaNews, SiswatiNews, SwahiliNews\n16\n17\n\n\n\n\nIsiZuluNews, KinNews\n\n\n\n\nSentiment\nClf\nNaijaSenti, AfriSenti, MultilingualSentiment\n11\n12\n\n\nHate speech\nClf\nAfriHate\n0\n14\n\n\nLID\nClf\nLanguageClassification, SouthAfricanLangClassification\n0\n18\n\n\n\nClf\nAfriSentiLangClassification\n\n\n\n\nIntent\nClf\nMassiveIntent, InjongoIntent\n3\n16\n\n\nScenario\nClf\nMassiveScenario\n0\n3\n\n\nEmotion\nMulti. Clf\nEmotionAnalysisPlus\n0\n14\n\n\nSemantic relatedness\nSTS\nSemRel24STS\n7\n7\n\n\nRetrieval\nRtrvl\nBelebele, MIRACL, MIRACLRetrievalHardNegatives,\n31\n31\n\n\n\n\nMrTidy, XQuAD, XM3600T2I\n\n\n\n\nClustering\nClust\nSIB200ClusteringFast, MasakhaNEWSClusteringP2P\n58\n58\n\n\n\n\nMasakhaNEWSClusteringS2S\n\n\n\n\nReranking\nRrnk\nMIRACLReranking\n0\n2",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Task</span></th>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Number of Languages</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Task</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Family</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Datasets (AfriMTEB Full)</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">in MMTEB</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">in AfriMTEB</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Bitext Mining</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Btxt</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">Flores, NTREX, BibleNLP, NollySenti, Tatoeba</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">59</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">59</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">NLI</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">Pr&#160;Clf</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-bg-color:#D9D9D9;\">XNLI, <span class=\"ltx_text ltx_font_bold\">AfriXNLI</span></span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">1</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">15</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">Topic class.</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">Clf</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-bg-color:#D9D9D9;\">SIB200Classification, <span class=\"ltx_text ltx_font_bold\">SIB200_14Classes</span></span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">0</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">56</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">News Topic class.</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">Clf</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-bg-color:#D9D9D9;\">MasakhaNEWS, TswanaNews, SiswatiNews, SwahiliNews</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">16</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">17</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-bg-color:#D9D9D9;\">IsiZuluNews, <span class=\"ltx_text ltx_font_bold\">KinNews</span></span></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Sentiment</td>\n<td class=\"ltx_td ltx_align_left\">Clf</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_italic\">NaijaSenti, AfriSenti, MultilingualSentiment</span></td>\n<td class=\"ltx_td ltx_align_right\">11</td>\n<td class=\"ltx_td ltx_align_right\">12</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">Hate speech</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">Clf</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"--ltx-bg-color:#D9D9D9;\">AfriHate</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">0</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">14</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">LID</td>\n<td class=\"ltx_td ltx_align_left\">Clf</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_italic\">LanguageClassification, SouthAfricanLangClassification</span></td>\n<td class=\"ltx_td ltx_align_right\">0</td>\n<td class=\"ltx_td ltx_align_right\">18</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">Clf</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_italic\">AfriSentiLangClassification</span></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">Intent</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">Clf</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-bg-color:#D9D9D9;\">MassiveIntent, <span class=\"ltx_text ltx_font_bold\">InjongoIntent</span></span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">3</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">16</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Scenario</td>\n<td class=\"ltx_td ltx_align_left\">Clf</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_italic\">MassiveScenario</span></td>\n<td class=\"ltx_td ltx_align_right\">0</td>\n<td class=\"ltx_td ltx_align_right\">3</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">Emotion</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">Multi. Clf</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"--ltx-bg-color:#D9D9D9;\">EmotionAnalysisPlus</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">0</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">14</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Semantic relatedness</td>\n<td class=\"ltx_td ltx_align_left\">STS</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_italic\">SemRel24STS</span></td>\n<td class=\"ltx_td ltx_align_right\">7</td>\n<td class=\"ltx_td ltx_align_right\">7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Retrieval</td>\n<td class=\"ltx_td ltx_align_left\">Rtrvl</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_italic\">Belebele, MIRACL, MIRACLRetrievalHardNegatives,</span></td>\n<td class=\"ltx_td ltx_align_right\">31</td>\n<td class=\"ltx_td ltx_align_right\">31</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_italic\">MrTidy, XQuAD, XM3600T2I</span></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Clustering</td>\n<td class=\"ltx_td ltx_align_left\">Clust</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_italic\">SIB200ClusteringFast, MasakhaNEWSClusteringP2P</span></td>\n<td class=\"ltx_td ltx_align_right\">58</td>\n<td class=\"ltx_td ltx_align_right\">58</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_italic\">MasakhaNEWSClusteringS2S</span></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Reranking</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Rrnk</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_italic\">MIRACLReranking</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">0</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">2</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "news",
            "swahilinews",
            "nli",
            "retrieval",
            "isizulunews",
            "afrisentilangclassification",
            "class",
            "mrtidy",
            "biblenlp",
            "clustering",
            "miraclretrievalhardnegatives",
            "family",
            "afrisenti",
            "semrel24sts",
            "massivescenario",
            "sib200classification",
            "mining",
            "southafricanlangclassification",
            "number",
            "semantic",
            "belebele",
            "sentiment",
            "btxt",
            "families",
            "clust",
            "datasets",
            "bitext",
            "classification",
            "tatoeba",
            "masakhanews",
            "reranking",
            "sts",
            "pair",
            "similarity",
            "afrimteb",
            "masakhanewsclusteringp2p",
            "tswananews",
            "emotion",
            "languages",
            "included",
            "tasks",
            "text",
            "miracl",
            "introduce",
            "kinnews",
            "sib200clusteringfast",
            "additional",
            "speech",
            "clf",
            "nollysenti",
            "highlighted",
            "massiveintent",
            "xm3600t2i",
            "mmteb",
            "afrihate",
            "rrnk",
            "afrixnli",
            "bold",
            "lid",
            "full",
            "rtrvl",
            "multilabel",
            "multilingualsentiment",
            "sib20014classes",
            "task",
            "xquad",
            "naijasenti",
            "flores",
            "topic",
            "scenario",
            "multi",
            "intent",
            "five",
            "masakhanewsclusterings2s",
            "relatedness",
            "hate",
            "xnli",
            "ntrex",
            "injongointent",
            "languageclassification",
            "miraclreranking",
            "emotionanalysisplus",
            "siswatinews"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> follows the <span class=\"ltx_text ltx_font_typewriter\">MTEB</span> taxonomy and groups tasks into eight families. <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S2.T1\" title=\"Table 1 &#8227; 2 AfriMTEB Benchmark &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> lists the tasks, task families and the datasets included in the <em class=\"ltx_emph ltx_font_italic\">Full</em> suite. Each dataset belongs to one of the eight task families: (1) Bibtext mining (2) Pair classification (3) Semantic Text Similarity (STS) (4) Clustering (5) Classification (6) Multi-label Classification (7) Retrieval (8). The first four task families performs a similarity related tasks based on two pairs of sentences, either to identity translation pairs, categorize them into a class (e.g., entailment vs. contradiction) or provide a similarity score. The next three tasks (5) - (6) classify a sentence or article into one or more classes. Finally, the last sets of tasks (7) and (8) are information retrieval tasks for retrieval or re-ranking based on a user query. We provide full description in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.SS1\" title=\"A.1 Task categories &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">AfriMTEB follows the MTEB taxonomy and groups tasks into eight families. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S2.T1\" title=\"Table 1 &#8227; 2 AfriMTEB Benchmark &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> lists the families and the datasets included in the <em class=\"ltx_emph ltx_font_italic\">Full</em> suite.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Text embeddings are an essential building component of several NLP tasks such as retrieval-augmented generation which is crucial for preventing hallucinations in LLMs. Despite the recent release of massively multilingual MTEB (MMTEB), African languages remain underrepresented, with existing tasks often repurposed from translation benchmarks such as FLORES clustering or SIB-200.\nIn this paper, we introduce AfriMTEB&#8212;a regional expansion of MMTEB covering 59 languages, 14 tasks, and 38 datasets, including six newly added datasets. Unlike many MMTEB datasets that include fewer than five languages, the new additions span 14 to 56 African languages and introduce entirely new tasks, such as hate speech detection, intent detection, and emotion classification, which were not previously covered. Complementing this, we present AfriE5, an adaptation of the instruction-tuned mE5 model to African languages through cross-lingual contrastive distillation. Our evaluation shows that AfriE5 achieves state-of-the-art performance, outperforming strong baselines such as Gemini-Embeddings and mE5.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "classification",
                    "clustering",
                    "hate",
                    "tasks",
                    "text",
                    "mmteb",
                    "emotion",
                    "five",
                    "introduce",
                    "flores",
                    "datasets",
                    "languages",
                    "intent"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "afrimteb",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Text embeddings are core building blocks for NLP systems in information retrieval, clustering, semantic similarity, and classification <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib15\" title=\"\">2022</a>; Feng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib13\" title=\"\">2022</a>)</cite>. However, evaluations on diverse tasks are often limited to a few high resource languages such as English&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib26\" title=\"\">2023</a>)</cite> or Chinese&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib48\" title=\"\">2024</a>)</cite>. Many under-represented languages are excluded due to lack of datasets or non-discoverability of community-created benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ojo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib34\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "clustering",
                    "tasks",
                    "text",
                    "similarity",
                    "retrieval",
                    "datasets",
                    "languages",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While recent text embedding benchmarks have improved language coverage in recent years, such as MMTEB&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Enevoldsen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib11\" title=\"\">2025</a>)</cite>, African languages remain under-represented, where many of the tasks covered are either based on massively evaluation of translation datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB-Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib33\" title=\"\">2022</a>; Federmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib12\" title=\"\">2022</a>)</cite>, and the tasks derived or repurposed from translation benchmarks such as Belebele&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bandarkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib5\" title=\"\">2024</a>)</cite> and SIB-200&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib1\" title=\"\">2024</a>)</cite>. As a result, the quality of text embeddings for languages in the African region remains unknown, as the region has few standardized tools for comparing models across tasks and languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alabi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib4\" title=\"\">2025</a>)</cite></p>\n\n",
                "matched_terms": [
                    "tasks",
                    "text",
                    "mmteb",
                    "datasets",
                    "languages",
                    "belebele"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">AfriMTEB</span>, a regional extension of MTEB tailored to African languages and tasks. <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;2</span></a> shows the full suite that covers 59 languages and 38 datasets spanning Bitext mining, Classification (single, pair and multi-label), Semantic Text Similarity, Retrieval, Clustering, and Reranking. To support fast-and-compute friendly evaluation, we also provide <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">AfriMTEB-Lite</span>, a compact suite over nine geographically-diverse languages (Amharic, Oromo, Igbo, Yoruba, Hausa, Swahili, Kinyarwanda, Xhosa, Zulu) constructed by selecting, within each task family, the maximally overlapping tasks such that every task includes all nine languages, ensuring uniform language coverage across the suite.</p>\n\n",
                "matched_terms": [
                    "task",
                    "tasks",
                    "text",
                    "introduce",
                    "multilabel",
                    "retrieval",
                    "datasets",
                    "bitext",
                    "classification",
                    "clustering",
                    "reranking",
                    "family",
                    "pair",
                    "similarity",
                    "afrimteb",
                    "mining",
                    "semantic",
                    "full",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to benchmarking, we develop <span class=\"ltx_text ltx_font_bold ltx_font_italic\">AfriE5-Large-Instruct</span>\nadapting strong embedding models to African languages.\nStarting from the instruction-tuned <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib42\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">c</a>)</cite>, we leveraged cross-lingual contrastive distillation. We construct supervision by translating MNLI/SNLI datasets<cite class=\"ltx_cite ltx_citemacro_citep\">(Williams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib47\" title=\"\">2018</a>; Bowman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib7\" title=\"\">2015</a>)</cite> in African languages with NLLB-200 (3.3B) <cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB-Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib33\" title=\"\">2022</a>)</cite>, followed by automatic filtering by SSA-COMET (an African COMET-based QE metric)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib19\" title=\"\">2025</a>)</cite>, and expanding each example into multiple sources or target directions to encourage cross-lingual alignment. We then use the receipe of BGE Re-ranker v2 m3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite> for text embedding model training.</p>\n\n",
                "matched_terms": [
                    "text",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate widely used baselines, including BGE-M3, E5 variants (e.g., e5-mistral-7b-instruct), Qwen embeddings, Gemini embedding-001, and Embedding Gemma <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">2024c</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib50\" title=\"\">2025</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib18\" title=\"\">2025</a>)</cite>. On <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB-Lite</span>, our adapted model <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span>, trained only on nine African languages, achieves an average score of 63.7, surpassing me5-large-instruct (62.0) and Gemini embedding-001 (63.1). Remarkably, despite being tuned with data and languages aligned to the <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB-Lite</span>, our model leveraged cross-lingual transfer to generalize to the broader <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> benchmark covering 59 languages and 38 datasets, where it also delivers the best performance with an average score of 62.4, ahead of me5-large-instruct (61.3) and Gemini embedding-001 (60.6) as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>. These results highlight that targeted cross-lingual adaptation on a carefully selected subset of languages can transfer effectively to a much larger set, yielding consistent improvements across task families while preserving the backbone&#8217;s general utility.</p>\n\n",
                "matched_terms": [
                    "task",
                    "families",
                    "afrimteb",
                    "datasets",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">AfriMTEB</span> is a regional extension of MTEB designed to evaluate text embeddings for African languages across multiple task families. To address persistent gaps in language coverage and enable systematic, comparable assessment, we introduce <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span>as a standardized benchmark for African languages. It consists of a <span class=\"ltx_text ltx_font_bold\">Full suite</span> covering <span class=\"ltx_text ltx_font_italic\">59 languages</span> and <span class=\"ltx_text ltx_font_italic\">38 datasets</span>, and a <span class=\"ltx_text ltx_font_bold\">Lite suite</span> that maintains uniform language coverage for <span class=\"ltx_text ltx_font_italic\">9 target languages</span> over <span class=\"ltx_text ltx_font_italic\">13 representative datasets</span>. Languages are selected to span major families and geographic regions while prioritizing practical relevance and the availability of reliable public datasets; tasks reflect core embedding applications and draw on established, well-curated resources. The Lite suite supports rapid iteration by enforcing consistent task coverage across languages through maximal task&#8211;language overlap.</p>\n\n",
                "matched_terms": [
                    "task",
                    "full",
                    "tasks",
                    "text",
                    "families",
                    "introduce",
                    "datasets",
                    "afrimteb",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> builds on the Massive Text Embedding Benchmark (MTEB) by selecting tasks that include African languages. From the original benchmark, we inherit datasets covering bitext mining (e.g., Flores, NTREX, Tatoeba), pair classification (XNLI), topic and sentiment classification (SIB-200, AfriSenti, MasakhaNEWS), semantic textual similarity (SemRel24), retrieval (MIRACL, XQuAD, XM3600), clustering (SIB-200, MasakhaNEWS), and reranking (MIRACL).</p>\n\n",
                "matched_terms": [
                    "miracl",
                    "text",
                    "xquad",
                    "tasks",
                    "flores",
                    "topic",
                    "retrieval",
                    "datasets",
                    "bitext",
                    "classification",
                    "clustering",
                    "tatoeba",
                    "masakhanews",
                    "reranking",
                    "afrisenti",
                    "xnli",
                    "ntrex",
                    "pair",
                    "similarity",
                    "afrimteb",
                    "mining",
                    "semantic",
                    "sentiment",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, language coverage in these datasets is uneven: some tasks include only a few African languages, while others span broader multilingual settings. To ensure fairness, we compute macro averages over languages within each task, then average across tasks and families to obtain the overall <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> score. This prevents any single task or language from dominating the benchmark.</p>\n\n",
                "matched_terms": [
                    "task",
                    "tasks",
                    "families",
                    "afrimteb",
                    "datasets",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <em class=\"ltx_emph ltx_font_italic\">Full</em> suite is built by selecting datasets from <span class=\"ltx_text ltx_font_typewriter\">MTEB</span> that include African languages and extending them with six additional datasets. These additions broaden task coverage, increase difficulty, and improve language representation.</p>\n\n",
                "matched_terms": [
                    "task",
                    "full",
                    "datasets",
                    "languages",
                    "additional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An African extension of the XNLI benchmark that provides natural language inference data to 15 African languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib3\" title=\"\">2025</a>)</cite>. By including AfriXNLI, we expand language coverage for the pair classification family beyond a single African language (Swahili), ensuring broader representation of African languages in entailment-style tasks.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "tasks",
                    "xnli",
                    "pair",
                    "afrixnli",
                    "languages",
                    "family"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A multilabel emotion data set that covers 32 languages including 14 African languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Belay et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib6\" title=\"\">2025</a>; Muhammad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib30\" title=\"\">2025b</a>)</cite>. Each sentence may be assigned multiple emotion labels such as <span class=\"ltx_text ltx_font_smallcaps\">joy</span>, <span class=\"ltx_text ltx_font_smallcaps\">anger</span>, <span class=\"ltx_text ltx_font_smallcaps\">sadness</span>, or <span class=\"ltx_text ltx_font_smallcaps\">fear</span>. By including this dataset, AfriMTEB introduces the first <em class=\"ltx_emph ltx_font_italic\">multi-label classification</em> task for African languages, thereby broadening the taxonomy beyond single-label settings.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "task",
                    "emotion",
                    "languages",
                    "afrimteb",
                    "multilabel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A multilingual hate-speech classification dataset covering 14 African languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Muhammad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib27\" title=\"\">2025a</a>)</cite>. Each instance is labeled as <span class=\"ltx_text ltx_font_smallcaps\">hate</span>, <span class=\"ltx_text ltx_font_smallcaps\">abusive</span>, or <span class=\"ltx_text ltx_font_smallcaps\">neutral</span>, providing a standardized benchmark for toxic content detection across diverse languages and registers. This dataset extends evaluation to socially relevant safety applications.</p>\n\n",
                "matched_terms": [
                    "hate",
                    "classification",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Injongo dataset is a multilingual resource for intent detection covering 16 African languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib49\" title=\"\">2025</a>)</cite>. It consists of short, conversational utterances annotated with 40 everyday intent categories, for example requests such as &#8220;freeze account&#8221; or &#8220;play music.&#8221; By focusing on dialogue-style classification, Injongo complements existing benchmarks like MASSIVE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">FitzGerald et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib14\" title=\"\">2023</a>)</cite>, but offers broader African language coverage.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "intent",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A Kinyarwanda news topic classification dataset with labels covering domains such as politics, business, and sports&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Niyongabo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib32\" title=\"\">2020</a>)</cite>. We include this dataset because <span class=\"ltx_text ltx_font_italic\">MasakhaNEWS</span> does not cover Kinyarwanda. Adding <span class=\"ltx_text ltx_font_italic\">KinNews</span> ensures we can cover Kinyarwanda in the AfriMTEB-Lite, since the Lite version requires maximally overlapping tasks.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "news",
                    "tasks",
                    "masakhanews",
                    "kinnews",
                    "topic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A more challenging variant of the SIB-200 dataset where labels are consolidated into 14 categories. This version still includes 56 African languages but is not limited to them&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib1\" title=\"\">2024</a>)</cite>. By merging fine-grained topics into broader classes, intra-class diversity increases, which raises task difficulty. The inclusion of this dataset not only strengthens African language evaluation but also increases the difficulty for other covered languages in SIB-200, leading to more robust cross-lingual assessment.</p>\n\n",
                "matched_terms": [
                    "task",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we added <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SIB200Classification</span> dataset to complement <span class=\"ltx_text ltx_font_italic\">SIB-200_14Classes</span> in the &#8220;<span class=\"ltx_text ltx_font_typewriter\">Clf</span>&#8221; task family, although not new. The only difference is that the former covers seven categories compared to the latter. The original MTEB only considers SIB-200 in the Clustering (<span class=\"ltx_text ltx_font_typewriter\">Clust</span>) task family.</p>\n\n",
                "matched_terms": [
                    "clustering",
                    "task",
                    "sib200classification",
                    "clust",
                    "sib20014classes",
                    "family"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <em class=\"ltx_emph ltx_font_italic\">Lite</em> suite focuses on nine geographically-diverse African languages:\n<span class=\"ltx_text ltx_font_italic\">Amharic, Oromo, Igbo, Yoruba, Hausa, Swahili, Kinyarwanda, Xhosa, Zulu</span>.\nWe selected only those datasets from MTEB that include nine languages to guarantee consistent language&#8211;task alignment across families. The result is a compact yet representative benchmark of 13 datasets&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We merged two datasets: MasakhaNEWS and KinNEWS since they have similar structure, thus we report 12 datasets</span></span></span> spanning classification, retrieval, bitext mining, clustering, pair classification, and multi-label classification.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "clustering",
                    "masakhanews",
                    "families",
                    "languages",
                    "pair",
                    "kinnews",
                    "retrieval",
                    "datasets",
                    "bitext",
                    "multilabel",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"s_{i,j}=\\mathbf{q}i^{\\top}\\mathbf{p}_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mi>&#119850;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>i</mi><mo>&#8868;</mo></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#119849;</mi><mi>j</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">s_{i,j}=\\mathbf{q}i^{\\top}\\mathbf{p}_{j}</annotation></semantics></math> is the similarity between query embedding <math alttext=\"\\mathbf{q}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#119850;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{q}_{i}</annotation></semantics></math> and passage embedding <math alttext=\"\\mathbf{p}_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#119849;</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{p}_{j}</annotation></semantics></math>, and <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m4\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> is a temperature hyperparameter. The numerator contains the similarity between query <math alttext=\"\\mathbf{q}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m5\" intent=\":literal\"><semantics><msub><mi>&#119850;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{q}_{i}</annotation></semantics></math> and its corresponding positive passage <math alttext=\"\\mathbf{p}{i\\cdot G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>&#119849;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><mi>G</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{p}{i\\cdot G}</annotation></semantics></math>. The denominator includes all similarities between <math alttext=\"\\mathbf{q}i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m7\" intent=\":literal\"><semantics><mrow><mi>&#119850;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{q}i</annotation></semantics></math> and all passages in the batch, comprising both pre-mined hard negatives (from NLI contradiction examples and hard negative mining) and in-batch negatives (other queries and passages from the same training batch).\nWith cross-device negative sharing enabled, the total negative set spans all examples across all GPUs in the distributed training setup, providing a rich set of challenging negative examples for each query.</p>\n\n",
                "matched_terms": [
                    "nli",
                    "similarity",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We constructed cross-lingual training data by leveraging large-scale natural language inference corpora, a supervision signal that has proven effective for learning sentence embeddings. <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib15\" title=\"\">2022</a>; Reimers and Gurevych, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib39\" title=\"\">2019</a>)</cite> Specifically, we used MultiNLI and SNLI <cite class=\"ltx_cite ltx_citemacro_citep\">(Williams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib47\" title=\"\">2018</a>; Bowman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib7\" title=\"\">2015</a>)</cite> as source datasets in English. Each sentence pair was translated into the nine African target languages using NLLB-200 (3.3B) <cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB-Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib33\" title=\"\">2022</a>)</cite>. We then estimated translation quality by SSA-COMET-MTL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib19\" title=\"\">2025</a>)</cite>, a COMET variant&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Rei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib38\" title=\"\">2020</a>)</cite>. and filtered pairs below a threshold of 0.75 to ensure the data quality.</p>\n\n",
                "matched_terms": [
                    "pair",
                    "datasets",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite being listed under small models (<math alttext=\"&lt;1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">&lt;1</annotation></semantics></math>B), the E5 family matches or surpasses the proprietary model on average <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> score. <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> attains the best macro average at 62.4, edging out <span class=\"ltx_text ltx_font_italic\">Gemini embedding</span> (60.6) and <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> (61.3) in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S2.T2\" title=\"Table 2 &#8227; 2.3 AfriMTEB-Lite construction &#8227; 2 AfriMTEB Benchmark &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>. This indicates that strong multilingual coverage and targeted adaptation can outweigh model size or access to proprietary training data.</p>\n\n",
                "matched_terms": [
                    "family",
                    "afrimteb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While E5 variants lead overall, <span class=\"ltx_text ltx_font_italic\">Gemini embedding 001</span> is strongest on most classification-style families: single-label classification (50.0 vs. 49.8 for <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span>and 49.7 for <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span>), multi-label classification (32.7 vs. 28.6/29.8), and pair classification (71.6 vs. 63.8/67.9). It also tops retrieval (77.5) and semantic textual similarity (65.0). These strengths suggest Gemini&#8217;s instruction/data mix particularly benefits discriminative judgment tasks and sentence-level similarity scoring.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "tasks",
                    "families",
                    "pair",
                    "similarity",
                    "retrieval",
                    "multilabel",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large 7B and 8B encoders do not translate into higher <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> scores. All 7B/8B models cluster in the low&#8211;mid 50s, for example, <span class=\"ltx_text ltx_font_italic\">gte-Qwen2-7B-instruct</span> at 50.9, <span class=\"ltx_text ltx_font_italic\">GritLM-7B</span> at 51.8, <span class=\"ltx_text ltx_font_italic\">Qwen3-Embedding-8B</span> at 50.5. Thay are clearly below the smaller E5 variants of around 61.3 point. This gap underscores that broad, balanced language coverage and task diversity matter more than parameter count alone.</p>\n\n",
                "matched_terms": [
                    "afrimteb",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On bitext mining, <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> and <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> have comparable performance of <math alttext=\"85\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mn>85</mn><annotation encoding=\"application/x-tex\">85</annotation></semantics></math> points,\nboth are far ahead of other opens and the API baseline (<span class=\"ltx_text ltx_font_italic\">Gemini embedding</span> at 72.2). For clustering, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> leads with 62.9 and <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> is next (61.9), while larger models and Gemini embeddings behind. These families reward models that learn language-agnostic semantic spaces with robust cross-lingual alignment, which appears to be a particular strength of the E5 lineage.</p>\n\n",
                "matched_terms": [
                    "clustering",
                    "families",
                    "mining",
                    "bitext",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> is adapted using supervision centered on nine African languages (<span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span>), it achieves the highest Full-suite average (62.4) across 59 languages. Relative to <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span>, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> shows consistent family-level gains on pair classification (+4.1), reranking (+2.1), retrieval (+1.3), clustering (+1.0), and multi-label classification (+1.2), with small trade-offs on bitext mining (&#8211;0.3), single-label classification (&#8211;0.1), and STS (&#8211;1.1). This pattern indicates that targeted cross-lingual distillation can yield transferable gains beyond the training languages, particularly for retrieval-style tasks. </p>\n\n",
                "matched_terms": [
                    "classification",
                    "clustering",
                    "tasks",
                    "languages",
                    "pair",
                    "reranking",
                    "retrieval",
                    "afrimteb",
                    "bitext",
                    "multilabel",
                    "sts",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the base model <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> records a reranking score of 61.9, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> raises this to 64.0. It also surpasses gemini embedding 001 (63.4) on the same task (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S2.T2\" title=\"Table 2 &#8227; 2.3 AfriMTEB-Lite construction &#8227; 2 AfriMTEB Benchmark &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). This improvement can be attributed to the training recipe, where <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> leverages knowledge distillation from the <span class=\"ltx_text ltx_font_smallcaps\">BGE-m3</span> cross-encoder in addition to contrastive supervision. The result suggests that incorporating reranker-derived soft labels enhances cross-lingual alignment for ranking-style objectives.</p>\n\n",
                "matched_terms": [
                    "reranking",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we described the overall aggregated results by datasets and languages in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S3.T3\" title=\"Table 3 &#8227; 3 Adapting Embedding Models to African Languages &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;3</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S4.F3\" title=\"Figure 3 &#8227; AfriE5 improves reranking with knowledge distillation &#8227; 4.1 AfriMTEB results &#8227; 4 Results &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;3</span></a>. The comprehensive results are in Appendix &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.SS4\" title=\"A.4 Detailed AfriMTEB-Lite results &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the Lite suite of nine African languages and twelve tasks, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> attains the best overall score with an average of 63.7, compared to 62.0 for <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> and the same performance of 63.1 as <span class=\"ltx_text ltx_font_italic\">Gemini embedding</span> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S3.T3\" title=\"Table 3 &#8227; 3 Adapting Embedding Models to African Languages &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Although the performance advantage is not uniform across all task families, AfriE5 consistently remains competitive and often surpasses both baselines on challenging benchmarks such as AfriXNLI and the SIB200 variants. These results highlight that adapting mE5 with cross-lingual contrastive distillation improves its effectiveness for African languages, even when trained on a compact set of nine languages.</p>\n\n",
                "matched_terms": [
                    "task",
                    "tasks",
                    "families",
                    "afrixnli",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Relative to <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span>, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> shows clear improvements on several classification benchmarks. It achieves higher scores on AfriSentiment (+3.7), AfriXNLI (+4.5), and SIB200 clustering (+1.8). The largest gain appears on SIB200-14 classification (+4.4; 26.2 vs. 21.8), a setting that requires grouping semantically diverse texts into broad topical categories. These improvements indicate that the cross-lingual, NLI-style contrastive learning used in AfriE5 strengthens semantic alignment across languages, leading to more effective cross-lingual transfer for classification and clustering tasks.\n</p>\n\n",
                "matched_terms": [
                    "classification",
                    "clustering",
                    "tasks",
                    "afrixnli",
                    "languages",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As described in Section &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S3.SS2\" title=\"3.2 Cross-lingual Training Data Construction &#8227; 3 Adapting Embedding Models to African Languages &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, comparing row 1 (<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>, 0.75) and row 3 (<math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math>, 0.75), the average increases from 62.3 to 63.2. The cross-lingual expansion of the data set exposes the model to richer cross-lingual contrasts: First, higher scores on bitext mining tasks show that the expansion improves the model&#8217;s ability to align semantically equivalent sentences between languages, producing a more coherent multilingual embedding space. Second, classification tasks such as SIB200-14 benefit from this stronger alignment, as the model learns to abstract over diverse topical domains without relying on language-specific cues.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "tasks",
                    "mining",
                    "bitext",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With expansion enabled (rows 2&#8211;4), the best overall score is achieved after filtering using a COMET variant, SSA-COMET-MTL QE=0.75 (63.2), compared to 62.5 at 0.67 and a sharp drop to 58.3 at 0.80. A low threshold (0.67) retains large noisy translations (around 433K training samples), which slightly hurts reasoning-heavy tasks. In contrast, a high threshold (0.80) filters out too much data (remaining only 7500 samples), reducing linguistic and semantic diversity and leading to weaker transfer, especially in classification. The middle ground (0.75) balances translation quality with coverage with around 60K samples, yielding the most reliable improvements across tasks. We provide the number of samples retained by SSA-COMET-MTL in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.SS3\" title=\"A.3 Detailed Statistics of Machine Translation Quality &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a></p>\n\n",
                "matched_terms": [
                    "classification",
                    "number",
                    "semantic",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A number of African NLP benchmarks focus on text classification and related tasks. AfroBench aggregates multiple tasks and languages with an emphasis on broad coverage <cite class=\"ltx_cite ltx_citemacro_citep\">(Ojo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib34\" title=\"\">2025</a>)</cite>. News topic classification resources include MasakhaNEWS and earlier Kinyarwanda/Kirundi corpora (KINNEWS/KIRNEWS) <cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib2\" title=\"\">2023</a>; Niyongabo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib32\" title=\"\">2020</a>)</cite>. Sentiment and toxicity are covered by AfriSenti, NaijaSenti, and AfriHate <cite class=\"ltx_cite ltx_citemacro_citep\">(Muhammad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib28\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib29\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib27\" title=\"\">2025a</a>)</cite>. The SIB-200 suite provides large-scale topic classification with stronger representation of African languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib1\" title=\"\">2024</a>)</cite>. Intent and slot-filling are addressed by INJONGO <cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib49\" title=\"\">2025</a>)</cite>. These resources underscore the need for region-specific evaluation and supply tasks that align well with sentence-embedding assessment.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "news",
                    "sentiment",
                    "tasks",
                    "text",
                    "masakhanews",
                    "afrihate",
                    "naijasenti",
                    "topic",
                    "languages",
                    "number",
                    "afrisenti",
                    "intent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Massive Text Embedding Benchmark (MTEB) introduced a common taxonomy and leaderboard for sentence embeddings <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib26\" title=\"\">2023</a>)</cite>. Since then, several language- or region-specific variants have emerged, including MMTEB (with Europe and Indic tracks), SEA-BED for Southeast Asia, PL-MTEB for Polish, and MTEB-French <cite class=\"ltx_cite ltx_citemacro_citep\">(Ponwitayarat et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib36\" title=\"\">2025</a>; Po&#347;wiata et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib37\" title=\"\">2024</a>; Ciancone et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib9\" title=\"\">2024</a>)</cite>. Additional efforts target specific language families or regions: C-MTEB (Chinese), German-focused suites, JMTEB (Japanese), Korean-focused suites, ruMTEB (Russian), FaMTEB (Persian/Farsi), and VN-MTEB (Vietnamese) <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib48\" title=\"\">2024</a>; Wehrli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib46\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib20\" title=\"\">2024</a>; Snegirev et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib40\" title=\"\">2025</a>; Zinvandi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib51\" title=\"\">2025</a>; Pham et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib35\" title=\"\">2025</a>)</cite>. Our work follows this trend by building an Africa-focused extension with broad task coverage.</p>\n\n",
                "matched_terms": [
                    "task",
                    "text",
                    "mmteb",
                    "families",
                    "additional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented AfriMTEB, a large-scale benchmark for African languages spanning 59 languages and 14 tasks, and AfriE5, an adaptation of mE5-large-instruct via cross-lingual contrastive distillation. AfriE5 achieves state-of-the-art results on both AfriMTEB-Full and Lite, surpassing strong baselines such as Gemini embedding-001. Our ablations show that cross-lingual dataset expansion and balanced translation filtering are crucial for these gains. AfriMTEB and AfriE5 provide standardized evaluation and a strong baseline to advance text embedding research for African languages.</p>\n\n",
                "matched_terms": [
                    "text",
                    "languages",
                    "afrimteb",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AfriMTEB expands coverage to 59 languages, yet many African languages, dialects, orthographies, and code-switched registers remain underrepresented. Several datasets inherit noise or heterogeneity from crowd labels, repurposed tasks, and preprocessing; our adaptation data further relies on machine translation (NLLB-200) and automatic quality estimation (SSA-COMET), which vary in reliability across language pairs and domains. Distillation from a single teacher (BGE Reranker v2 m3) can also transfer its biases, potentially advantaging languages and styles that align with the MT/QE pipeline and the teacher&#8217;s preferences.</p>\n\n",
                "matched_terms": [
                    "afrimteb",
                    "languages",
                    "datasets",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation focuses on text-only sentence/paragraph embeddings and macro-averages across tasks and languages; alternative weightings (e.g., by population or application criticality) and additional metrics (calibration, robustness, fairness) could yield different conclusions. End-to-end RAG quality, multimodal retrieval, and very long-context document embeddings are out of scope, and domain coverage skews toward formal/news text over colloquial or specialized domains. True parity with closed-weight baselines (e.g., data mixtures, architectures, inference settings) is infeasible; repeated API evaluations may be affected by nondeterminism, and some open models may be sensitive to prompt formats or poolers we did not exhaustively tune.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "text",
                    "retrieval",
                    "languages",
                    "additional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, benchmark contamination remains possible because public corpora are widely used in pretraining, which can inflate absolute scores. Our single-GPU fine-tuning recipe favors accessibility over peak performance and may limit stability, and releasing full translation/provenance artefacts is nontrivial due to size and licensing. While AfriE5 trained on nine languages generalizes well to 59 in our tests, transfer may degrade for typologically distant or extremely low-resource languages; broader community datasets and multi-teacher/multi-signal training are promising avenues to mitigate these limitations.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "full",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given two sentence sets from different languages, the task is to identify translation pairs. Embeddings are used to compute similarities and find the best match for each sentence.</p>\n\n",
                "matched_terms": [
                    "task",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task involves a pair of input sentences with a binary or categorical relationship label (e.g., entailment vs. contradiction). Predictions are based on embedding similarity.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "pair",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Single-text classification where each input is mapped to one label among several categories (e.g., topic, sentiment, hate speech, or language ID). A linear classifier is trained on top of embeddings.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "classification",
                    "sentiment",
                    "hate",
                    "topic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task measures the degree of semantic similarity between sentence pairs, either within or across languages, based on embedding similarity.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "task",
                    "semantic",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a query, the task is to retrieve relevant documents from a large corpus. Both queries and documents are embedded, and similarity scores determine ranking.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">mmBERT-base is a massively multilingual ModernBERT encoder trained on <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>T,+ tokens covering <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>1,800 languages, extending the ModernBERT architecture (fast encoder with long context) to the multilingual regime <cite class=\"ltx_cite ltx_citemacro_citep\">(Warner et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib45\" title=\"\">2025</a>)</cite>. The training recipe introduces curriculum-style annealed language learning, inverse masking, and temperature-based sampling to emphasize low-resource languages while retaining strong performance on high-resource ones <cite class=\"ltx_cite ltx_citemacro_citep\">(Marone et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib23\" title=\"\">2025</a>)</cite>. Reported results show that <span class=\"ltx_text ltx_font_typewriter\">mmBERT-base</span> surpasses prior multilingual encoders such as XLM-R on standard NLU and retrieval benchmarks, approaching the English-only ModernBERT on GLUE despite being trained predominantly on non-English data <cite class=\"ltx_cite ltx_citemacro_citep\">(Marone et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib23\" title=\"\">2025</a>)</cite>. Inference follows standard encoder usage (mean pooling over last hidden states and L2 normalization).</p>\n\n",
                "matched_terms": [
                    "retrieval",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">KaLM-Embedding is a multilingual embedding family that prioritizes training-data quality over sheer scale, combining (i) persona-based synthetic examples distilled from LLMs, (ii) ranking-consistency filtering, and (iii) semi-homogeneous task batching for efficient contrastive learning <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib16\" title=\"\">2025</a>)</cite>. Many public checkpoints are built on compact Qwen2 backbones (e.g., <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.5B) and instruction-tuned variants for downstream retrieval and semantic similarity. Technical reports describe v1.5/v2 updates with improved data curation and training strategy, yielding strong MTEB performance for their size <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib16\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "task",
                    "similarity",
                    "retrieval",
                    "family",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Embedding is a purpose-built series of dense encoders for text embeddings and reranking, offered in 0.6B/4B/8B sizes with a 32k token context window and coverage of 100+ languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib50\" title=\"\">2025</a>)</cite>. The models are instruction-aware and support flexible output dimensionalities (via MRL-style prefix truncation), with typical maximum embedding sizes of 1,024 (0.6B), 2,560 (4B), and 4,096 (8B); matching reranker models are available at each size. Training leverages Qwen3 LLMs both as backbones and as data synthesizers across domains and languages, improving robustness for retrieval/reranking workloads. Inference uses mean pooling and L2 normalization; common deployment stacks expose a user-selectable output dimensionality for storage/latency trade-offs.</p>\n\n",
                "matched_terms": [
                    "text",
                    "reranking",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BGE-M3 (BAAI) is a versatile embedding model unifying three capabilities in a single encoder: Multi-Functionality (dense, multi-vector, and sparse retrieval), Multi-Linguality (100+ languages), and Multi-Granularity (robust from short queries to long documents, up to <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>8,192 tokens) <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite>. The training pipeline uses self-knowledge distillation across retrieval functions to align representations and enables hybrid retrieval without switching models <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite>. It is widely used as a strong multilingual baseline for retrieval, clustering, and classification.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "clustering",
                    "retrieval",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">mE5-large and mE5-large-instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">2024c</a>)</cite> are multilingual members of the E5 family built on XLM-RoBERTa-large <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib10\" title=\"\">2020</a>)</cite>, trained with a two-stage recipe that first performs weakly supervised contrastive pre-training on roughly one billion multilingual text pairs and then supervised fine-tuning on curated embedding tasks; the instruction variant further formats supervision with concise task instructions to specialize representations for retrieval and related tasks. Both use a 24-layer encoder that produces 1,024-dimensional vectors and inherit broad (&#160;100 language) coverage from the XLM-RoBERTa backbone. At inference time they follow the E5 prompt conventions (e.g., query / passage style inputs), apply mean pooling over the last hidden states, and L2-normalize the output, yielding strong performance on retrieval, semantic similarity, clustering, and classification benchmarks in both monolingual and cross-lingual settings.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "clustering",
                    "task",
                    "tasks",
                    "text",
                    "similarity",
                    "retrieval",
                    "family",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">gte-Qwen2-7B-instruct (Alibaba-NLP) is a 7B-parameter instruction-tuned General Text Embedding model built on Qwen2-7B, targeting high-quality multilingual embeddings with a 32k context window <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib22\" title=\"\">2023b</a>)</cite>. At release, it reported leading scores on MTEB English/Chinese subsets, reflecting a training mixture that combines instruction-style contrastive objectives and curated negatives. It is used as a strong large-model baseline for retrieval, reranking, and semantic similarity.</p>\n\n",
                "matched_terms": [
                    "text",
                    "reranking",
                    "similarity",
                    "retrieval",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">Linq-Embed-Mistral</span> is a Mistral-7B&#8211;based embedding model developed with task-tailored data crafting, filtering, and hard-negative mining to improve retrieval quality <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib17\" title=\"\">2024</a>)</cite>. The technical report and model card document leading MTEB retrieval scores at release (e.g., average <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px8.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>68.2 on 56 datasets; retrieval score <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px8.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>60.2), achieved through homogeneous task ordering and mixed-task fine-tuning strategies <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib17\" title=\"\">2024</a>)</cite>. It is frequently used as a competitive 7B encoder baseline for dense retrieval.</p>\n\n",
                "matched_terms": [
                    "retrieval",
                    "mining",
                    "task",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SFR-Embedding-Mistral (Salesforce Research) applies transfer learning on top of E5-mistral-7b-instruct and Mistral-7B-v0.1, with additional multi-task training and optimized negative sampling aimed at retrieval tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib24\" title=\"\">2024</a>)</cite>. Public materials position it as a top-performing 7B embedding model for search, clustering, and classification workloads <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib24\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "clustering",
                    "tasks",
                    "retrieval",
                    "additional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.T6\" title=\"Table 6 &#8227; A.3 Detailed Statistics of Machine Translation Quality &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we observe a clear trade-off between dataset size and quality. At a relaxed threshold of 0.67, over 430k sentence pairs are retained, ensuring broad coverage across all nine target languages. Increasing the threshold to 0.75 reduces the pool to around 60k examples, striking a balance between filtering noise and preserving sufficient training data. At the strictest cutoff of 0.80, only 7.5k pairs remain, indicating that high-quality translations are relatively scarce. Language-level differences are also evident: Swahili, Oromo, and Kinyarwanda consistently contribute the largest number of pairs, while Igbo and Amharic see the sharpest reductions under stricter filtering, reflecting variation in MT quality across languages.</p>\n\n",
                "matched_terms": [
                    "number",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#A1.T7\" title=\"Table 7 &#8227; A.4 Detailed AfriMTEB-Lite results &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;7</span></a> shows the comprehensive results across all evaluated datasets and languages in <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB-Lite</span>.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "languages"
                ]
            }
        ]
    },
    "S2.T2": {
        "source_file": "AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages",
        "caption": "Table 2: AfriMTEB results. Average performance of embedding models across 59 African and regionally relevant languages grouped by task family. The final column reports the unweighted macro average across task families. Best scores in each column are highlighted in bold.",
        "body": "Model\nBtxt\nClf\nClust\nMulti. Clf\nPr Clf\nRrnk\nRtrvl\nSTS\nAverage\n\n\n\nSmall models (<1<\\!1B)\n\n\nmmBERT-base\n3.0\n48.5\n33.1\n24.6\n54.1\n6.8\n4.9\n38.0\n26.6\n\n\nKaLM\n49.9\n36.3\n46.8\n25.9\n60.0\n49.3\n52.8\n53.4\n46.8\n\n\nQwen3-Embedding 0.6B\n33.6\n35.9\n37.6\n25.2\n58.0\n52.9\n52.0\n54.1\n43.7\n\n\nbge-m3\n70.0\n40.0\n47.3\n26.8\n66.4\n66.8\n70.2\n58.7\n55.8\n\n\nmE5-large\n79.7\n43.3\n45.3\n27.7\n64.3\n65.2\n69.2\n62.5\n57.2\n\n\nmE5-large-instruct\n85.8\n49.8\n61.9\n28.6\n63.8\n61.9\n74.1\n64.8\n61.3\n\n\nAfriE5-large-instruct\n85.5\n49.7\n62.9\n29.8\n67.9\n64.0\n75.4\n63.7\n62.4\n\n\n\nMedium models (≈4\\approx 4B)\n\n\nQwen3-Embedding-4B\n42.5\n42.9\n39.6\n25.8\n57.4\n60.6\n61.5\n54.9\n48.2\n\n\n\nLarge models (≥7\\geq\\!7B)\n\n\ngte-Qwen2-7B-instruct\n52.7\n41.0\n56.6\n25.1\n58.1\n58.8\n60.3\n54.9\n50.9\n\n\nGritLM-7B\n45.2\n43.4\n54.0\n26.6\n59.6\n65.4\n61.0\n59.1\n51.8\n\n\nLinq-Embed-Mistral\n45.2\n43.2\n55.4\n27.1\n59.4\n65.1\n59.2\n62.5\n52.1\n\n\nSFR-Embedding-Mistral\n46.3\n42.5\n58.0\n26.2\n58.9\n63.8\n58.1\n62.0\n52.0\n\n\nE5 mistral 7b instruct\n46.4\n41.9\n58.5\n26.0\n58.7\n64.1\n57.3\n61.3\n51.8\n\n\nQwen3-Embedding-8B\n48.4\n43.7\n41.8\n27.2\n58.3\n60.0\n69.9\n54.7\n50.5\n\n\nUndisclosed size\n\n\ngemini embedding 001\n72.2\n50.0\n52.7\n32.7\n71.6\n63.4\n77.5\n65.0\n60.6",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Btxt</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Clf</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Clust</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Multi. Clf</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Pr Clf</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Rrnk</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Rtrvl</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">STS</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Average</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"10\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<em class=\"ltx_emph ltx_font_italic\">Small models</em> (<math alttext=\"&lt;\\!1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">&lt;\\!1</annotation></semantics></math>B)</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">mmBERT-base</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">48.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">33.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">24.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">54.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">6.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">38.0</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">26.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">KaLM</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">49.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">36.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">46.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">25.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">60.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">49.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">52.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">53.4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">46.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">Qwen3-Embedding 0.6B</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">33.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">35.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">37.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">25.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">58.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">52.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">52.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">54.1</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">43.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">bge-m3</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">70.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">40.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">47.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">26.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">66.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">66.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">70.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">58.7</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">55.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">mE5-large</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">79.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">43.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">45.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">27.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">64.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">65.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">69.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">62.5</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">57.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">mE5-large-instruct</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">85.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">49.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">61.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">28.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">63.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">61.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">74.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">64.8</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">61.3</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\" style=\"--ltx-bg-color:#D9D9D9;\">AfriE5-large-instruct</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">85.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">49.7</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">62.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">29.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">67.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">64.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">75.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">63.7</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">62.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"10\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<em class=\"ltx_emph ltx_font_italic\">Medium models</em> (<math alttext=\"\\approx 4\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 4</annotation></semantics></math>B)</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">Qwen3-Embedding-4B</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">42.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">42.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">39.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">25.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">57.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">60.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">61.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">54.9</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">48.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"10\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<em class=\"ltx_emph ltx_font_italic\">Large models</em> (<math alttext=\"\\geq\\!7\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m3\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8805;</mo><mn>7</mn></mrow><annotation encoding=\"application/x-tex\">\\geq\\!7</annotation></semantics></math>B)</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">gte-Qwen2-7B-instruct</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">52.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">41.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">56.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">25.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">58.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">58.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">60.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">54.9</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">50.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">GritLM-7B</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">45.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">43.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">54.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">26.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">59.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">65.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">61.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">59.1</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">51.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">Linq-Embed-Mistral</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">45.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">43.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">55.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">27.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">59.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">65.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">59.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">62.5</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">52.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">SFR-Embedding-Mistral</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">46.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">42.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">58.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">26.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">58.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">63.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">58.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">62.0</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">52.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">E5 mistral 7b instruct</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">46.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">41.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">58.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">26.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">58.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">64.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">57.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">61.3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">51.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">Qwen3-Embedding-8B</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">48.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">43.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">41.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">27.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">58.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">60.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">69.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">54.7</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">50.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"10\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><em class=\"ltx_emph ltx_font_italic\">Undisclosed size</em></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">gemini embedding 001</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">72.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">50.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">52.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">32.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">71.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">63.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">77.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">65.0</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">60.6</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "afrie5largeinstruct",
            "linqembedmistral",
            "task",
            "medium",
            "size",
            "families",
            "mistral",
            "relevant",
            "clust",
            "multi",
            "qwen3embedding8b",
            "embedding",
            "macro",
            "large",
            "gemini",
            "each",
            "06b",
            "regionally",
            "across",
            "african",
            "mmbertbase",
            "clf",
            "me5large",
            "sfrembeddingmistral",
            "highlighted",
            "sts",
            "me5largeinstruct",
            "≈4approx",
            "reports",
            "family",
            "results",
            "unweighted",
            "column",
            "instruct",
            "model",
            "grouped",
            "11b",
            "gritlm7b",
            "rrnk",
            "gteqwen27binstruct",
            "undisclosed",
            "afrimteb",
            "average",
            "bold",
            "performance",
            "qwen3embedding",
            "≥7geq7b",
            "qwen3embedding4b",
            "rtrvl",
            "final",
            "btxt",
            "models",
            "scores",
            "small",
            "best",
            "kalm",
            "languages",
            "bgem3"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Despite being listed under small models (<math alttext=\"&lt;1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">&lt;1</annotation></semantics></math>B), the E5 family matches or surpasses the proprietary model on average <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> score. <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> attains the best macro average at 62.4, edging out <span class=\"ltx_text ltx_font_italic\">Gemini embedding</span> (60.6) and <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> (61.3) in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S2.T2\" title=\"Table 2 &#8227; 2.3 AfriMTEB-Lite construction &#8227; 2 AfriMTEB Benchmark &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>. This indicates that strong multilingual coverage and targeted adaptation can outweigh model size or access to proprietary training data.</p>\n\n",
            "<p class=\"ltx_p\">Although the base model <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> records a reranking score of 61.9, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> raises this to 64.0. It also surpasses gemini embedding 001 (63.4) on the same task (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S2.T2\" title=\"Table 2 &#8227; 2.3 AfriMTEB-Lite construction &#8227; 2 AfriMTEB Benchmark &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). This improvement can be attributed to the training recipe, where <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> leverages knowledge distillation from the <span class=\"ltx_text ltx_font_smallcaps\">BGE-m3</span> cross-encoder in addition to contrastive supervision. The result suggests that incorporating reranker-derived soft labels enhances cross-lingual alignment for ranking-style objectives.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Text embeddings are an essential building component of several NLP tasks such as retrieval-augmented generation which is crucial for preventing hallucinations in LLMs. Despite the recent release of massively multilingual MTEB (MMTEB), African languages remain underrepresented, with existing tasks often repurposed from translation benchmarks such as FLORES clustering or SIB-200.\nIn this paper, we introduce AfriMTEB&#8212;a regional expansion of MMTEB covering 59 languages, 14 tasks, and 38 datasets, including six newly added datasets. Unlike many MMTEB datasets that include fewer than five languages, the new additions span 14 to 56 African languages and introduce entirely new tasks, such as hate speech detection, intent detection, and emotion classification, which were not previously covered. Complementing this, we present AfriE5, an adaptation of the instruction-tuned mE5 model to African languages through cross-lingual contrastive distillation. Our evaluation shows that AfriE5 achieves state-of-the-art performance, outperforming strong baselines such as Gemini-Embeddings and mE5.</p>\n\n",
                "matched_terms": [
                    "african",
                    "performance",
                    "model",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "african",
                    "afrimteb",
                    "languages",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While recent text embedding benchmarks have improved language coverage in recent years, such as MMTEB&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Enevoldsen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib11\" title=\"\">2025</a>)</cite>, African languages remain under-represented, where many of the tasks covered are either based on massively evaluation of translation datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB-Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib33\" title=\"\">2022</a>; Federmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib12\" title=\"\">2022</a>)</cite>, and the tasks derived or repurposed from translation benchmarks such as Belebele&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bandarkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib5\" title=\"\">2024</a>)</cite> and SIB-200&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib1\" title=\"\">2024</a>)</cite>. As a result, the quality of text embeddings for languages in the African region remains unknown, as the region has few standardized tools for comparing models across tasks and languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alabi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib4\" title=\"\">2025</a>)</cite></p>\n\n",
                "matched_terms": [
                    "across",
                    "models",
                    "african",
                    "languages",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">AfriMTEB</span>, a regional extension of MTEB tailored to African languages and tasks. <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;2</span></a> shows the full suite that covers 59 languages and 38 datasets spanning Bitext mining, Classification (single, pair and multi-label), Semantic Text Similarity, Retrieval, Clustering, and Reranking. To support fast-and-compute friendly evaluation, we also provide <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">AfriMTEB-Lite</span>, a compact suite over nine geographically-diverse languages (Amharic, Oromo, Igbo, Yoruba, Hausa, Swahili, Kinyarwanda, Xhosa, Zulu) constructed by selecting, within each task family, the maximally overlapping tasks such that every task includes all nine languages, ensuring uniform language coverage across the suite.</p>\n\n",
                "matched_terms": [
                    "each",
                    "task",
                    "across",
                    "african",
                    "afrimteb",
                    "languages",
                    "family"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to benchmarking, we develop <span class=\"ltx_text ltx_font_bold ltx_font_italic\">AfriE5-Large-Instruct</span>\nadapting strong embedding models to African languages.\nStarting from the instruction-tuned <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib42\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">c</a>)</cite>, we leveraged cross-lingual contrastive distillation. We construct supervision by translating MNLI/SNLI datasets<cite class=\"ltx_cite ltx_citemacro_citep\">(Williams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib47\" title=\"\">2018</a>; Bowman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib7\" title=\"\">2015</a>)</cite> in African languages with NLLB-200 (3.3B) <cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB-Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib33\" title=\"\">2022</a>)</cite>, followed by automatic filtering by SSA-COMET (an African COMET-based QE metric)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib19\" title=\"\">2025</a>)</cite>, and expanding each example into multiple sources or target directions to encourage cross-lingual alignment. We then use the receipe of BGE Re-ranker v2 m3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite> for text embedding model training.</p>\n\n",
                "matched_terms": [
                    "afrie5largeinstruct",
                    "each",
                    "model",
                    "african",
                    "models",
                    "me5largeinstruct",
                    "languages",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate widely used baselines, including BGE-M3, E5 variants (e.g., e5-mistral-7b-instruct), Qwen embeddings, Gemini embedding-001, and Embedding Gemma <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">2024c</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib50\" title=\"\">2025</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib18\" title=\"\">2025</a>)</cite>. On <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB-Lite</span>, our adapted model <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span>, trained only on nine African languages, achieves an average score of 63.7, surpassing me5-large-instruct (62.0) and Gemini embedding-001 (63.1). Remarkably, despite being tuned with data and languages aligned to the <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB-Lite</span>, our model leveraged cross-lingual transfer to generalize to the broader <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> benchmark covering 59 languages and 38 datasets, where it also delivers the best performance with an average score of 62.4, ahead of me5-large-instruct (61.3) and Gemini embedding-001 (60.6) as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>. These results highlight that targeted cross-lingual adaptation on a carefully selected subset of languages can transfer effectively to a much larger set, yielding consistent improvements across task families while preserving the backbone&#8217;s general utility.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "afrie5largeinstruct",
                    "model",
                    "task",
                    "across",
                    "african",
                    "families",
                    "best",
                    "afrimteb",
                    "me5largeinstruct",
                    "languages",
                    "average",
                    "bgem3",
                    "performance",
                    "embedding",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">AfriMTEB</span> is a regional extension of MTEB designed to evaluate text embeddings for African languages across multiple task families. To address persistent gaps in language coverage and enable systematic, comparable assessment, we introduce <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span>as a standardized benchmark for African languages. It consists of a <span class=\"ltx_text ltx_font_bold\">Full suite</span> covering <span class=\"ltx_text ltx_font_italic\">59 languages</span> and <span class=\"ltx_text ltx_font_italic\">38 datasets</span>, and a <span class=\"ltx_text ltx_font_bold\">Lite suite</span> that maintains uniform language coverage for <span class=\"ltx_text ltx_font_italic\">9 target languages</span> over <span class=\"ltx_text ltx_font_italic\">13 representative datasets</span>. Languages are selected to span major families and geographic regions while prioritizing practical relevance and the availability of reliable public datasets; tasks reflect core embedding applications and draw on established, well-curated resources. The Lite suite supports rapid iteration by enforcing consistent task coverage across languages through maximal task&#8211;language overlap.</p>\n\n",
                "matched_terms": [
                    "task",
                    "across",
                    "african",
                    "families",
                    "afrimteb",
                    "languages",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> follows the <span class=\"ltx_text ltx_font_typewriter\">MTEB</span> taxonomy and groups tasks into eight families. <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S2.T1\" title=\"Table 1 &#8227; 2 AfriMTEB Benchmark &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> lists the tasks, task families and the datasets included in the <em class=\"ltx_emph ltx_font_italic\">Full</em> suite. Each dataset belongs to one of the eight task families: (1) Bibtext mining (2) Pair classification (3) Semantic Text Similarity (STS) (4) Clustering (5) Classification (6) Multi-label Classification (7) Retrieval (8). The first four task families performs a similarity related tasks based on two pairs of sentences, either to identity translation pairs, categorize them into a class (e.g., entailment vs. contradiction) or provide a similarity score. The next three tasks (5) - (6) classify a sentence or article into one or more classes. Finally, the last sets of tasks (7) and (8) are information retrieval tasks for retrieval or re-ranking based on a user query. We provide full description in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.SS1\" title=\"A.1 Task categories &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "task",
                    "families",
                    "afrimteb",
                    "sts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> builds on the Massive Text Embedding Benchmark (MTEB) by selecting tasks that include African languages. From the original benchmark, we inherit datasets covering bitext mining (e.g., Flores, NTREX, Tatoeba), pair classification (XNLI), topic and sentiment classification (SIB-200, AfriSenti, MasakhaNEWS), semantic textual similarity (SemRel24), retrieval (MIRACL, XQuAD, XM3600), clustering (SIB-200, MasakhaNEWS), and reranking (MIRACL).</p>\n\n",
                "matched_terms": [
                    "african",
                    "embedding",
                    "afrimteb",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, language coverage in these datasets is uneven: some tasks include only a few African languages, while others span broader multilingual settings. To ensure fairness, we compute macro averages over languages within each task, then average across tasks and families to obtain the overall <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> score. This prevents any single task or language from dominating the benchmark.</p>\n\n",
                "matched_terms": [
                    "each",
                    "task",
                    "across",
                    "african",
                    "families",
                    "afrimteb",
                    "languages",
                    "average",
                    "macro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <em class=\"ltx_emph ltx_font_italic\">Full</em> suite is built by selecting datasets from <span class=\"ltx_text ltx_font_typewriter\">MTEB</span> that include African languages and extending them with six additional datasets. These additions broaden task coverage, increase difficulty, and improve language representation.</p>\n\n",
                "matched_terms": [
                    "african",
                    "task",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An African extension of the XNLI benchmark that provides natural language inference data to 15 African languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib3\" title=\"\">2025</a>)</cite>. By including AfriXNLI, we expand language coverage for the pair classification family beyond a single African language (Swahili), ensuring broader representation of African languages in entailment-style tasks.</p>\n\n",
                "matched_terms": [
                    "african",
                    "family",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A multilabel emotion data set that covers 32 languages including 14 African languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Belay et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib6\" title=\"\">2025</a>; Muhammad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib30\" title=\"\">2025b</a>)</cite>. Each sentence may be assigned multiple emotion labels such as <span class=\"ltx_text ltx_font_smallcaps\">joy</span>, <span class=\"ltx_text ltx_font_smallcaps\">anger</span>, <span class=\"ltx_text ltx_font_smallcaps\">sadness</span>, or <span class=\"ltx_text ltx_font_smallcaps\">fear</span>. By including this dataset, AfriMTEB introduces the first <em class=\"ltx_emph ltx_font_italic\">multi-label classification</em> task for African languages, thereby broadening the taxonomy beyond single-label settings.</p>\n\n",
                "matched_terms": [
                    "each",
                    "task",
                    "african",
                    "afrimteb",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A multilingual hate-speech classification dataset covering 14 African languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Muhammad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib27\" title=\"\">2025a</a>)</cite>. Each instance is labeled as <span class=\"ltx_text ltx_font_smallcaps\">hate</span>, <span class=\"ltx_text ltx_font_smallcaps\">abusive</span>, or <span class=\"ltx_text ltx_font_smallcaps\">neutral</span>, providing a standardized benchmark for toxic content detection across diverse languages and registers. This dataset extends evaluation to socially relevant safety applications.</p>\n\n",
                "matched_terms": [
                    "each",
                    "across",
                    "african",
                    "relevant",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Injongo dataset is a multilingual resource for intent detection covering 16 African languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib49\" title=\"\">2025</a>)</cite>. It consists of short, conversational utterances annotated with 40 everyday intent categories, for example requests such as &#8220;freeze account&#8221; or &#8220;play music.&#8221; By focusing on dialogue-style classification, Injongo complements existing benchmarks like MASSIVE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">FitzGerald et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib14\" title=\"\">2023</a>)</cite>, but offers broader African language coverage.</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A more challenging variant of the SIB-200 dataset where labels are consolidated into 14 categories. This version still includes 56 African languages but is not limited to them&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib1\" title=\"\">2024</a>)</cite>. By merging fine-grained topics into broader classes, intra-class diversity increases, which raises task difficulty. The inclusion of this dataset not only strengthens African language evaluation but also increases the difficulty for other covered languages in SIB-200, leading to more robust cross-lingual assessment.</p>\n\n",
                "matched_terms": [
                    "african",
                    "task",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we added <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SIB200Classification</span> dataset to complement <span class=\"ltx_text ltx_font_italic\">SIB-200_14Classes</span> in the &#8220;<span class=\"ltx_text ltx_font_typewriter\">Clf</span>&#8221; task family, although not new. The only difference is that the former covers seven categories compared to the latter. The original MTEB only considers SIB-200 in the Clustering (<span class=\"ltx_text ltx_font_typewriter\">Clust</span>) task family.</p>\n\n",
                "matched_terms": [
                    "clust",
                    "family",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <em class=\"ltx_emph ltx_font_italic\">Lite</em> suite focuses on nine geographically-diverse African languages:\n<span class=\"ltx_text ltx_font_italic\">Amharic, Oromo, Igbo, Yoruba, Hausa, Swahili, Kinyarwanda, Xhosa, Zulu</span>.\nWe selected only those datasets from MTEB that include nine languages to guarantee consistent language&#8211;task alignment across families. The result is a compact yet representative benchmark of 13 datasets&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We merged two datasets: MasakhaNEWS and KinNEWS since they have similar structure, thus we report 12 datasets</span></span></span> spanning classification, retrieval, bitext mining, clustering, pair classification, and multi-label classification.</p>\n\n",
                "matched_terms": [
                    "african",
                    "families",
                    "across",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our approach combines contrastive learning with knowledge distillation in a unified objective. Given a batch of <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> query&#8211;document groups, each with group size <math alttext=\"G\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>G</mi><annotation encoding=\"application/x-tex\">G</annotation></semantics></math>, the total loss is:</p>\n\n",
                "matched_terms": [
                    "each",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"s_{i,j}=\\mathbf{q}i^{\\top}\\mathbf{p}_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mi>&#119850;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>i</mi><mo>&#8868;</mo></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#119849;</mi><mi>j</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">s_{i,j}=\\mathbf{q}i^{\\top}\\mathbf{p}_{j}</annotation></semantics></math> is the similarity between query embedding <math alttext=\"\\mathbf{q}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#119850;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{q}_{i}</annotation></semantics></math> and passage embedding <math alttext=\"\\mathbf{p}_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#119849;</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{p}_{j}</annotation></semantics></math>, and <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m4\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> is a temperature hyperparameter. The numerator contains the similarity between query <math alttext=\"\\mathbf{q}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m5\" intent=\":literal\"><semantics><msub><mi>&#119850;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{q}_{i}</annotation></semantics></math> and its corresponding positive passage <math alttext=\"\\mathbf{p}{i\\cdot G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>&#119849;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><mi>G</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{p}{i\\cdot G}</annotation></semantics></math>. The denominator includes all similarities between <math alttext=\"\\mathbf{q}i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m7\" intent=\":literal\"><semantics><mrow><mi>&#119850;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{q}i</annotation></semantics></math> and all passages in the batch, comprising both pre-mined hard negatives (from NLI contradiction examples and hard negative mining) and in-batch negatives (other queries and passages from the same training batch).\nWith cross-device negative sharing enabled, the total negative set spans all examples across all GPUs in the distributed training setup, providing a rich set of challenging negative examples for each query.</p>\n\n",
                "matched_terms": [
                    "each",
                    "embedding",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We constructed cross-lingual training data by leveraging large-scale natural language inference corpora, a supervision signal that has proven effective for learning sentence embeddings. <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib15\" title=\"\">2022</a>; Reimers and Gurevych, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib39\" title=\"\">2019</a>)</cite> Specifically, we used MultiNLI and SNLI <cite class=\"ltx_cite ltx_citemacro_citep\">(Williams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib47\" title=\"\">2018</a>; Bowman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib7\" title=\"\">2015</a>)</cite> as source datasets in English. Each sentence pair was translated into the nine African target languages using NLLB-200 (3.3B) <cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB-Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib33\" title=\"\">2022</a>)</cite>. We then estimated translation quality by SSA-COMET-MTL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib19\" title=\"\">2025</a>)</cite>, a COMET variant&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Rei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib38\" title=\"\">2020</a>)</cite>. and filtered pairs below a threshold of 0.75 to ensure the data quality.</p>\n\n",
                "matched_terms": [
                    "each",
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To encourage <span class=\"ltx_text ltx_font_bold\">cross-lingual alignment</span>, each example was expanded into multiple configurations: (i) premise in target language and hypothesis in source, (ii) premise in source and hypothesis in target, (iii) both in target, and (iv) both in source, that is,is, English. For MultiNLI, we reformulated examples as query&#8211;positive/negative pairs (entailment as <span class=\"ltx_text ltx_font_italic\">pos</span>, contradiction as <span class=\"ltx_text ltx_font_italic\">neg</span>). For SNLI, we followed the same strategy but included all 3-way annotations. We further increased training difficulty by sampling up to 15 hard negatives per instance using mE5-large-instruct.</p>\n\n",
                "matched_terms": [
                    "each",
                    "me5largeinstruct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To improve supervision, we distilled teacher scores from the BGE Reranker v2 m3 cross-encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite>. Each datapoint was thus annotated with both positive/negative pairs and soft teacher distributions.</p>\n\n",
                "matched_terms": [
                    "each",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tuned <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> using the open-sourced <span class=\"ltx_text ltx_font_typewriter\">FlagEmbedding</span> training repository based on BGE Reranker v2 m3 recipe&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite>. Training was performed with <span class=\"ltx_text ltx_font_typewriter\">torchrun</span> on a single GPU. The key settings are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.T5\" title=\"Table 5 &#8227; Reranking. &#8227; A.1 Task categories &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. We followed the default implementation of contrastive learning with cross-device negatives and enabled knowledge distillation with teacher scores provided by BGE Reranker v2 m3.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "me5largeinstruct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We trained for one epoch over the curated cross-lingual dataset, with logging every 100 steps and checkpoint saving every 100 steps. All samples in a batch were drawn from the same dataset (<span class=\"ltx_text ltx_font_typewriter\">--same_dataset_within_batch</span>) to maintain consistent supervision. The resulting model is referred to as <span class=\"ltx_text ltx_font_bold ltx_font_italic\">AfriE5-Large-Instruct</span>. We provide brief descriptions of the baseline models in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.SS2\" title=\"A.2 Baseline Model Descriptions &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "afrie5largeinstruct",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While E5 variants lead overall, <span class=\"ltx_text ltx_font_italic\">Gemini embedding 001</span> is strongest on most classification-style families: single-label classification (50.0 vs. 49.8 for <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span>and 49.7 for <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span>), multi-label classification (32.7 vs. 28.6/29.8), and pair classification (71.6 vs. 63.8/67.9). It also tops retrieval (77.5) and semantic textual similarity (65.0). These strengths suggest Gemini&#8217;s instruction/data mix particularly benefits discriminative judgment tasks and sentence-level similarity scoring.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "afrie5largeinstruct",
                    "families",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large 7B and 8B encoders do not translate into higher <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> scores. All 7B/8B models cluster in the low&#8211;mid 50s, for example, <span class=\"ltx_text ltx_font_italic\">gte-Qwen2-7B-instruct</span> at 50.9, <span class=\"ltx_text ltx_font_italic\">GritLM-7B</span> at 51.8, <span class=\"ltx_text ltx_font_italic\">Qwen3-Embedding-8B</span> at 50.5. Thay are clearly below the smaller E5 variants of around 61.3 point. This gap underscores that broad, balanced language coverage and task diversity matter more than parameter count alone.</p>\n\n",
                "matched_terms": [
                    "task",
                    "gritlm7b",
                    "models",
                    "scores",
                    "gteqwen27binstruct",
                    "afrimteb",
                    "qwen3embedding8b",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On bitext mining, <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> and <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> have comparable performance of <math alttext=\"85\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mn>85</mn><annotation encoding=\"application/x-tex\">85</annotation></semantics></math> points,\nboth are far ahead of other opens and the API baseline (<span class=\"ltx_text ltx_font_italic\">Gemini embedding</span> at 72.2). For clustering, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> leads with 62.9 and <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> is next (61.9), while larger models and Gemini embeddings behind. These families reward models that learn language-agnostic semantic spaces with robust cross-lingual alignment, which appears to be a particular strength of the E5 lineage.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "afrie5largeinstruct",
                    "models",
                    "families",
                    "me5largeinstruct",
                    "performance",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> is adapted using supervision centered on nine African languages (<span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span>), it achieves the highest Full-suite average (62.4) across 59 languages. Relative to <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span>, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> shows consistent family-level gains on pair classification (+4.1), reranking (+2.1), retrieval (+1.3), clustering (+1.0), and multi-label classification (+1.2), with small trade-offs on bitext mining (&#8211;0.3), single-label classification (&#8211;0.1), and STS (&#8211;1.1). This pattern indicates that targeted cross-lingual distillation can yield transferable gains beyond the training languages, particularly for retrieval-style tasks. </p>\n\n",
                "matched_terms": [
                    "afrie5largeinstruct",
                    "across",
                    "african",
                    "languages",
                    "small",
                    "afrimteb",
                    "me5largeinstruct",
                    "sts",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we described the overall aggregated results by datasets and languages in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S3.T3\" title=\"Table 3 &#8227; 3 Adapting Embedding Models to African Languages &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;3</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S4.F3\" title=\"Figure 3 &#8227; AfriE5 improves reranking with knowledge distillation &#8227; 4.1 AfriMTEB results &#8227; 4 Results &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;3</span></a>. The comprehensive results are in Appendix &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.SS4\" title=\"A.4 Detailed AfriMTEB-Lite results &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the Lite suite of nine African languages and twelve tasks, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> attains the best overall score with an average of 63.7, compared to 62.0 for <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> and the same performance of 63.1 as <span class=\"ltx_text ltx_font_italic\">Gemini embedding</span> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S3.T3\" title=\"Table 3 &#8227; 3 Adapting Embedding Models to African Languages &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Although the performance advantage is not uniform across all task families, AfriE5 consistently remains competitive and often surpasses both baselines on challenging benchmarks such as AfriXNLI and the SIB200 variants. These results highlight that adapting mE5 with cross-lingual contrastive distillation improves its effectiveness for African languages, even when trained on a compact set of nine languages.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "afrie5largeinstruct",
                    "task",
                    "across",
                    "african",
                    "families",
                    "best",
                    "me5largeinstruct",
                    "languages",
                    "average",
                    "performance",
                    "embedding",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Relative to <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span>, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> shows clear improvements on several classification benchmarks. It achieves higher scores on AfriSentiment (+3.7), AfriXNLI (+4.5), and SIB200 clustering (+1.8). The largest gain appears on SIB200-14 classification (+4.4; 26.2 vs. 21.8), a setting that requires grouping semantically diverse texts into broad topical categories. These improvements indicate that the cross-lingual, NLI-style contrastive learning used in AfriE5 strengthens semantic alignment across languages, leading to more effective cross-lingual transfer for classification and clustering tasks.\n</p>\n\n",
                "matched_terms": [
                    "afrie5largeinstruct",
                    "across",
                    "scores",
                    "me5largeinstruct",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As described in Section &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S3.SS2\" title=\"3.2 Cross-lingual Training Data Construction &#8227; 3 Adapting Embedding Models to African Languages &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, comparing row 1 (<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>, 0.75) and row 3 (<math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math>, 0.75), the average increases from 62.3 to 63.2. The cross-lingual expansion of the data set exposes the model to richer cross-lingual contrasts: First, higher scores on bitext mining tasks show that the expansion improves the model&#8217;s ability to align semantically equivalent sentences between languages, producing a more coherent multilingual embedding space. Second, classification tasks such as SIB200-14 benefit from this stronger alignment, as the model learns to abstract over diverse topical domains without relying on language-specific cues.</p>\n\n",
                "matched_terms": [
                    "model",
                    "scores",
                    "languages",
                    "average",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With expansion enabled (rows 2&#8211;4), the best overall score is achieved after filtering using a COMET variant, SSA-COMET-MTL QE=0.75 (63.2), compared to 62.5 at 0.67 and a sharp drop to 58.3 at 0.80. A low threshold (0.67) retains large noisy translations (around 433K training samples), which slightly hurts reasoning-heavy tasks. In contrast, a high threshold (0.80) filters out too much data (remaining only 7500 samples), reducing linguistic and semantic diversity and leading to weaker transfer, especially in classification. The middle ground (0.75) balances translation quality with coverage with around 60K samples, yielding the most reliable improvements across tasks. We provide the number of samples retained by SSA-COMET-MTL in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.SS3\" title=\"A.3 Detailed Statistics of Machine Translation Quality &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a></p>\n\n",
                "matched_terms": [
                    "best",
                    "across",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A number of African NLP benchmarks focus on text classification and related tasks. AfroBench aggregates multiple tasks and languages with an emphasis on broad coverage <cite class=\"ltx_cite ltx_citemacro_citep\">(Ojo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib34\" title=\"\">2025</a>)</cite>. News topic classification resources include MasakhaNEWS and earlier Kinyarwanda/Kirundi corpora (KINNEWS/KIRNEWS) <cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib2\" title=\"\">2023</a>; Niyongabo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib32\" title=\"\">2020</a>)</cite>. Sentiment and toxicity are covered by AfriSenti, NaijaSenti, and AfriHate <cite class=\"ltx_cite ltx_citemacro_citep\">(Muhammad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib28\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib29\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib27\" title=\"\">2025a</a>)</cite>. The SIB-200 suite provides large-scale topic classification with stronger representation of African languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib1\" title=\"\">2024</a>)</cite>. Intent and slot-filling are addressed by INJONGO <cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib49\" title=\"\">2025</a>)</cite>. These resources underscore the need for region-specific evaluation and supply tasks that align well with sentence-embedding assessment.</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Massive Text Embedding Benchmark (MTEB) introduced a common taxonomy and leaderboard for sentence embeddings <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib26\" title=\"\">2023</a>)</cite>. Since then, several language- or region-specific variants have emerged, including MMTEB (with Europe and Indic tracks), SEA-BED for Southeast Asia, PL-MTEB for Polish, and MTEB-French <cite class=\"ltx_cite ltx_citemacro_citep\">(Ponwitayarat et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib36\" title=\"\">2025</a>; Po&#347;wiata et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib37\" title=\"\">2024</a>; Ciancone et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib9\" title=\"\">2024</a>)</cite>. Additional efforts target specific language families or regions: C-MTEB (Chinese), German-focused suites, JMTEB (Japanese), Korean-focused suites, ruMTEB (Russian), FaMTEB (Persian/Farsi), and VN-MTEB (Vietnamese) <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib48\" title=\"\">2024</a>; Wehrli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib46\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib20\" title=\"\">2024</a>; Snegirev et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib40\" title=\"\">2025</a>; Zinvandi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib51\" title=\"\">2025</a>; Pham et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib35\" title=\"\">2025</a>)</cite>. Our work follows this trend by building an Africa-focused extension with broad task coverage.</p>\n\n",
                "matched_terms": [
                    "families",
                    "embedding",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Commercial/API models widely used in practice include OpenAI&#8217;s text-embedding-3 series, Google&#8217;s gemini-embedding-001, and Cohere&#8217;s Embed v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Neelakantan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib31\" title=\"\">2022</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib18\" title=\"\">2025</a>)</cite>. Open-weight models are an active research area: me5, e5, and e5-mistral-7b-instruct provide strong general-purpose and instruction-tuned baselines <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">2024c</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib42\" title=\"\">a</a>)</cite>; Qwen3-Embedding and Embedding Gemma offer lightweight multilingual options <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib50\" title=\"\">2025</a>; Vera et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib41\" title=\"\">2025</a>)</cite>; GTE proposes efficient, general-purpose embeddings <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib21\" title=\"\">2023a</a>)</cite>; and classic multilingual encoders LaBSE remain strong references <cite class=\"ltx_cite ltx_citemacro_citep\">(Feng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib13\" title=\"\">2022</a>)</cite>. The recent BGE-M3 model integrates multilingual, multi-function training and is a competitive open baseline <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite>. Despite progress, coverage and performance on many African languages remain uneven, motivating region-specific evaluation and targeted adaptation.</p>\n\n",
                "matched_terms": [
                    "qwen3embedding",
                    "model",
                    "african",
                    "models",
                    "languages",
                    "bgem3",
                    "performance",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented AfriMTEB, a large-scale benchmark for African languages spanning 59 languages and 14 tasks, and AfriE5, an adaptation of mE5-large-instruct via cross-lingual contrastive distillation. AfriE5 achieves state-of-the-art results on both AfriMTEB-Full and Lite, surpassing strong baselines such as Gemini embedding-001. Our ablations show that cross-lingual dataset expansion and balanced translation filtering are crucial for these gains. AfriMTEB and AfriE5 provide standardized evaluation and a strong baseline to advance text embedding research for African languages.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "african",
                    "afrimteb",
                    "me5largeinstruct",
                    "languages",
                    "embedding",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AfriMTEB expands coverage to 59 languages, yet many African languages, dialects, orthographies, and code-switched registers remain underrepresented. Several datasets inherit noise or heterogeneity from crowd labels, repurposed tasks, and preprocessing; our adaptation data further relies on machine translation (NLLB-200) and automatic quality estimation (SSA-COMET), which vary in reliability across language pairs and domains. Distillation from a single teacher (BGE Reranker v2 m3) can also transfer its biases, potentially advantaging languages and styles that align with the MT/QE pipeline and the teacher&#8217;s preferences.</p>\n\n",
                "matched_terms": [
                    "african",
                    "afrimteb",
                    "across",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation focuses on text-only sentence/paragraph embeddings and macro-averages across tasks and languages; alternative weightings (e.g., by population or application criticality) and additional metrics (calibration, robustness, fairness) could yield different conclusions. End-to-end RAG quality, multimodal retrieval, and very long-context document embeddings are out of scope, and domain coverage skews toward formal/news text over colloquial or specialized domains. True parity with closed-weight baselines (e.g., data mixtures, architectures, inference settings) is infeasible; repeated API evaluations may be affected by nondeterminism, and some open models may be sensitive to prompt formats or poolers we did not exhaustively tune.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, benchmark contamination remains possible because public corpora are widely used in pretraining, which can inflate absolute scores. Our single-GPU fine-tuning recipe favors accessibility over peak performance and may limit stability, and releasing full translation/provenance artefacts is nontrivial due to size and licensing. While AfriE5 trained on nine languages generalizes well to 59 in our tests, transfer may degrade for typologically distant or extremely low-resource languages; broader community datasets and multi-teacher/multi-signal training are promising avenues to mitigate these limitations.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "scores",
                    "size",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AfriMTEB follows the MTEB taxonomy and groups tasks into eight families. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S2.T1\" title=\"Table 1 &#8227; 2 AfriMTEB Benchmark &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> lists the families and the datasets included in the <em class=\"ltx_emph ltx_font_italic\">Full</em> suite.</p>\n\n",
                "matched_terms": [
                    "families",
                    "afrimteb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given two sentence sets from different languages, the task is to identify translation pairs. Embeddings are used to compute similarities and find the best match for each sentence.</p>\n\n",
                "matched_terms": [
                    "each",
                    "best",
                    "task",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task involves a pair of input sentences with a binary or categorical relationship label (e.g., entailment vs. contradiction). Predictions are based on embedding similarity.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task measures the degree of semantic similarity between sentence pairs, either within or across languages, based on embedding similarity.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "task",
                    "across",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a query, the task is to retrieve relevant documents from a large corpus. Both queries and documents are embedded, and similarity scores determine ranking.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "task",
                    "large",
                    "relevant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">mmBERT-base is a massively multilingual ModernBERT encoder trained on <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>T,+ tokens covering <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>1,800 languages, extending the ModernBERT architecture (fast encoder with long context) to the multilingual regime <cite class=\"ltx_cite ltx_citemacro_citep\">(Warner et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib45\" title=\"\">2025</a>)</cite>. The training recipe introduces curriculum-style annealed language learning, inverse masking, and temperature-based sampling to emphasize low-resource languages while retaining strong performance on high-resource ones <cite class=\"ltx_cite ltx_citemacro_citep\">(Marone et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib23\" title=\"\">2025</a>)</cite>. Reported results show that <span class=\"ltx_text ltx_font_typewriter\">mmBERT-base</span> surpasses prior multilingual encoders such as XLM-R on standard NLU and retrieval benchmarks, approaching the English-only ModernBERT on GLUE despite being trained predominantly on non-English data <cite class=\"ltx_cite ltx_citemacro_citep\">(Marone et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib23\" title=\"\">2025</a>)</cite>. Inference follows standard encoder usage (mean pooling over last hidden states and L2 normalization).</p>\n\n",
                "matched_terms": [
                    "performance",
                    "mmbertbase",
                    "results",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">KaLM-Embedding is a multilingual embedding family that prioritizes training-data quality over sheer scale, combining (i) persona-based synthetic examples distilled from LLMs, (ii) ranking-consistency filtering, and (iii) semi-homogeneous task batching for efficient contrastive learning <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib16\" title=\"\">2025</a>)</cite>. Many public checkpoints are built on compact Qwen2 backbones (e.g., <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.5B) and instruction-tuned variants for downstream retrieval and semantic similarity. Technical reports describe v1.5/v2 updates with improved data curation and training strategy, yielding strong MTEB performance for their size <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib16\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "task",
                    "size",
                    "embedding",
                    "reports",
                    "performance",
                    "family"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Embedding is a purpose-built series of dense encoders for text embeddings and reranking, offered in 0.6B/4B/8B sizes with a 32k token context window and coverage of 100+ languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib50\" title=\"\">2025</a>)</cite>. The models are instruction-aware and support flexible output dimensionalities (via MRL-style prefix truncation), with typical maximum embedding sizes of 1,024 (0.6B), 2,560 (4B), and 4,096 (8B); matching reranker models are available at each size. Training leverages Qwen3 LLMs both as backbones and as data synthesizers across domains and languages, improving robustness for retrieval/reranking workloads. Inference uses mean pooling and L2 normalization; common deployment stacks expose a user-selectable output dimensionality for storage/latency trade-offs.</p>\n\n",
                "matched_terms": [
                    "each",
                    "06b",
                    "qwen3embedding",
                    "size",
                    "across",
                    "models",
                    "languages",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BGE-M3 (BAAI) is a versatile embedding model unifying three capabilities in a single encoder: Multi-Functionality (dense, multi-vector, and sparse retrieval), Multi-Linguality (100+ languages), and Multi-Granularity (robust from short queries to long documents, up to <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>8,192 tokens) <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite>. The training pipeline uses self-knowledge distillation across retrieval functions to align representations and enables hybrid retrieval without switching models <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite>. It is widely used as a strong multilingual baseline for retrieval, clustering, and classification.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across",
                    "models",
                    "languages",
                    "bgem3",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">mE5-large and mE5-large-instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">2024c</a>)</cite> are multilingual members of the E5 family built on XLM-RoBERTa-large <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib10\" title=\"\">2020</a>)</cite>, trained with a two-stage recipe that first performs weakly supervised contrastive pre-training on roughly one billion multilingual text pairs and then supervised fine-tuning on curated embedding tasks; the instruction variant further formats supervision with concise task instructions to specialize representations for retrieval and related tasks. Both use a 24-layer encoder that produces 1,024-dimensional vectors and inherit broad (&#160;100 language) coverage from the XLM-RoBERTa backbone. At inference time they follow the E5 prompt conventions (e.g., query / passage style inputs), apply mean pooling over the last hidden states, and L2-normalize the output, yielding strong performance on retrieval, semantic similarity, clustering, and classification benchmarks in both monolingual and cross-lingual settings.</p>\n\n",
                "matched_terms": [
                    "task",
                    "me5large",
                    "embedding",
                    "me5largeinstruct",
                    "performance",
                    "family"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">gte-Qwen2-7B-instruct (Alibaba-NLP) is a 7B-parameter instruction-tuned General Text Embedding model built on Qwen2-7B, targeting high-quality multilingual embeddings with a 32k context window <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib22\" title=\"\">2023b</a>)</cite>. At release, it reported leading scores on MTEB English/Chinese subsets, reflecting a training mixture that combines instruction-style contrastive objectives and curated negatives. It is used as a strong large-model baseline for retrieval, reranking, and semantic similarity.</p>\n\n",
                "matched_terms": [
                    "gteqwen27binstruct",
                    "model",
                    "embedding",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">GritLM-7B (Contextual AI) unifies generation and embeddings in one model via Generative Representational Instruction Tuning (GRIT) <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib25\" title=\"\">2024</a>)</cite>. A custom modeling component adds bidirectional attention paths so that the same backbone can function as a strong encoder for embeddings without sacrificing generative performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib25\" title=\"\">2024</a>)</cite>. The paper reports SOTA-level MTEB results for 7B-class open models alongside strong generative benchmarks, demonstrating that a single model can excel at both modalities <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib25\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "gritlm7b",
                    "models",
                    "reports",
                    "performance",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">Linq-Embed-Mistral</span> is a Mistral-7B&#8211;based embedding model developed with task-tailored data crafting, filtering, and hard-negative mining to improve retrieval quality <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib17\" title=\"\">2024</a>)</cite>. The technical report and model card document leading MTEB retrieval scores at release (e.g., average <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px8.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>68.2 on 56 datasets; retrieval score <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px8.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>60.2), achieved through homogeneous task ordering and mixed-task fine-tuning strategies <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib17\" title=\"\">2024</a>)</cite>. It is frequently used as a competitive 7B encoder baseline for dense retrieval.</p>\n\n",
                "matched_terms": [
                    "linqembedmistral",
                    "model",
                    "task",
                    "scores",
                    "average",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SFR-Embedding-Mistral (Salesforce Research) applies transfer learning on top of E5-mistral-7b-instruct and Mistral-7B-v0.1, with additional multi-task training and optimized negative sampling aimed at retrieval tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib24\" title=\"\">2024</a>)</cite>. Public materials position it as a top-performing 7B embedding model for search, clustering, and classification workloads <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib24\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "sfrembeddingmistral",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">E5-Mistral-7B-instruct initializes the E5 instruction-tuning recipe from Mistral-7B-v0.1, producing a 7B encoder specialized for instruction-aware embeddings <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib43\" title=\"\">2024b</a>)</cite>. As with other E5 variants, inference benefits from task instructions plus <span class=\"ltx_text ltx_font_typewriter\">query:</span>/<span class=\"ltx_text ltx_font_typewriter\">passage:</span> prefixes; mean pooling with L2 normalization is used for final vectors <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib43\" title=\"\">2024b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib42\" title=\"\">a</a>)</cite>. It serves both as a competitive baseline and as a foundation for further transfer learning (e.g., SFR-Embedding-Mistral).</p>\n\n",
                "matched_terms": [
                    "sfrembeddingmistral",
                    "task",
                    "final"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">gemini-embedding-001 is Google&#8217;s multilingual text-embedding model available via the Gemini API and Vertex AI, trained with Matryoshka Representation Learning (MRL) so that leading vector prefixes remain useful at smaller dimensions <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib18\" title=\"\">2025</a>)</cite>. The default output is 3,072 dimensions, but APIs allow setting output dimensionality (e.g., 1,536 or 768) with minimal quality loss, enabling flexible storage/latency trade-offs <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib18\" title=\"\">2025</a>)</cite>. The model supports 100+ languages and has been a strong performer on multilingual MTEB since its early releases.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "model",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the quality of the machine translation data used in AfriMTEB, we rely on automatic evaluation with <span class=\"ltx_text ltx_font_bold\">SSA-COMET</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib19\" title=\"\">2025</a>)</cite>. SSA-COMET is a recently released metric trained on <span class=\"ltx_text ltx_font_smallcaps\">SSA-MTE</span>, a large-scale human-annotated evaluation dataset covering 13 African language pairs with over 63,000 sentence-level judgments. Compared to earlier African-focused metrics such as AfriCOMET, SSA-COMET provides stronger correlation with human ratings and better robustness in low-resource settings.</p>\n\n",
                "matched_terms": [
                    "african",
                    "afrimteb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.T6\" title=\"Table 6 &#8227; A.3 Detailed Statistics of Machine Translation Quality &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we observe a clear trade-off between dataset size and quality. At a relaxed threshold of 0.67, over 430k sentence pairs are retained, ensuring broad coverage across all nine target languages. Increasing the threshold to 0.75 reduces the pool to around 60k examples, striking a balance between filtering noise and preserving sufficient training data. At the strictest cutoff of 0.80, only 7.5k pairs remain, indicating that high-quality translations are relatively scarce. Language-level differences are also evident: Swahili, Oromo, and Kinyarwanda consistently contribute the largest number of pairs, while Igbo and Amharic see the sharpest reductions under stricter filtering, reflecting variation in MT quality across languages.</p>\n\n",
                "matched_terms": [
                    "across",
                    "size",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#A1.T7\" title=\"Table 7 &#8227; A.4 Detailed AfriMTEB-Lite results &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;7</span></a> shows the comprehensive results across all evaluated datasets and languages in <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB-Lite</span>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "across",
                    "languages"
                ]
            }
        ]
    },
    "S3.T3": {
        "source_file": "AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages",
        "caption": "Table 3: AfriMTEB-Lite results. Average performance of embedding models across nine African languages (AMH, GAZ, HAU, IBO, KIN, SWA, XHO, YOR, ZUL) on 12 tasks. Columns report task-level averages, and the final column gives the unweighted macro average across tasks. Best scores per column are highlighted in bold.",
        "body": "Belebele\n\nFlores\n\nNews\nNTREX\nSIB-200\n\n\n\nModel\nHate\nSenti\nNLI\nRetrvl\nEmo\nBtxt\nIntent\nTC\nBtxt\n14Classes\nClass\nClust\nAvg\n\n\n\nSmall models (<1<\\!1B)\n\n\nmmBERT-base\n48.2\n39.9\n54.0\n4.0\n27.3\n1.8\n72.5\n55.1\n2.2\n2.4\n34.2\n5.0\n28.9\n\n\nKaLM\n48.5\n44.3\n61.0\n51.0\n28.9\n53.2\n63.4\n74.6\n58.6\n7.3\n48.9\n16.8\n46.4\n\n\nQwen3-Embedding 0.6B\n48.6\n41.5\n58.1\n46.3\n28.1\n33.0\n68.5\n70.5\n38.1\n3.5\n40.0\n12.0\n40.7\n\n\nEmbeddingGemma 300m\n45.4\n39.0\n53.6\n10.6\n26.6\n12.2\n49.4\n59.0\n17.9\n1.4\n29.6\n4.6\n29.1\n\n\nbge-m3\n50.1\n47.9\n68.7\n69.9\n29.4\n78.1\n75.4\n72.7\n80.4\n10.2\n55.6\n21.0\n55.0\n\n\nmE5-large\n49.8\n48.9\n65.2\n69.9\n31.3\n86.9\n77.1\n77.7\n88.7\n11.6\n60.4\n25.6\n57.8\n\n\nmE5-large-instruct\n51.5\n47.0\n64.5\n75.7\n31.5\n91.4\n75.5\n78.8\n91.5\n22.0\n71.2\n43.9\n62.0\n\n\nAfriE5-large-instruct\n51.7\n50.7\n69.0\n77.7\n32.8\n91.2\n75.4\n79.5\n92.0\n26.2\n72.0\n45.7\n63.7\n\n\n\nMedium models (≈4\\approx 4B)\n\n\nQwen3-Embedding-4B\n50.0\n41.2\n56.8\n58.3\n28.3\n47.7\n70.1\n76.0\n49.0\n7.9\n49.2\n17.5\n46.0\n\n\n\nLarge models (≥7\\geq\\!7B)\n\n\ngte-Qwen2-7B-instruct\n46.0\n43.5\n58.6\n55.6\n27.8\n58.4\n57.1\n79.8\n60.0\n10.4\n50.8\n22.4\n47.5\n\n\nGritLM-7B\n51.4\n45.6\n59.8\n55.0\n29.6\n46.5\n70.8\n75.3\n52.0\n8.8\n50.8\n22.2\n47.3\n\n\nLinq-Embed-Mistral\n51.0\n43.8\n59.7\n52.1\n30.0\n46.7\n70.3\n76.5\n52.0\n7.6\n50.7\n22.1\n46.9\n\n\nSFR-Embedding-Mistral\n49.3\n44.5\n58.9\n50.9\n28.8\n47.7\n62.5\n77.0\n53.0\n8.5\n49.7\n21.9\n46.1\n\n\nE5 mistral 7b instruct\n49.0\n45.7\n58.7\n50.2\n28.6\n47.9\n61.8\n76.0\n53.3\n7.2\n49.2\n21.8\n45.8\n\n\nQwen3-Embedding 8B\n50.8\n46.2\n58.8\n68.3\n29.1\n58.3\n67.7\n77.6\n57.3\n8.0\n53.5\n20.9\n49.7\n\n\nUndisclosed size\n\n\ngemini embedding 001\n55.0\n53.8\n75.3\n83.6\n35.5\n88.1\n83.9\n76.8\n84.2\n17.7\n69.2\n34.6\n63.1",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<td class=\"ltx_td ltx_border_tt\" colspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Belebele</span></td>\n<td class=\"ltx_td ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Flores</span></td>\n<td class=\"ltx_td ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">News</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">NTREX</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">SIB-200</span></td>\n<td class=\"ltx_td ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Hate</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Senti</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">NLI</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Retrvl</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Emo</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Btxt</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Intent</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">TC</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Btxt</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">14Classes</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Class</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Clust</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Avg</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"14\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<em class=\"ltx_emph ltx_font_italic\">Small models</em> (<math alttext=\"&lt;\\!1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">&lt;\\!1</annotation></semantics></math>B)</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">mmBERT-base</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">48.2</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">39.9</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">54.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">27.3</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.8</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">72.5</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">55.1</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.2</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.4</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.2</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">5.0</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">28.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">KaLM</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">48.5</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">44.3</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">61.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">51.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">28.9</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">53.2</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">63.4</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">74.6</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">58.6</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">7.3</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">48.9</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">16.8</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">46.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">Qwen3-Embedding 0.6B</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">48.6</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">41.5</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">58.1</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">46.3</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">28.1</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">33.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">68.5</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">70.5</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">38.1</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.5</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">40.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">12.0</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">40.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">EmbeddingGemma 300m</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">45.4</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">39.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">53.6</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">10.6</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">26.6</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">12.2</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">49.4</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">59.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">17.9</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.4</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">29.6</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4.6</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">29.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">bge-m3</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">50.1</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">47.9</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">68.7</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">69.9</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">29.4</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">78.1</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">75.4</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">72.7</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">80.4</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">10.2</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">55.6</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">21.0</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">55.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">mE5-large</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">49.8</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">48.9</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">65.2</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">69.9</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">31.3</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">86.9</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">77.1</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">77.7</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">88.7</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">11.6</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">60.4</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">25.6</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">57.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">mE5-large-instruct</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">51.5</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">47.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">64.5</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">75.7</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">31.5</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">91.4</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">75.5</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">78.8</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">91.5</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">22.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">71.2</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">43.9</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">62.0</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\" style=\"--ltx-bg-color:#D9D9D9;\">AfriE5-large-instruct</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">51.7</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">50.7</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">69.0</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">77.7</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">32.8</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">91.2</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">75.4</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">79.5</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">92.0</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">26.2</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">72.0</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">45.7</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">63.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"14\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<em class=\"ltx_emph ltx_font_italic\">Medium models</em> (<math alttext=\"\\approx 4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 4</annotation></semantics></math>B)</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">Qwen3-Embedding-4B</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">50.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">41.2</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">56.8</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">58.3</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">28.3</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">47.7</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">70.1</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">76.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">49.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">7.9</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">49.2</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">17.5</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">46.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"14\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<em class=\"ltx_emph ltx_font_italic\">Large models</em> (<math alttext=\"\\geq\\!7\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m3\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8805;</mo><mn>7</mn></mrow><annotation encoding=\"application/x-tex\">\\geq\\!7</annotation></semantics></math>B)</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">gte-Qwen2-7B-instruct</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">46.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">43.5</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">58.6</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">55.6</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">27.8</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">58.4</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">57.1</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">79.8</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">60.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">10.4</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">50.8</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">22.4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">47.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">GritLM-7B</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">51.4</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">45.6</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">59.8</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">55.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">29.6</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">46.5</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">70.8</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">75.3</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">52.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">8.8</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">50.8</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">22.2</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">47.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">Linq-Embed-Mistral</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">51.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">43.8</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">59.7</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">52.1</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">30.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">46.7</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">70.3</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">76.5</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">52.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">7.6</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">50.7</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">22.1</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">46.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">SFR-Embedding-Mistral</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">49.3</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">44.5</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">58.9</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">50.9</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">28.8</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">47.7</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">62.5</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">77.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">53.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">8.5</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">49.7</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">21.9</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">46.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">E5 mistral 7b instruct</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">49.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">45.7</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">58.7</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">50.2</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">28.6</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">47.9</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">61.8</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">76.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">53.3</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">7.2</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">49.2</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">21.8</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">45.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">Qwen3-Embedding 8B</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">50.8</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">46.2</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">58.8</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">68.3</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">29.1</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">58.3</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">67.7</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">77.6</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">57.3</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">8.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">53.5</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">20.9</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">49.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"14\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><em class=\"ltx_emph ltx_font_italic\">Undisclosed size</em></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">gemini embedding 001</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">55.0</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">53.8</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">75.3</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">83.6</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">35.5</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">88.1</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">83.9</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">76.8</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">84.2</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">17.7</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">69.2</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.6</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">63.1</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "amh",
            "news",
            "afrie5largeinstruct",
            "linqembedmistral",
            "medium",
            "size",
            "tasks",
            "tasklevel",
            "xho",
            "gives",
            "sib200",
            "nli",
            "retrvl",
            "flores",
            "mistral",
            "avg",
            "clust",
            "columns",
            "kin",
            "14classes",
            "class",
            "embedding",
            "macro",
            "intent",
            "large",
            "gemini",
            "06b",
            "across",
            "averages",
            "emo",
            "african",
            "mmbertbase",
            "me5large",
            "zul",
            "senti",
            "sfrembeddingmistral",
            "highlighted",
            "me5largeinstruct",
            "≈4approx",
            "yor",
            "results",
            "unweighted",
            "hate",
            "column",
            "instruct",
            "report",
            "model",
            "11b",
            "gritlm7b",
            "ntrex",
            "300m",
            "swa",
            "gteqwen27binstruct",
            "undisclosed",
            "hau",
            "average",
            "bold",
            "performance",
            "qwen3embedding",
            "≥7geq7b",
            "belebele",
            "qwen3embedding4b",
            "gaz",
            "embeddinggemma",
            "afrimteblite",
            "final",
            "btxt",
            "models",
            "scores",
            "ibo",
            "small",
            "best",
            "nine",
            "kalm",
            "languages",
            "bgem3"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Here, we described the overall aggregated results by datasets and languages in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S3.T3\" title=\"Table 3 &#8227; 3 Adapting Embedding Models to African Languages &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;3</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S4.F3\" title=\"Figure 3 &#8227; AfriE5 improves reranking with knowledge distillation &#8227; 4.1 AfriMTEB results &#8227; 4 Results &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;3</span></a>. The comprehensive results are in Appendix &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.SS4\" title=\"A.4 Detailed AfriMTEB-Lite results &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">On the Lite suite of nine African languages and twelve tasks, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> attains the best overall score with an average of 63.7, compared to 62.0 for <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> and the same performance of 63.1 as <span class=\"ltx_text ltx_font_italic\">Gemini embedding</span> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S3.T3\" title=\"Table 3 &#8227; 3 Adapting Embedding Models to African Languages &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Although the performance advantage is not uniform across all task families, AfriE5 consistently remains competitive and often surpasses both baselines on challenging benchmarks such as AfriXNLI and the SIB200 variants. These results highlight that adapting mE5 with cross-lingual contrastive distillation improves its effectiveness for African languages, even when trained on a compact set of nine languages.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Text embeddings are an essential building component of several NLP tasks such as retrieval-augmented generation which is crucial for preventing hallucinations in LLMs. Despite the recent release of massively multilingual MTEB (MMTEB), African languages remain underrepresented, with existing tasks often repurposed from translation benchmarks such as FLORES clustering or SIB-200.\nIn this paper, we introduce AfriMTEB&#8212;a regional expansion of MMTEB covering 59 languages, 14 tasks, and 38 datasets, including six newly added datasets. Unlike many MMTEB datasets that include fewer than five languages, the new additions span 14 to 56 African languages and introduce entirely new tasks, such as hate speech detection, intent detection, and emotion classification, which were not previously covered. Complementing this, we present AfriE5, an adaptation of the instruction-tuned mE5 model to African languages through cross-lingual contrastive distillation. Our evaluation shows that AfriE5 achieves state-of-the-art performance, outperforming strong baselines such as Gemini-Embeddings and mE5.</p>\n\n",
                "matched_terms": [
                    "hate",
                    "model",
                    "tasks",
                    "sib200",
                    "african",
                    "flores",
                    "languages",
                    "performance",
                    "intent"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "african",
                    "embedding",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Text embeddings are core building blocks for NLP systems in information retrieval, clustering, semantic similarity, and classification <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib15\" title=\"\">2022</a>; Feng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib13\" title=\"\">2022</a>)</cite>. However, evaluations on diverse tasks are often limited to a few high resource languages such as English&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib26\" title=\"\">2023</a>)</cite> or Chinese&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib48\" title=\"\">2024</a>)</cite>. Many under-represented languages are excluded due to lack of datasets or non-discoverability of community-created benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ojo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib34\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While recent text embedding benchmarks have improved language coverage in recent years, such as MMTEB&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Enevoldsen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib11\" title=\"\">2025</a>)</cite>, African languages remain under-represented, where many of the tasks covered are either based on massively evaluation of translation datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB-Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib33\" title=\"\">2022</a>; Federmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib12\" title=\"\">2022</a>)</cite>, and the tasks derived or repurposed from translation benchmarks such as Belebele&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bandarkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib5\" title=\"\">2024</a>)</cite> and SIB-200&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib1\" title=\"\">2024</a>)</cite>. As a result, the quality of text embeddings for languages in the African region remains unknown, as the region has few standardized tools for comparing models across tasks and languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alabi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib4\" title=\"\">2025</a>)</cite></p>\n\n",
                "matched_terms": [
                    "across",
                    "tasks",
                    "sib200",
                    "african",
                    "models",
                    "languages",
                    "embedding",
                    "belebele"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">AfriMTEB</span>, a regional extension of MTEB tailored to African languages and tasks. <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;2</span></a> shows the full suite that covers 59 languages and 38 datasets spanning Bitext mining, Classification (single, pair and multi-label), Semantic Text Similarity, Retrieval, Clustering, and Reranking. To support fast-and-compute friendly evaluation, we also provide <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">AfriMTEB-Lite</span>, a compact suite over nine geographically-diverse languages (Amharic, Oromo, Igbo, Yoruba, Hausa, Swahili, Kinyarwanda, Xhosa, Zulu) constructed by selecting, within each task family, the maximally overlapping tasks such that every task includes all nine languages, ensuring uniform language coverage across the suite.</p>\n\n",
                "matched_terms": [
                    "afrimteblite",
                    "across",
                    "tasks",
                    "african",
                    "nine",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to benchmarking, we develop <span class=\"ltx_text ltx_font_bold ltx_font_italic\">AfriE5-Large-Instruct</span>\nadapting strong embedding models to African languages.\nStarting from the instruction-tuned <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib42\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">c</a>)</cite>, we leveraged cross-lingual contrastive distillation. We construct supervision by translating MNLI/SNLI datasets<cite class=\"ltx_cite ltx_citemacro_citep\">(Williams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib47\" title=\"\">2018</a>; Bowman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib7\" title=\"\">2015</a>)</cite> in African languages with NLLB-200 (3.3B) <cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB-Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib33\" title=\"\">2022</a>)</cite>, followed by automatic filtering by SSA-COMET (an African COMET-based QE metric)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib19\" title=\"\">2025</a>)</cite>, and expanding each example into multiple sources or target directions to encourage cross-lingual alignment. We then use the receipe of BGE Re-ranker v2 m3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite> for text embedding model training.</p>\n\n",
                "matched_terms": [
                    "afrie5largeinstruct",
                    "model",
                    "african",
                    "models",
                    "me5largeinstruct",
                    "languages",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate widely used baselines, including BGE-M3, E5 variants (e.g., e5-mistral-7b-instruct), Qwen embeddings, Gemini embedding-001, and Embedding Gemma <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">2024c</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib50\" title=\"\">2025</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib18\" title=\"\">2025</a>)</cite>. On <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB-Lite</span>, our adapted model <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span>, trained only on nine African languages, achieves an average score of 63.7, surpassing me5-large-instruct (62.0) and Gemini embedding-001 (63.1). Remarkably, despite being tuned with data and languages aligned to the <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB-Lite</span>, our model leveraged cross-lingual transfer to generalize to the broader <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> benchmark covering 59 languages and 38 datasets, where it also delivers the best performance with an average score of 62.4, ahead of me5-large-instruct (61.3) and Gemini embedding-001 (60.6) as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>. These results highlight that targeted cross-lingual adaptation on a carefully selected subset of languages can transfer effectively to a much larger set, yielding consistent improvements across task families while preserving the backbone&#8217;s general utility.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "afrie5largeinstruct",
                    "model",
                    "afrimteblite",
                    "across",
                    "african",
                    "best",
                    "nine",
                    "me5largeinstruct",
                    "languages",
                    "average",
                    "bgem3",
                    "performance",
                    "embedding",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">AfriMTEB</span> is a regional extension of MTEB designed to evaluate text embeddings for African languages across multiple task families. To address persistent gaps in language coverage and enable systematic, comparable assessment, we introduce <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span>as a standardized benchmark for African languages. It consists of a <span class=\"ltx_text ltx_font_bold\">Full suite</span> covering <span class=\"ltx_text ltx_font_italic\">59 languages</span> and <span class=\"ltx_text ltx_font_italic\">38 datasets</span>, and a <span class=\"ltx_text ltx_font_bold\">Lite suite</span> that maintains uniform language coverage for <span class=\"ltx_text ltx_font_italic\">9 target languages</span> over <span class=\"ltx_text ltx_font_italic\">13 representative datasets</span>. Languages are selected to span major families and geographic regions while prioritizing practical relevance and the availability of reliable public datasets; tasks reflect core embedding applications and draw on established, well-curated resources. The Lite suite supports rapid iteration by enforcing consistent task coverage across languages through maximal task&#8211;language overlap.</p>\n\n",
                "matched_terms": [
                    "across",
                    "tasks",
                    "african",
                    "languages",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> follows the <span class=\"ltx_text ltx_font_typewriter\">MTEB</span> taxonomy and groups tasks into eight families. <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S2.T1\" title=\"Table 1 &#8227; 2 AfriMTEB Benchmark &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> lists the tasks, task families and the datasets included in the <em class=\"ltx_emph ltx_font_italic\">Full</em> suite. Each dataset belongs to one of the eight task families: (1) Bibtext mining (2) Pair classification (3) Semantic Text Similarity (STS) (4) Clustering (5) Classification (6) Multi-label Classification (7) Retrieval (8). The first four task families performs a similarity related tasks based on two pairs of sentences, either to identity translation pairs, categorize them into a class (e.g., entailment vs. contradiction) or provide a similarity score. The next three tasks (5) - (6) classify a sentence or article into one or more classes. Finally, the last sets of tasks (7) and (8) are information retrieval tasks for retrieval or re-ranking based on a user query. We provide full description in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.SS1\" title=\"A.1 Task categories &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "class",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> builds on the Massive Text Embedding Benchmark (MTEB) by selecting tasks that include African languages. From the original benchmark, we inherit datasets covering bitext mining (e.g., Flores, NTREX, Tatoeba), pair classification (XNLI), topic and sentiment classification (SIB-200, AfriSenti, MasakhaNEWS), semantic textual similarity (SemRel24), retrieval (MIRACL, XQuAD, XM3600), clustering (SIB-200, MasakhaNEWS), and reranking (MIRACL).</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "sib200",
                    "african",
                    "ntrex",
                    "flores",
                    "languages",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, language coverage in these datasets is uneven: some tasks include only a few African languages, while others span broader multilingual settings. To ensure fairness, we compute macro averages over languages within each task, then average across tasks and families to obtain the overall <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> score. This prevents any single task or language from dominating the benchmark.</p>\n\n",
                "matched_terms": [
                    "across",
                    "averages",
                    "tasks",
                    "african",
                    "languages",
                    "average",
                    "macro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <em class=\"ltx_emph ltx_font_italic\">Full</em> suite is built by selecting datasets from <span class=\"ltx_text ltx_font_typewriter\">MTEB</span> that include African languages and extending them with six additional datasets. These additions broaden task coverage, increase difficulty, and improve language representation.</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An African extension of the XNLI benchmark that provides natural language inference data to 15 African languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib3\" title=\"\">2025</a>)</cite>. By including AfriXNLI, we expand language coverage for the pair classification family beyond a single African language (Swahili), ensuring broader representation of African languages in entailment-style tasks.</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A multilabel emotion data set that covers 32 languages including 14 African languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Belay et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib6\" title=\"\">2025</a>; Muhammad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib30\" title=\"\">2025b</a>)</cite>. Each sentence may be assigned multiple emotion labels such as <span class=\"ltx_text ltx_font_smallcaps\">joy</span>, <span class=\"ltx_text ltx_font_smallcaps\">anger</span>, <span class=\"ltx_text ltx_font_smallcaps\">sadness</span>, or <span class=\"ltx_text ltx_font_smallcaps\">fear</span>. By including this dataset, AfriMTEB introduces the first <em class=\"ltx_emph ltx_font_italic\">multi-label classification</em> task for African languages, thereby broadening the taxonomy beyond single-label settings.</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A multilingual hate-speech classification dataset covering 14 African languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Muhammad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib27\" title=\"\">2025a</a>)</cite>. Each instance is labeled as <span class=\"ltx_text ltx_font_smallcaps\">hate</span>, <span class=\"ltx_text ltx_font_smallcaps\">abusive</span>, or <span class=\"ltx_text ltx_font_smallcaps\">neutral</span>, providing a standardized benchmark for toxic content detection across diverse languages and registers. This dataset extends evaluation to socially relevant safety applications.</p>\n\n",
                "matched_terms": [
                    "hate",
                    "african",
                    "across",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Injongo dataset is a multilingual resource for intent detection covering 16 African languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib49\" title=\"\">2025</a>)</cite>. It consists of short, conversational utterances annotated with 40 everyday intent categories, for example requests such as &#8220;freeze account&#8221; or &#8220;play music.&#8221; By focusing on dialogue-style classification, Injongo complements existing benchmarks like MASSIVE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">FitzGerald et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib14\" title=\"\">2023</a>)</cite>, but offers broader African language coverage.</p>\n\n",
                "matched_terms": [
                    "african",
                    "intent",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A Kinyarwanda news topic classification dataset with labels covering domains such as politics, business, and sports&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Niyongabo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib32\" title=\"\">2020</a>)</cite>. We include this dataset because <span class=\"ltx_text ltx_font_italic\">MasakhaNEWS</span> does not cover Kinyarwanda. Adding <span class=\"ltx_text ltx_font_italic\">KinNews</span> ensures we can cover Kinyarwanda in the AfriMTEB-Lite, since the Lite version requires maximally overlapping tasks.</p>\n\n",
                "matched_terms": [
                    "news",
                    "afrimteblite",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A more challenging variant of the SIB-200 dataset where labels are consolidated into 14 categories. This version still includes 56 African languages but is not limited to them&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib1\" title=\"\">2024</a>)</cite>. By merging fine-grained topics into broader classes, intra-class diversity increases, which raises task difficulty. The inclusion of this dataset not only strengthens African language evaluation but also increases the difficulty for other covered languages in SIB-200, leading to more robust cross-lingual assessment.</p>\n\n",
                "matched_terms": [
                    "sib200",
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we added <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SIB200Classification</span> dataset to complement <span class=\"ltx_text ltx_font_italic\">SIB-200_14Classes</span> in the &#8220;<span class=\"ltx_text ltx_font_typewriter\">Clf</span>&#8221; task family, although not new. The only difference is that the former covers seven categories compared to the latter. The original MTEB only considers SIB-200 in the Clustering (<span class=\"ltx_text ltx_font_typewriter\">Clust</span>) task family.</p>\n\n",
                "matched_terms": [
                    "sib200",
                    "clust"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <em class=\"ltx_emph ltx_font_italic\">Lite</em> suite focuses on nine geographically-diverse African languages:\n<span class=\"ltx_text ltx_font_italic\">Amharic, Oromo, Igbo, Yoruba, Hausa, Swahili, Kinyarwanda, Xhosa, Zulu</span>.\nWe selected only those datasets from MTEB that include nine languages to guarantee consistent language&#8211;task alignment across families. The result is a compact yet representative benchmark of 13 datasets&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We merged two datasets: MasakhaNEWS and KinNEWS since they have similar structure, thus we report 12 datasets</span></span></span> spanning classification, retrieval, bitext mining, clustering, pair classification, and multi-label classification.</p>\n\n",
                "matched_terms": [
                    "report",
                    "across",
                    "african",
                    "nine",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"s_{i,j}=\\mathbf{q}i^{\\top}\\mathbf{p}_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mi>&#119850;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>i</mi><mo>&#8868;</mo></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#119849;</mi><mi>j</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">s_{i,j}=\\mathbf{q}i^{\\top}\\mathbf{p}_{j}</annotation></semantics></math> is the similarity between query embedding <math alttext=\"\\mathbf{q}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#119850;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{q}_{i}</annotation></semantics></math> and passage embedding <math alttext=\"\\mathbf{p}_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#119849;</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{p}_{j}</annotation></semantics></math>, and <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m4\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> is a temperature hyperparameter. The numerator contains the similarity between query <math alttext=\"\\mathbf{q}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m5\" intent=\":literal\"><semantics><msub><mi>&#119850;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{q}_{i}</annotation></semantics></math> and its corresponding positive passage <math alttext=\"\\mathbf{p}{i\\cdot G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>&#119849;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><mi>G</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{p}{i\\cdot G}</annotation></semantics></math>. The denominator includes all similarities between <math alttext=\"\\mathbf{q}i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m7\" intent=\":literal\"><semantics><mrow><mi>&#119850;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{q}i</annotation></semantics></math> and all passages in the batch, comprising both pre-mined hard negatives (from NLI contradiction examples and hard negative mining) and in-batch negatives (other queries and passages from the same training batch).\nWith cross-device negative sharing enabled, the total negative set spans all examples across all GPUs in the distributed training setup, providing a rich set of challenging negative examples for each query.</p>\n\n",
                "matched_terms": [
                    "nli",
                    "embedding",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We constructed cross-lingual training data by leveraging large-scale natural language inference corpora, a supervision signal that has proven effective for learning sentence embeddings. <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib15\" title=\"\">2022</a>; Reimers and Gurevych, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib39\" title=\"\">2019</a>)</cite> Specifically, we used MultiNLI and SNLI <cite class=\"ltx_cite ltx_citemacro_citep\">(Williams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib47\" title=\"\">2018</a>; Bowman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib7\" title=\"\">2015</a>)</cite> as source datasets in English. Each sentence pair was translated into the nine African target languages using NLLB-200 (3.3B) <cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB-Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib33\" title=\"\">2022</a>)</cite>. We then estimated translation quality by SSA-COMET-MTL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib19\" title=\"\">2025</a>)</cite>, a COMET variant&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Rei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib38\" title=\"\">2020</a>)</cite>. and filtered pairs below a threshold of 0.75 to ensure the data quality.</p>\n\n",
                "matched_terms": [
                    "african",
                    "nine",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tuned <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> using the open-sourced <span class=\"ltx_text ltx_font_typewriter\">FlagEmbedding</span> training repository based on BGE Reranker v2 m3 recipe&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite>. Training was performed with <span class=\"ltx_text ltx_font_typewriter\">torchrun</span> on a single GPU. The key settings are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.T5\" title=\"Table 5 &#8227; Reranking. &#8227; A.1 Task categories &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. We followed the default implementation of contrastive learning with cross-device negatives and enabled knowledge distillation with teacher scores provided by BGE Reranker v2 m3.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "me5largeinstruct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We trained for one epoch over the curated cross-lingual dataset, with logging every 100 steps and checkpoint saving every 100 steps. All samples in a batch were drawn from the same dataset (<span class=\"ltx_text ltx_font_typewriter\">--same_dataset_within_batch</span>) to maintain consistent supervision. The resulting model is referred to as <span class=\"ltx_text ltx_font_bold ltx_font_italic\">AfriE5-Large-Instruct</span>. We provide brief descriptions of the baseline models in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.SS2\" title=\"A.2 Baseline Model Descriptions &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "afrie5largeinstruct",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite being listed under small models (<math alttext=\"&lt;1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">&lt;1</annotation></semantics></math>B), the E5 family matches or surpasses the proprietary model on average <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> score. <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> attains the best macro average at 62.4, edging out <span class=\"ltx_text ltx_font_italic\">Gemini embedding</span> (60.6) and <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> (61.3) in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S2.T2\" title=\"Table 2 &#8227; 2.3 AfriMTEB-Lite construction &#8227; 2 AfriMTEB Benchmark &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>. This indicates that strong multilingual coverage and targeted adaptation can outweigh model size or access to proprietary training data.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "afrie5largeinstruct",
                    "model",
                    "size",
                    "11b",
                    "models",
                    "small",
                    "best",
                    "me5largeinstruct",
                    "average",
                    "embedding",
                    "macro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While E5 variants lead overall, <span class=\"ltx_text ltx_font_italic\">Gemini embedding 001</span> is strongest on most classification-style families: single-label classification (50.0 vs. 49.8 for <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span>and 49.7 for <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span>), multi-label classification (32.7 vs. 28.6/29.8), and pair classification (71.6 vs. 63.8/67.9). It also tops retrieval (77.5) and semantic textual similarity (65.0). These strengths suggest Gemini&#8217;s instruction/data mix particularly benefits discriminative judgment tasks and sentence-level similarity scoring.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "afrie5largeinstruct",
                    "embedding",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large 7B and 8B encoders do not translate into higher <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> scores. All 7B/8B models cluster in the low&#8211;mid 50s, for example, <span class=\"ltx_text ltx_font_italic\">gte-Qwen2-7B-instruct</span> at 50.9, <span class=\"ltx_text ltx_font_italic\">GritLM-7B</span> at 51.8, <span class=\"ltx_text ltx_font_italic\">Qwen3-Embedding-8B</span> at 50.5. Thay are clearly below the smaller E5 variants of around 61.3 point. This gap underscores that broad, balanced language coverage and task diversity matter more than parameter count alone.</p>\n\n",
                "matched_terms": [
                    "models",
                    "gritlm7b",
                    "scores",
                    "gteqwen27binstruct",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On bitext mining, <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> and <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> have comparable performance of <math alttext=\"85\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mn>85</mn><annotation encoding=\"application/x-tex\">85</annotation></semantics></math> points,\nboth are far ahead of other opens and the API baseline (<span class=\"ltx_text ltx_font_italic\">Gemini embedding</span> at 72.2). For clustering, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> leads with 62.9 and <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> is next (61.9), while larger models and Gemini embeddings behind. These families reward models that learn language-agnostic semantic spaces with robust cross-lingual alignment, which appears to be a particular strength of the E5 lineage.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "afrie5largeinstruct",
                    "models",
                    "me5largeinstruct",
                    "performance",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> is adapted using supervision centered on nine African languages (<span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span>), it achieves the highest Full-suite average (62.4) across 59 languages. Relative to <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span>, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> shows consistent family-level gains on pair classification (+4.1), reranking (+2.1), retrieval (+1.3), clustering (+1.0), and multi-label classification (+1.2), with small trade-offs on bitext mining (&#8211;0.3), single-label classification (&#8211;0.1), and STS (&#8211;1.1). This pattern indicates that targeted cross-lingual distillation can yield transferable gains beyond the training languages, particularly for retrieval-style tasks. </p>\n\n",
                "matched_terms": [
                    "afrie5largeinstruct",
                    "across",
                    "tasks",
                    "african",
                    "small",
                    "nine",
                    "me5largeinstruct",
                    "languages",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the base model <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> records a reranking score of 61.9, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> raises this to 64.0. It also surpasses gemini embedding 001 (63.4) on the same task (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S2.T2\" title=\"Table 2 &#8227; 2.3 AfriMTEB-Lite construction &#8227; 2 AfriMTEB Benchmark &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). This improvement can be attributed to the training recipe, where <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> leverages knowledge distillation from the <span class=\"ltx_text ltx_font_smallcaps\">BGE-m3</span> cross-encoder in addition to contrastive supervision. The result suggests that incorporating reranker-derived soft labels enhances cross-lingual alignment for ranking-style objectives.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "afrie5largeinstruct",
                    "model",
                    "me5largeinstruct",
                    "bgem3",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Relative to <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span>, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> shows clear improvements on several classification benchmarks. It achieves higher scores on AfriSentiment (+3.7), AfriXNLI (+4.5), and SIB200 clustering (+1.8). The largest gain appears on SIB200-14 classification (+4.4; 26.2 vs. 21.8), a setting that requires grouping semantically diverse texts into broad topical categories. These improvements indicate that the cross-lingual, NLI-style contrastive learning used in AfriE5 strengthens semantic alignment across languages, leading to more effective cross-lingual transfer for classification and clustering tasks.\n</p>\n\n",
                "matched_terms": [
                    "afrie5largeinstruct",
                    "across",
                    "tasks",
                    "sib200",
                    "scores",
                    "me5largeinstruct",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct controlled experiments where all training settings are held fixed (base architecture, loss, knowledge distillation from BGE reranker, batch/group sizes, number of steps, and sampling) while varying a single factor at a time. We report AfriMTEB-Lite results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S4.T4\" title=\"Table 4 &#8227; Our adaptation boosts classification tasks, especially SIB200-14 &#8227; 4.2 AfriMTEB-Lite Results &#8227; 4 Results &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "afrimteblite",
                    "report"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As described in Section &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S3.SS2\" title=\"3.2 Cross-lingual Training Data Construction &#8227; 3 Adapting Embedding Models to African Languages &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, comparing row 1 (<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>, 0.75) and row 3 (<math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math>, 0.75), the average increases from 62.3 to 63.2. The cross-lingual expansion of the data set exposes the model to richer cross-lingual contrasts: First, higher scores on bitext mining tasks show that the expansion improves the model&#8217;s ability to align semantically equivalent sentences between languages, producing a more coherent multilingual embedding space. Second, classification tasks such as SIB200-14 benefit from this stronger alignment, as the model learns to abstract over diverse topical domains without relying on language-specific cues.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tasks",
                    "scores",
                    "languages",
                    "average",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With expansion enabled (rows 2&#8211;4), the best overall score is achieved after filtering using a COMET variant, SSA-COMET-MTL QE=0.75 (63.2), compared to 62.5 at 0.67 and a sharp drop to 58.3 at 0.80. A low threshold (0.67) retains large noisy translations (around 433K training samples), which slightly hurts reasoning-heavy tasks. In contrast, a high threshold (0.80) filters out too much data (remaining only 7500 samples), reducing linguistic and semantic diversity and leading to weaker transfer, especially in classification. The middle ground (0.75) balances translation quality with coverage with around 60K samples, yielding the most reliable improvements across tasks. We provide the number of samples retained by SSA-COMET-MTL in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.SS3\" title=\"A.3 Detailed Statistics of Machine Translation Quality &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a></p>\n\n",
                "matched_terms": [
                    "best",
                    "across",
                    "large",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A number of African NLP benchmarks focus on text classification and related tasks. AfroBench aggregates multiple tasks and languages with an emphasis on broad coverage <cite class=\"ltx_cite ltx_citemacro_citep\">(Ojo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib34\" title=\"\">2025</a>)</cite>. News topic classification resources include MasakhaNEWS and earlier Kinyarwanda/Kirundi corpora (KINNEWS/KIRNEWS) <cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib2\" title=\"\">2023</a>; Niyongabo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib32\" title=\"\">2020</a>)</cite>. Sentiment and toxicity are covered by AfriSenti, NaijaSenti, and AfriHate <cite class=\"ltx_cite ltx_citemacro_citep\">(Muhammad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib28\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib29\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib27\" title=\"\">2025a</a>)</cite>. The SIB-200 suite provides large-scale topic classification with stronger representation of African languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib1\" title=\"\">2024</a>)</cite>. Intent and slot-filling are addressed by INJONGO <cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib49\" title=\"\">2025</a>)</cite>. These resources underscore the need for region-specific evaluation and supply tasks that align well with sentence-embedding assessment.</p>\n\n",
                "matched_terms": [
                    "news",
                    "tasks",
                    "sib200",
                    "african",
                    "languages",
                    "intent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Commercial/API models widely used in practice include OpenAI&#8217;s text-embedding-3 series, Google&#8217;s gemini-embedding-001, and Cohere&#8217;s Embed v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Neelakantan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib31\" title=\"\">2022</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib18\" title=\"\">2025</a>)</cite>. Open-weight models are an active research area: me5, e5, and e5-mistral-7b-instruct provide strong general-purpose and instruction-tuned baselines <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">2024c</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib42\" title=\"\">a</a>)</cite>; Qwen3-Embedding and Embedding Gemma offer lightweight multilingual options <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib50\" title=\"\">2025</a>; Vera et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib41\" title=\"\">2025</a>)</cite>; GTE proposes efficient, general-purpose embeddings <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib21\" title=\"\">2023a</a>)</cite>; and classic multilingual encoders LaBSE remain strong references <cite class=\"ltx_cite ltx_citemacro_citep\">(Feng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib13\" title=\"\">2022</a>)</cite>. The recent BGE-M3 model integrates multilingual, multi-function training and is a competitive open baseline <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite>. Despite progress, coverage and performance on many African languages remain uneven, motivating region-specific evaluation and targeted adaptation.</p>\n\n",
                "matched_terms": [
                    "qwen3embedding",
                    "model",
                    "african",
                    "models",
                    "languages",
                    "bgem3",
                    "performance",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented AfriMTEB, a large-scale benchmark for African languages spanning 59 languages and 14 tasks, and AfriE5, an adaptation of mE5-large-instruct via cross-lingual contrastive distillation. AfriE5 achieves state-of-the-art results on both AfriMTEB-Full and Lite, surpassing strong baselines such as Gemini embedding-001. Our ablations show that cross-lingual dataset expansion and balanced translation filtering are crucial for these gains. AfriMTEB and AfriE5 provide standardized evaluation and a strong baseline to advance text embedding research for African languages.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "tasks",
                    "african",
                    "me5largeinstruct",
                    "languages",
                    "embedding",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AfriMTEB expands coverage to 59 languages, yet many African languages, dialects, orthographies, and code-switched registers remain underrepresented. Several datasets inherit noise or heterogeneity from crowd labels, repurposed tasks, and preprocessing; our adaptation data further relies on machine translation (NLLB-200) and automatic quality estimation (SSA-COMET), which vary in reliability across language pairs and domains. Distillation from a single teacher (BGE Reranker v2 m3) can also transfer its biases, potentially advantaging languages and styles that align with the MT/QE pipeline and the teacher&#8217;s preferences.</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages",
                    "across",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation focuses on text-only sentence/paragraph embeddings and macro-averages across tasks and languages; alternative weightings (e.g., by population or application criticality) and additional metrics (calibration, robustness, fairness) could yield different conclusions. End-to-end RAG quality, multimodal retrieval, and very long-context document embeddings are out of scope, and domain coverage skews toward formal/news text over colloquial or specialized domains. True parity with closed-weight baselines (e.g., data mixtures, architectures, inference settings) is infeasible; repeated API evaluations may be affected by nondeterminism, and some open models may be sensitive to prompt formats or poolers we did not exhaustively tune.</p>\n\n",
                "matched_terms": [
                    "models",
                    "languages",
                    "across",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, benchmark contamination remains possible because public corpora are widely used in pretraining, which can inflate absolute scores. Our single-GPU fine-tuning recipe favors accessibility over peak performance and may limit stability, and releasing full translation/provenance artefacts is nontrivial due to size and licensing. While AfriE5 trained on nine languages generalizes well to 59 in our tests, transfer may degrade for typologically distant or extremely low-resource languages; broader community datasets and multi-teacher/multi-signal training are promising avenues to mitigate these limitations.</p>\n\n",
                "matched_terms": [
                    "size",
                    "scores",
                    "nine",
                    "languages",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given two sentence sets from different languages, the task is to identify translation pairs. Embeddings are used to compute similarities and find the best match for each sentence.</p>\n\n",
                "matched_terms": [
                    "best",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task measures the degree of semantic similarity between sentence pairs, either within or across languages, based on embedding similarity.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "across",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a query, the task is to retrieve relevant documents from a large corpus. Both queries and documents are embedded, and similarity scores determine ranking.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">mmBERT-base is a massively multilingual ModernBERT encoder trained on <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>T,+ tokens covering <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>1,800 languages, extending the ModernBERT architecture (fast encoder with long context) to the multilingual regime <cite class=\"ltx_cite ltx_citemacro_citep\">(Warner et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib45\" title=\"\">2025</a>)</cite>. The training recipe introduces curriculum-style annealed language learning, inverse masking, and temperature-based sampling to emphasize low-resource languages while retaining strong performance on high-resource ones <cite class=\"ltx_cite ltx_citemacro_citep\">(Marone et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib23\" title=\"\">2025</a>)</cite>. Reported results show that <span class=\"ltx_text ltx_font_typewriter\">mmBERT-base</span> surpasses prior multilingual encoders such as XLM-R on standard NLU and retrieval benchmarks, approaching the English-only ModernBERT on GLUE despite being trained predominantly on non-English data <cite class=\"ltx_cite ltx_citemacro_citep\">(Marone et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib23\" title=\"\">2025</a>)</cite>. Inference follows standard encoder usage (mean pooling over last hidden states and L2 normalization).</p>\n\n",
                "matched_terms": [
                    "performance",
                    "mmbertbase",
                    "results",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">KaLM-Embedding is a multilingual embedding family that prioritizes training-data quality over sheer scale, combining (i) persona-based synthetic examples distilled from LLMs, (ii) ranking-consistency filtering, and (iii) semi-homogeneous task batching for efficient contrastive learning <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib16\" title=\"\">2025</a>)</cite>. Many public checkpoints are built on compact Qwen2 backbones (e.g., <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.5B) and instruction-tuned variants for downstream retrieval and semantic similarity. Technical reports describe v1.5/v2 updates with improved data curation and training strategy, yielding strong MTEB performance for their size <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib16\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "embedding",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Embedding is a purpose-built series of dense encoders for text embeddings and reranking, offered in 0.6B/4B/8B sizes with a 32k token context window and coverage of 100+ languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib50\" title=\"\">2025</a>)</cite>. The models are instruction-aware and support flexible output dimensionalities (via MRL-style prefix truncation), with typical maximum embedding sizes of 1,024 (0.6B), 2,560 (4B), and 4,096 (8B); matching reranker models are available at each size. Training leverages Qwen3 LLMs both as backbones and as data synthesizers across domains and languages, improving robustness for retrieval/reranking workloads. Inference uses mean pooling and L2 normalization; common deployment stacks expose a user-selectable output dimensionality for storage/latency trade-offs.</p>\n\n",
                "matched_terms": [
                    "06b",
                    "size",
                    "across",
                    "models",
                    "embedding",
                    "languages",
                    "qwen3embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BGE-M3 (BAAI) is a versatile embedding model unifying three capabilities in a single encoder: Multi-Functionality (dense, multi-vector, and sparse retrieval), Multi-Linguality (100+ languages), and Multi-Granularity (robust from short queries to long documents, up to <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>8,192 tokens) <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite>. The training pipeline uses self-knowledge distillation across retrieval functions to align representations and enables hybrid retrieval without switching models <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite>. It is widely used as a strong multilingual baseline for retrieval, clustering, and classification.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across",
                    "models",
                    "languages",
                    "bgem3",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">mE5-large and mE5-large-instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">2024c</a>)</cite> are multilingual members of the E5 family built on XLM-RoBERTa-large <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib10\" title=\"\">2020</a>)</cite>, trained with a two-stage recipe that first performs weakly supervised contrastive pre-training on roughly one billion multilingual text pairs and then supervised fine-tuning on curated embedding tasks; the instruction variant further formats supervision with concise task instructions to specialize representations for retrieval and related tasks. Both use a 24-layer encoder that produces 1,024-dimensional vectors and inherit broad (&#160;100 language) coverage from the XLM-RoBERTa backbone. At inference time they follow the E5 prompt conventions (e.g., query / passage style inputs), apply mean pooling over the last hidden states, and L2-normalize the output, yielding strong performance on retrieval, semantic similarity, clustering, and classification benchmarks in both monolingual and cross-lingual settings.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "me5large",
                    "me5largeinstruct",
                    "performance",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">gte-Qwen2-7B-instruct (Alibaba-NLP) is a 7B-parameter instruction-tuned General Text Embedding model built on Qwen2-7B, targeting high-quality multilingual embeddings with a 32k context window <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib22\" title=\"\">2023b</a>)</cite>. At release, it reported leading scores on MTEB English/Chinese subsets, reflecting a training mixture that combines instruction-style contrastive objectives and curated negatives. It is used as a strong large-model baseline for retrieval, reranking, and semantic similarity.</p>\n\n",
                "matched_terms": [
                    "gteqwen27binstruct",
                    "model",
                    "embedding",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">GritLM-7B (Contextual AI) unifies generation and embeddings in one model via Generative Representational Instruction Tuning (GRIT) <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib25\" title=\"\">2024</a>)</cite>. A custom modeling component adds bidirectional attention paths so that the same backbone can function as a strong encoder for embeddings without sacrificing generative performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib25\" title=\"\">2024</a>)</cite>. The paper reports SOTA-level MTEB results for 7B-class open models alongside strong generative benchmarks, demonstrating that a single model can excel at both modalities <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib25\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "gritlm7b",
                    "performance",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">Linq-Embed-Mistral</span> is a Mistral-7B&#8211;based embedding model developed with task-tailored data crafting, filtering, and hard-negative mining to improve retrieval quality <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib17\" title=\"\">2024</a>)</cite>. The technical report and model card document leading MTEB retrieval scores at release (e.g., average <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px8.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>68.2 on 56 datasets; retrieval score <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px8.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>60.2), achieved through homogeneous task ordering and mixed-task fine-tuning strategies <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib17\" title=\"\">2024</a>)</cite>. It is frequently used as a competitive 7B encoder baseline for dense retrieval.</p>\n\n",
                "matched_terms": [
                    "linqembedmistral",
                    "model",
                    "report",
                    "scores",
                    "average",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SFR-Embedding-Mistral (Salesforce Research) applies transfer learning on top of E5-mistral-7b-instruct and Mistral-7B-v0.1, with additional multi-task training and optimized negative sampling aimed at retrieval tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib24\" title=\"\">2024</a>)</cite>. Public materials position it as a top-performing 7B embedding model for search, clustering, and classification workloads <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib24\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "sfrembeddingmistral",
                    "embedding",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">E5-Mistral-7B-instruct initializes the E5 instruction-tuning recipe from Mistral-7B-v0.1, producing a 7B encoder specialized for instruction-aware embeddings <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib43\" title=\"\">2024b</a>)</cite>. As with other E5 variants, inference benefits from task instructions plus <span class=\"ltx_text ltx_font_typewriter\">query:</span>/<span class=\"ltx_text ltx_font_typewriter\">passage:</span> prefixes; mean pooling with L2 normalization is used for final vectors <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib43\" title=\"\">2024b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib42\" title=\"\">a</a>)</cite>. It serves both as a competitive baseline and as a foundation for further transfer learning (e.g., SFR-Embedding-Mistral).</p>\n\n",
                "matched_terms": [
                    "sfrembeddingmistral",
                    "final"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">gemini-embedding-001 is Google&#8217;s multilingual text-embedding model available via the Gemini API and Vertex AI, trained with Matryoshka Representation Learning (MRL) so that leading vector prefixes remain useful at smaller dimensions <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib18\" title=\"\">2025</a>)</cite>. The default output is 3,072 dimensions, but APIs allow setting output dimensionality (e.g., 1,536 or 768) with minimal quality loss, enabling flexible storage/latency trade-offs <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib18\" title=\"\">2025</a>)</cite>. The model supports 100+ languages and has been a strong performer on multilingual MTEB since its early releases.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "model",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.T6\" title=\"Table 6 &#8227; A.3 Detailed Statistics of Machine Translation Quality &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we observe a clear trade-off between dataset size and quality. At a relaxed threshold of 0.67, over 430k sentence pairs are retained, ensuring broad coverage across all nine target languages. Increasing the threshold to 0.75 reduces the pool to around 60k examples, striking a balance between filtering noise and preserving sufficient training data. At the strictest cutoff of 0.80, only 7.5k pairs remain, indicating that high-quality translations are relatively scarce. Language-level differences are also evident: Swahili, Oromo, and Kinyarwanda consistently contribute the largest number of pairs, while Igbo and Amharic see the sharpest reductions under stricter filtering, reflecting variation in MT quality across languages.</p>\n\n",
                "matched_terms": [
                    "nine",
                    "across",
                    "size",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#A1.T7\" title=\"Table 7 &#8227; A.4 Detailed AfriMTEB-Lite results &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;7</span></a> shows the comprehensive results across all evaluated datasets and languages in <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB-Lite</span>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "afrimteblite",
                    "across",
                    "languages"
                ]
            }
        ]
    },
    "S4.T4": {
        "source_file": "AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages",
        "caption": "Table 4: Ablation study of AfriE5-large-instruct on AfriMTEB-Lite. Columns match the benchmark layout in Table 3. We vary the use of cross-lingual dataset expansion and the translation quality threshold (QE) during filtering.",
        "body": "Flores\n\nNews\nNTREX\nSIB-200\nSIB-200\nSIB-200\nAvg\n\n\nCross-lingual Expansion\nQE Thres.\nHate\nSenti\nNLI\nEmo\nBtxt\nIntent\nTC\nBtxt\n14Classes\nClass\nClust\nAvg\n\n\n\n\n✓\\checkmark\n0.67\n52.2\n50.9\n66.3\n32.3\n91.6\n77.6\n82.7\n94.1\n23.8\n71.7\n44.8\n62.5\n\n\n✓\\checkmark\n0.70\n51.3\n52.0\n69.0\n32.6\n91.3\n75.2\n79.5\n92.2\n26.9\n73.1\n45.5\n62.6\n\n\n✓\\checkmark\n0.75\n51.4\n53.3\n69.1\n32.8\n91.2\n77.1\n82.6\n93.8\n26.2\n72.0\n45.7\n63.2\n\n\n✓\\checkmark\n0.80\n51.5\n49.9\n68.2\n29.8\n88.2\n85.9\n79.9\n91.0\n11.2\n61.8\n24.2\n58.3\n\n\n×\\times\n0.75\n52.3\n52.8\n68.2\n32.0\n88.5\n81.2\n82.0\n91.2\n21.8\n70.6\n45.2\n62.3",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Flores</span></th>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">News</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">NTREX</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">SIB-200</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">SIB-200</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">SIB-200</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Avg</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Cross-lingual Expansion</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">QE Thres.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Hate</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Senti</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">NLI</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Emo</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Btxt</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Intent</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">TC</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Btxt</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">14Classes</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Class</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Clust</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Avg</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.67</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">52.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">50.9</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">66.3</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.3</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">91.6</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">77.6</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">82.7</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">94.1</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">23.8</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">71.7</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">44.8</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">62.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.70</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">51.3</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">52.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">69.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.6</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">91.3</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">75.2</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">79.5</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">92.2</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">26.9</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">73.1</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">45.5</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">62.6</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m3\" intent=\":literal\" style=\"--ltx-bg-color:#D9D9D9;\"><semantics><mi mathbackground=\"#D9D9D9\" mathvariant=\"normal\" style=\"--ltx-bg-color:#D9D9D9;\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">0.75</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">51.4</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">53.3</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">69.1</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">32.8</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">91.2</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">77.1</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">82.6</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">93.8</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">26.2</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">72.0</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">45.7</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">63.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.80</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">51.5</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">49.9</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">68.2</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">29.8</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">88.2</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">85.9</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">79.9</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">91.0</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">11.2</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">61.8</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">24.2</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">58.3</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m5\" intent=\":literal\" style=\"--ltx-bg-color:#D9D9D9;\"><semantics><mo mathbackground=\"#D9D9D9\" style=\"--ltx-bg-color:#D9D9D9;\">&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">0.75</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">52.3</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">52.8</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">68.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">32.0</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">88.5</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">81.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">82.0</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">91.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">21.8</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">70.6</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">45.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">62.3</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "afrie5largeinstruct",
            "filtering",
            "news",
            "quality",
            "ablation",
            "threshold",
            "sib200",
            "nli",
            "flores",
            "avg",
            "clust",
            "columns",
            "14classes",
            "class",
            "layout",
            "benchmark",
            "thres",
            "intent",
            "emo",
            "vary",
            "senti",
            "translation",
            "hate",
            "expansion",
            "match",
            "ntrex",
            "dataset",
            "×times",
            "afrimteblite",
            "study",
            "btxt",
            "✓checkmark",
            "during",
            "use",
            "crosslingual"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We conduct controlled experiments where all training settings are held fixed (base architecture, loss, knowledge distillation from BGE reranker, batch/group sizes, number of steps, and sampling) while varying a single factor at a time. We report AfriMTEB-Lite results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S4.T4\" title=\"Table 4 &#8227; Our adaptation boosts classification tasks, especially SIB200-14 &#8227; 4.2 AfriMTEB-Lite Results &#8227; 4 Results &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Text embeddings are an essential building component of several NLP tasks such as retrieval-augmented generation which is crucial for preventing hallucinations in LLMs. Despite the recent release of massively multilingual MTEB (MMTEB), African languages remain underrepresented, with existing tasks often repurposed from translation benchmarks such as FLORES clustering or SIB-200.\nIn this paper, we introduce AfriMTEB&#8212;a regional expansion of MMTEB covering 59 languages, 14 tasks, and 38 datasets, including six newly added datasets. Unlike many MMTEB datasets that include fewer than five languages, the new additions span 14 to 56 African languages and introduce entirely new tasks, such as hate speech detection, intent detection, and emotion classification, which were not previously covered. Complementing this, we present AfriE5, an adaptation of the instruction-tuned mE5 model to African languages through cross-lingual contrastive distillation. Our evaluation shows that AfriE5 achieves state-of-the-art performance, outperforming strong baselines such as Gemini-Embeddings and mE5.</p>\n\n",
                "matched_terms": [
                    "hate",
                    "expansion",
                    "sib200",
                    "flores",
                    "translation",
                    "crosslingual",
                    "intent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While recent text embedding benchmarks have improved language coverage in recent years, such as MMTEB&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Enevoldsen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib11\" title=\"\">2025</a>)</cite>, African languages remain under-represented, where many of the tasks covered are either based on massively evaluation of translation datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB-Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib33\" title=\"\">2022</a>; Federmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib12\" title=\"\">2022</a>)</cite>, and the tasks derived or repurposed from translation benchmarks such as Belebele&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bandarkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib5\" title=\"\">2024</a>)</cite> and SIB-200&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib1\" title=\"\">2024</a>)</cite>. As a result, the quality of text embeddings for languages in the African region remains unknown, as the region has few standardized tools for comparing models across tasks and languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alabi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib4\" title=\"\">2025</a>)</cite></p>\n\n",
                "matched_terms": [
                    "sib200",
                    "translation",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to benchmarking, we develop <span class=\"ltx_text ltx_font_bold ltx_font_italic\">AfriE5-Large-Instruct</span>\nadapting strong embedding models to African languages.\nStarting from the instruction-tuned <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib42\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">c</a>)</cite>, we leveraged cross-lingual contrastive distillation. We construct supervision by translating MNLI/SNLI datasets<cite class=\"ltx_cite ltx_citemacro_citep\">(Williams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib47\" title=\"\">2018</a>; Bowman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib7\" title=\"\">2015</a>)</cite> in African languages with NLLB-200 (3.3B) <cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB-Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib33\" title=\"\">2022</a>)</cite>, followed by automatic filtering by SSA-COMET (an African COMET-based QE metric)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib19\" title=\"\">2025</a>)</cite>, and expanding each example into multiple sources or target directions to encourage cross-lingual alignment. We then use the receipe of BGE Re-ranker v2 m3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite> for text embedding model training.</p>\n\n",
                "matched_terms": [
                    "afrie5largeinstruct",
                    "filtering",
                    "crosslingual",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate widely used baselines, including BGE-M3, E5 variants (e.g., e5-mistral-7b-instruct), Qwen embeddings, Gemini embedding-001, and Embedding Gemma <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">2024c</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib50\" title=\"\">2025</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib18\" title=\"\">2025</a>)</cite>. On <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB-Lite</span>, our adapted model <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span>, trained only on nine African languages, achieves an average score of 63.7, surpassing me5-large-instruct (62.0) and Gemini embedding-001 (63.1). Remarkably, despite being tuned with data and languages aligned to the <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB-Lite</span>, our model leveraged cross-lingual transfer to generalize to the broader <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> benchmark covering 59 languages and 38 datasets, where it also delivers the best performance with an average score of 62.4, ahead of me5-large-instruct (61.3) and Gemini embedding-001 (60.6) as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>. These results highlight that targeted cross-lingual adaptation on a carefully selected subset of languages can transfer effectively to a much larger set, yielding consistent improvements across task families while preserving the backbone&#8217;s general utility.</p>\n\n",
                "matched_terms": [
                    "afrie5largeinstruct",
                    "crosslingual",
                    "benchmark",
                    "afrimteblite"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> follows the <span class=\"ltx_text ltx_font_typewriter\">MTEB</span> taxonomy and groups tasks into eight families. <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S2.T1\" title=\"Table 1 &#8227; 2 AfriMTEB Benchmark &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> lists the tasks, task families and the datasets included in the <em class=\"ltx_emph ltx_font_italic\">Full</em> suite. Each dataset belongs to one of the eight task families: (1) Bibtext mining (2) Pair classification (3) Semantic Text Similarity (STS) (4) Clustering (5) Classification (6) Multi-label Classification (7) Retrieval (8). The first four task families performs a similarity related tasks based on two pairs of sentences, either to identity translation pairs, categorize them into a class (e.g., entailment vs. contradiction) or provide a similarity score. The next three tasks (5) - (6) classify a sentence or article into one or more classes. Finally, the last sets of tasks (7) and (8) are information retrieval tasks for retrieval or re-ranking based on a user query. We provide full description in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.SS1\" title=\"A.1 Task categories &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "class",
                    "translation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> builds on the Massive Text Embedding Benchmark (MTEB) by selecting tasks that include African languages. From the original benchmark, we inherit datasets covering bitext mining (e.g., Flores, NTREX, Tatoeba), pair classification (XNLI), topic and sentiment classification (SIB-200, AfriSenti, MasakhaNEWS), semantic textual similarity (SemRel24), retrieval (MIRACL, XQuAD, XM3600), clustering (SIB-200, MasakhaNEWS), and reranking (MIRACL).</p>\n\n",
                "matched_terms": [
                    "sib200",
                    "ntrex",
                    "benchmark",
                    "flores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A multilingual hate-speech classification dataset covering 14 African languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Muhammad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib27\" title=\"\">2025a</a>)</cite>. Each instance is labeled as <span class=\"ltx_text ltx_font_smallcaps\">hate</span>, <span class=\"ltx_text ltx_font_smallcaps\">abusive</span>, or <span class=\"ltx_text ltx_font_smallcaps\">neutral</span>, providing a standardized benchmark for toxic content detection across diverse languages and registers. This dataset extends evaluation to socially relevant safety applications.</p>\n\n",
                "matched_terms": [
                    "hate",
                    "benchmark",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Injongo dataset is a multilingual resource for intent detection covering 16 African languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib49\" title=\"\">2025</a>)</cite>. It consists of short, conversational utterances annotated with 40 everyday intent categories, for example requests such as &#8220;freeze account&#8221; or &#8220;play music.&#8221; By focusing on dialogue-style classification, Injongo complements existing benchmarks like MASSIVE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">FitzGerald et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib14\" title=\"\">2023</a>)</cite>, but offers broader African language coverage.</p>\n\n",
                "matched_terms": [
                    "intent",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A Kinyarwanda news topic classification dataset with labels covering domains such as politics, business, and sports&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Niyongabo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib32\" title=\"\">2020</a>)</cite>. We include this dataset because <span class=\"ltx_text ltx_font_italic\">MasakhaNEWS</span> does not cover Kinyarwanda. Adding <span class=\"ltx_text ltx_font_italic\">KinNews</span> ensures we can cover Kinyarwanda in the AfriMTEB-Lite, since the Lite version requires maximally overlapping tasks.</p>\n\n",
                "matched_terms": [
                    "news",
                    "afrimteblite",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A more challenging variant of the SIB-200 dataset where labels are consolidated into 14 categories. This version still includes 56 African languages but is not limited to them&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib1\" title=\"\">2024</a>)</cite>. By merging fine-grained topics into broader classes, intra-class diversity increases, which raises task difficulty. The inclusion of this dataset not only strengthens African language evaluation but also increases the difficulty for other covered languages in SIB-200, leading to more robust cross-lingual assessment.</p>\n\n",
                "matched_terms": [
                    "sib200",
                    "crosslingual",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we added <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SIB200Classification</span> dataset to complement <span class=\"ltx_text ltx_font_italic\">SIB-200_14Classes</span> in the &#8220;<span class=\"ltx_text ltx_font_typewriter\">Clf</span>&#8221; task family, although not new. The only difference is that the former covers seven categories compared to the latter. The original MTEB only considers SIB-200 in the Clustering (<span class=\"ltx_text ltx_font_typewriter\">Clust</span>) task family.</p>\n\n",
                "matched_terms": [
                    "sib200",
                    "clust",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We constructed cross-lingual training data by leveraging large-scale natural language inference corpora, a supervision signal that has proven effective for learning sentence embeddings. <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib15\" title=\"\">2022</a>; Reimers and Gurevych, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib39\" title=\"\">2019</a>)</cite> Specifically, we used MultiNLI and SNLI <cite class=\"ltx_cite ltx_citemacro_citep\">(Williams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib47\" title=\"\">2018</a>; Bowman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib7\" title=\"\">2015</a>)</cite> as source datasets in English. Each sentence pair was translated into the nine African target languages using NLLB-200 (3.3B) <cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB-Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib33\" title=\"\">2022</a>)</cite>. We then estimated translation quality by SSA-COMET-MTL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib19\" title=\"\">2025</a>)</cite>, a COMET variant&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Rei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib38\" title=\"\">2020</a>)</cite>. and filtered pairs below a threshold of 0.75 to ensure the data quality.</p>\n\n",
                "matched_terms": [
                    "threshold",
                    "translation",
                    "crosslingual",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We trained for one epoch over the curated cross-lingual dataset, with logging every 100 steps and checkpoint saving every 100 steps. All samples in a batch were drawn from the same dataset (<span class=\"ltx_text ltx_font_typewriter\">--same_dataset_within_batch</span>) to maintain consistent supervision. The resulting model is referred to as <span class=\"ltx_text ltx_font_bold ltx_font_italic\">AfriE5-Large-Instruct</span>. We provide brief descriptions of the baseline models in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.SS2\" title=\"A.2 Baseline Model Descriptions &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "afrie5largeinstruct",
                    "crosslingual",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On bitext mining, <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> and <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> have comparable performance of <math alttext=\"85\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mn>85</mn><annotation encoding=\"application/x-tex\">85</annotation></semantics></math> points,\nboth are far ahead of other opens and the API baseline (<span class=\"ltx_text ltx_font_italic\">Gemini embedding</span> at 72.2). For clustering, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> leads with 62.9 and <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> is next (61.9), while larger models and Gemini embeddings behind. These families reward models that learn language-agnostic semantic spaces with robust cross-lingual alignment, which appears to be a particular strength of the E5 lineage.</p>\n\n",
                "matched_terms": [
                    "afrie5largeinstruct",
                    "crosslingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> is adapted using supervision centered on nine African languages (<span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span>), it achieves the highest Full-suite average (62.4) across 59 languages. Relative to <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span>, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> shows consistent family-level gains on pair classification (+4.1), reranking (+2.1), retrieval (+1.3), clustering (+1.0), and multi-label classification (+1.2), with small trade-offs on bitext mining (&#8211;0.3), single-label classification (&#8211;0.1), and STS (&#8211;1.1). This pattern indicates that targeted cross-lingual distillation can yield transferable gains beyond the training languages, particularly for retrieval-style tasks. </p>\n\n",
                "matched_terms": [
                    "afrie5largeinstruct",
                    "crosslingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the base model <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> records a reranking score of 61.9, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> raises this to 64.0. It also surpasses gemini embedding 001 (63.4) on the same task (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S2.T2\" title=\"Table 2 &#8227; 2.3 AfriMTEB-Lite construction &#8227; 2 AfriMTEB Benchmark &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). This improvement can be attributed to the training recipe, where <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> leverages knowledge distillation from the <span class=\"ltx_text ltx_font_smallcaps\">BGE-m3</span> cross-encoder in addition to contrastive supervision. The result suggests that incorporating reranker-derived soft labels enhances cross-lingual alignment for ranking-style objectives.</p>\n\n",
                "matched_terms": [
                    "afrie5largeinstruct",
                    "crosslingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the Lite suite of nine African languages and twelve tasks, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> attains the best overall score with an average of 63.7, compared to 62.0 for <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> and the same performance of 63.1 as <span class=\"ltx_text ltx_font_italic\">Gemini embedding</span> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S3.T3\" title=\"Table 3 &#8227; 3 Adapting Embedding Models to African Languages &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Although the performance advantage is not uniform across all task families, AfriE5 consistently remains competitive and often surpasses both baselines on challenging benchmarks such as AfriXNLI and the SIB200 variants. These results highlight that adapting mE5 with cross-lingual contrastive distillation improves its effectiveness for African languages, even when trained on a compact set of nine languages.</p>\n\n",
                "matched_terms": [
                    "sib200",
                    "afrie5largeinstruct",
                    "crosslingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Relative to <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span>, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> shows clear improvements on several classification benchmarks. It achieves higher scores on AfriSentiment (+3.7), AfriXNLI (+4.5), and SIB200 clustering (+1.8). The largest gain appears on SIB200-14 classification (+4.4; 26.2 vs. 21.8), a setting that requires grouping semantically diverse texts into broad topical categories. These improvements indicate that the cross-lingual, NLI-style contrastive learning used in AfriE5 strengthens semantic alignment across languages, leading to more effective cross-lingual transfer for classification and clustering tasks.\n</p>\n\n",
                "matched_terms": [
                    "sib200",
                    "afrie5largeinstruct",
                    "crosslingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As described in Section &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S3.SS2\" title=\"3.2 Cross-lingual Training Data Construction &#8227; 3 Adapting Embedding Models to African Languages &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, comparing row 1 (<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>, 0.75) and row 3 (<math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math>, 0.75), the average increases from 62.3 to 63.2. The cross-lingual expansion of the data set exposes the model to richer cross-lingual contrasts: First, higher scores on bitext mining tasks show that the expansion improves the model&#8217;s ability to align semantically equivalent sentences between languages, producing a more coherent multilingual embedding space. Second, classification tasks such as SIB200-14 benefit from this stronger alignment, as the model learns to abstract over diverse topical domains without relying on language-specific cues.</p>\n\n",
                "matched_terms": [
                    "✓checkmark",
                    "expansion",
                    "crosslingual",
                    "×times"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With expansion enabled (rows 2&#8211;4), the best overall score is achieved after filtering using a COMET variant, SSA-COMET-MTL QE=0.75 (63.2), compared to 62.5 at 0.67 and a sharp drop to 58.3 at 0.80. A low threshold (0.67) retains large noisy translations (around 433K training samples), which slightly hurts reasoning-heavy tasks. In contrast, a high threshold (0.80) filters out too much data (remaining only 7500 samples), reducing linguistic and semantic diversity and leading to weaker transfer, especially in classification. The middle ground (0.75) balances translation quality with coverage with around 60K samples, yielding the most reliable improvements across tasks. We provide the number of samples retained by SSA-COMET-MTL in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.SS3\" title=\"A.3 Detailed Statistics of Machine Translation Quality &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a></p>\n\n",
                "matched_terms": [
                    "filtering",
                    "expansion",
                    "quality",
                    "threshold",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A number of African NLP benchmarks focus on text classification and related tasks. AfroBench aggregates multiple tasks and languages with an emphasis on broad coverage <cite class=\"ltx_cite ltx_citemacro_citep\">(Ojo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib34\" title=\"\">2025</a>)</cite>. News topic classification resources include MasakhaNEWS and earlier Kinyarwanda/Kirundi corpora (KINNEWS/KIRNEWS) <cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib2\" title=\"\">2023</a>; Niyongabo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib32\" title=\"\">2020</a>)</cite>. Sentiment and toxicity are covered by AfriSenti, NaijaSenti, and AfriHate <cite class=\"ltx_cite ltx_citemacro_citep\">(Muhammad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib28\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib29\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib27\" title=\"\">2025a</a>)</cite>. The SIB-200 suite provides large-scale topic classification with stronger representation of African languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib1\" title=\"\">2024</a>)</cite>. Intent and slot-filling are addressed by INJONGO <cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib49\" title=\"\">2025</a>)</cite>. These resources underscore the need for region-specific evaluation and supply tasks that align well with sentence-embedding assessment.</p>\n\n",
                "matched_terms": [
                    "sib200",
                    "news",
                    "intent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented AfriMTEB, a large-scale benchmark for African languages spanning 59 languages and 14 tasks, and AfriE5, an adaptation of mE5-large-instruct via cross-lingual contrastive distillation. AfriE5 achieves state-of-the-art results on both AfriMTEB-Full and Lite, surpassing strong baselines such as Gemini embedding-001. Our ablations show that cross-lingual dataset expansion and balanced translation filtering are crucial for these gains. AfriMTEB and AfriE5 provide standardized evaluation and a strong baseline to advance text embedding research for African languages.</p>\n\n",
                "matched_terms": [
                    "filtering",
                    "expansion",
                    "dataset",
                    "translation",
                    "crosslingual",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AfriMTEB expands coverage to 59 languages, yet many African languages, dialects, orthographies, and code-switched registers remain underrepresented. Several datasets inherit noise or heterogeneity from crowd labels, repurposed tasks, and preprocessing; our adaptation data further relies on machine translation (NLLB-200) and automatic quality estimation (SSA-COMET), which vary in reliability across language pairs and domains. Distillation from a single teacher (BGE Reranker v2 m3) can also transfer its biases, potentially advantaging languages and styles that align with the MT/QE pipeline and the teacher&#8217;s preferences.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "vary",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given two sentence sets from different languages, the task is to identify translation pairs. Embeddings are used to compute similarities and find the best match for each sentence.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "match"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">KaLM-Embedding is a multilingual embedding family that prioritizes training-data quality over sheer scale, combining (i) persona-based synthetic examples distilled from LLMs, (ii) ranking-consistency filtering, and (iii) semi-homogeneous task batching for efficient contrastive learning <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib16\" title=\"\">2025</a>)</cite>. Many public checkpoints are built on compact Qwen2 backbones (e.g., <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.5B) and instruction-tuned variants for downstream retrieval and semantic similarity. Technical reports describe v1.5/v2 updates with improved data curation and training strategy, yielding strong MTEB performance for their size <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib16\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "filtering",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">mE5-large and mE5-large-instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">2024c</a>)</cite> are multilingual members of the E5 family built on XLM-RoBERTa-large <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib10\" title=\"\">2020</a>)</cite>, trained with a two-stage recipe that first performs weakly supervised contrastive pre-training on roughly one billion multilingual text pairs and then supervised fine-tuning on curated embedding tasks; the instruction variant further formats supervision with concise task instructions to specialize representations for retrieval and related tasks. Both use a 24-layer encoder that produces 1,024-dimensional vectors and inherit broad (&#160;100 language) coverage from the XLM-RoBERTa backbone. At inference time they follow the E5 prompt conventions (e.g., query / passage style inputs), apply mean pooling over the last hidden states, and L2-normalize the output, yielding strong performance on retrieval, semantic similarity, clustering, and classification benchmarks in both monolingual and cross-lingual settings.</p>\n\n",
                "matched_terms": [
                    "crosslingual",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">Linq-Embed-Mistral</span> is a Mistral-7B&#8211;based embedding model developed with task-tailored data crafting, filtering, and hard-negative mining to improve retrieval quality <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib17\" title=\"\">2024</a>)</cite>. The technical report and model card document leading MTEB retrieval scores at release (e.g., average <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px8.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>68.2 on 56 datasets; retrieval score <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px8.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>60.2), achieved through homogeneous task ordering and mixed-task fine-tuning strategies <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib17\" title=\"\">2024</a>)</cite>. It is frequently used as a competitive 7B encoder baseline for dense retrieval.</p>\n\n",
                "matched_terms": [
                    "filtering",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the quality of the machine translation data used in AfriMTEB, we rely on automatic evaluation with <span class=\"ltx_text ltx_font_bold\">SSA-COMET</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib19\" title=\"\">2025</a>)</cite>. SSA-COMET is a recently released metric trained on <span class=\"ltx_text ltx_font_smallcaps\">SSA-MTE</span>, a large-scale human-annotated evaluation dataset covering 13 African language pairs with over 63,000 sentence-level judgments. Compared to earlier African-focused metrics such as AfriCOMET, SSA-COMET provides stronger correlation with human ratings and better robustness in low-resource settings.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "quality",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.T6\" title=\"Table 6 &#8227; A.3 Detailed Statistics of Machine Translation Quality &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we observe a clear trade-off between dataset size and quality. At a relaxed threshold of 0.67, over 430k sentence pairs are retained, ensuring broad coverage across all nine target languages. Increasing the threshold to 0.75 reduces the pool to around 60k examples, striking a balance between filtering noise and preserving sufficient training data. At the strictest cutoff of 0.80, only 7.5k pairs remain, indicating that high-quality translations are relatively scarce. Language-level differences are also evident: Swahili, Oromo, and Kinyarwanda consistently contribute the largest number of pairs, while Igbo and Amharic see the sharpest reductions under stricter filtering, reflecting variation in MT quality across languages.</p>\n\n",
                "matched_terms": [
                    "threshold",
                    "filtering",
                    "quality",
                    "dataset"
                ]
            }
        ]
    },
    "A1.T5": {
        "source_file": "AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages",
        "caption": "Table 5: Key training configurations used in fine-tuning.",
        "body": "Parameter\nValue\n\n\n\n\nTraining epochs\n1\n\n\nBatch size (per device)\n8\n\n\nGroup size\n8\n\n\nLearning rate\n1e-5\n\n\nWarmup ratio\n0.1\n\n\nMax query length\n512\n\n\nMax passage length\n512\n\n\nPadding multiple\n8\n\n\nKnowledge distillation\nTrue (KL divergence)\n\n\nNegatives cross-device\nEnabled\n\n\nEmbedding normalization\nTrue (L2)\n\n\nSentence pooling\nMean pooling\n\n\nTemperature\n0.02\n\n\nPrecision\nFP16",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Parameter</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Value</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Training epochs</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Batch size (per device)</th>\n<td class=\"ltx_td ltx_align_left\">8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Group size</th>\n<td class=\"ltx_td ltx_align_left\">8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Learning rate</th>\n<td class=\"ltx_td ltx_align_left\">1e-5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Warmup ratio</th>\n<td class=\"ltx_td ltx_align_left\">0.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Max query length</th>\n<td class=\"ltx_td ltx_align_left\">512</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Max passage length</th>\n<td class=\"ltx_td ltx_align_left\">512</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Padding multiple</th>\n<td class=\"ltx_td ltx_align_left\">8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Knowledge distillation</th>\n<td class=\"ltx_td ltx_align_left\">True (KL divergence)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Negatives cross-device</th>\n<td class=\"ltx_td ltx_align_left\">Enabled</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Embedding normalization</th>\n<td class=\"ltx_td ltx_align_left\">True (L2)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Sentence pooling</th>\n<td class=\"ltx_td ltx_align_left\">Mean pooling</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Temperature</th>\n<td class=\"ltx_td ltx_align_left\">0.02</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Precision</th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">FP16</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "training",
            "size",
            "sentence",
            "rate",
            "parameter",
            "max",
            "temperature",
            "device",
            "padding",
            "length",
            "knowledge",
            "finetuning",
            "used",
            "embedding",
            "key",
            "learning",
            "true",
            "mean",
            "passage",
            "query",
            "group",
            "crossdevice",
            "negatives",
            "1e5",
            "enabled",
            "multiple",
            "normalization",
            "ratio",
            "epochs",
            "fp16",
            "pooling",
            "distillation",
            "divergence",
            "value",
            "batch",
            "configurations",
            "warmup",
            "precision"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We fine-tuned <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> using the open-sourced <span class=\"ltx_text ltx_font_typewriter\">FlagEmbedding</span> training repository based on BGE Reranker v2 m3 recipe&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite>. Training was performed with <span class=\"ltx_text ltx_font_typewriter\">torchrun</span> on a single GPU. The key settings are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.T5\" title=\"Table 5 &#8227; Reranking. &#8227; A.1 Task categories &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. We followed the default implementation of contrastive learning with cross-device negatives and enabled knowledge distillation with teacher scores provided by BGE Reranker v2 m3.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">In addition to benchmarking, we develop <span class=\"ltx_text ltx_font_bold ltx_font_italic\">AfriE5-Large-Instruct</span>\nadapting strong embedding models to African languages.\nStarting from the instruction-tuned <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib42\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">c</a>)</cite>, we leveraged cross-lingual contrastive distillation. We construct supervision by translating MNLI/SNLI datasets<cite class=\"ltx_cite ltx_citemacro_citep\">(Williams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib47\" title=\"\">2018</a>; Bowman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib7\" title=\"\">2015</a>)</cite> in African languages with NLLB-200 (3.3B) <cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB-Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib33\" title=\"\">2022</a>)</cite>, followed by automatic filtering by SSA-COMET (an African COMET-based QE metric)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib19\" title=\"\">2025</a>)</cite>, and expanding each example into multiple sources or target directions to encourage cross-lingual alignment. We then use the receipe of BGE Re-ranker v2 m3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite> for text embedding model training.</p>\n\n",
                "matched_terms": [
                    "multiple",
                    "distillation",
                    "embedding",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate widely used baselines, including BGE-M3, E5 variants (e.g., e5-mistral-7b-instruct), Qwen embeddings, Gemini embedding-001, and Embedding Gemma <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">2024c</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib50\" title=\"\">2025</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib18\" title=\"\">2025</a>)</cite>. On <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB-Lite</span>, our adapted model <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span>, trained only on nine African languages, achieves an average score of 63.7, surpassing me5-large-instruct (62.0) and Gemini embedding-001 (63.1). Remarkably, despite being tuned with data and languages aligned to the <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB-Lite</span>, our model leveraged cross-lingual transfer to generalize to the broader <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> benchmark covering 59 languages and 38 datasets, where it also delivers the best performance with an average score of 62.4, ahead of me5-large-instruct (61.3) and Gemini embedding-001 (60.6) as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>. These results highlight that targeted cross-lingual adaptation on a carefully selected subset of languages can transfer effectively to a much larger set, yielding consistent improvements across task families while preserving the backbone&#8217;s general utility.</p>\n\n",
                "matched_terms": [
                    "used",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">AfriMTEB</span> is a regional extension of MTEB designed to evaluate text embeddings for African languages across multiple task families. To address persistent gaps in language coverage and enable systematic, comparable assessment, we introduce <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span>as a standardized benchmark for African languages. It consists of a <span class=\"ltx_text ltx_font_bold\">Full suite</span> covering <span class=\"ltx_text ltx_font_italic\">59 languages</span> and <span class=\"ltx_text ltx_font_italic\">38 datasets</span>, and a <span class=\"ltx_text ltx_font_bold\">Lite suite</span> that maintains uniform language coverage for <span class=\"ltx_text ltx_font_italic\">9 target languages</span> over <span class=\"ltx_text ltx_font_italic\">13 representative datasets</span>. Languages are selected to span major families and geographic regions while prioritizing practical relevance and the availability of reliable public datasets; tasks reflect core embedding applications and draw on established, well-curated resources. The Lite suite supports rapid iteration by enforcing consistent task coverage across languages through maximal task&#8211;language overlap.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "multiple"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> follows the <span class=\"ltx_text ltx_font_typewriter\">MTEB</span> taxonomy and groups tasks into eight families. <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S2.T1\" title=\"Table 1 &#8227; 2 AfriMTEB Benchmark &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> lists the tasks, task families and the datasets included in the <em class=\"ltx_emph ltx_font_italic\">Full</em> suite. Each dataset belongs to one of the eight task families: (1) Bibtext mining (2) Pair classification (3) Semantic Text Similarity (STS) (4) Clustering (5) Classification (6) Multi-label Classification (7) Retrieval (8). The first four task families performs a similarity related tasks based on two pairs of sentences, either to identity translation pairs, categorize them into a class (e.g., entailment vs. contradiction) or provide a similarity score. The next three tasks (5) - (6) classify a sentence or article into one or more classes. Finally, the last sets of tasks (7) and (8) are information retrieval tasks for retrieval or re-ranking based on a user query. We provide full description in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.SS1\" title=\"A.1 Task categories &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "query",
                    "sentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A multilabel emotion data set that covers 32 languages including 14 African languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Belay et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib6\" title=\"\">2025</a>; Muhammad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib30\" title=\"\">2025b</a>)</cite>. Each sentence may be assigned multiple emotion labels such as <span class=\"ltx_text ltx_font_smallcaps\">joy</span>, <span class=\"ltx_text ltx_font_smallcaps\">anger</span>, <span class=\"ltx_text ltx_font_smallcaps\">sadness</span>, or <span class=\"ltx_text ltx_font_smallcaps\">fear</span>. By including this dataset, AfriMTEB introduces the first <em class=\"ltx_emph ltx_font_italic\">multi-label classification</em> task for African languages, thereby broadening the taxonomy beyond single-label settings.</p>\n\n",
                "matched_terms": [
                    "sentence",
                    "multiple"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our approach combines contrastive learning with knowledge distillation in a unified objective. Given a batch of <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> query&#8211;document groups, each with group size <math alttext=\"G\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>G</mi><annotation encoding=\"application/x-tex\">G</annotation></semantics></math>, the total loss is:</p>\n\n",
                "matched_terms": [
                    "distillation",
                    "size",
                    "learning",
                    "batch",
                    "knowledge",
                    "group"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The distillation term aligns the student&#8217;s predicted distribution with the teacher&#8217;s scores using KL divergence. Teacher&#8217;s scores are gained during training data creation.:</p>\n\n",
                "matched_terms": [
                    "distillation",
                    "training",
                    "divergence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The contrastive term is computed with in-batch (and cross-device) negatives:</p>\n\n",
                "matched_terms": [
                    "crossdevice",
                    "negatives"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"s_{i,j}=\\mathbf{q}i^{\\top}\\mathbf{p}_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mi>&#119850;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>i</mi><mo>&#8868;</mo></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#119849;</mi><mi>j</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">s_{i,j}=\\mathbf{q}i^{\\top}\\mathbf{p}_{j}</annotation></semantics></math> is the similarity between query embedding <math alttext=\"\\mathbf{q}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#119850;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{q}_{i}</annotation></semantics></math> and passage embedding <math alttext=\"\\mathbf{p}_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#119849;</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{p}_{j}</annotation></semantics></math>, and <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m4\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> is a temperature hyperparameter. The numerator contains the similarity between query <math alttext=\"\\mathbf{q}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m5\" intent=\":literal\"><semantics><msub><mi>&#119850;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{q}_{i}</annotation></semantics></math> and its corresponding positive passage <math alttext=\"\\mathbf{p}{i\\cdot G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>&#119849;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><mi>G</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{p}{i\\cdot G}</annotation></semantics></math>. The denominator includes all similarities between <math alttext=\"\\mathbf{q}i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m7\" intent=\":literal\"><semantics><mrow><mi>&#119850;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{q}i</annotation></semantics></math> and all passages in the batch, comprising both pre-mined hard negatives (from NLI contradiction examples and hard negative mining) and in-batch negatives (other queries and passages from the same training batch).\nWith cross-device negative sharing enabled, the total negative set spans all examples across all GPUs in the distributed training setup, providing a rich set of challenging negative examples for each query.</p>\n\n",
                "matched_terms": [
                    "training",
                    "enabled",
                    "crossdevice",
                    "batch",
                    "temperature",
                    "passage",
                    "query",
                    "embedding",
                    "negatives"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We constructed cross-lingual training data by leveraging large-scale natural language inference corpora, a supervision signal that has proven effective for learning sentence embeddings. <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib15\" title=\"\">2022</a>; Reimers and Gurevych, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib39\" title=\"\">2019</a>)</cite> Specifically, we used MultiNLI and SNLI <cite class=\"ltx_cite ltx_citemacro_citep\">(Williams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib47\" title=\"\">2018</a>; Bowman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib7\" title=\"\">2015</a>)</cite> as source datasets in English. Each sentence pair was translated into the nine African target languages using NLLB-200 (3.3B) <cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB-Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib33\" title=\"\">2022</a>)</cite>. We then estimated translation quality by SSA-COMET-MTL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib19\" title=\"\">2025</a>)</cite>, a COMET variant&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Rei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib38\" title=\"\">2020</a>)</cite>. and filtered pairs below a threshold of 0.75 to ensure the data quality.</p>\n\n",
                "matched_terms": [
                    "sentence",
                    "used",
                    "learning",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To encourage <span class=\"ltx_text ltx_font_bold\">cross-lingual alignment</span>, each example was expanded into multiple configurations: (i) premise in target language and hypothesis in source, (ii) premise in source and hypothesis in target, (iii) both in target, and (iv) both in source, that is,is, English. For MultiNLI, we reformulated examples as query&#8211;positive/negative pairs (entailment as <span class=\"ltx_text ltx_font_italic\">pos</span>, contradiction as <span class=\"ltx_text ltx_font_italic\">neg</span>). For SNLI, we followed the same strategy but included all 3-way annotations. We further increased training difficulty by sampling up to 15 hard negatives per instance using mE5-large-instruct.</p>\n\n",
                "matched_terms": [
                    "multiple",
                    "training",
                    "configurations",
                    "negatives"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite being listed under small models (<math alttext=\"&lt;1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">&lt;1</annotation></semantics></math>B), the E5 family matches or surpasses the proprietary model on average <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> score. <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> attains the best macro average at 62.4, edging out <span class=\"ltx_text ltx_font_italic\">Gemini embedding</span> (60.6) and <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> (61.3) in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S2.T2\" title=\"Table 2 &#8227; 2.3 AfriMTEB-Lite construction &#8227; 2 AfriMTEB Benchmark &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>. This indicates that strong multilingual coverage and targeted adaptation can outweigh model size or access to proprietary training data.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "training",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> is adapted using supervision centered on nine African languages (<span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span>), it achieves the highest Full-suite average (62.4) across 59 languages. Relative to <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span>, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> shows consistent family-level gains on pair classification (+4.1), reranking (+2.1), retrieval (+1.3), clustering (+1.0), and multi-label classification (+1.2), with small trade-offs on bitext mining (&#8211;0.3), single-label classification (&#8211;0.1), and STS (&#8211;1.1). This pattern indicates that targeted cross-lingual distillation can yield transferable gains beyond the training languages, particularly for retrieval-style tasks. </p>\n\n",
                "matched_terms": [
                    "distillation",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the base model <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> records a reranking score of 61.9, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> raises this to 64.0. It also surpasses gemini embedding 001 (63.4) on the same task (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S2.T2\" title=\"Table 2 &#8227; 2.3 AfriMTEB-Lite construction &#8227; 2 AfriMTEB Benchmark &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). This improvement can be attributed to the training recipe, where <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> leverages knowledge distillation from the <span class=\"ltx_text ltx_font_smallcaps\">BGE-m3</span> cross-encoder in addition to contrastive supervision. The result suggests that incorporating reranker-derived soft labels enhances cross-lingual alignment for ranking-style objectives.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "distillation",
                    "embedding",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the Lite suite of nine African languages and twelve tasks, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> attains the best overall score with an average of 63.7, compared to 62.0 for <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> and the same performance of 63.1 as <span class=\"ltx_text ltx_font_italic\">Gemini embedding</span> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S3.T3\" title=\"Table 3 &#8227; 3 Adapting Embedding Models to African Languages &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Although the performance advantage is not uniform across all task families, AfriE5 consistently remains competitive and often surpasses both baselines on challenging benchmarks such as AfriXNLI and the SIB200 variants. These results highlight that adapting mE5 with cross-lingual contrastive distillation improves its effectiveness for African languages, even when trained on a compact set of nine languages.</p>\n\n",
                "matched_terms": [
                    "distillation",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Relative to <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span>, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> shows clear improvements on several classification benchmarks. It achieves higher scores on AfriSentiment (+3.7), AfriXNLI (+4.5), and SIB200 clustering (+1.8). The largest gain appears on SIB200-14 classification (+4.4; 26.2 vs. 21.8), a setting that requires grouping semantically diverse texts into broad topical categories. These improvements indicate that the cross-lingual, NLI-style contrastive learning used in AfriE5 strengthens semantic alignment across languages, leading to more effective cross-lingual transfer for classification and clustering tasks.\n</p>\n\n",
                "matched_terms": [
                    "used",
                    "learning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct controlled experiments where all training settings are held fixed (base architecture, loss, knowledge distillation from BGE reranker, batch/group sizes, number of steps, and sampling) while varying a single factor at a time. We report AfriMTEB-Lite results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S4.T4\" title=\"Table 4 &#8227; Our adaptation boosts classification tasks, especially SIB200-14 &#8227; 4.2 AfriMTEB-Lite Results &#8227; 4 Results &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "distillation",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With expansion enabled (rows 2&#8211;4), the best overall score is achieved after filtering using a COMET variant, SSA-COMET-MTL QE=0.75 (63.2), compared to 62.5 at 0.67 and a sharp drop to 58.3 at 0.80. A low threshold (0.67) retains large noisy translations (around 433K training samples), which slightly hurts reasoning-heavy tasks. In contrast, a high threshold (0.80) filters out too much data (remaining only 7500 samples), reducing linguistic and semantic diversity and leading to weaker transfer, especially in classification. The middle ground (0.75) balances translation quality with coverage with around 60K samples, yielding the most reliable improvements across tasks. We provide the number of samples retained by SSA-COMET-MTL in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.SS3\" title=\"A.3 Detailed Statistics of Machine Translation Quality &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a></p>\n\n",
                "matched_terms": [
                    "enabled",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Massive Text Embedding Benchmark (MTEB) introduced a common taxonomy and leaderboard for sentence embeddings <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib26\" title=\"\">2023</a>)</cite>. Since then, several language- or region-specific variants have emerged, including MMTEB (with Europe and Indic tracks), SEA-BED for Southeast Asia, PL-MTEB for Polish, and MTEB-French <cite class=\"ltx_cite ltx_citemacro_citep\">(Ponwitayarat et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib36\" title=\"\">2025</a>; Po&#347;wiata et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib37\" title=\"\">2024</a>; Ciancone et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib9\" title=\"\">2024</a>)</cite>. Additional efforts target specific language families or regions: C-MTEB (Chinese), German-focused suites, JMTEB (Japanese), Korean-focused suites, ruMTEB (Russian), FaMTEB (Persian/Farsi), and VN-MTEB (Vietnamese) <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib48\" title=\"\">2024</a>; Wehrli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib46\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib20\" title=\"\">2024</a>; Snegirev et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib40\" title=\"\">2025</a>; Zinvandi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib51\" title=\"\">2025</a>; Pham et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib35\" title=\"\">2025</a>)</cite>. Our work follows this trend by building an Africa-focused extension with broad task coverage.</p>\n\n",
                "matched_terms": [
                    "sentence",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Commercial/API models widely used in practice include OpenAI&#8217;s text-embedding-3 series, Google&#8217;s gemini-embedding-001, and Cohere&#8217;s Embed v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Neelakantan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib31\" title=\"\">2022</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib18\" title=\"\">2025</a>)</cite>. Open-weight models are an active research area: me5, e5, and e5-mistral-7b-instruct provide strong general-purpose and instruction-tuned baselines <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">2024c</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib42\" title=\"\">a</a>)</cite>; Qwen3-Embedding and Embedding Gemma offer lightweight multilingual options <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib50\" title=\"\">2025</a>; Vera et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib41\" title=\"\">2025</a>)</cite>; GTE proposes efficient, general-purpose embeddings <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib21\" title=\"\">2023a</a>)</cite>; and classic multilingual encoders LaBSE remain strong references <cite class=\"ltx_cite ltx_citemacro_citep\">(Feng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib13\" title=\"\">2022</a>)</cite>. The recent BGE-M3 model integrates multilingual, multi-function training and is a competitive open baseline <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite>. Despite progress, coverage and performance on many African languages remain uneven, motivating region-specific evaluation and targeted adaptation.</p>\n\n",
                "matched_terms": [
                    "used",
                    "embedding",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented AfriMTEB, a large-scale benchmark for African languages spanning 59 languages and 14 tasks, and AfriE5, an adaptation of mE5-large-instruct via cross-lingual contrastive distillation. AfriE5 achieves state-of-the-art results on both AfriMTEB-Full and Lite, surpassing strong baselines such as Gemini embedding-001. Our ablations show that cross-lingual dataset expansion and balanced translation filtering are crucial for these gains. AfriMTEB and AfriE5 provide standardized evaluation and a strong baseline to advance text embedding research for African languages.</p>\n\n",
                "matched_terms": [
                    "distillation",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, benchmark contamination remains possible because public corpora are widely used in pretraining, which can inflate absolute scores. Our single-GPU fine-tuning recipe favors accessibility over peak performance and may limit stability, and releasing full translation/provenance artefacts is nontrivial due to size and licensing. While AfriE5 trained on nine languages generalizes well to 59 in our tests, transfer may degrade for typologically distant or extremely low-resource languages; broader community datasets and multi-teacher/multi-signal training are promising avenues to mitigate these limitations.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "used",
                    "training",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given two sentence sets from different languages, the task is to identify translation pairs. Embeddings are used to compute similarities and find the best match for each sentence.</p>\n\n",
                "matched_terms": [
                    "sentence",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task measures the degree of semantic similarity between sentence pairs, either within or across languages, based on embedding similarity.</p>\n\n",
                "matched_terms": [
                    "sentence",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">mmBERT-base is a massively multilingual ModernBERT encoder trained on <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>T,+ tokens covering <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>1,800 languages, extending the ModernBERT architecture (fast encoder with long context) to the multilingual regime <cite class=\"ltx_cite ltx_citemacro_citep\">(Warner et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib45\" title=\"\">2025</a>)</cite>. The training recipe introduces curriculum-style annealed language learning, inverse masking, and temperature-based sampling to emphasize low-resource languages while retaining strong performance on high-resource ones <cite class=\"ltx_cite ltx_citemacro_citep\">(Marone et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib23\" title=\"\">2025</a>)</cite>. Reported results show that <span class=\"ltx_text ltx_font_typewriter\">mmBERT-base</span> surpasses prior multilingual encoders such as XLM-R on standard NLU and retrieval benchmarks, approaching the English-only ModernBERT on GLUE despite being trained predominantly on non-English data <cite class=\"ltx_cite ltx_citemacro_citep\">(Marone et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib23\" title=\"\">2025</a>)</cite>. Inference follows standard encoder usage (mean pooling over last hidden states and L2 normalization).</p>\n\n",
                "matched_terms": [
                    "training",
                    "learning",
                    "normalization",
                    "mean",
                    "pooling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">KaLM-Embedding is a multilingual embedding family that prioritizes training-data quality over sheer scale, combining (i) persona-based synthetic examples distilled from LLMs, (ii) ranking-consistency filtering, and (iii) semi-homogeneous task batching for efficient contrastive learning <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib16\" title=\"\">2025</a>)</cite>. Many public checkpoints are built on compact Qwen2 backbones (e.g., <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.5B) and instruction-tuned variants for downstream retrieval and semantic similarity. Technical reports describe v1.5/v2 updates with improved data curation and training strategy, yielding strong MTEB performance for their size <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib16\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "embedding",
                    "training",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Embedding is a purpose-built series of dense encoders for text embeddings and reranking, offered in 0.6B/4B/8B sizes with a 32k token context window and coverage of 100+ languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib50\" title=\"\">2025</a>)</cite>. The models are instruction-aware and support flexible output dimensionalities (via MRL-style prefix truncation), with typical maximum embedding sizes of 1,024 (0.6B), 2,560 (4B), and 4,096 (8B); matching reranker models are available at each size. Training leverages Qwen3 LLMs both as backbones and as data synthesizers across domains and languages, improving robustness for retrieval/reranking workloads. Inference uses mean pooling and L2 normalization; common deployment stacks expose a user-selectable output dimensionality for storage/latency trade-offs.</p>\n\n",
                "matched_terms": [
                    "training",
                    "size",
                    "normalization",
                    "mean",
                    "embedding",
                    "pooling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BGE-M3 (BAAI) is a versatile embedding model unifying three capabilities in a single encoder: Multi-Functionality (dense, multi-vector, and sparse retrieval), Multi-Linguality (100+ languages), and Multi-Granularity (robust from short queries to long documents, up to <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>8,192 tokens) <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite>. The training pipeline uses self-knowledge distillation across retrieval functions to align representations and enables hybrid retrieval without switching models <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite>. It is widely used as a strong multilingual baseline for retrieval, clustering, and classification.</p>\n\n",
                "matched_terms": [
                    "distillation",
                    "used",
                    "embedding",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">mE5-large and mE5-large-instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">2024c</a>)</cite> are multilingual members of the E5 family built on XLM-RoBERTa-large <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib10\" title=\"\">2020</a>)</cite>, trained with a two-stage recipe that first performs weakly supervised contrastive pre-training on roughly one billion multilingual text pairs and then supervised fine-tuning on curated embedding tasks; the instruction variant further formats supervision with concise task instructions to specialize representations for retrieval and related tasks. Both use a 24-layer encoder that produces 1,024-dimensional vectors and inherit broad (&#160;100 language) coverage from the XLM-RoBERTa backbone. At inference time they follow the E5 prompt conventions (e.g., query / passage style inputs), apply mean pooling over the last hidden states, and L2-normalize the output, yielding strong performance on retrieval, semantic similarity, clustering, and classification benchmarks in both monolingual and cross-lingual settings.</p>\n\n",
                "matched_terms": [
                    "mean",
                    "passage",
                    "query",
                    "finetuning",
                    "embedding",
                    "pooling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">gte-Qwen2-7B-instruct (Alibaba-NLP) is a 7B-parameter instruction-tuned General Text Embedding model built on Qwen2-7B, targeting high-quality multilingual embeddings with a 32k context window <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib22\" title=\"\">2023b</a>)</cite>. At release, it reported leading scores on MTEB English/Chinese subsets, reflecting a training mixture that combines instruction-style contrastive objectives and curated negatives. It is used as a strong large-model baseline for retrieval, reranking, and semantic similarity.</p>\n\n",
                "matched_terms": [
                    "used",
                    "embedding",
                    "training",
                    "negatives"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">Linq-Embed-Mistral</span> is a Mistral-7B&#8211;based embedding model developed with task-tailored data crafting, filtering, and hard-negative mining to improve retrieval quality <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib17\" title=\"\">2024</a>)</cite>. The technical report and model card document leading MTEB retrieval scores at release (e.g., average <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px8.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>68.2 on 56 datasets; retrieval score <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px8.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>60.2), achieved through homogeneous task ordering and mixed-task fine-tuning strategies <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib17\" title=\"\">2024</a>)</cite>. It is frequently used as a competitive 7B encoder baseline for dense retrieval.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "used",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SFR-Embedding-Mistral (Salesforce Research) applies transfer learning on top of E5-mistral-7b-instruct and Mistral-7B-v0.1, with additional multi-task training and optimized negative sampling aimed at retrieval tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib24\" title=\"\">2024</a>)</cite>. Public materials position it as a top-performing 7B embedding model for search, clustering, and classification workloads <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib24\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "embedding",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">E5-Mistral-7B-instruct initializes the E5 instruction-tuning recipe from Mistral-7B-v0.1, producing a 7B encoder specialized for instruction-aware embeddings <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib43\" title=\"\">2024b</a>)</cite>. As with other E5 variants, inference benefits from task instructions plus <span class=\"ltx_text ltx_font_typewriter\">query:</span>/<span class=\"ltx_text ltx_font_typewriter\">passage:</span> prefixes; mean pooling with L2 normalization is used for final vectors <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib43\" title=\"\">2024b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib42\" title=\"\">a</a>)</cite>. It serves both as a competitive baseline and as a foundation for further transfer learning (e.g., SFR-Embedding-Mistral).</p>\n\n",
                "matched_terms": [
                    "learning",
                    "normalization",
                    "mean",
                    "used",
                    "pooling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.T6\" title=\"Table 6 &#8227; A.3 Detailed Statistics of Machine Translation Quality &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we observe a clear trade-off between dataset size and quality. At a relaxed threshold of 0.67, over 430k sentence pairs are retained, ensuring broad coverage across all nine target languages. Increasing the threshold to 0.75 reduces the pool to around 60k examples, striking a balance between filtering noise and preserving sufficient training data. At the strictest cutoff of 0.80, only 7.5k pairs remain, indicating that high-quality translations are relatively scarce. Language-level differences are also evident: Swahili, Oromo, and Kinyarwanda consistently contribute the largest number of pairs, while Igbo and Amharic see the sharpest reductions under stricter filtering, reflecting variation in MT quality across languages.</p>\n\n",
                "matched_terms": [
                    "sentence",
                    "training",
                    "size"
                ]
            }
        ]
    },
    "A1.T6": {
        "source_file": "AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages",
        "caption": "Table 6: Number of translation pairs retained after filtering with SSA-COMET at three thresholds. Lower thresholds yield more data, while stricter thresholds retain fewer but higher-quality examples.",
        "body": "Language\n0.67\n0.75\n0.80\n\n\n\n\nAmharic (amh_Ethi)\n44,166\n4,617\n281\n\n\nOromo (gaz_Latn)\n73,846\n14,598\n2,818\n\n\nHausa (hau_Latn)\n31,799\n5,851\n1,056\n\n\nIgbo (ibo_Latn)\n16,778\n1,279\n136\n\n\nKinyarwanda (kin_Latn)\n81,003\n4,867\n337\n\n\nSwahili (swh_Latn)\n116,338\n21,996\n1,849\n\n\nXhosa (xho_Latn)\n18,143\n2,007\n351\n\n\nYoruba (yor_Latn)\n31,377\n3,316\n411\n\n\nZulu (zul_Latn)\n20,229\n1,535\n224\n\n\nTotal\n433,629\n60,066\n7,463",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Language</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">0.67</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">0.75</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">0.80</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Amharic (amh_Ethi)</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">44,166</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">4,617</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">281</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Oromo (gaz_Latn)</td>\n<td class=\"ltx_td ltx_align_right\">73,846</td>\n<td class=\"ltx_td ltx_align_right\">14,598</td>\n<td class=\"ltx_td ltx_align_right\">2,818</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Hausa (hau_Latn)</td>\n<td class=\"ltx_td ltx_align_right\">31,799</td>\n<td class=\"ltx_td ltx_align_right\">5,851</td>\n<td class=\"ltx_td ltx_align_right\">1,056</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Igbo (ibo_Latn)</td>\n<td class=\"ltx_td ltx_align_right\">16,778</td>\n<td class=\"ltx_td ltx_align_right\">1,279</td>\n<td class=\"ltx_td ltx_align_right\">136</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Kinyarwanda (kin_Latn)</td>\n<td class=\"ltx_td ltx_align_right\">81,003</td>\n<td class=\"ltx_td ltx_align_right\">4,867</td>\n<td class=\"ltx_td ltx_align_right\">337</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Swahili (swh_Latn)</td>\n<td class=\"ltx_td ltx_align_right\">116,338</td>\n<td class=\"ltx_td ltx_align_right\">21,996</td>\n<td class=\"ltx_td ltx_align_right\">1,849</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Xhosa (xho_Latn)</td>\n<td class=\"ltx_td ltx_align_right\">18,143</td>\n<td class=\"ltx_td ltx_align_right\">2,007</td>\n<td class=\"ltx_td ltx_align_right\">351</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Yoruba (yor_Latn)</td>\n<td class=\"ltx_td ltx_align_right\">31,377</td>\n<td class=\"ltx_td ltx_align_right\">3,316</td>\n<td class=\"ltx_td ltx_align_right\">411</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Zulu (zul_Latn)</td>\n<td class=\"ltx_td ltx_align_right\">20,229</td>\n<td class=\"ltx_td ltx_align_right\">1,535</td>\n<td class=\"ltx_td ltx_align_right\">224</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Total</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">433,629</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">60,066</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">7,463</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "filtering",
            "xhosa",
            "igbo",
            "swhlatn",
            "lower",
            "more",
            "zulu",
            "three",
            "swahili",
            "yield",
            "translation",
            "amhethi",
            "ibolatn",
            "language",
            "higherquality",
            "haulatn",
            "examples",
            "fewer",
            "zullatn",
            "ssacomet",
            "pairs",
            "gazlatn",
            "kinlatn",
            "xholatn",
            "number",
            "retain",
            "amharic",
            "stricter",
            "yoruba",
            "retained",
            "thresholds",
            "total",
            "while",
            "hausa",
            "yorlatn",
            "data",
            "after",
            "oromo",
            "kinyarwanda"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.T6\" title=\"Table 6 &#8227; A.3 Detailed Statistics of Machine Translation Quality &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we observe a clear trade-off between dataset size and quality. At a relaxed threshold of 0.67, over 430k sentence pairs are retained, ensuring broad coverage across all nine target languages. Increasing the threshold to 0.75 reduces the pool to around 60k examples, striking a balance between filtering noise and preserving sufficient training data. At the strictest cutoff of 0.80, only 7.5k pairs remain, indicating that high-quality translations are relatively scarce. Language-level differences are also evident: Swahili, Oromo, and Kinyarwanda consistently contribute the largest number of pairs, while Igbo and Amharic see the sharpest reductions under stricter filtering, reflecting variation in MT quality across languages.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Text embeddings are an essential building component of several NLP tasks such as retrieval-augmented generation which is crucial for preventing hallucinations in LLMs. Despite the recent release of massively multilingual MTEB (MMTEB), African languages remain underrepresented, with existing tasks often repurposed from translation benchmarks such as FLORES clustering or SIB-200.\nIn this paper, we introduce AfriMTEB&#8212;a regional expansion of MMTEB covering 59 languages, 14 tasks, and 38 datasets, including six newly added datasets. Unlike many MMTEB datasets that include fewer than five languages, the new additions span 14 to 56 African languages and introduce entirely new tasks, such as hate speech detection, intent detection, and emotion classification, which were not previously covered. Complementing this, we present AfriE5, an adaptation of the instruction-tuned mE5 model to African languages through cross-lingual contrastive distillation. Our evaluation shows that AfriE5 achieves state-of-the-art performance, outperforming strong baselines such as Gemini-Embeddings and mE5.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "fewer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While recent text embedding benchmarks have improved language coverage in recent years, such as MMTEB&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Enevoldsen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib11\" title=\"\">2025</a>)</cite>, African languages remain under-represented, where many of the tasks covered are either based on massively evaluation of translation datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB-Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib33\" title=\"\">2022</a>; Federmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib12\" title=\"\">2022</a>)</cite>, and the tasks derived or repurposed from translation benchmarks such as Belebele&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bandarkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib5\" title=\"\">2024</a>)</cite> and SIB-200&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib1\" title=\"\">2024</a>)</cite>. As a result, the quality of text embeddings for languages in the African region remains unknown, as the region has few standardized tools for comparing models across tasks and languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alabi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib4\" title=\"\">2025</a>)</cite></p>\n\n",
                "matched_terms": [
                    "translation",
                    "language",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">AfriMTEB</span>, a regional extension of MTEB tailored to African languages and tasks. <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;2</span></a> shows the full suite that covers 59 languages and 38 datasets spanning Bitext mining, Classification (single, pair and multi-label), Semantic Text Similarity, Retrieval, Clustering, and Reranking. To support fast-and-compute friendly evaluation, we also provide <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">AfriMTEB-Lite</span>, a compact suite over nine geographically-diverse languages (Amharic, Oromo, Igbo, Yoruba, Hausa, Swahili, Kinyarwanda, Xhosa, Zulu) constructed by selecting, within each task family, the maximally overlapping tasks such that every task includes all nine languages, ensuring uniform language coverage across the suite.</p>\n\n",
                "matched_terms": [
                    "yoruba",
                    "language",
                    "swahili",
                    "xhosa",
                    "igbo",
                    "zulu",
                    "hausa",
                    "amharic",
                    "oromo",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to benchmarking, we develop <span class=\"ltx_text ltx_font_bold ltx_font_italic\">AfriE5-Large-Instruct</span>\nadapting strong embedding models to African languages.\nStarting from the instruction-tuned <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib42\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">c</a>)</cite>, we leveraged cross-lingual contrastive distillation. We construct supervision by translating MNLI/SNLI datasets<cite class=\"ltx_cite ltx_citemacro_citep\">(Williams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib47\" title=\"\">2018</a>; Bowman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib7\" title=\"\">2015</a>)</cite> in African languages with NLLB-200 (3.3B) <cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB-Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib33\" title=\"\">2022</a>)</cite>, followed by automatic filtering by SSA-COMET (an African COMET-based QE metric)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib19\" title=\"\">2025</a>)</cite>, and expanding each example into multiple sources or target directions to encourage cross-lingual alignment. We then use the receipe of BGE Re-ranker v2 m3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite> for text embedding model training.</p>\n\n",
                "matched_terms": [
                    "filtering",
                    "ssacomet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate widely used baselines, including BGE-M3, E5 variants (e.g., e5-mistral-7b-instruct), Qwen embeddings, Gemini embedding-001, and Embedding Gemma <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">2024c</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib50\" title=\"\">2025</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib18\" title=\"\">2025</a>)</cite>. On <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB-Lite</span>, our adapted model <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span>, trained only on nine African languages, achieves an average score of 63.7, surpassing me5-large-instruct (62.0) and Gemini embedding-001 (63.1). Remarkably, despite being tuned with data and languages aligned to the <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB-Lite</span>, our model leveraged cross-lingual transfer to generalize to the broader <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> benchmark covering 59 languages and 38 datasets, where it also delivers the best performance with an average score of 62.4, ahead of me5-large-instruct (61.3) and Gemini embedding-001 (60.6) as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>. These results highlight that targeted cross-lingual adaptation on a carefully selected subset of languages can transfer effectively to a much larger set, yielding consistent improvements across task families while preserving the backbone&#8217;s general utility.</p>\n\n",
                "matched_terms": [
                    "data",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">AfriMTEB</span> is a regional extension of MTEB designed to evaluate text embeddings for African languages across multiple task families. To address persistent gaps in language coverage and enable systematic, comparable assessment, we introduce <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span>as a standardized benchmark for African languages. It consists of a <span class=\"ltx_text ltx_font_bold\">Full suite</span> covering <span class=\"ltx_text ltx_font_italic\">59 languages</span> and <span class=\"ltx_text ltx_font_italic\">38 datasets</span>, and a <span class=\"ltx_text ltx_font_bold\">Lite suite</span> that maintains uniform language coverage for <span class=\"ltx_text ltx_font_italic\">9 target languages</span> over <span class=\"ltx_text ltx_font_italic\">13 representative datasets</span>. Languages are selected to span major families and geographic regions while prioritizing practical relevance and the availability of reliable public datasets; tasks reflect core embedding applications and draw on established, well-curated resources. The Lite suite supports rapid iteration by enforcing consistent task coverage across languages through maximal task&#8211;language overlap.</p>\n\n",
                "matched_terms": [
                    "language",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> follows the <span class=\"ltx_text ltx_font_typewriter\">MTEB</span> taxonomy and groups tasks into eight families. <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S2.T1\" title=\"Table 1 &#8227; 2 AfriMTEB Benchmark &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> lists the tasks, task families and the datasets included in the <em class=\"ltx_emph ltx_font_italic\">Full</em> suite. Each dataset belongs to one of the eight task families: (1) Bibtext mining (2) Pair classification (3) Semantic Text Similarity (STS) (4) Clustering (5) Classification (6) Multi-label Classification (7) Retrieval (8). The first four task families performs a similarity related tasks based on two pairs of sentences, either to identity translation pairs, categorize them into a class (e.g., entailment vs. contradiction) or provide a similarity score. The next three tasks (5) - (6) classify a sentence or article into one or more classes. Finally, the last sets of tasks (7) and (8) are information retrieval tasks for retrieval or re-ranking based on a user query. We provide full description in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.SS1\" title=\"A.1 Task categories &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "pairs",
                    "three",
                    "more"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, language coverage in these datasets is uneven: some tasks include only a few African languages, while others span broader multilingual settings. To ensure fairness, we compute macro averages over languages within each task, then average across tasks and families to obtain the overall <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> score. This prevents any single task or language from dominating the benchmark.</p>\n\n",
                "matched_terms": [
                    "language",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An African extension of the XNLI benchmark that provides natural language inference data to 15 African languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib3\" title=\"\">2025</a>)</cite>. By including AfriXNLI, we expand language coverage for the pair classification family beyond a single African language (Swahili), ensuring broader representation of African languages in entailment-style tasks.</p>\n\n",
                "matched_terms": [
                    "language",
                    "data",
                    "swahili"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A more challenging variant of the SIB-200 dataset where labels are consolidated into 14 categories. This version still includes 56 African languages but is not limited to them&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib1\" title=\"\">2024</a>)</cite>. By merging fine-grained topics into broader classes, intra-class diversity increases, which raises task difficulty. The inclusion of this dataset not only strengthens African language evaluation but also increases the difficulty for other covered languages in SIB-200, leading to more robust cross-lingual assessment.</p>\n\n",
                "matched_terms": [
                    "language",
                    "more"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <em class=\"ltx_emph ltx_font_italic\">Lite</em> suite focuses on nine geographically-diverse African languages:\n<span class=\"ltx_text ltx_font_italic\">Amharic, Oromo, Igbo, Yoruba, Hausa, Swahili, Kinyarwanda, Xhosa, Zulu</span>.\nWe selected only those datasets from MTEB that include nine languages to guarantee consistent language&#8211;task alignment across families. The result is a compact yet representative benchmark of 13 datasets&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We merged two datasets: MasakhaNEWS and KinNEWS since they have similar structure, thus we report 12 datasets</span></span></span> spanning classification, retrieval, bitext mining, clustering, pair classification, and multi-label classification.</p>\n\n",
                "matched_terms": [
                    "yoruba",
                    "swahili",
                    "xhosa",
                    "igbo",
                    "zulu",
                    "hausa",
                    "amharic",
                    "oromo",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"s_{i,j}=\\mathbf{q}i^{\\top}\\mathbf{p}_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mi>&#119850;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>i</mi><mo>&#8868;</mo></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#119849;</mi><mi>j</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">s_{i,j}=\\mathbf{q}i^{\\top}\\mathbf{p}_{j}</annotation></semantics></math> is the similarity between query embedding <math alttext=\"\\mathbf{q}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#119850;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{q}_{i}</annotation></semantics></math> and passage embedding <math alttext=\"\\mathbf{p}_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#119849;</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{p}_{j}</annotation></semantics></math>, and <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m4\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> is a temperature hyperparameter. The numerator contains the similarity between query <math alttext=\"\\mathbf{q}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m5\" intent=\":literal\"><semantics><msub><mi>&#119850;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{q}_{i}</annotation></semantics></math> and its corresponding positive passage <math alttext=\"\\mathbf{p}{i\\cdot G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>&#119849;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><mi>G</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{p}{i\\cdot G}</annotation></semantics></math>. The denominator includes all similarities between <math alttext=\"\\mathbf{q}i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m7\" intent=\":literal\"><semantics><mrow><mi>&#119850;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{q}i</annotation></semantics></math> and all passages in the batch, comprising both pre-mined hard negatives (from NLI contradiction examples and hard negative mining) and in-batch negatives (other queries and passages from the same training batch).\nWith cross-device negative sharing enabled, the total negative set spans all examples across all GPUs in the distributed training setup, providing a rich set of challenging negative examples for each query.</p>\n\n",
                "matched_terms": [
                    "total",
                    "examples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We constructed cross-lingual training data by leveraging large-scale natural language inference corpora, a supervision signal that has proven effective for learning sentence embeddings. <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib15\" title=\"\">2022</a>; Reimers and Gurevych, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib39\" title=\"\">2019</a>)</cite> Specifically, we used MultiNLI and SNLI <cite class=\"ltx_cite ltx_citemacro_citep\">(Williams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib47\" title=\"\">2018</a>; Bowman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib7\" title=\"\">2015</a>)</cite> as source datasets in English. Each sentence pair was translated into the nine African target languages using NLLB-200 (3.3B) <cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB-Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib33\" title=\"\">2022</a>)</cite>. We then estimated translation quality by SSA-COMET-MTL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib19\" title=\"\">2025</a>)</cite>, a COMET variant&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Rei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib38\" title=\"\">2020</a>)</cite>. and filtered pairs below a threshold of 0.75 to ensure the data quality.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "language",
                    "data",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To encourage <span class=\"ltx_text ltx_font_bold\">cross-lingual alignment</span>, each example was expanded into multiple configurations: (i) premise in target language and hypothesis in source, (ii) premise in source and hypothesis in target, (iii) both in target, and (iv) both in source, that is,is, English. For MultiNLI, we reformulated examples as query&#8211;positive/negative pairs (entailment as <span class=\"ltx_text ltx_font_italic\">pos</span>, contradiction as <span class=\"ltx_text ltx_font_italic\">neg</span>). For SNLI, we followed the same strategy but included all 3-way annotations. We further increased training difficulty by sampling up to 15 hard negatives per instance using mE5-large-instruct.</p>\n\n",
                "matched_terms": [
                    "language",
                    "pairs",
                    "examples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large 7B and 8B encoders do not translate into higher <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> scores. All 7B/8B models cluster in the low&#8211;mid 50s, for example, <span class=\"ltx_text ltx_font_italic\">gte-Qwen2-7B-instruct</span> at 50.9, <span class=\"ltx_text ltx_font_italic\">GritLM-7B</span> at 51.8, <span class=\"ltx_text ltx_font_italic\">Qwen3-Embedding-8B</span> at 50.5. Thay are clearly below the smaller E5 variants of around 61.3 point. This gap underscores that broad, balanced language coverage and task diversity matter more than parameter count alone.</p>\n\n",
                "matched_terms": [
                    "language",
                    "more"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct controlled experiments where all training settings are held fixed (base architecture, loss, knowledge distillation from BGE reranker, batch/group sizes, number of steps, and sampling) while varying a single factor at a time. We report AfriMTEB-Lite results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S4.T4\" title=\"Table 4 &#8227; Our adaptation boosts classification tasks, especially SIB200-14 &#8227; 4.2 AfriMTEB-Lite Results &#8227; 4 Results &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "number",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As described in Section &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S3.SS2\" title=\"3.2 Cross-lingual Training Data Construction &#8227; 3 Adapting Embedding Models to African Languages &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, comparing row 1 (<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>, 0.75) and row 3 (<math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math>, 0.75), the average increases from 62.3 to 63.2. The cross-lingual expansion of the data set exposes the model to richer cross-lingual contrasts: First, higher scores on bitext mining tasks show that the expansion improves the model&#8217;s ability to align semantically equivalent sentences between languages, producing a more coherent multilingual embedding space. Second, classification tasks such as SIB200-14 benefit from this stronger alignment, as the model learns to abstract over diverse topical domains without relying on language-specific cues.</p>\n\n",
                "matched_terms": [
                    "data",
                    "more"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With expansion enabled (rows 2&#8211;4), the best overall score is achieved after filtering using a COMET variant, SSA-COMET-MTL QE=0.75 (63.2), compared to 62.5 at 0.67 and a sharp drop to 58.3 at 0.80. A low threshold (0.67) retains large noisy translations (around 433K training samples), which slightly hurts reasoning-heavy tasks. In contrast, a high threshold (0.80) filters out too much data (remaining only 7500 samples), reducing linguistic and semantic diversity and leading to weaker transfer, especially in classification. The middle ground (0.75) balances translation quality with coverage with around 60K samples, yielding the most reliable improvements across tasks. We provide the number of samples retained by SSA-COMET-MTL in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.SS3\" title=\"A.3 Detailed Statistics of Machine Translation Quality &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a></p>\n\n",
                "matched_terms": [
                    "filtering",
                    "retained",
                    "after",
                    "translation",
                    "data",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented AfriMTEB, a large-scale benchmark for African languages spanning 59 languages and 14 tasks, and AfriE5, an adaptation of mE5-large-instruct via cross-lingual contrastive distillation. AfriE5 achieves state-of-the-art results on both AfriMTEB-Full and Lite, surpassing strong baselines such as Gemini embedding-001. Our ablations show that cross-lingual dataset expansion and balanced translation filtering are crucial for these gains. AfriMTEB and AfriE5 provide standardized evaluation and a strong baseline to advance text embedding research for African languages.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "filtering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AfriMTEB expands coverage to 59 languages, yet many African languages, dialects, orthographies, and code-switched registers remain underrepresented. Several datasets inherit noise or heterogeneity from crowd labels, repurposed tasks, and preprocessing; our adaptation data further relies on machine translation (NLLB-200) and automatic quality estimation (SSA-COMET), which vary in reliability across language pairs and domains. Distillation from a single teacher (BGE Reranker v2 m3) can also transfer its biases, potentially advantaging languages and styles that align with the MT/QE pipeline and the teacher&#8217;s preferences.</p>\n\n",
                "matched_terms": [
                    "language",
                    "translation",
                    "ssacomet",
                    "pairs",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation focuses on text-only sentence/paragraph embeddings and macro-averages across tasks and languages; alternative weightings (e.g., by population or application criticality) and additional metrics (calibration, robustness, fairness) could yield different conclusions. End-to-end RAG quality, multimodal retrieval, and very long-context document embeddings are out of scope, and domain coverage skews toward formal/news text over colloquial or specialized domains. True parity with closed-weight baselines (e.g., data mixtures, architectures, inference settings) is infeasible; repeated API evaluations may be affected by nondeterminism, and some open models may be sensitive to prompt formats or poolers we did not exhaustively tune.</p>\n\n",
                "matched_terms": [
                    "data",
                    "yield"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given two sentence sets from different languages, the task is to identify translation pairs. Embeddings are used to compute similarities and find the best match for each sentence.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">mmBERT-base is a massively multilingual ModernBERT encoder trained on <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>T,+ tokens covering <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>1,800 languages, extending the ModernBERT architecture (fast encoder with long context) to the multilingual regime <cite class=\"ltx_cite ltx_citemacro_citep\">(Warner et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib45\" title=\"\">2025</a>)</cite>. The training recipe introduces curriculum-style annealed language learning, inverse masking, and temperature-based sampling to emphasize low-resource languages while retaining strong performance on high-resource ones <cite class=\"ltx_cite ltx_citemacro_citep\">(Marone et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib23\" title=\"\">2025</a>)</cite>. Reported results show that <span class=\"ltx_text ltx_font_typewriter\">mmBERT-base</span> surpasses prior multilingual encoders such as XLM-R on standard NLU and retrieval benchmarks, approaching the English-only ModernBERT on GLUE despite being trained predominantly on non-English data <cite class=\"ltx_cite ltx_citemacro_citep\">(Marone et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib23\" title=\"\">2025</a>)</cite>. Inference follows standard encoder usage (mean pooling over last hidden states and L2 normalization).</p>\n\n",
                "matched_terms": [
                    "language",
                    "while",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">KaLM-Embedding is a multilingual embedding family that prioritizes training-data quality over sheer scale, combining (i) persona-based synthetic examples distilled from LLMs, (ii) ranking-consistency filtering, and (iii) semi-homogeneous task batching for efficient contrastive learning <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib16\" title=\"\">2025</a>)</cite>. Many public checkpoints are built on compact Qwen2 backbones (e.g., <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.5B) and instruction-tuned variants for downstream retrieval and semantic similarity. Technical reports describe v1.5/v2 updates with improved data curation and training strategy, yielding strong MTEB performance for their size <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib16\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "filtering",
                    "data",
                    "examples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">mE5-large and mE5-large-instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">2024c</a>)</cite> are multilingual members of the E5 family built on XLM-RoBERTa-large <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib10\" title=\"\">2020</a>)</cite>, trained with a two-stage recipe that first performs weakly supervised contrastive pre-training on roughly one billion multilingual text pairs and then supervised fine-tuning on curated embedding tasks; the instruction variant further formats supervision with concise task instructions to specialize representations for retrieval and related tasks. Both use a 24-layer encoder that produces 1,024-dimensional vectors and inherit broad (&#160;100 language) coverage from the XLM-RoBERTa backbone. At inference time they follow the E5 prompt conventions (e.g., query / passage style inputs), apply mean pooling over the last hidden states, and L2-normalize the output, yielding strong performance on retrieval, semantic similarity, clustering, and classification benchmarks in both monolingual and cross-lingual settings.</p>\n\n",
                "matched_terms": [
                    "language",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">Linq-Embed-Mistral</span> is a Mistral-7B&#8211;based embedding model developed with task-tailored data crafting, filtering, and hard-negative mining to improve retrieval quality <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib17\" title=\"\">2024</a>)</cite>. The technical report and model card document leading MTEB retrieval scores at release (e.g., average <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px8.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>68.2 on 56 datasets; retrieval score <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px8.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>60.2), achieved through homogeneous task ordering and mixed-task fine-tuning strategies <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib17\" title=\"\">2024</a>)</cite>. It is frequently used as a competitive 7B encoder baseline for dense retrieval.</p>\n\n",
                "matched_terms": [
                    "filtering",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the quality of the machine translation data used in AfriMTEB, we rely on automatic evaluation with <span class=\"ltx_text ltx_font_bold\">SSA-COMET</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib19\" title=\"\">2025</a>)</cite>. SSA-COMET is a recently released metric trained on <span class=\"ltx_text ltx_font_smallcaps\">SSA-MTE</span>, a large-scale human-annotated evaluation dataset covering 13 African language pairs with over 63,000 sentence-level judgments. Compared to earlier African-focused metrics such as AfriCOMET, SSA-COMET provides stronger correlation with human ratings and better robustness in low-resource settings.</p>\n\n",
                "matched_terms": [
                    "language",
                    "translation",
                    "ssacomet",
                    "pairs",
                    "data"
                ]
            }
        ]
    },
    "A1.T7": {
        "source_file": "AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages",
        "caption": "Table 7: Task-wise and per-language Performance. Comparison of the selected models across all African languages for each task.",
        "body": "Dataset\namh\ngaz\nhau\nibo\nkin\nswa\nxho\nyor\nzul\nAvg.\n\n\n\n\nAfriHateClassification\n\n\n\n\n\n\n\n\n\n\n\n\nbge-m3\n52.52\n52.19\n47.32\n56.03\n51.71\n64.81\n36.85\n50.77\n38.49\n50.08\n\n\ngemini embedding 001\n54.31\n52.31\n52.4\n63.47\n57.75\n74.07\n38.94\n56.3\n45.3\n54.98\n\n\nmE5-large-instruct\n50.78\n51.92\n42.95\n57.16\n54.85\n67.12\n40.18\n51.56\n47.03\n51.51\n\n\nAfriE5-large-instruct\n52.76\n53.79\n44.77\n58.95\n56.47\n64.94\n37.8\n51.67\n43.93\n51.68\n\n\nAfriSentiClassification\n\n\n\n\n\n\n\n\n\n\n\n\nbge-m3\n48.45\n34.8\n68.83\n54.43\n46.12\n44.52\n–\n38.21\n–\n47.91\n\n\ngemini embedding 001\n59.75\n34.35\n75.02\n61.65\n58.58\n45.94\n–\n41.25\n–\n53.79\n\n\nmE5-large-instruct\n44.07\n35.8\n68.64\n49.52\n50.19\n41.95\n–\n38.91\n–\n47.01\n\n\nAfriE5-large-instruct\n51.41\n35.7\n71.78\n53.67\n53.98\n44.13\n–\n44.57\n–\n50.75\n\n\nNewsClassification\n\n\n\n\n\n\n\n\n\n\n\n\nbge-m3\n84.73\n74.62\n78.23\n64.23\n49.4\n73.8\n77.85\n79.05\n–\n72.74\n\n\ngemini embedding 001\n84.41\n82.09\n77.14\n68.9\n58.58\n71.95\n87.31\n84.21\n–\n76.82\n\n\nmE5-large-instruct\n87.82\n79.54\n80.71\n77.59\n57.27\n79.43\n85.29\n83.07\n–\n78.84\n\n\nAfriE5-large-instruct\n89.52\n81.75\n81.52\n78.03\n58.31\n78.15\n85.59\n82.99\n–\n79.48\n\n\nAfriXNLI\n\n\n\n\n\n\n\n\n\n\n\n\nbge-m3\n75.64\n66.39\n69.67\n64.33\n65.27\n73.88\n69.41\n64.03\n69.87\n68.72\n\n\ngemini embedding 001\n81.32\n66.8\n78.57\n78.05\n73.79\n78.34\n75.42\n71.72\n73.38\n75.27\n\n\nmE5-large-instruct\n66.85\n62.96\n63.51\n65.05\n62.01\n68.09\n66.66\n61.89\n63.7\n64.52\n\n\nAfriE5-large-instruct\n72.28\n68.57\n67.89\n70.36\n64.68\n72.1\n72.41\n65.43\n67.52\n69.03\n\n\nEmotionAnalysisPlus\n\n\n\n\n\n\n\n\n\n\n\n\nbge-m3\n20.99\n28.54\n24.92\n23.49\n33.9\n37.14\n21.52\n40.58\n33.86\n29.44\n\n\ngemini embedding 001\n28.38\n30.8\n36.03\n30.71\n39.52\n39.67\n27.76\n45.66\n40.6\n35.46\n\n\nmE5-large-instruct\n22.27\n30.41\n27.03\n27.59\n34.27\n39.89\n24.17\n40.88\n37.05\n31.51\n\n\nAfriE5-large-instruct\n24.07\n30.74\n28.37\n30.18\n36.21\n38.8\n28.13\n42.2\n36.35\n32.78\n\n\nFloresBitextMining\n\n\n\n\n\n\n\n\n\n\n\n\nbge-m3\n83.73\n71.4\n82.71\n72.78\n76.63\n86.13\n81.99\n65.33\n82.24\n78.1\n\n\ngemini embedding 001\n91.3\n77.8\n90.01\n87.9\n90.13\n91.43\n90.37\n83.99\n90.28\n88.13\n\n\nmE5-large-instruct\n92.58\n88.51\n91.5\n91.65\n92.05\n93.22\n92.58\n87.88\n92.72\n91.41\n\n\nAfriE5-large-instruct\n92.37\n88.67\n91.32\n91.32\n91.83\n92.91\n92.29\n87.92\n92.51\n91.24\n\n\nInjongoIntent\n\n\n\n\n\n\n\n\n\n\n\n\nbge-m3\n85.84\n61.19\n86.5\n71.86\n65.66\n90.36\n74.62\n74.11\n68.05\n75.35\n\n\ngemini embedding 001\n89.5\n65.23\n95.25\n83.14\n77.72\n93.64\n87.38\n84.28\n79.22\n83.93\n\n\nmE5-large-instruct\n80.86\n64.89\n85.05\n74.45\n65.95\n80.17\n78.11\n79.38\n70.25\n75.46\n\n\nAfriE5-large-instruct\n82.84\n62.31\n85.95\n72.3\n65.2\n84.42\n79.67\n77.73\n68.36\n75.42\n\n\nNTREXBitextMining\n\n\n\n\n\n\n\n\n\n\n\n\nbge-m3\n79.36\n60.9\n86.3\n81.64\n79.04\n93.91\n83.76\n72.93\n86.07\n80.43\n\n\ngemini embedding 001\n87.2\n65.69\n88.16\n87.03\n81.02\n94.49\n86.33\n77.45\n90.35\n84.19\n\n\nmE5-large-instruct\n92.07\n73.8\n94.81\n94.57\n92.65\n97.45\n93.82\n89.26\n95.15\n91.51\n\n\nAfriE5-large-instruct\n92.87\n78.04\n94.67\n94.63\n91.83\n97.43\n93.88\n89.67\n95.37\n92.04\n\n\nSIB200-14Classes\n\n\n\n\n\n\n\n\n\n\n\n\nbge-m3\n16.05\n6.08\n13.59\n6.85\n9.23\n16.92\n9.83\n4.08\n9.31\n10.22\n\n\ngemini embedding 001\n18.27\n8.13\n18.58\n16.24\n23.23\n20.03\n18.46\n13.64\n23.07\n17.74\n\n\nmE5-large-instruct\n21.24\n12.61\n22.11\n21.49\n23.8\n32.44\n21.63\n17.23\n25.51\n22.01\n\n\nAfriE5-large-instruct\n30.21\n13.28\n27.48\n25.91\n29.02\n33.05\n27.06\n20.71\n28.82\n26.17\n\n\nSIB200Classification\n\n\n\n\n\n\n\n\n\n\n\n\nbge-m3\n64.9\n44.22\n60.85\n51.16\n53.44\n67.99\n56.41\n46.73\n55.07\n55.64\n\n\ngemini embedding 001\n71.72\n57.15\n69.65\n69.84\n73.69\n73.4\n71.01\n65\n71.3\n69.2\n\n\nmE5-large-instruct\n72.87\n57.09\n71.27\n73.7\n74.68\n76.63\n74.1\n66.05\n74.14\n71.17\n\n\nAfriE5-large-instruct\n75.47\n59.82\n72.57\n73.69\n75.16\n76.46\n73.8\n67.1\n74.04\n72.01\n\n\nSIB200ClusteringS2S\n\n\n\n\n\n\n\n\n\n\n\n\nbge-m3\n29.82\n10.84\n22.91\n15.01\n18.84\n34.49\n21.51\n13.4\n22.02\n20.98\n\n\ngemini embedding 001\n39.18\n20.92\n40.94\n37\n37.29\n38.3\n34.57\n27.63\n35.68\n34.61\n\n\nmE5-large-instruct\n48.96\n30.47\n44.47\n46.11\n47.42\n50.32\n46.65\n36.03\n45.04\n43.94\n\n\nAfriE5-large-instruct\n54.35\n31.61\n44.87\n46.56\n48.65\n49.95\n49.53\n38.31\n47.52\n45.71\n\n\nBelebeleRetrieval\n\n\n\n\n\n\n\n\n\n\n\n\nBGE-m3\n79.99\n58.88\n75.48\n61.26\n65.58\n87.92\n69.13\n60.84\n69.59\n69.85\n\n\nGemini Embedding\n91.07\n68.23\n86.46\n80.26\n86.01\n92.65\n86.33\n75\n86.61\n83.62\n\n\nmE5-Large-Instruct\n81.08\n66.34\n77.54\n73.72\n76.93\n86.97\n77.63\n62.97\n77.97\n75.68\n\n\nAfriE5-Large-Instruct\n83.34\n69.42\n78.71\n75.71\n78.34\n87.99\n80.16\n65.25\n80.26\n77.69",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">amh</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">gaz</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">hau</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">ibo</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">kin</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">swa</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">xho</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">yor</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">zul</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">AfriHateClassification</span></td>\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_tt\"/>\n<td class=\"ltx_td ltx_border_tt\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">bge-m3</td>\n<td class=\"ltx_td ltx_align_right\">52.52</td>\n<td class=\"ltx_td ltx_align_right\">52.19</td>\n<td class=\"ltx_td ltx_align_right\">47.32</td>\n<td class=\"ltx_td ltx_align_right\">56.03</td>\n<td class=\"ltx_td ltx_align_right\">51.71</td>\n<td class=\"ltx_td ltx_align_right\">64.81</td>\n<td class=\"ltx_td ltx_align_right\">36.85</td>\n<td class=\"ltx_td ltx_align_right\">50.77</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">38.49</td>\n<td class=\"ltx_td ltx_align_right\">50.08</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">gemini embedding 001</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">54.31</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">52.31</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">52.4</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">63.47</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">57.75</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">74.07</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">38.94</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">56.3</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">45.3</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">54.98</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">mE5-large-instruct</td>\n<td class=\"ltx_td ltx_align_right\">50.78</td>\n<td class=\"ltx_td ltx_align_right\">51.92</td>\n<td class=\"ltx_td ltx_align_right\">42.95</td>\n<td class=\"ltx_td ltx_align_right\">57.16</td>\n<td class=\"ltx_td ltx_align_right\">54.85</td>\n<td class=\"ltx_td ltx_align_right\">67.12</td>\n<td class=\"ltx_td ltx_align_right\">40.18</td>\n<td class=\"ltx_td ltx_align_right\">51.56</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">47.03</td>\n<td class=\"ltx_td ltx_align_right\">51.51</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">AfriE5-large-instruct</td>\n<td class=\"ltx_td ltx_align_right\">52.76</td>\n<td class=\"ltx_td ltx_align_right\">53.79</td>\n<td class=\"ltx_td ltx_align_right\">44.77</td>\n<td class=\"ltx_td ltx_align_right\">58.95</td>\n<td class=\"ltx_td ltx_align_right\">56.47</td>\n<td class=\"ltx_td ltx_align_right\">64.94</td>\n<td class=\"ltx_td ltx_align_right\">37.8</td>\n<td class=\"ltx_td ltx_align_right\">51.67</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">43.93</td>\n<td class=\"ltx_td ltx_align_right\">51.68</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">AfriSentiClassification</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">bge-m3</td>\n<td class=\"ltx_td ltx_align_right\">48.45</td>\n<td class=\"ltx_td ltx_align_right\">34.8</td>\n<td class=\"ltx_td ltx_align_right\">68.83</td>\n<td class=\"ltx_td ltx_align_right\">54.43</td>\n<td class=\"ltx_td ltx_align_right\">46.12</td>\n<td class=\"ltx_td ltx_align_right\">44.52</td>\n<td class=\"ltx_td ltx_align_right\">&#8211;</td>\n<td class=\"ltx_td ltx_align_right\">38.21</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">&#8211;</td>\n<td class=\"ltx_td ltx_align_right\">47.91</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">gemini embedding 001</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">59.75</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">34.35</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">75.02</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">61.65</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">58.58</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">45.94</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">41.25</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">53.79</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">mE5-large-instruct</td>\n<td class=\"ltx_td ltx_align_right\">44.07</td>\n<td class=\"ltx_td ltx_align_right\">35.8</td>\n<td class=\"ltx_td ltx_align_right\">68.64</td>\n<td class=\"ltx_td ltx_align_right\">49.52</td>\n<td class=\"ltx_td ltx_align_right\">50.19</td>\n<td class=\"ltx_td ltx_align_right\">41.95</td>\n<td class=\"ltx_td ltx_align_right\">&#8211;</td>\n<td class=\"ltx_td ltx_align_right\">38.91</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">&#8211;</td>\n<td class=\"ltx_td ltx_align_right\">47.01</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">AfriE5-large-instruct</td>\n<td class=\"ltx_td ltx_align_right\">51.41</td>\n<td class=\"ltx_td ltx_align_right\">35.7</td>\n<td class=\"ltx_td ltx_align_right\">71.78</td>\n<td class=\"ltx_td ltx_align_right\">53.67</td>\n<td class=\"ltx_td ltx_align_right\">53.98</td>\n<td class=\"ltx_td ltx_align_right\">44.13</td>\n<td class=\"ltx_td ltx_align_right\">&#8211;</td>\n<td class=\"ltx_td ltx_align_right\">44.57</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">&#8211;</td>\n<td class=\"ltx_td ltx_align_right\">50.75</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">NewsClassification</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">bge-m3</td>\n<td class=\"ltx_td ltx_align_right\">84.73</td>\n<td class=\"ltx_td ltx_align_right\">74.62</td>\n<td class=\"ltx_td ltx_align_right\">78.23</td>\n<td class=\"ltx_td ltx_align_right\">64.23</td>\n<td class=\"ltx_td ltx_align_right\">49.4</td>\n<td class=\"ltx_td ltx_align_right\">73.8</td>\n<td class=\"ltx_td ltx_align_right\">77.85</td>\n<td class=\"ltx_td ltx_align_right\">79.05</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">&#8211;</td>\n<td class=\"ltx_td ltx_align_right\">72.74</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">gemini embedding 001</td>\n<td class=\"ltx_td ltx_align_right\">84.41</td>\n<td class=\"ltx_td ltx_align_right\">82.09</td>\n<td class=\"ltx_td ltx_align_right\">77.14</td>\n<td class=\"ltx_td ltx_align_right\">68.9</td>\n<td class=\"ltx_td ltx_align_right\">58.58</td>\n<td class=\"ltx_td ltx_align_right\">71.95</td>\n<td class=\"ltx_td ltx_align_right\">87.31</td>\n<td class=\"ltx_td ltx_align_right\">84.21</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">&#8211;</td>\n<td class=\"ltx_td ltx_align_right\">76.82</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">mE5-large-instruct</td>\n<td class=\"ltx_td ltx_align_right\">87.82</td>\n<td class=\"ltx_td ltx_align_right\">79.54</td>\n<td class=\"ltx_td ltx_align_right\">80.71</td>\n<td class=\"ltx_td ltx_align_right\">77.59</td>\n<td class=\"ltx_td ltx_align_right\">57.27</td>\n<td class=\"ltx_td ltx_align_right\">79.43</td>\n<td class=\"ltx_td ltx_align_right\">85.29</td>\n<td class=\"ltx_td ltx_align_right\">83.07</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">&#8211;</td>\n<td class=\"ltx_td ltx_align_right\">78.84</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">AfriE5-large-instruct</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">89.52</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">81.75</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">81.52</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">78.03</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">58.31</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">78.15</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">85.59</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">82.99</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">79.48</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">AfriXNLI</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">bge-m3</td>\n<td class=\"ltx_td ltx_align_right\">75.64</td>\n<td class=\"ltx_td ltx_align_right\">66.39</td>\n<td class=\"ltx_td ltx_align_right\">69.67</td>\n<td class=\"ltx_td ltx_align_right\">64.33</td>\n<td class=\"ltx_td ltx_align_right\">65.27</td>\n<td class=\"ltx_td ltx_align_right\">73.88</td>\n<td class=\"ltx_td ltx_align_right\">69.41</td>\n<td class=\"ltx_td ltx_align_right\">64.03</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">69.87</td>\n<td class=\"ltx_td ltx_align_right\">68.72</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">gemini embedding 001</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">81.32</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">66.8</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">78.57</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">78.05</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">73.79</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">78.34</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">75.42</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">71.72</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">73.38</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">75.27</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">mE5-large-instruct</td>\n<td class=\"ltx_td ltx_align_right\">66.85</td>\n<td class=\"ltx_td ltx_align_right\">62.96</td>\n<td class=\"ltx_td ltx_align_right\">63.51</td>\n<td class=\"ltx_td ltx_align_right\">65.05</td>\n<td class=\"ltx_td ltx_align_right\">62.01</td>\n<td class=\"ltx_td ltx_align_right\">68.09</td>\n<td class=\"ltx_td ltx_align_right\">66.66</td>\n<td class=\"ltx_td ltx_align_right\">61.89</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">63.7</td>\n<td class=\"ltx_td ltx_align_right\">64.52</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">AfriE5-large-instruct</td>\n<td class=\"ltx_td ltx_align_right\">72.28</td>\n<td class=\"ltx_td ltx_align_right\">68.57</td>\n<td class=\"ltx_td ltx_align_right\">67.89</td>\n<td class=\"ltx_td ltx_align_right\">70.36</td>\n<td class=\"ltx_td ltx_align_right\">64.68</td>\n<td class=\"ltx_td ltx_align_right\">72.1</td>\n<td class=\"ltx_td ltx_align_right\">72.41</td>\n<td class=\"ltx_td ltx_align_right\">65.43</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">67.52</td>\n<td class=\"ltx_td ltx_align_right\">69.03</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">EmotionAnalysisPlus</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">bge-m3</td>\n<td class=\"ltx_td ltx_align_right\">20.99</td>\n<td class=\"ltx_td ltx_align_right\">28.54</td>\n<td class=\"ltx_td ltx_align_right\">24.92</td>\n<td class=\"ltx_td ltx_align_right\">23.49</td>\n<td class=\"ltx_td ltx_align_right\">33.9</td>\n<td class=\"ltx_td ltx_align_right\">37.14</td>\n<td class=\"ltx_td ltx_align_right\">21.52</td>\n<td class=\"ltx_td ltx_align_right\">40.58</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">33.86</td>\n<td class=\"ltx_td ltx_align_right\">29.44</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">gemini embedding 001</td>\n<td class=\"ltx_td ltx_align_right\">28.38</td>\n<td class=\"ltx_td ltx_align_right\">30.8</td>\n<td class=\"ltx_td ltx_align_right\">36.03</td>\n<td class=\"ltx_td ltx_align_right\">30.71</td>\n<td class=\"ltx_td ltx_align_right\">39.52</td>\n<td class=\"ltx_td ltx_align_right\">39.67</td>\n<td class=\"ltx_td ltx_align_right\">27.76</td>\n<td class=\"ltx_td ltx_align_right\">45.66</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">40.6</td>\n<td class=\"ltx_td ltx_align_right\">35.46</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">mE5-large-instruct</td>\n<td class=\"ltx_td ltx_align_right\">22.27</td>\n<td class=\"ltx_td ltx_align_right\">30.41</td>\n<td class=\"ltx_td ltx_align_right\">27.03</td>\n<td class=\"ltx_td ltx_align_right\">27.59</td>\n<td class=\"ltx_td ltx_align_right\">34.27</td>\n<td class=\"ltx_td ltx_align_right\">39.89</td>\n<td class=\"ltx_td ltx_align_right\">24.17</td>\n<td class=\"ltx_td ltx_align_right\">40.88</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">37.05</td>\n<td class=\"ltx_td ltx_align_right\">31.51</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">AfriE5-large-instruct</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">24.07</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">30.74</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">28.37</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">30.18</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">36.21</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">38.8</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">28.13</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">42.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">36.35</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">32.78</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">FloresBitextMining</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">bge-m3</td>\n<td class=\"ltx_td ltx_align_right\">83.73</td>\n<td class=\"ltx_td ltx_align_right\">71.4</td>\n<td class=\"ltx_td ltx_align_right\">82.71</td>\n<td class=\"ltx_td ltx_align_right\">72.78</td>\n<td class=\"ltx_td ltx_align_right\">76.63</td>\n<td class=\"ltx_td ltx_align_right\">86.13</td>\n<td class=\"ltx_td ltx_align_right\">81.99</td>\n<td class=\"ltx_td ltx_align_right\">65.33</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">82.24</td>\n<td class=\"ltx_td ltx_align_right\">78.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">gemini embedding 001</td>\n<td class=\"ltx_td ltx_align_right\">91.3</td>\n<td class=\"ltx_td ltx_align_right\">77.8</td>\n<td class=\"ltx_td ltx_align_right\">90.01</td>\n<td class=\"ltx_td ltx_align_right\">87.9</td>\n<td class=\"ltx_td ltx_align_right\">90.13</td>\n<td class=\"ltx_td ltx_align_right\">91.43</td>\n<td class=\"ltx_td ltx_align_right\">90.37</td>\n<td class=\"ltx_td ltx_align_right\">83.99</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">90.28</td>\n<td class=\"ltx_td ltx_align_right\">88.13</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">mE5-large-instruct</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">92.58</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">88.51</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">91.5</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">91.65</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">92.05</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">93.22</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">92.58</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">87.88</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">92.72</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">91.41</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">AfriE5-large-instruct</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">92.37</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">88.67</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">91.32</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">91.32</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">91.83</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">92.91</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">92.29</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">87.92</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">92.51</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">91.24</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">InjongoIntent</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">bge-m3</td>\n<td class=\"ltx_td ltx_align_right\">85.84</td>\n<td class=\"ltx_td ltx_align_right\">61.19</td>\n<td class=\"ltx_td ltx_align_right\">86.5</td>\n<td class=\"ltx_td ltx_align_right\">71.86</td>\n<td class=\"ltx_td ltx_align_right\">65.66</td>\n<td class=\"ltx_td ltx_align_right\">90.36</td>\n<td class=\"ltx_td ltx_align_right\">74.62</td>\n<td class=\"ltx_td ltx_align_right\">74.11</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">68.05</td>\n<td class=\"ltx_td ltx_align_right\">75.35</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">gemini embedding 001</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">89.5</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">65.23</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">95.25</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">83.14</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">77.72</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">93.64</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">87.38</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">84.28</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">79.22</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">83.93</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">mE5-large-instruct</td>\n<td class=\"ltx_td ltx_align_right\">80.86</td>\n<td class=\"ltx_td ltx_align_right\">64.89</td>\n<td class=\"ltx_td ltx_align_right\">85.05</td>\n<td class=\"ltx_td ltx_align_right\">74.45</td>\n<td class=\"ltx_td ltx_align_right\">65.95</td>\n<td class=\"ltx_td ltx_align_right\">80.17</td>\n<td class=\"ltx_td ltx_align_right\">78.11</td>\n<td class=\"ltx_td ltx_align_right\">79.38</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">70.25</td>\n<td class=\"ltx_td ltx_align_right\">75.46</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">AfriE5-large-instruct</td>\n<td class=\"ltx_td ltx_align_right\">82.84</td>\n<td class=\"ltx_td ltx_align_right\">62.31</td>\n<td class=\"ltx_td ltx_align_right\">85.95</td>\n<td class=\"ltx_td ltx_align_right\">72.3</td>\n<td class=\"ltx_td ltx_align_right\">65.2</td>\n<td class=\"ltx_td ltx_align_right\">84.42</td>\n<td class=\"ltx_td ltx_align_right\">79.67</td>\n<td class=\"ltx_td ltx_align_right\">77.73</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">68.36</td>\n<td class=\"ltx_td ltx_align_right\">75.42</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">NTREXBitextMining</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">bge-m3</td>\n<td class=\"ltx_td ltx_align_right\">79.36</td>\n<td class=\"ltx_td ltx_align_right\">60.9</td>\n<td class=\"ltx_td ltx_align_right\">86.3</td>\n<td class=\"ltx_td ltx_align_right\">81.64</td>\n<td class=\"ltx_td ltx_align_right\">79.04</td>\n<td class=\"ltx_td ltx_align_right\">93.91</td>\n<td class=\"ltx_td ltx_align_right\">83.76</td>\n<td class=\"ltx_td ltx_align_right\">72.93</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">86.07</td>\n<td class=\"ltx_td ltx_align_right\">80.43</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">gemini embedding 001</td>\n<td class=\"ltx_td ltx_align_right\">87.2</td>\n<td class=\"ltx_td ltx_align_right\">65.69</td>\n<td class=\"ltx_td ltx_align_right\">88.16</td>\n<td class=\"ltx_td ltx_align_right\">87.03</td>\n<td class=\"ltx_td ltx_align_right\">81.02</td>\n<td class=\"ltx_td ltx_align_right\">94.49</td>\n<td class=\"ltx_td ltx_align_right\">86.33</td>\n<td class=\"ltx_td ltx_align_right\">77.45</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">90.35</td>\n<td class=\"ltx_td ltx_align_right\">84.19</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">mE5-large-instruct</td>\n<td class=\"ltx_td ltx_align_right\">92.07</td>\n<td class=\"ltx_td ltx_align_right\">73.8</td>\n<td class=\"ltx_td ltx_align_right\">94.81</td>\n<td class=\"ltx_td ltx_align_right\">94.57</td>\n<td class=\"ltx_td ltx_align_right\">92.65</td>\n<td class=\"ltx_td ltx_align_right\">97.45</td>\n<td class=\"ltx_td ltx_align_right\">93.82</td>\n<td class=\"ltx_td ltx_align_right\">89.26</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">95.15</td>\n<td class=\"ltx_td ltx_align_right\">91.51</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">AfriE5-large-instruct</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">92.87</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">78.04</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">94.67</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">94.63</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">91.83</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">97.43</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">93.88</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">89.67</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">95.37</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">92.04</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">SIB200-14Classes</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">bge-m3</td>\n<td class=\"ltx_td ltx_align_right\">16.05</td>\n<td class=\"ltx_td ltx_align_right\">6.08</td>\n<td class=\"ltx_td ltx_align_right\">13.59</td>\n<td class=\"ltx_td ltx_align_right\">6.85</td>\n<td class=\"ltx_td ltx_align_right\">9.23</td>\n<td class=\"ltx_td ltx_align_right\">16.92</td>\n<td class=\"ltx_td ltx_align_right\">9.83</td>\n<td class=\"ltx_td ltx_align_right\">4.08</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">9.31</td>\n<td class=\"ltx_td ltx_align_right\">10.22</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">gemini embedding 001</td>\n<td class=\"ltx_td ltx_align_right\">18.27</td>\n<td class=\"ltx_td ltx_align_right\">8.13</td>\n<td class=\"ltx_td ltx_align_right\">18.58</td>\n<td class=\"ltx_td ltx_align_right\">16.24</td>\n<td class=\"ltx_td ltx_align_right\">23.23</td>\n<td class=\"ltx_td ltx_align_right\">20.03</td>\n<td class=\"ltx_td ltx_align_right\">18.46</td>\n<td class=\"ltx_td ltx_align_right\">13.64</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">23.07</td>\n<td class=\"ltx_td ltx_align_right\">17.74</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">mE5-large-instruct</td>\n<td class=\"ltx_td ltx_align_right\">21.24</td>\n<td class=\"ltx_td ltx_align_right\">12.61</td>\n<td class=\"ltx_td ltx_align_right\">22.11</td>\n<td class=\"ltx_td ltx_align_right\">21.49</td>\n<td class=\"ltx_td ltx_align_right\">23.8</td>\n<td class=\"ltx_td ltx_align_right\">32.44</td>\n<td class=\"ltx_td ltx_align_right\">21.63</td>\n<td class=\"ltx_td ltx_align_right\">17.23</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">25.51</td>\n<td class=\"ltx_td ltx_align_right\">22.01</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">AfriE5-large-instruct</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">30.21</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">13.28</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">27.48</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">25.91</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">29.02</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">33.05</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">27.06</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">20.71</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">28.82</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">26.17</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">SIB200Classification</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">bge-m3</td>\n<td class=\"ltx_td ltx_align_right\">64.9</td>\n<td class=\"ltx_td ltx_align_right\">44.22</td>\n<td class=\"ltx_td ltx_align_right\">60.85</td>\n<td class=\"ltx_td ltx_align_right\">51.16</td>\n<td class=\"ltx_td ltx_align_right\">53.44</td>\n<td class=\"ltx_td ltx_align_right\">67.99</td>\n<td class=\"ltx_td ltx_align_right\">56.41</td>\n<td class=\"ltx_td ltx_align_right\">46.73</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">55.07</td>\n<td class=\"ltx_td ltx_align_right\">55.64</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">gemini embedding 001</td>\n<td class=\"ltx_td ltx_align_right\">71.72</td>\n<td class=\"ltx_td ltx_align_right\">57.15</td>\n<td class=\"ltx_td ltx_align_right\">69.65</td>\n<td class=\"ltx_td ltx_align_right\">69.84</td>\n<td class=\"ltx_td ltx_align_right\">73.69</td>\n<td class=\"ltx_td ltx_align_right\">73.4</td>\n<td class=\"ltx_td ltx_align_right\">71.01</td>\n<td class=\"ltx_td ltx_align_right\">65</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">71.3</td>\n<td class=\"ltx_td ltx_align_right\">69.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">mE5-large-instruct</td>\n<td class=\"ltx_td ltx_align_right\">72.87</td>\n<td class=\"ltx_td ltx_align_right\">57.09</td>\n<td class=\"ltx_td ltx_align_right\">71.27</td>\n<td class=\"ltx_td ltx_align_right\">73.7</td>\n<td class=\"ltx_td ltx_align_right\">74.68</td>\n<td class=\"ltx_td ltx_align_right\">76.63</td>\n<td class=\"ltx_td ltx_align_right\">74.1</td>\n<td class=\"ltx_td ltx_align_right\">66.05</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">74.14</td>\n<td class=\"ltx_td ltx_align_right\">71.17</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">AfriE5-large-instruct</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">75.47</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">59.82</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">72.57</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">73.69</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">75.16</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">76.46</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">73.8</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">67.1</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">74.04</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">72.01</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">SIB200ClusteringS2S</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">bge-m3</td>\n<td class=\"ltx_td ltx_align_right\">29.82</td>\n<td class=\"ltx_td ltx_align_right\">10.84</td>\n<td class=\"ltx_td ltx_align_right\">22.91</td>\n<td class=\"ltx_td ltx_align_right\">15.01</td>\n<td class=\"ltx_td ltx_align_right\">18.84</td>\n<td class=\"ltx_td ltx_align_right\">34.49</td>\n<td class=\"ltx_td ltx_align_right\">21.51</td>\n<td class=\"ltx_td ltx_align_right\">13.4</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">22.02</td>\n<td class=\"ltx_td ltx_align_right\">20.98</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">gemini embedding 001</td>\n<td class=\"ltx_td ltx_align_right\">39.18</td>\n<td class=\"ltx_td ltx_align_right\">20.92</td>\n<td class=\"ltx_td ltx_align_right\">40.94</td>\n<td class=\"ltx_td ltx_align_right\">37</td>\n<td class=\"ltx_td ltx_align_right\">37.29</td>\n<td class=\"ltx_td ltx_align_right\">38.3</td>\n<td class=\"ltx_td ltx_align_right\">34.57</td>\n<td class=\"ltx_td ltx_align_right\">27.63</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">35.68</td>\n<td class=\"ltx_td ltx_align_right\">34.61</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">mE5-large-instruct</td>\n<td class=\"ltx_td ltx_align_right\">48.96</td>\n<td class=\"ltx_td ltx_align_right\">30.47</td>\n<td class=\"ltx_td ltx_align_right\">44.47</td>\n<td class=\"ltx_td ltx_align_right\">46.11</td>\n<td class=\"ltx_td ltx_align_right\">47.42</td>\n<td class=\"ltx_td ltx_align_right\">50.32</td>\n<td class=\"ltx_td ltx_align_right\">46.65</td>\n<td class=\"ltx_td ltx_align_right\">36.03</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">45.04</td>\n<td class=\"ltx_td ltx_align_right\">43.94</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">AfriE5-large-instruct</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">54.35</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">31.61</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">44.87</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">46.56</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">48.65</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">49.95</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">49.53</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">38.31</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">47.52</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">45.71</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">BelebeleRetrieval</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">BGE-m3</td>\n<td class=\"ltx_td ltx_align_right\">79.99</td>\n<td class=\"ltx_td ltx_align_right\">58.88</td>\n<td class=\"ltx_td ltx_align_right\">75.48</td>\n<td class=\"ltx_td ltx_align_right\">61.26</td>\n<td class=\"ltx_td ltx_align_right\">65.58</td>\n<td class=\"ltx_td ltx_align_right\">87.92</td>\n<td class=\"ltx_td ltx_align_right\">69.13</td>\n<td class=\"ltx_td ltx_align_right\">60.84</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">69.59</td>\n<td class=\"ltx_td ltx_align_right\">69.85</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">Gemini Embedding</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">91.07</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">68.23</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">86.46</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">80.26</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">86.01</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">92.65</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">86.33</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">75</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">86.61</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D9D9;\">83.62</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">mE5-Large-Instruct</td>\n<td class=\"ltx_td ltx_align_right\">81.08</td>\n<td class=\"ltx_td ltx_align_right\">66.34</td>\n<td class=\"ltx_td ltx_align_right\">77.54</td>\n<td class=\"ltx_td ltx_align_right\">73.72</td>\n<td class=\"ltx_td ltx_align_right\">76.93</td>\n<td class=\"ltx_td ltx_align_right\">86.97</td>\n<td class=\"ltx_td ltx_align_right\">77.63</td>\n<td class=\"ltx_td ltx_align_right\">62.97</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">77.97</td>\n<td class=\"ltx_td ltx_align_right\">75.68</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">AfriE5-Large-Instruct</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">83.34</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">69.42</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">78.71</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">75.71</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">78.34</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">87.99</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">80.16</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">65.25</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\">80.26</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">77.69</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "amh",
            "afrie5largeinstruct",
            "belebeleretrieval",
            "task",
            "xho",
            "newsclassification",
            "avg",
            "kin",
            "ntrexbitextmining",
            "embedding",
            "afrisenticlassification",
            "gemini",
            "each",
            "sib200clusterings2s",
            "across",
            "african",
            "all",
            "zul",
            "me5largeinstruct",
            "yor",
            "afrihateclassification",
            "selected",
            "dataset",
            "swa",
            "afrixnli",
            "sib200classification",
            "hau",
            "performance",
            "floresbitextmining",
            "taskwise",
            "gaz",
            "injongointent",
            "models",
            "ibo",
            "emotionanalysisplus",
            "languages",
            "perlanguage",
            "bgem3",
            "sib20014classes",
            "comparison"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#A1.T7\" title=\"Table 7 &#8227; A.4 Detailed AfriMTEB-Lite results &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;7</span></a> shows the comprehensive results across all evaluated datasets and languages in <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB-Lite</span>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Text embeddings are an essential building component of several NLP tasks such as retrieval-augmented generation which is crucial for preventing hallucinations in LLMs. Despite the recent release of massively multilingual MTEB (MMTEB), African languages remain underrepresented, with existing tasks often repurposed from translation benchmarks such as FLORES clustering or SIB-200.\nIn this paper, we introduce AfriMTEB&#8212;a regional expansion of MMTEB covering 59 languages, 14 tasks, and 38 datasets, including six newly added datasets. Unlike many MMTEB datasets that include fewer than five languages, the new additions span 14 to 56 African languages and introduce entirely new tasks, such as hate speech detection, intent detection, and emotion classification, which were not previously covered. Complementing this, we present AfriE5, an adaptation of the instruction-tuned mE5 model to African languages through cross-lingual contrastive distillation. Our evaluation shows that AfriE5 achieves state-of-the-art performance, outperforming strong baselines such as Gemini-Embeddings and mE5.</p>\n\n",
                "matched_terms": [
                    "african",
                    "performance",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "african",
                    "embedding",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While recent text embedding benchmarks have improved language coverage in recent years, such as MMTEB&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Enevoldsen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib11\" title=\"\">2025</a>)</cite>, African languages remain under-represented, where many of the tasks covered are either based on massively evaluation of translation datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB-Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib33\" title=\"\">2022</a>; Federmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib12\" title=\"\">2022</a>)</cite>, and the tasks derived or repurposed from translation benchmarks such as Belebele&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bandarkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib5\" title=\"\">2024</a>)</cite> and SIB-200&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib1\" title=\"\">2024</a>)</cite>. As a result, the quality of text embeddings for languages in the African region remains unknown, as the region has few standardized tools for comparing models across tasks and languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alabi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib4\" title=\"\">2025</a>)</cite></p>\n\n",
                "matched_terms": [
                    "across",
                    "models",
                    "african",
                    "languages",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">AfriMTEB</span>, a regional extension of MTEB tailored to African languages and tasks. <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;2</span></a> shows the full suite that covers 59 languages and 38 datasets spanning Bitext mining, Classification (single, pair and multi-label), Semantic Text Similarity, Retrieval, Clustering, and Reranking. To support fast-and-compute friendly evaluation, we also provide <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">AfriMTEB-Lite</span>, a compact suite over nine geographically-diverse languages (Amharic, Oromo, Igbo, Yoruba, Hausa, Swahili, Kinyarwanda, Xhosa, Zulu) constructed by selecting, within each task family, the maximally overlapping tasks such that every task includes all nine languages, ensuring uniform language coverage across the suite.</p>\n\n",
                "matched_terms": [
                    "each",
                    "task",
                    "across",
                    "african",
                    "all",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to benchmarking, we develop <span class=\"ltx_text ltx_font_bold ltx_font_italic\">AfriE5-Large-Instruct</span>\nadapting strong embedding models to African languages.\nStarting from the instruction-tuned <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib42\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">c</a>)</cite>, we leveraged cross-lingual contrastive distillation. We construct supervision by translating MNLI/SNLI datasets<cite class=\"ltx_cite ltx_citemacro_citep\">(Williams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib47\" title=\"\">2018</a>; Bowman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib7\" title=\"\">2015</a>)</cite> in African languages with NLLB-200 (3.3B) <cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB-Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib33\" title=\"\">2022</a>)</cite>, followed by automatic filtering by SSA-COMET (an African COMET-based QE metric)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib19\" title=\"\">2025</a>)</cite>, and expanding each example into multiple sources or target directions to encourage cross-lingual alignment. We then use the receipe of BGE Re-ranker v2 m3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite> for text embedding model training.</p>\n\n",
                "matched_terms": [
                    "afrie5largeinstruct",
                    "each",
                    "african",
                    "models",
                    "me5largeinstruct",
                    "languages",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate widely used baselines, including BGE-M3, E5 variants (e.g., e5-mistral-7b-instruct), Qwen embeddings, Gemini embedding-001, and Embedding Gemma <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">2024c</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib50\" title=\"\">2025</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib18\" title=\"\">2025</a>)</cite>. On <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB-Lite</span>, our adapted model <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span>, trained only on nine African languages, achieves an average score of 63.7, surpassing me5-large-instruct (62.0) and Gemini embedding-001 (63.1). Remarkably, despite being tuned with data and languages aligned to the <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB-Lite</span>, our model leveraged cross-lingual transfer to generalize to the broader <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> benchmark covering 59 languages and 38 datasets, where it also delivers the best performance with an average score of 62.4, ahead of me5-large-instruct (61.3) and Gemini embedding-001 (60.6) as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>. These results highlight that targeted cross-lingual adaptation on a carefully selected subset of languages can transfer effectively to a much larger set, yielding consistent improvements across task families while preserving the backbone&#8217;s general utility.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "afrie5largeinstruct",
                    "task",
                    "across",
                    "african",
                    "me5largeinstruct",
                    "languages",
                    "selected",
                    "bgem3",
                    "performance",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">AfriMTEB</span> is a regional extension of MTEB designed to evaluate text embeddings for African languages across multiple task families. To address persistent gaps in language coverage and enable systematic, comparable assessment, we introduce <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span>as a standardized benchmark for African languages. It consists of a <span class=\"ltx_text ltx_font_bold\">Full suite</span> covering <span class=\"ltx_text ltx_font_italic\">59 languages</span> and <span class=\"ltx_text ltx_font_italic\">38 datasets</span>, and a <span class=\"ltx_text ltx_font_bold\">Lite suite</span> that maintains uniform language coverage for <span class=\"ltx_text ltx_font_italic\">9 target languages</span> over <span class=\"ltx_text ltx_font_italic\">13 representative datasets</span>. Languages are selected to span major families and geographic regions while prioritizing practical relevance and the availability of reliable public datasets; tasks reflect core embedding applications and draw on established, well-curated resources. The Lite suite supports rapid iteration by enforcing consistent task coverage across languages through maximal task&#8211;language overlap.</p>\n\n",
                "matched_terms": [
                    "task",
                    "across",
                    "african",
                    "languages",
                    "selected",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> follows the <span class=\"ltx_text ltx_font_typewriter\">MTEB</span> taxonomy and groups tasks into eight families. <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S2.T1\" title=\"Table 1 &#8227; 2 AfriMTEB Benchmark &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> lists the tasks, task families and the datasets included in the <em class=\"ltx_emph ltx_font_italic\">Full</em> suite. Each dataset belongs to one of the eight task families: (1) Bibtext mining (2) Pair classification (3) Semantic Text Similarity (STS) (4) Clustering (5) Classification (6) Multi-label Classification (7) Retrieval (8). The first four task families performs a similarity related tasks based on two pairs of sentences, either to identity translation pairs, categorize them into a class (e.g., entailment vs. contradiction) or provide a similarity score. The next three tasks (5) - (6) classify a sentence or article into one or more classes. Finally, the last sets of tasks (7) and (8) are information retrieval tasks for retrieval or re-ranking based on a user query. We provide full description in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.SS1\" title=\"A.1 Task categories &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "task",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> builds on the Massive Text Embedding Benchmark (MTEB) by selecting tasks that include African languages. From the original benchmark, we inherit datasets covering bitext mining (e.g., Flores, NTREX, Tatoeba), pair classification (XNLI), topic and sentiment classification (SIB-200, AfriSenti, MasakhaNEWS), semantic textual similarity (SemRel24), retrieval (MIRACL, XQuAD, XM3600), clustering (SIB-200, MasakhaNEWS), and reranking (MIRACL).</p>\n\n",
                "matched_terms": [
                    "african",
                    "embedding",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, language coverage in these datasets is uneven: some tasks include only a few African languages, while others span broader multilingual settings. To ensure fairness, we compute macro averages over languages within each task, then average across tasks and families to obtain the overall <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> score. This prevents any single task or language from dominating the benchmark.</p>\n\n",
                "matched_terms": [
                    "each",
                    "task",
                    "across",
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <em class=\"ltx_emph ltx_font_italic\">Full</em> suite is built by selecting datasets from <span class=\"ltx_text ltx_font_typewriter\">MTEB</span> that include African languages and extending them with six additional datasets. These additions broaden task coverage, increase difficulty, and improve language representation.</p>\n\n",
                "matched_terms": [
                    "african",
                    "task",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An African extension of the XNLI benchmark that provides natural language inference data to 15 African languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib3\" title=\"\">2025</a>)</cite>. By including AfriXNLI, we expand language coverage for the pair classification family beyond a single African language (Swahili), ensuring broader representation of African languages in entailment-style tasks.</p>\n\n",
                "matched_terms": [
                    "african",
                    "afrixnli",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A multilabel emotion data set that covers 32 languages including 14 African languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Belay et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib6\" title=\"\">2025</a>; Muhammad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib30\" title=\"\">2025b</a>)</cite>. Each sentence may be assigned multiple emotion labels such as <span class=\"ltx_text ltx_font_smallcaps\">joy</span>, <span class=\"ltx_text ltx_font_smallcaps\">anger</span>, <span class=\"ltx_text ltx_font_smallcaps\">sadness</span>, or <span class=\"ltx_text ltx_font_smallcaps\">fear</span>. By including this dataset, AfriMTEB introduces the first <em class=\"ltx_emph ltx_font_italic\">multi-label classification</em> task for African languages, thereby broadening the taxonomy beyond single-label settings.</p>\n\n",
                "matched_terms": [
                    "each",
                    "task",
                    "african",
                    "dataset",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A multilingual hate-speech classification dataset covering 14 African languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Muhammad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib27\" title=\"\">2025a</a>)</cite>. Each instance is labeled as <span class=\"ltx_text ltx_font_smallcaps\">hate</span>, <span class=\"ltx_text ltx_font_smallcaps\">abusive</span>, or <span class=\"ltx_text ltx_font_smallcaps\">neutral</span>, providing a standardized benchmark for toxic content detection across diverse languages and registers. This dataset extends evaluation to socially relevant safety applications.</p>\n\n",
                "matched_terms": [
                    "each",
                    "across",
                    "african",
                    "dataset",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Injongo dataset is a multilingual resource for intent detection covering 16 African languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib49\" title=\"\">2025</a>)</cite>. It consists of short, conversational utterances annotated with 40 everyday intent categories, for example requests such as &#8220;freeze account&#8221; or &#8220;play music.&#8221; By focusing on dialogue-style classification, Injongo complements existing benchmarks like MASSIVE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">FitzGerald et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib14\" title=\"\">2023</a>)</cite>, but offers broader African language coverage.</p>\n\n",
                "matched_terms": [
                    "african",
                    "dataset",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A more challenging variant of the SIB-200 dataset where labels are consolidated into 14 categories. This version still includes 56 African languages but is not limited to them&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib1\" title=\"\">2024</a>)</cite>. By merging fine-grained topics into broader classes, intra-class diversity increases, which raises task difficulty. The inclusion of this dataset not only strengthens African language evaluation but also increases the difficulty for other covered languages in SIB-200, leading to more robust cross-lingual assessment.</p>\n\n",
                "matched_terms": [
                    "african",
                    "task",
                    "dataset",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we added <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SIB200Classification</span> dataset to complement <span class=\"ltx_text ltx_font_italic\">SIB-200_14Classes</span> in the &#8220;<span class=\"ltx_text ltx_font_typewriter\">Clf</span>&#8221; task family, although not new. The only difference is that the former covers seven categories compared to the latter. The original MTEB only considers SIB-200 in the Clustering (<span class=\"ltx_text ltx_font_typewriter\">Clust</span>) task family.</p>\n\n",
                "matched_terms": [
                    "sib20014classes",
                    "sib200classification",
                    "task",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <em class=\"ltx_emph ltx_font_italic\">Lite</em> suite focuses on nine geographically-diverse African languages:\n<span class=\"ltx_text ltx_font_italic\">Amharic, Oromo, Igbo, Yoruba, Hausa, Swahili, Kinyarwanda, Xhosa, Zulu</span>.\nWe selected only those datasets from MTEB that include nine languages to guarantee consistent language&#8211;task alignment across families. The result is a compact yet representative benchmark of 13 datasets&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We merged two datasets: MasakhaNEWS and KinNEWS since they have similar structure, thus we report 12 datasets</span></span></span> spanning classification, retrieval, bitext mining, clustering, pair classification, and multi-label classification.</p>\n\n",
                "matched_terms": [
                    "selected",
                    "african",
                    "across",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"s_{i,j}=\\mathbf{q}i^{\\top}\\mathbf{p}_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mi>&#119850;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>i</mi><mo>&#8868;</mo></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#119849;</mi><mi>j</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">s_{i,j}=\\mathbf{q}i^{\\top}\\mathbf{p}_{j}</annotation></semantics></math> is the similarity between query embedding <math alttext=\"\\mathbf{q}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#119850;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{q}_{i}</annotation></semantics></math> and passage embedding <math alttext=\"\\mathbf{p}_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#119849;</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{p}_{j}</annotation></semantics></math>, and <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m4\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> is a temperature hyperparameter. The numerator contains the similarity between query <math alttext=\"\\mathbf{q}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m5\" intent=\":literal\"><semantics><msub><mi>&#119850;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{q}_{i}</annotation></semantics></math> and its corresponding positive passage <math alttext=\"\\mathbf{p}{i\\cdot G}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>&#119849;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><mi>G</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{p}{i\\cdot G}</annotation></semantics></math>. The denominator includes all similarities between <math alttext=\"\\mathbf{q}i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p3.m7\" intent=\":literal\"><semantics><mrow><mi>&#119850;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{q}i</annotation></semantics></math> and all passages in the batch, comprising both pre-mined hard negatives (from NLI contradiction examples and hard negative mining) and in-batch negatives (other queries and passages from the same training batch).\nWith cross-device negative sharing enabled, the total negative set spans all examples across all GPUs in the distributed training setup, providing a rich set of challenging negative examples for each query.</p>\n\n",
                "matched_terms": [
                    "each",
                    "all",
                    "embedding",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We constructed cross-lingual training data by leveraging large-scale natural language inference corpora, a supervision signal that has proven effective for learning sentence embeddings. <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib15\" title=\"\">2022</a>; Reimers and Gurevych, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib39\" title=\"\">2019</a>)</cite> Specifically, we used MultiNLI and SNLI <cite class=\"ltx_cite ltx_citemacro_citep\">(Williams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib47\" title=\"\">2018</a>; Bowman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib7\" title=\"\">2015</a>)</cite> as source datasets in English. Each sentence pair was translated into the nine African target languages using NLLB-200 (3.3B) <cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB-Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib33\" title=\"\">2022</a>)</cite>. We then estimated translation quality by SSA-COMET-MTL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib19\" title=\"\">2025</a>)</cite>, a COMET variant&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Rei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib38\" title=\"\">2020</a>)</cite>. and filtered pairs below a threshold of 0.75 to ensure the data quality.</p>\n\n",
                "matched_terms": [
                    "each",
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To encourage <span class=\"ltx_text ltx_font_bold\">cross-lingual alignment</span>, each example was expanded into multiple configurations: (i) premise in target language and hypothesis in source, (ii) premise in source and hypothesis in target, (iii) both in target, and (iv) both in source, that is,is, English. For MultiNLI, we reformulated examples as query&#8211;positive/negative pairs (entailment as <span class=\"ltx_text ltx_font_italic\">pos</span>, contradiction as <span class=\"ltx_text ltx_font_italic\">neg</span>). For SNLI, we followed the same strategy but included all 3-way annotations. We further increased training difficulty by sampling up to 15 hard negatives per instance using mE5-large-instruct.</p>\n\n",
                "matched_terms": [
                    "each",
                    "all",
                    "me5largeinstruct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We trained for one epoch over the curated cross-lingual dataset, with logging every 100 steps and checkpoint saving every 100 steps. All samples in a batch were drawn from the same dataset (<span class=\"ltx_text ltx_font_typewriter\">--same_dataset_within_batch</span>) to maintain consistent supervision. The resulting model is referred to as <span class=\"ltx_text ltx_font_bold ltx_font_italic\">AfriE5-Large-Instruct</span>. We provide brief descriptions of the baseline models in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.SS2\" title=\"A.2 Baseline Model Descriptions &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "afrie5largeinstruct",
                    "all",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite being listed under small models (<math alttext=\"&lt;1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">&lt;1</annotation></semantics></math>B), the E5 family matches or surpasses the proprietary model on average <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> score. <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> attains the best macro average at 62.4, edging out <span class=\"ltx_text ltx_font_italic\">Gemini embedding</span> (60.6) and <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> (61.3) in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23896v1#S2.T2\" title=\"Table 2 &#8227; 2.3 AfriMTEB-Lite construction &#8227; 2 AfriMTEB Benchmark &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>. This indicates that strong multilingual coverage and targeted adaptation can outweigh model size or access to proprietary training data.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "afrie5largeinstruct",
                    "models",
                    "me5largeinstruct",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While E5 variants lead overall, <span class=\"ltx_text ltx_font_italic\">Gemini embedding 001</span> is strongest on most classification-style families: single-label classification (50.0 vs. 49.8 for <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span>and 49.7 for <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span>), multi-label classification (32.7 vs. 28.6/29.8), and pair classification (71.6 vs. 63.8/67.9). It also tops retrieval (77.5) and semantic textual similarity (65.0). These strengths suggest Gemini&#8217;s instruction/data mix particularly benefits discriminative judgment tasks and sentence-level similarity scoring.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "afrie5largeinstruct",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large 7B and 8B encoders do not translate into higher <span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span> scores. All 7B/8B models cluster in the low&#8211;mid 50s, for example, <span class=\"ltx_text ltx_font_italic\">gte-Qwen2-7B-instruct</span> at 50.9, <span class=\"ltx_text ltx_font_italic\">GritLM-7B</span> at 51.8, <span class=\"ltx_text ltx_font_italic\">Qwen3-Embedding-8B</span> at 50.5. Thay are clearly below the smaller E5 variants of around 61.3 point. This gap underscores that broad, balanced language coverage and task diversity matter more than parameter count alone.</p>\n\n",
                "matched_terms": [
                    "models",
                    "all",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On bitext mining, <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> and <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> have comparable performance of <math alttext=\"85\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mn>85</mn><annotation encoding=\"application/x-tex\">85</annotation></semantics></math> points,\nboth are far ahead of other opens and the API baseline (<span class=\"ltx_text ltx_font_italic\">Gemini embedding</span> at 72.2). For clustering, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> leads with 62.9 and <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> is next (61.9), while larger models and Gemini embeddings behind. These families reward models that learn language-agnostic semantic spaces with robust cross-lingual alignment, which appears to be a particular strength of the E5 lineage.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "afrie5largeinstruct",
                    "models",
                    "me5largeinstruct",
                    "performance",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> is adapted using supervision centered on nine African languages (<span class=\"ltx_text ltx_font_typewriter\">AfriMTEB</span>), it achieves the highest Full-suite average (62.4) across 59 languages. Relative to <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span>, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> shows consistent family-level gains on pair classification (+4.1), reranking (+2.1), retrieval (+1.3), clustering (+1.0), and multi-label classification (+1.2), with small trade-offs on bitext mining (&#8211;0.3), single-label classification (&#8211;0.1), and STS (&#8211;1.1). This pattern indicates that targeted cross-lingual distillation can yield transferable gains beyond the training languages, particularly for retrieval-style tasks. </p>\n\n",
                "matched_terms": [
                    "afrie5largeinstruct",
                    "across",
                    "african",
                    "me5largeinstruct",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the base model <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> records a reranking score of 61.9, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> raises this to 64.0. It also surpasses gemini embedding 001 (63.4) on the same task (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S2.T2\" title=\"Table 2 &#8227; 2.3 AfriMTEB-Lite construction &#8227; 2 AfriMTEB Benchmark &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). This improvement can be attributed to the training recipe, where <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> leverages knowledge distillation from the <span class=\"ltx_text ltx_font_smallcaps\">BGE-m3</span> cross-encoder in addition to contrastive supervision. The result suggests that incorporating reranker-derived soft labels enhances cross-lingual alignment for ranking-style objectives.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "afrie5largeinstruct",
                    "task",
                    "me5largeinstruct",
                    "bgem3",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the Lite suite of nine African languages and twelve tasks, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> attains the best overall score with an average of 63.7, compared to 62.0 for <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span> and the same performance of 63.1 as <span class=\"ltx_text ltx_font_italic\">Gemini embedding</span> (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S3.T3\" title=\"Table 3 &#8227; 3 Adapting Embedding Models to African Languages &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Although the performance advantage is not uniform across all task families, AfriE5 consistently remains competitive and often surpasses both baselines on challenging benchmarks such as AfriXNLI and the SIB200 variants. These results highlight that adapting mE5 with cross-lingual contrastive distillation improves its effectiveness for African languages, even when trained on a compact set of nine languages.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "afrie5largeinstruct",
                    "task",
                    "across",
                    "african",
                    "all",
                    "afrixnli",
                    "me5largeinstruct",
                    "languages",
                    "performance",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Relative to <span class=\"ltx_text ltx_font_italic\">mE5-Large-Instruct</span>, <span class=\"ltx_text ltx_font_italic\">AfriE5-Large-Instruct</span> shows clear improvements on several classification benchmarks. It achieves higher scores on AfriSentiment (+3.7), AfriXNLI (+4.5), and SIB200 clustering (+1.8). The largest gain appears on SIB200-14 classification (+4.4; 26.2 vs. 21.8), a setting that requires grouping semantically diverse texts into broad topical categories. These improvements indicate that the cross-lingual, NLI-style contrastive learning used in AfriE5 strengthens semantic alignment across languages, leading to more effective cross-lingual transfer for classification and clustering tasks.\n</p>\n\n",
                "matched_terms": [
                    "afrie5largeinstruct",
                    "across",
                    "afrixnli",
                    "me5largeinstruct",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As described in Section &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#S3.SS2\" title=\"3.2 Cross-lingual Training Data Construction &#8227; 3 Adapting Embedding Models to African Languages &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, comparing row 1 (<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>, 0.75) and row 3 (<math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math>, 0.75), the average increases from 62.3 to 63.2. The cross-lingual expansion of the data set exposes the model to richer cross-lingual contrasts: First, higher scores on bitext mining tasks show that the expansion improves the model&#8217;s ability to align semantically equivalent sentences between languages, producing a more coherent multilingual embedding space. Second, classification tasks such as SIB200-14 benefit from this stronger alignment, as the model learns to abstract over diverse topical domains without relying on language-specific cues.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A number of African NLP benchmarks focus on text classification and related tasks. AfroBench aggregates multiple tasks and languages with an emphasis on broad coverage <cite class=\"ltx_cite ltx_citemacro_citep\">(Ojo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib34\" title=\"\">2025</a>)</cite>. News topic classification resources include MasakhaNEWS and earlier Kinyarwanda/Kirundi corpora (KINNEWS/KIRNEWS) <cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib2\" title=\"\">2023</a>; Niyongabo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib32\" title=\"\">2020</a>)</cite>. Sentiment and toxicity are covered by AfriSenti, NaijaSenti, and AfriHate <cite class=\"ltx_cite ltx_citemacro_citep\">(Muhammad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib28\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib29\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib27\" title=\"\">2025a</a>)</cite>. The SIB-200 suite provides large-scale topic classification with stronger representation of African languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib1\" title=\"\">2024</a>)</cite>. Intent and slot-filling are addressed by INJONGO <cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib49\" title=\"\">2025</a>)</cite>. These resources underscore the need for region-specific evaluation and supply tasks that align well with sentence-embedding assessment.</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Massive Text Embedding Benchmark (MTEB) introduced a common taxonomy and leaderboard for sentence embeddings <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib26\" title=\"\">2023</a>)</cite>. Since then, several language- or region-specific variants have emerged, including MMTEB (with Europe and Indic tracks), SEA-BED for Southeast Asia, PL-MTEB for Polish, and MTEB-French <cite class=\"ltx_cite ltx_citemacro_citep\">(Ponwitayarat et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib36\" title=\"\">2025</a>; Po&#347;wiata et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib37\" title=\"\">2024</a>; Ciancone et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib9\" title=\"\">2024</a>)</cite>. Additional efforts target specific language families or regions: C-MTEB (Chinese), German-focused suites, JMTEB (Japanese), Korean-focused suites, ruMTEB (Russian), FaMTEB (Persian/Farsi), and VN-MTEB (Vietnamese) <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib48\" title=\"\">2024</a>; Wehrli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib46\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib20\" title=\"\">2024</a>; Snegirev et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib40\" title=\"\">2025</a>; Zinvandi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib51\" title=\"\">2025</a>; Pham et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib35\" title=\"\">2025</a>)</cite>. Our work follows this trend by building an Africa-focused extension with broad task coverage.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Commercial/API models widely used in practice include OpenAI&#8217;s text-embedding-3 series, Google&#8217;s gemini-embedding-001, and Cohere&#8217;s Embed v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Neelakantan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib31\" title=\"\">2022</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib18\" title=\"\">2025</a>)</cite>. Open-weight models are an active research area: me5, e5, and e5-mistral-7b-instruct provide strong general-purpose and instruction-tuned baselines <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">2024c</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib42\" title=\"\">a</a>)</cite>; Qwen3-Embedding and Embedding Gemma offer lightweight multilingual options <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib50\" title=\"\">2025</a>; Vera et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib41\" title=\"\">2025</a>)</cite>; GTE proposes efficient, general-purpose embeddings <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib21\" title=\"\">2023a</a>)</cite>; and classic multilingual encoders LaBSE remain strong references <cite class=\"ltx_cite ltx_citemacro_citep\">(Feng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib13\" title=\"\">2022</a>)</cite>. The recent BGE-M3 model integrates multilingual, multi-function training and is a competitive open baseline <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite>. Despite progress, coverage and performance on many African languages remain uneven, motivating region-specific evaluation and targeted adaptation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "african",
                    "languages",
                    "bgem3",
                    "performance",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented AfriMTEB, a large-scale benchmark for African languages spanning 59 languages and 14 tasks, and AfriE5, an adaptation of mE5-large-instruct via cross-lingual contrastive distillation. AfriE5 achieves state-of-the-art results on both AfriMTEB-Full and Lite, surpassing strong baselines such as Gemini embedding-001. Our ablations show that cross-lingual dataset expansion and balanced translation filtering are crucial for these gains. AfriMTEB and AfriE5 provide standardized evaluation and a strong baseline to advance text embedding research for African languages.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "african",
                    "dataset",
                    "me5largeinstruct",
                    "languages",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AfriMTEB expands coverage to 59 languages, yet many African languages, dialects, orthographies, and code-switched registers remain underrepresented. Several datasets inherit noise or heterogeneity from crowd labels, repurposed tasks, and preprocessing; our adaptation data further relies on machine translation (NLLB-200) and automatic quality estimation (SSA-COMET), which vary in reliability across language pairs and domains. Distillation from a single teacher (BGE Reranker v2 m3) can also transfer its biases, potentially advantaging languages and styles that align with the MT/QE pipeline and the teacher&#8217;s preferences.</p>\n\n",
                "matched_terms": [
                    "african",
                    "across",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation focuses on text-only sentence/paragraph embeddings and macro-averages across tasks and languages; alternative weightings (e.g., by population or application criticality) and additional metrics (calibration, robustness, fairness) could yield different conclusions. End-to-end RAG quality, multimodal retrieval, and very long-context document embeddings are out of scope, and domain coverage skews toward formal/news text over colloquial or specialized domains. True parity with closed-weight baselines (e.g., data mixtures, architectures, inference settings) is infeasible; repeated API evaluations may be affected by nondeterminism, and some open models may be sensitive to prompt formats or poolers we did not exhaustively tune.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, benchmark contamination remains possible because public corpora are widely used in pretraining, which can inflate absolute scores. Our single-GPU fine-tuning recipe favors accessibility over peak performance and may limit stability, and releasing full translation/provenance artefacts is nontrivial due to size and licensing. While AfriE5 trained on nine languages generalizes well to 59 in our tests, transfer may degrade for typologically distant or extremely low-resource languages; broader community datasets and multi-teacher/multi-signal training are promising avenues to mitigate these limitations.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given two sentence sets from different languages, the task is to identify translation pairs. Embeddings are used to compute similarities and find the best match for each sentence.</p>\n\n",
                "matched_terms": [
                    "each",
                    "task",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task involves a pair of input sentences with a binary or categorical relationship label (e.g., entailment vs. contradiction). Predictions are based on embedding similarity.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task measures the degree of semantic similarity between sentence pairs, either within or across languages, based on embedding similarity.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "task",
                    "across",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">mmBERT-base is a massively multilingual ModernBERT encoder trained on <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>T,+ tokens covering <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>1,800 languages, extending the ModernBERT architecture (fast encoder with long context) to the multilingual regime <cite class=\"ltx_cite ltx_citemacro_citep\">(Warner et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib45\" title=\"\">2025</a>)</cite>. The training recipe introduces curriculum-style annealed language learning, inverse masking, and temperature-based sampling to emphasize low-resource languages while retaining strong performance on high-resource ones <cite class=\"ltx_cite ltx_citemacro_citep\">(Marone et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib23\" title=\"\">2025</a>)</cite>. Reported results show that <span class=\"ltx_text ltx_font_typewriter\">mmBERT-base</span> surpasses prior multilingual encoders such as XLM-R on standard NLU and retrieval benchmarks, approaching the English-only ModernBERT on GLUE despite being trained predominantly on non-English data <cite class=\"ltx_cite ltx_citemacro_citep\">(Marone et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib23\" title=\"\">2025</a>)</cite>. Inference follows standard encoder usage (mean pooling over last hidden states and L2 normalization).</p>\n\n",
                "matched_terms": [
                    "performance",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">KaLM-Embedding is a multilingual embedding family that prioritizes training-data quality over sheer scale, combining (i) persona-based synthetic examples distilled from LLMs, (ii) ranking-consistency filtering, and (iii) semi-homogeneous task batching for efficient contrastive learning <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib16\" title=\"\">2025</a>)</cite>. Many public checkpoints are built on compact Qwen2 backbones (e.g., <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.5B) and instruction-tuned variants for downstream retrieval and semantic similarity. Technical reports describe v1.5/v2 updates with improved data curation and training strategy, yielding strong MTEB performance for their size <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib16\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "embedding",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen3-Embedding is a purpose-built series of dense encoders for text embeddings and reranking, offered in 0.6B/4B/8B sizes with a 32k token context window and coverage of 100+ languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib50\" title=\"\">2025</a>)</cite>. The models are instruction-aware and support flexible output dimensionalities (via MRL-style prefix truncation), with typical maximum embedding sizes of 1,024 (0.6B), 2,560 (4B), and 4,096 (8B); matching reranker models are available at each size. Training leverages Qwen3 LLMs both as backbones and as data synthesizers across domains and languages, improving robustness for retrieval/reranking workloads. Inference uses mean pooling and L2 normalization; common deployment stacks expose a user-selectable output dimensionality for storage/latency trade-offs.</p>\n\n",
                "matched_terms": [
                    "each",
                    "across",
                    "models",
                    "languages",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BGE-M3 (BAAI) is a versatile embedding model unifying three capabilities in a single encoder: Multi-Functionality (dense, multi-vector, and sparse retrieval), Multi-Linguality (100+ languages), and Multi-Granularity (robust from short queries to long documents, up to <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>8,192 tokens) <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite>. The training pipeline uses self-knowledge distillation across retrieval functions to align representations and enables hybrid retrieval without switching models <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib8\" title=\"\">2024</a>)</cite>. It is widely used as a strong multilingual baseline for retrieval, clustering, and classification.</p>\n\n",
                "matched_terms": [
                    "across",
                    "models",
                    "languages",
                    "bgem3",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">mE5-large and mE5-large-instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib44\" title=\"\">2024c</a>)</cite> are multilingual members of the E5 family built on XLM-RoBERTa-large <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib10\" title=\"\">2020</a>)</cite>, trained with a two-stage recipe that first performs weakly supervised contrastive pre-training on roughly one billion multilingual text pairs and then supervised fine-tuning on curated embedding tasks; the instruction variant further formats supervision with concise task instructions to specialize representations for retrieval and related tasks. Both use a 24-layer encoder that produces 1,024-dimensional vectors and inherit broad (&#160;100 language) coverage from the XLM-RoBERTa backbone. At inference time they follow the E5 prompt conventions (e.g., query / passage style inputs), apply mean pooling over the last hidden states, and L2-normalize the output, yielding strong performance on retrieval, semantic similarity, clustering, and classification benchmarks in both monolingual and cross-lingual settings.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "embedding",
                    "task",
                    "me5largeinstruct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">GritLM-7B (Contextual AI) unifies generation and embeddings in one model via Generative Representational Instruction Tuning (GRIT) <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib25\" title=\"\">2024</a>)</cite>. A custom modeling component adds bidirectional attention paths so that the same backbone can function as a strong encoder for embeddings without sacrificing generative performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib25\" title=\"\">2024</a>)</cite>. The paper reports SOTA-level MTEB results for 7B-class open models alongside strong generative benchmarks, demonstrating that a single model can excel at both modalities <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib25\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">Linq-Embed-Mistral</span> is a Mistral-7B&#8211;based embedding model developed with task-tailored data crafting, filtering, and hard-negative mining to improve retrieval quality <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib17\" title=\"\">2024</a>)</cite>. The technical report and model card document leading MTEB retrieval scores at release (e.g., average <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px8.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>68.2 on 56 datasets; retrieval score <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px8.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>60.2), achieved through homogeneous task ordering and mixed-task fine-tuning strategies <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib17\" title=\"\">2024</a>)</cite>. It is frequently used as a competitive 7B encoder baseline for dense retrieval.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">gemini-embedding-001 is Google&#8217;s multilingual text-embedding model available via the Gemini API and Vertex AI, trained with Matryoshka Representation Learning (MRL) so that leading vector prefixes remain useful at smaller dimensions <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib18\" title=\"\">2025</a>)</cite>. The default output is 3,072 dimensions, but APIs allow setting output dimensionality (e.g., 1,536 or 768) with minimal quality loss, enabling flexible storage/latency trade-offs <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib18\" title=\"\">2025</a>)</cite>. The model supports 100+ languages and has been a strong performer on multilingual MTEB since its early releases.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the quality of the machine translation data used in AfriMTEB, we rely on automatic evaluation with <span class=\"ltx_text ltx_font_bold\">SSA-COMET</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#bib.bib19\" title=\"\">2025</a>)</cite>. SSA-COMET is a recently released metric trained on <span class=\"ltx_text ltx_font_smallcaps\">SSA-MTE</span>, a large-scale human-annotated evaluation dataset covering 13 African language pairs with over 63,000 sentence-level judgments. Compared to earlier African-focused metrics such as AfriCOMET, SSA-COMET provides stronger correlation with human ratings and better robustness in low-resource settings.</p>\n\n",
                "matched_terms": [
                    "african",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23896v1#A1.T6\" title=\"Table 6 &#8227; A.3 Detailed Statistics of Machine Translation Quality &#8227; Appendix A Appendix &#8227; AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we observe a clear trade-off between dataset size and quality. At a relaxed threshold of 0.67, over 430k sentence pairs are retained, ensuring broad coverage across all nine target languages. Increasing the threshold to 0.75 reduces the pool to around 60k examples, striking a balance between filtering noise and preserving sufficient training data. At the strictest cutoff of 0.80, only 7.5k pairs remain, indicating that high-quality translations are relatively scarce. Language-level differences are also evident: Swahili, Oromo, and Kinyarwanda consistently contribute the largest number of pairs, while Igbo and Amharic see the sharpest reductions under stricter filtering, reflecting variation in MT quality across languages.</p>\n\n",
                "matched_terms": [
                    "all",
                    "dataset",
                    "across",
                    "languages"
                ]
            }
        ]
    }
}