{
    "S2.T1": {
        "source_file": "Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap",
        "caption": "Table 1: Representative benchmarks at a glance. Columns are grouped by primary focus. Legend: ✓present, ⚫partial, ✗not included.",
        "body": "MMAR\n\n\n(Ma et al., 2025)",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">MMAR</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib21\" title=\"\">2025</a>)</cite></td>\n</tr>\n</table>\n",
        "informative_terms_identified": [
            "legend",
            "benchmarks",
            "grouped",
            "⚫partial",
            "mmar",
            "glance",
            "representative",
            "columns",
            "primary",
            "focus",
            "✗not",
            "included",
            "✓present"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Existing voice benchmarks, while valuable, have not evaluated the ability of models to perform general-purpose reasoning through a real-time conversational interface.\nInstead, prior work has focused on two distinct areas: a model&#8217;s ability to understand the acoustic signal itself, and its ability to manage conversational mechanics. Benchmarks like SUPERB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib50\" title=\"\">2021</a>)</cite>, AudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib39\" title=\"\">2024a</a>)</cite>, and even more recent ones like MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib32\" title=\"\">2024</a>)</cite> and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib21\" title=\"\">2025</a>)</cite>, evaluate <span class=\"ltx_text ltx_font_bold\">audio-content understanding, often with reasoning about sound</span>&#8212;tasks such as identifying events from sounds, analyzing acoustic scenes, or answering questions about the properties of the audio signal.\nSeparately, the spoken language understanding (SLU) and spoken-QA literature targets mapping speech to meaning, including intent and slot filling, dialog state tracking, and extractive or conversational QA, with representative corpora such as Spoken SQuAD, ODSQA, Spoken-CoQA, HeySQuAD, and the SLUE suite (Phase-1/2)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib15\" title=\"\">2018b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib14\" title=\"\">a</a>; You et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib51\" title=\"\">2022</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib46\" title=\"\">2023</a>; Shon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib34\" title=\"\">2022</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nThese datasets assess comprehension of recorded speech but generally lack explicit real-time constraints and do not provide text&#8211;versus&#8211;voice comparisons on reasoning problems.\nConcurrently, a separate line of work on full-duplex systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib30\" title=\"\">2025</a>; Arora et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib2\" title=\"\">2025</a>)</cite> has focused on the <span class=\"ltx_text ltx_font_bold\">mechanics of dialogue</span>, such as turn-taking and interruption handling, without evaluating the substantive reasoning that must occur within that conversation. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a comparative overview of representative benchmarks across these areas.</p>\n\n",
            "<p class=\"ltx_p\">As Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates (with a more comprehensive catalog in Appendix Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A1.T4\" title=\"Table 4 &#8227; Appendix A Previous Benchmarks &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), this focus on distinct capabilities has created a clear evaluation gap. The field measures whether a model can <em class=\"ltx_emph ltx_font_italic\">hear</em> (Audio Understanding), <em class=\"ltx_emph ltx_font_italic\">understand</em> spoken language, or <em class=\"ltx_emph ltx_font_italic\">handle</em> interaction mechanics (full-duplex/latency), but not whether it can <span class=\"ltx_text ltx_font_bold\">think on general problems while talking</span>.\nNo existing benchmark combines <span class=\"ltx_text ltx_font_bold\">(1) multi-step, general-purpose reasoning</span> with <span class=\"ltx_text ltx_font_bold\">(2) explicit real-time latency constraints</span> and <span class=\"ltx_text ltx_font_bold\">(3) a direct, cross-modal text&#8211;versus&#8211;voice comparison on identical tasks</span>.\nThis gap helps explain why the severe reasoning degradation we document has gone unquantified.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> is the first to occupy this intersection, providing a focused diagnostic tool for the trade-offs between conversational fluency and reasoning depth in modern voice systems.</p>\n\n"
        ],
        "contextual_paragraphs": []
    },
    "S3.T2": {
        "source_file": "Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap",
        "caption": "Table 2: VERA composition and adaptation statistics. Avg. Duration is the length of the spoken prompt; for the Context track the long evidence is supplied as a separate text document (not spoken).",
        "body": "Track\nEpisodes\nSource Dataset\nDomain\nAvg. Quality\nAvg. Duration\nSpeaking Rate\n\n\nMath\n115\nAIME 2020-2025\nCompetition Math\n8.9\n43.8s\n169.5 WPM\n\n\nWeb\n1,107\nBrowseComp\nInformation Retrieval\n9.2\n40.2s\n172.0 WPM\n\n\nScience\n161\nGPQA Diamond\nGraduate Science\n8.9\n40.2s\n153.7 WPM\n\n\nContext\n548\nMRCR\nCo-reference Resolution\n8.0\n4.2s\n186.1 WPM\n\n\nFactual\n1,000\nSimpleQA\nKnowledge Retrieval\n9.4\n7.8s\n170.1 WPM\n\n\nTotal\n2,931\nMulti-source\nCross-domain\n9.0\n22.6s\n172.9 WPM",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Track</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Episodes</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Source Dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Domain</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Avg. Quality</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Avg. Duration</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Speaking Rate</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Math</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">115</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AIME 2020-2025</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Competition Math</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">8.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">43.8s</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">169.5 WPM</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Web</td>\n<td class=\"ltx_td ltx_align_center\">1,107</td>\n<td class=\"ltx_td ltx_align_center\">BrowseComp</td>\n<td class=\"ltx_td ltx_align_center\">Information Retrieval</td>\n<td class=\"ltx_td ltx_align_center\">9.2</td>\n<td class=\"ltx_td ltx_align_center\">40.2s</td>\n<td class=\"ltx_td ltx_align_center\">172.0 WPM</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Science</td>\n<td class=\"ltx_td ltx_align_center\">161</td>\n<td class=\"ltx_td ltx_align_center\">GPQA Diamond</td>\n<td class=\"ltx_td ltx_align_center\">Graduate Science</td>\n<td class=\"ltx_td ltx_align_center\">8.9</td>\n<td class=\"ltx_td ltx_align_center\">40.2s</td>\n<td class=\"ltx_td ltx_align_center\">153.7 WPM</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Context</td>\n<td class=\"ltx_td ltx_align_center\">548</td>\n<td class=\"ltx_td ltx_align_center\">MRCR</td>\n<td class=\"ltx_td ltx_align_center\">Co-reference Resolution</td>\n<td class=\"ltx_td ltx_align_center\">8.0</td>\n<td class=\"ltx_td ltx_align_center\">4.2s</td>\n<td class=\"ltx_td ltx_align_center\">186.1 WPM</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Factual</td>\n<td class=\"ltx_td ltx_align_center\">1,000</td>\n<td class=\"ltx_td ltx_align_center\">SimpleQA</td>\n<td class=\"ltx_td ltx_align_center\">Knowledge Retrieval</td>\n<td class=\"ltx_td ltx_align_center\">9.4</td>\n<td class=\"ltx_td ltx_align_center\">7.8s</td>\n<td class=\"ltx_td ltx_align_center\">170.1 WPM</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Total</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2,931</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Multi-source</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Cross-domain</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">9.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">22.6s</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">172.9 WPM</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "spoken",
            "quality",
            "source",
            "text",
            "78s",
            "rate",
            "avg",
            "retrieval",
            "math",
            "42s",
            "length",
            "evidence",
            "knowledge",
            "statistics",
            "information",
            "track",
            "gpqa",
            "adaptation",
            "document",
            "supplied",
            "simpleqa",
            "226s",
            "multisource",
            "wpm",
            "not",
            "402s",
            "web",
            "438s",
            "competition",
            "duration",
            "dataset",
            "browsecomp",
            "context",
            "long",
            "vera",
            "episodes",
            "speaking",
            "domain",
            "prompt",
            "resolution",
            "composition",
            "science",
            "factual",
            "separate",
            "crossdomain",
            "graduate",
            "total",
            "diamond",
            "aime",
            "mrcr",
            "coreference"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">VERA</span> comprises 2,931 voice-optimized episodes that are systematically derived from five established benchmarks, with detailed statistics for each track presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S3.T2\" title=\"Table 2 &#8227; 3.3 Dataset Composition &#8227; 3 The VERA Benchmark &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Voice Evaluation of Reasoning Ability (<span class=\"ltx_text ltx_font_smallcaps\">VERA</span>), a benchmark for evaluating <em class=\"ltx_emph ltx_font_italic\">reasoning</em> ability in voice-interactive systems under real-time conversational constraints.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> comprises 2,931 voice-native episodes derived from established text benchmarks and organized into five tracks (Math, Web, Science, Long-Context, Factual).\nEach item is adapted for speech interaction while preserving reasoning difficulty.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> enables direct text&#8211;voice comparison within model families and supports analysis of how architectural choices affect reliability.\nWe assess 12 contemporary voice systems alongside strong text baselines and observe large, consistent modality gaps: on competition mathematics a leading text model attains 74.8% accuracy while its voice counterpart reaches 6.1%; macro-averaged across tracks the best text models achieve 54.0% versus 11.3% for voice.\nLatency&#8211;accuracy analyses reveal a low-latency plateau, where fast voice systems cluster around <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>10% accuracy, while approaching text performance requires sacrificing real-time interaction.\nDiagnostic experiments indicate that common mitigations are insufficient.\nIncreasing &#8220;thinking time&#8221; yields negligible gains; a decoupled cascade that separates reasoning from narration improves accuracy but still falls well short of text and introduces characteristic grounding/consistency errors.\nFailure analyses further show distinct error signatures across native streaming, end-to-end, and cascade designs.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> provides a reproducible testbed and targeted diagnostics for architectures that decouple <em class=\"ltx_emph ltx_font_italic\">thinking</em> from <em class=\"ltx_emph ltx_font_italic\">speaking</em>, offering a principled way to measure progress toward real-time voice assistants that are both fluent and reliably reasoned.</p>\n\n",
                "matched_terms": [
                    "competition",
                    "text",
                    "science",
                    "factual",
                    "math",
                    "web",
                    "vera",
                    "episodes",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a systematic evaluation of reasoning in today&#8217;s voice-interactive systems, documenting a significant and consistent performance degradation we term the Voice Reasoning Gap (<span class=\"ltx_text ltx_font_smallcaps\">VRG</span>). This gap is most pronounced on complex, multi-step reasoning tasks. For example, in our study, a leading voice assistant, GPT-realtime&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib27\" title=\"\">2024b</a>)</cite>, achieves 6.1% accuracy on mathematical problems, whereas a top-performing text model from the same developer, GPT-5&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib29\" title=\"\">2025b</a>)</cite>, achieves 74.8%. This 68.7-point difference is not an isolated finding but is representative of a broader pattern where models optimized for low-latency streaming show consistently lower performance.</p>\n\n",
                "matched_terms": [
                    "text",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The problem does not appear to be purely acoustic. Existing benchmarks show that current voice models are highly proficient at audio understanding, capable of transcribing speech with near-human accuracy and analyzing complex acoustic scenes&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib50\" title=\"\">2021</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib39\" title=\"\">2024a</a>)</cite>. While these capabilities confirm that the models can effectively &#8220;hear&#8221; a user&#8217;s request, they are separate from the cognitive processes required for general-purpose reasoning. We hypothesize that the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> is instead a consequence of a fundamental architectural tension: the design of real-time voice systems, which prioritizes an <span class=\"ltx_text ltx_font_italic\">irreversible, low-latency stream of audio</span>, is in direct conflict with the <span class=\"ltx_text ltx_font_italic\">iterative, revisable computation</span> that underpins complex reasoning in text-based models.</p>\n\n",
                "matched_terms": [
                    "separate",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate this hypothesis, we introduce the <span class=\"ltx_text ltx_font_bold\">Voice Evaluation of Reasoning Ability (<span class=\"ltx_text ltx_font_smallcaps\">VERA</span>)</span>, a benchmark designed to measure reasoning under real-time constraints. Our analysis with <span class=\"ltx_text ltx_font_smallcaps\">VERA</span>, summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, reveals a clear <span class=\"ltx_text ltx_font_italic\">latency-accuracy trade-off</span>. The data shows a <span class=\"ltx_text ltx_font_bold\">low-latency plateau</span>, where the fastest voice models remain shallow in their reasoning, and a <span class=\"ltx_text ltx_font_bold\">cascade lift, not parity</span>, where even a powerful text reasoner decoupled from the voice interface improves accuracy but still falls significantly short of its native text performance. Together, these patterns demonstrate that the ideal <span class=\"ltx_text ltx_font_italic\">fast-and-accurate upper-left corner of the frontier remains empty</span>, suggesting the gap is a systemic challenge for current architectures, not merely an efficiency issue. This work provides a framework for diagnosing these trade-offs, complementing (rather than replacing) existing audio-understanding evaluation. Our analysis uncovers distinct failure signatures tied to system architecture; for instance, native streaming models tend to produce fluent but incorrect responses, while decoupled cascades are more prone to grounding errors. The patterns we observe highlight key opportunities for improvement and suggest promising research directions. Our main contributions are:</p>\n\n",
                "matched_terms": [
                    "text",
                    "vera",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Characterizes distinct failure signatures tied to voice architectures.</span> Through analysis of 2,931 episodes, we provide the first systematic evidence showing that different voice system designs (e.g., native streaming vs. decoupled cascade) fail in predictably different ways, creating a diagnostic fingerprint for the underlying architectural trade-offs.</p>\n\n",
                "matched_terms": [
                    "episodes",
                    "evidence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing voice benchmarks, while valuable, have not evaluated the ability of models to perform general-purpose reasoning through a real-time conversational interface.\nInstead, prior work has focused on two distinct areas: a model&#8217;s ability to understand the acoustic signal itself, and its ability to manage conversational mechanics. Benchmarks like SUPERB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib50\" title=\"\">2021</a>)</cite>, AudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib39\" title=\"\">2024a</a>)</cite>, and even more recent ones like MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib32\" title=\"\">2024</a>)</cite> and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib21\" title=\"\">2025</a>)</cite>, evaluate <span class=\"ltx_text ltx_font_bold\">audio-content understanding, often with reasoning about sound</span>&#8212;tasks such as identifying events from sounds, analyzing acoustic scenes, or answering questions about the properties of the audio signal.\nSeparately, the spoken language understanding (SLU) and spoken-QA literature targets mapping speech to meaning, including intent and slot filling, dialog state tracking, and extractive or conversational QA, with representative corpora such as Spoken SQuAD, ODSQA, Spoken-CoQA, HeySQuAD, and the SLUE suite (Phase-1/2)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib15\" title=\"\">2018b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib14\" title=\"\">a</a>; You et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib51\" title=\"\">2022</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib46\" title=\"\">2023</a>; Shon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib34\" title=\"\">2022</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nThese datasets assess comprehension of recorded speech but generally lack explicit real-time constraints and do not provide text&#8211;versus&#8211;voice comparisons on reasoning problems.\nConcurrently, a separate line of work on full-duplex systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib30\" title=\"\">2025</a>; Arora et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib2\" title=\"\">2025</a>)</cite> has focused on the <span class=\"ltx_text ltx_font_bold\">mechanics of dialogue</span>, such as turn-taking and interruption handling, without evaluating the substantive reasoning that must occur within that conversation. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a comparative overview of representative benchmarks across these areas.</p>\n\n",
                "matched_terms": [
                    "separate",
                    "spoken",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates (with a more comprehensive catalog in Appendix Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A1.T4\" title=\"Table 4 &#8227; Appendix A Previous Benchmarks &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), this focus on distinct capabilities has created a clear evaluation gap. The field measures whether a model can <em class=\"ltx_emph ltx_font_italic\">hear</em> (Audio Understanding), <em class=\"ltx_emph ltx_font_italic\">understand</em> spoken language, or <em class=\"ltx_emph ltx_font_italic\">handle</em> interaction mechanics (full-duplex/latency), but not whether it can <span class=\"ltx_text ltx_font_bold\">think on general problems while talking</span>.\nNo existing benchmark combines <span class=\"ltx_text ltx_font_bold\">(1) multi-step, general-purpose reasoning</span> with <span class=\"ltx_text ltx_font_bold\">(2) explicit real-time latency constraints</span> and <span class=\"ltx_text ltx_font_bold\">(3) a direct, cross-modal text&#8211;versus&#8211;voice comparison on identical tasks</span>.\nThis gap helps explain why the severe reasoning degradation we document has gone unquantified.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> is the first to occupy this intersection, providing a focused diagnostic tool for the trade-offs between conversational fluency and reasoning depth in modern voice systems.</p>\n\n",
                "matched_terms": [
                    "document",
                    "vera",
                    "spoken",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study provides a <span class=\"ltx_text ltx_font_bold\">diagnostic characterization</span> of the current voice systems&#8217; landscape, not a controlled experiment designed to prove causality. Because we evaluate heterogeneous commercial systems with different architectures and training objectives, <span class=\"ltx_text ltx_font_bold\">we cannot isolate the causal impact of modality alone</span>. Rather, our goal is to systematically document system performance and identify recurring, cross-model patterns that point toward underlying architectural challenges. The consistency of the gap we find across 12 systems strongly suggests that these challenges are fundamental and merit this investigation, for which we provide a reproducible benchmark.</p>\n\n",
                "matched_terms": [
                    "document",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Voice suitability filter.</span>\nFor each source question, a filtering agent screens for (i) <em class=\"ltx_emph ltx_font_italic\">visual dependence</em> (must not require diagrams/tables), (ii) <em class=\"ltx_emph ltx_font_italic\">audio memory load</em> (3&#8211;4 salient entities), (iii) <em class=\"ltx_emph ltx_font_italic\">multi-step structure</em> (interruptible reasoning), and (iv) <em class=\"ltx_emph ltx_font_italic\">articulatory feasibility</em> (clear tokenization for TTS). Items failing any criterion are excluded.</p>\n\n",
                "matched_terms": [
                    "not",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An episode is retained iff <math alttext=\"Q_{\\text{overall}}\\geq\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><msub><mi>Q</mi><mtext>overall</mtext></msub><mo>&#8805;</mo><mi>&#964;</mi></mrow><annotation encoding=\"application/x-tex\">Q_{\\text{overall}}\\geq\\tau</annotation></semantics></math> and <math alttext=\"Q_{\\text{reason}}\\geq 7.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><mrow><msub><mi>Q</mi><mtext>reason</mtext></msub><mo>&#8805;</mo><mn>7.0</mn></mrow><annotation encoding=\"application/x-tex\">Q_{\\text{reason}}\\geq 7.0</annotation></semantics></math>, with <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> set by track difficulty (7.0&#8211;8.5). The quality score <math alttext=\"Q_{\\text{overall}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>Q</mi><mtext>overall</mtext></msub><annotation encoding=\"application/x-tex\">Q_{\\text{overall}}</annotation></semantics></math> represents the LLM validator&#8217;s assessment on a 0-10 scale, with accepted episodes achieving a mean score of 9.0.</p>\n\n",
                "matched_terms": [
                    "track",
                    "episodes",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech generation.</span>\nValidated text episodes are rendered to 24kHz audio using Higgs-Audio v2 <cite class=\"ltx_cite ltx_citemacro_cite\">Boson AI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib3\" title=\"\">2025</a>)</cite>, which generates naturalistic speech with automatic variation in timbre, tone, and emotion based on textual content. This TTS system produces acoustically diverse outputs through its inherent voice variation, ensuring models are evaluated on reasoning rather than adaptation to specific acoustic patterns (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S3.SS3\" title=\"3.3 Dataset Composition &#8227; 3 The VERA Benchmark &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a> for diversity analysis).</p>\n\n",
                "matched_terms": [
                    "text",
                    "adaptation",
                    "episodes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our benchmark is structured around five complementary tracks, each designed to isolate a distinct failure mode in voice-based reasoning. <span class=\"ltx_text ltx_font_bold\">Mathematical reasoning</span>, using 115 problems from the AIME math competition&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mathematical Association of America, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib22\" title=\"\">2025</a>)</cite>, tests solution coherence while speaking. <span class=\"ltx_text ltx_font_bold\">Web-grounded synthesis</span>, with 1,107 questions from the BrowseComp web-navigation benchmark&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib45\" title=\"\">2025</a>)</cite>, evaluates information integration under streaming constraints. <span class=\"ltx_text ltx_font_bold\">Scientific expertise</span>, drawn from 161 graduate-level GPQA Diamond questions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib31\" title=\"\">2023</a>)</cite>, probes knowledge access under the cognitive load of simultaneous speech generation. <span class=\"ltx_text ltx_font_bold\">Long-context memory</span>, using 548 MRCR episodes&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib28\" title=\"\">2025a</a>)</cite> with contexts up to 100K characters, examines state tracking during extended interactions. Finally, a crucial baseline of <span class=\"ltx_text ltx_font_bold\">Factual recall</span>, with 1,000 SimpleQA questions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib44\" title=\"\">2024</a>)</cite>, isolates architectural overhead from reasoning complexity.</p>\n\n",
                "matched_terms": [
                    "competition",
                    "gpqa",
                    "simpleqa",
                    "factual",
                    "browsecomp",
                    "diamond",
                    "aime",
                    "math",
                    "mrcr",
                    "knowledge",
                    "episodes",
                    "information",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The creation of these 2,931 episodes involved a rigorous curation process that filtered approximately 22,000 source items to prioritize diagnostic clarity. Each adapted episode first achieved a mean quality score of 9.0, as assessed by an LLM validator, before being rendered to 24kHz audio using Higgs-Audio v2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Boson AI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib3\" title=\"\">2025</a>)</cite>. This TTS system is critical to the benchmark&#8217;s design, as it automatically varies timbre, tone, and emotion based on textual content to produce acoustically diverse speech. To ensure the final benchmark&#8217;s integrity, we validated both its semantic and acoustic properties. A manual audit of 200 episodes (6.8%) confirmed that semantic and logical structures were preserved, while an analysis of speaker embeddings using WeSpeaker&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib40\" title=\"\">2023a</a>)</cite> verified the acoustic diversity of the generated audio (<math alttext=\"\\mu=0.000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#956;</mi><mo>=</mo><mn>0.000</mn></mrow><annotation encoding=\"application/x-tex\">\\mu=0.000</annotation></semantics></math>, <math alttext=\"\\sigma=0.120\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><mi>&#963;</mi><mo>=</mo><mn>0.120</mn></mrow><annotation encoding=\"application/x-tex\">\\sigma=0.120</annotation></semantics></math>), confirming the absence of systemic acoustic bias.</p>\n\n",
                "matched_terms": [
                    "episodes",
                    "quality",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Fidelity Assessment.</span> We evaluate generated speech using Word Error Rate (WER), comparing ASR transcripts against ground truth. Our LLM-based normalizer standardizes both the reference text and ASR\ntranscript to canonical mathematical notation (e.g., &#8220;f of sixteen equals fifty four&#8221; &#8594; &#8220;f(16) = 54&#8221;, &#8220;twenty twenty-four&#8221; &#8594; &#8220;2024&#8221;) before comparison. This normalization, with further examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A2\" title=\"Appendix B ASR Transcript Normalization &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>, ensures a fair comparison between mathematical expressions in written form and their spoken equivalents&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sproat &amp; Jaitly, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib37\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "spoken",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Accuracy Evaluation.</span> We assess task accuracy using an LLM-as-a-judge protocol&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib52\" title=\"\">2023</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib20\" title=\"\">2023</a>)</cite>. This approach is highly effective for <span class=\"ltx_text ltx_font_smallcaps\">VERA</span> because our benchmark tasks, while challenging, are designed to have <span class=\"ltx_text ltx_font_bold\">well-defined ground truth answers with minimal ambiguity</span>, making them suitable for reliable automated grading. We employ GPT-4o&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib26\" title=\"\">2024a</a>)</cite> as the grader, using the normalized ASR transcript for voice model outputs. Each prediction undergoes <span class=\"ltx_text ltx_font_bold\">three independent evaluations</span> to mitigate judgment stochasticity, with the final label (Correct, Incorrect, or Not Attempted) determined by majority vote.</p>\n\n",
                "matched_terms": [
                    "vera",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Calibration.</span> To validate our LLM-based evaluation, we conducted human evaluation on 1,000 randomly sampled predictions across all tracks and models. GPT-4o&#8217;s judgments achieved 97.8% agreement with human evaluation (95% CI: 96.8-98.7%), ranging from perfect agreement on Math (100%) to 84.3% on Science where answers require more nuanced interpretation. Cross-vendor validation using Gemini-2.5-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Google Cloud, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib10\" title=\"\">2025a</a>)</cite> achieved 98.7% agreement with human evaluation and 98.1% with GPT-4o, confirming minimal vendor bias and consistent evaluation standards across judges. Detailed analyses are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A3\" title=\"Appendix C Human Evaluation and Judge Validation &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "science",
                    "math"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To diagnose the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span>, we evaluate a comprehensive set of models on the <span class=\"ltx_text ltx_font_smallcaps\">VERA</span> benchmark. Our evaluation spans three categories of voice systems: <span class=\"ltx_text ltx_font_bold\">commercial realtime APIs</span> (GPT-realtime, Gemini-2.5-Flash-audio, Amazon Nova Sonic); <span class=\"ltx_text ltx_font_bold\">open voice models</span> (Qwen2-Audio, UltraVox, Audio Flamingo 3, Phi-4-multimodal); and <span class=\"ltx_text ltx_font_bold\">end-to-end architectures</span> that directly generate speech (Moshi, Freeze-Omni, Qwen2.5-Omni). Against these, we benchmark two critical references to isolate the source of the performance drop. First, a <span class=\"ltx_text ltx_font_bold\">text-only upper bound</span> (GPT-4o, GPT-5, Gemini-2.5 Pro/Flash) quantifies maximum achievable accuracy by isolating reasoning capacity from modality constraints. Second, we construct a sophisticated <span class=\"ltx_text ltx_font_bold\">cascade baseline</span>, <em class=\"ltx_emph ltx_font_italic\">LiveAnswer</em>, to simulate an architecture that separates deep reasoning from real-time narration. <em class=\"ltx_emph ltx_font_italic\">LiveAnswer</em> uses GPT-5 as a powerful core reasoner and a faster Llama-3.3-70B-Instruct as a narration synthesizer to convert the detailed reasoning into a concise, fluent spoken response, allowing us to test whether the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> persists even when thinking and speaking are decoupled. Full implementation details and citations for all models are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A4\" title=\"Appendix D Model Implementation Details &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "vera",
                    "spoken",
                    "speaking",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation (table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.T3\" title=\"Table 3 &#8227; 5.1 What is the gap and how does it vary by task? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) reveals a stark <span class=\"ltx_text ltx_font_smallcaps\">VRG</span>: an average accuracy drop of 40.4 percentage points for voice models that widens dramatically on tasks requiring complex, multi-step reasoning.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Unless otherwise stated, gaps are computed against the text baseline (GPT-5, effort=low) while the text upper bound refers to GPT-5 (effort=high) and is shown as the dashed line in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</span></span></span>\nThis gap scales systematically with the complexity of the reasoning required.\nFor instance, while factual retrieval shows moderate degradation (GPT-5 text: 48.3% vs. GPT-realtime voice: 27.4%), the gap widens dramatically for tasks requiring multi-step reasoning, with mathematical reasoning exhibiting a near-total collapse in performance (GPT-5: 74.8% vs. GPT-realtime: 6.1%).\nThis suggests that certain tasks, such as the multi-hop synthesis required in our Web track, become particularly intractable under the constraints of a streaming voice interface.\nStatistical validation using McNemar&#8217;s test <cite class=\"ltx_cite ltx_citemacro_citep\">(McNemar, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib23\" title=\"\">1947</a>)</cite> confirms these differences are highly significant (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), as detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A5\" title=\"Appendix E Statistical Validation &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.\nThis pattern of differential failure extends universally across the diverse voice architectures we evaluated.\nThey consistently perform best on retrieval or short-answer tasks while failing on complex reasoning. GPT-realtime achieves its highest score on Factual questions (27.4%) but drops to 6.1% on Math.\nSome models exhibit extreme specialization; UltraVox, for example, maintains 26.6% accuracy on Context while scoring 0.0% on Math, suggesting an optimization for conversational continuity at the expense of deep reasoning.\nThis trend holds for Gemini&#8217;s audio model (18.8% on Context vs. 3.5% on Math) and open-source models like Phi-4-multimodal (12.0% on Context vs. 0.0% on Math).\nThis consistent pattern across 12 diverse voice systems demonstrates that the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> is not a model-specific artifact but a universal property of current voice technology, with the gap scaling systematically from moderate on simple retrieval tasks to severe on complex reasoning.</p>\n\n",
                "matched_terms": [
                    "track",
                    "text",
                    "factual",
                    "context",
                    "retrieval",
                    "math",
                    "not",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.F4\" title=\"Figure 4 &#8227; 5.1 What is the gap and how does it vary by task? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> further demonstrates this pattern for several model families.\nPanel (a) shows GPT-5 text maintaining robust multi-domain performance (54% radar chart coverage) while GPT-realtime voice achieves only 11% coverage, with moderate performance on Factual (27.4%) but severe weakness across reasoning tasks.\nPanel (b) confirms generalization to Gemini models, with text variants achieving 40-50% coverage versus 11% for voice.\nPanel (c) reveals that even diverse voice architectures&#8212;including an <em class=\"ltx_emph ltx_font_italic\">audio-encoder + LLM text-decoder</em> design (Qwen2-Audio), an <em class=\"ltx_emph ltx_font_italic\">end-to-end Thinker&#8211;Talker</em> model that jointly generates text and speech (Qwen2.5-Omni), and a Whisper-style encoder + LLM with on-demand reasoning (Audio Flamingo 3)&#8212;remain confined below 5% accuracy on reasoning tasks.\nThe variance within voice models (<math alttext=\"\\sigma^{2}=3.66\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><msup><mi>&#963;</mi><mn>2</mn></msup><mo>=</mo><mn>3.66</mn></mrow><annotation encoding=\"application/x-tex\">\\sigma^{2}=3.66</annotation></semantics></math> across Math scores) is 171&#215; smaller than between modalities (<math alttext=\"\\sigma^{2}=625.92\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><msup><mi>&#963;</mi><mn>2</mn></msup><mo>=</mo><mn>625.92</mn></mrow><annotation encoding=\"application/x-tex\">\\sigma^{2}=625.92</annotation></semantics></math>), confirming that architectural variations within the voice paradigm produce marginal improvements compared to the fundamental gap.\nThis pattern holds even for models featuring a &#8220;thinking mode,&#8221; which, as our analysis in Section 5.2 shows, fails to improve reasoning despite a significant increase in latency.</p>\n\n",
                "matched_terms": [
                    "text",
                    "math",
                    "factual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our diagnostic experiments indicate the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> stems not from simple engineering limitations, but from a deeper architectural conflict between real-time streaming and complex reasoning.\nFirst, extended thinking time provides negligible benefit: Audio Flamingo 3&#8217;s thinking mode increases latency from 2.40s to 15.14s (a 530% increase) to allow internal deliberation before speaking, yet accuracy actually decreases from 1.7% to 1.5% overall while Context performance degrades from 3.8% to 1.8%.\nThe latency-accuracy frontier in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> confirms this pattern across all models, showing voice systems plateau below 10% accuracy regardless of response time, with no voice systems achieving both sub-1.5s latency and above-11% accuracy.\nSecond, the LiveAnswer cascade experiment isolates the modality penalty by using the same powerful GPT-5 model as our text upper bound.\nEven in this ideal setup, a persistent 15.7 percentage point gap remained on the Math track (59.1% vs. the text model&#8217;s 74.8%).\nThis drop is largely attributable to the narration synthesizer, which must translate the reasoner&#8217;s complex output into fluent speech periodically.\nThis translation process introduced logical inconsistencies and was particularly detrimental to tasks requiring exact string matching, causing a near-total failure on the Context track (0.2%).\nAs detailed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.F5\" title=\"Figure 5 &#8227; 5.2 Why does the gap exist? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the time-to-first-response for this system averages 10.5s, dominated by the Speech-to-Text step.\nThis demonstrates that even a sophisticated, decoupled architecture still cannot fully close the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span>, reinforcing the need for more fundamental architectural innovation to bridge the gap between deep reasoning and real-time narration.\nThird, output quality measurements confirm that speech synthesis is not the bottleneck: speech clarity does not determine success, as models across the WER spectrum from 7.9% (Gemini-2.5-Flash-Audio) to 19.8% (Freeze-Omni) show uniformly poor reasoning performance.\nCollectively, these diagnostic experiments demonstrate that the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> is not a simple engineering artifact that can be fixed by allocating more time, decoupling the architecture, or improving speech quality. The persistence of the gap across these conditions points instead to a fundamental constraint in how current streaming architectures support multi-step computation.</p>\n\n",
                "matched_terms": [
                    "track",
                    "quality",
                    "text",
                    "context",
                    "math",
                    "not",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings indicate that achieving human-level reasoning in voice assistants will require architectural innovations beyond incremental improvements.\nThe convergent evidence from our analysis establishes that the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> appears not to be explained by the engineering factors we ablate, indicating architectural changes may be needed.\nThe 40.4 percentage point average gap resists all conventional solutions, single models show large performance differentials between retrieval and reasoning, and even architectural decoupling yields an irreducible 15.7-point penalty.\nThe systematic failure patterns in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.F6\" title=\"Figure 6 &#8227; 5.3 How do the models fail differently? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, particularly streaming commitment errors&#8212;manifesting primarily as <span class=\"ltx_text ltx_font_smallcaps\">off_target</span> and <span class=\"ltx_text ltx_font_smallcaps\">no_final_answer</span> deviations that <em class=\"ltx_emph ltx_font_italic\">vary by architecture</em> (underproduced for native voice, overproduced for cascades)&#8212;mechanistically explain why incremental improvements cannot bridge this gap.\nThese findings point toward our central design principle: architectures must decouple thinking from speaking through an editable internal state separate from the speech output buffer.\nThis principle suggests several research directions including asynchronous architectures <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib18\" title=\"\">2025c</a>)</cite> where backend reasoning models operate with higher latency while frontend verbalizers maintain conversational flow, and chunked reasoning with parallel processing <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib5\" title=\"\">2025</a>)</cite> where models use audio playback time to compute next reasoning steps.\nOur LiveAnswer analysis (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.F5\" title=\"Figure 5 &#8227; 5.2 Why does the gap exist? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) reveals specific engineering challenges: managing the latency-accuracy trade-off through streaming ASR with confidence-gated handoff and answer-first narration strategies, and ensuring cross-stage consistency to prevent the grounding failures (<span class=\"ltx_text ltx_font_smallcaps\">unsupported_fact</span> at +0.27) that arise when decoupling modules.\nAchieving human-like reasoning in voice assistants ultimately requires unique architectures that strategically combine pre-computation, parallel processing, and selective verbalization to deliver systems that are both deeply intelligent and naturally conversational.</p>\n\n",
                "matched_terms": [
                    "separate",
                    "retrieval",
                    "not",
                    "evidence",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work systematically documents and diagnoses the Voice Reasoning Gap, a significant and consistent performance drop observed when current language models operate through a voice interface compared to text.\nUsing our purpose-built benchmark, <span class=\"ltx_text ltx_font_smallcaps\">VERA</span>, we provide the first quantitative characterization of this gap across a range of models and complex reasoning tasks.\nOur diagnostic experiments show that this performance degradation is not a simple engineering artifact, as it persists even when granting models extended thinking time, ensuring high audio fidelity, or employing a sophisticated cascade architecture that separates the reasoning core from audio I/O.\nInstead, our analysis suggests a fundamental tension between the architectural demands of low-latency streaming and the iterative, revisable computation required for deep reasoning.\nWe identified distinct failure signatures tied to different architectures, finding that native streaming models tend to fail by producing fluent but incorrect responses, while decoupled systems introduce grounding and consistency errors.\nThese findings indicate that bridging the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> will likely require a paradigm shift away from monolithic architectures toward novel systems that explicitly decouple reasoning from real-time narration.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> provides a critical diagnostic tool to guide and measure progress toward this goal, paving the way for voice assistants that are not only fluent but also genuinely intelligent.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vera",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Organization</span> This Appendix provides comprehensive details on benchmark construction, evaluation methodology, and additional analyses not covered in the main paper. The sections are ordered following their introduction in the main text, with supplementary materials at the end. The document is organized as follows:</p>\n\n",
                "matched_terms": [
                    "text",
                    "not",
                    "document"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Filter, adaptation, quality check, and grading prompts</p>\n\n",
                "matched_terms": [
                    "adaptation",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Source dataset statistics and adaptation details</p>\n\n",
                "matched_terms": [
                    "statistics",
                    "adaptation",
                    "dataset",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure fair comparison between spoken and written mathematical expressions, we employ an LLM-based normalizer that converts both ASR transcripts and reference texts to canonical mathematical notation before computing Word Error Rate (WER). This approach handles the complex variety of ways mathematical content can be verbalized.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use GPT-4o with a deterministic prompt to normalize spoken mathematical expressions into standard notation. The normalizer is instructed to:</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This LLM-based normalization ensures that WER reflects genuine transcription errors rather than superficial formatting differences between spoken and written mathematical expressions. The same normalization is applied to both the ground truth and ASR output to maintain consistency. The full normalization prompt is available in our released code repository.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sampled 1,000 model outputs stratified across tracks (Math: 46, Web: 490, Science: 70, Factual: 394) for human validation. Each output was evaluated as correct or incorrect given the ground truth answer.</p>\n\n",
                "matched_terms": [
                    "science",
                    "web",
                    "math",
                    "factual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The near-perfect agreement on Math, Web, and Factual tracks reflects the objective nature of these tasks with clear correct answers. The lower but still strong agreement on Science (84.3-92.9%) appropriately captures the greater interpretive complexity in graduate-level scientific reasoning. These validation results confirm that our LLM-based evaluation provides reliable and consistent judgments aligned with human assessment.</p>\n\n",
                "matched_terms": [
                    "science",
                    "web",
                    "math",
                    "factual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GPT-realtime.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib27\" title=\"\">2024b</a>)</cite> A commercial, full-duplex voice model with streaming audio input and low-latency speech output. We use it as a native voice baseline: the model listens while speaking, produces incremental audio tokens, and has no separate text-reasoning stage exposed to the user. It serves as a representative of end-to-end, latency-optimized voice agents.</p>\n\n",
                "matched_terms": [
                    "separate",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Nova-Sonic.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Amazon Web Services, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib1\" title=\"\">2025</a>)</cite> A commercial real-time voice system with streaming speech in/out. We include it to broaden the coverage of native, production-grade voice agents. We do not modify decoding parameters beyond the provider defaults.</p>\n\n",
                "matched_terms": [
                    "web",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Core Reasoner.</span> The first module is the <span class=\"ltx_text ltx_font_typewriter\">ProblemSolver</span>, which serves as the powerful but potentially slow cognitive core of the system. It is responsible for the actual problem-solving, leveraging <span class=\"ltx_text ltx_font_bold\">GPT-5</span> through its responses endpoint. This module is equipped with tools like web search and a code interpreter to handle complex, multi-hop reasoning tasks. Instead of generating a single, final text block, the solver produces a stream of structured &#8220;thoughts&#8221; that represent its internal state. This includes reasoning summaries, tool call invocations, and finally, the computed answer. These thoughts are not sent directly to the user but are pushed to the Narration Synthesizer via the <span class=\"ltx_text ltx_font_typewriter\">push_thought</span> method.</p>\n\n",
                "matched_terms": [
                    "text",
                    "web",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Incremental Updates:</span> As the Core Reasoner pushes new thoughts (e.g., updates from a web search), the synthesizer incorporates this new information into its ongoing narration. It includes logic to generate natural-sounding filler text (e.g., &#8220;I&#8217;m still thinking about this&#8230;&#8221;) if the Core Reasoner is taking a long time between thoughts, preventing awkward silences.</p>\n\n",
                "matched_terms": [
                    "text",
                    "long",
                    "web",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">WER Analysis.</span> We run ASR on model-generated speech and apply an LLM-based normalizer to canonicalize spoken math and notation before scoring.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "math"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Configuration Notes.</span> For all voice-native systems we enable streaming and full-duplex whenever supported by the provider. Unless otherwise stated, we do not allow web tools or retrieval beyond what the model natively exposes. Text upper bounds are evaluated with the same prompts and answer formats as their voice counterparts, differing only in modality and (for &#8220;effort=high&#8221;) decode-time budget.</p>\n\n",
                "matched_terms": [
                    "text",
                    "web",
                    "retrieval",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All primary comparisons show highly significant differences (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), confirming that the Voice Reasoning Gap is not due to measurement noise or random variation. The gap persists even when using identical text models with voice I/O wrappers (LiveAnswer), indicating that modality constraints rather than model capacity drive the performance degradation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>The Web track shows no significant difference in the LiveAnswer comparison (p = 0.636), likely due to low baseline performance (<math alttext=\"\\approx 12\\%\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>12</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 12\\%</annotation></semantics></math>) in both modalities.</span></span></span></p>\n\n",
                "matched_terms": [
                    "text",
                    "track",
                    "web",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Note on anomalies:</span> The Web track shows no significant difference in the LiveAnswer comparison (<math alttext=\"p=0.636\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.636</mn></mrow><annotation encoding=\"application/x-tex\">p=0.636</annotation></semantics></math>), likely because both modalities struggle equally with multi-hop synthesis where base performance is low (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>12%). The Context track exhibits anomalously low LiveAnswer performance (0.2%), suggesting a possible system-specific failure that warrants investigation.</p>\n\n",
                "matched_terms": [
                    "track",
                    "context",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Source: 120 problems from AIME 2020-2025 (8 examination sittings)\n<br class=\"ltx_break\"/>Excluded: 5 problems requiring geometric diagrams or extensive symbolic manipulation\n<br class=\"ltx_break\"/>Retained: 115 problems\n<br class=\"ltx_break\"/>Key constraints: Integer answers in range [0, 999] for pronunciation clarity\n<br class=\"ltx_break\"/>Verbalization example: <math alttext=\"x^{2}+3x-2\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></mrow><mo>&#8722;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">x^{2}+3x-2</annotation></semantics></math> rendered as &#8220;x squared plus three x minus two&#8221;</p>\n\n",
                "matched_terms": [
                    "aime",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Retained: 1,107 episodes\n<br class=\"ltx_break\"/>Adaptation: URL citations transformed to spoken attributions (e.g., &#8220;according to a 2014 journal article&#8221;)</p>\n\n",
                "matched_terms": [
                    "episodes",
                    "adaptation",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Source: 198 questions from GPQA Diamond subset\n<br class=\"ltx_break\"/>Domain distribution: Physics (61), Chemistry (52), Biology (48)\n<br class=\"ltx_break\"/>Excluded: 37 questions with visual dependencies (chemical structures, circuit schematics, complex derivations)\n<br class=\"ltx_break\"/>Retained: 161 questions\n<br class=\"ltx_break\"/>Performance baseline: PhD experts 65%, skilled non-experts with web access 34%\n<br class=\"ltx_break\"/>Notation adaptation: H<sub class=\"ltx_sub\">2</sub>SO<sub class=\"ltx_sub\">4</sub> verbalized as &#8220;H two S O four&#8221;</p>\n\n",
                "matched_terms": [
                    "domain",
                    "gpqa",
                    "adaptation",
                    "source",
                    "diamond",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Source: 2,400 synthetic conversations from Multi-Round Coreference Resolution\n<br class=\"ltx_break\"/>Context length filter: Episodes with contexts up to 100,000 characters\n<br class=\"ltx_break\"/>Temporal constraint: Source materials from 2022-2025\n<br class=\"ltx_break\"/>Key adaptation: Random identifiers replaced with natural ordinal references (&#8220;the second poem about nature&#8221;)\n<br class=\"ltx_break\"/>Retained: 548 episodes</p>\n\n",
                "matched_terms": [
                    "resolution",
                    "adaptation",
                    "source",
                    "context",
                    "coreference",
                    "length",
                    "episodes"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap",
        "caption": "Table 3: VERA evaluation results. Best text model in bold; best voice/cascade model underlined.\nAccuracies are macro-averaged across tracks (equal weight per track).\nTTFR (s) denotes time-to-first-response: (i) time to first audio byte for streaming/realtime voice models; (ii) time to first audio token for non-streaming voice models; (iii) time to first text token for text models.\n† Web search enabled. ‡ Cascade baseline.",
        "body": "Model\nMath\nWeb\nScience\nContext\nFactual\nAvg.\nTTFR (s)\nWER (%)\n\n\nCommercial APIs\n\n\nGPT-realtime\n6.1\n0.8\n13.0\n9.3\n27.4\n11.3\n2.69\n9.6\n\n\nGemini-2.5-Flash-audio†\n\n3.5\n1.1\n11.2\n18.8\n17.0\n10.3\n0.87\n7.9\n\n\nNova-Sonic\n0.0\n0.1\n0.0\n2.6\n1.3\n0.8\n0.94\nN/A\n\n\nOpen Voice Models\n\n\nQwen2-Audio\n0.0\n0.4\n4.4\n0.2\n2.1\n1.4\n1.26\nN/A\n\n\nUltraVox\n0.0\n0.2\n1.2\n26.6\n1.4\n5.9\n3.42\nN/A\n\n\nAudio Flamingo 3\n0.0\n0.3\n3.1\n3.8\n1.5\n1.7\n2.40\nN/A\n\n\nAudio Flamingo 3 (thinking)\n0.0\n0.4\n4.4\n1.8\n1.1\n1.5\n15.14\nN/A\n\n\nPhi-4-multimodal\n0.0\n0.5\n1.2\n12.0\n2.6\n3.3\n2.22\nN/A\n\n\nEnd-to-End Voice Models\n\n\nMoshi\n0.0\n0.2\n0.6\n0.0\n0.8\n0.3\n0.13\n12.2\n\n\nFreeze-Omni\n0.8\n0.0\n2.8\n0.0\n0.0\n0.7\n1.23\n19.8\n\n\nQwen2.5-Omni\n0.0\n0.1\n1.9\n1.4\n1.0\n0.9\n0.06\n19.0\n\n\nCascade Baseline\n\n\nLiveAnswer†,‡\n\n59.1\n13.0\n31.7\n0.2\n31.0\n27.0\n10.50\n7.5\n\n\nText-Only Upper Bounds\n\n\nGPT-4o†\n\n10.4\n0.8\n21.7\n12.2\n37.5\n16.5\n1.04\nN/A\n\n\nGPT-5† (effort=low)\n74.8\n12.3\n42.2\n80.8\n48.3\n51.7\n4.54\nN/A\n\n\nGPT-5† (effort=high)\n63.5\n16.4\n50.3\n90.5\n49.5\n54.0\n35.9\nN/A\n\n\nGemini-2.5-Pro†\n\n50.4\n4.6\n44.7\n94.3\n56.1\n50.0\n21.10\nN/A\n\n\nGemini-2.5-Flash†\n\n37.4\n3.6\n38.5\n86.7\n31.6\n39.6\n26.67\nN/A",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Math</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Web</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Science</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Context</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Factual</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Avg.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">TTFR (s)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">WER (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"9\"><span class=\"ltx_text ltx_font_italic\">Commercial APIs</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">GPT-realtime</td>\n<td class=\"ltx_td ltx_align_center\">6.1</td>\n<td class=\"ltx_td ltx_align_center\">0.8</td>\n<td class=\"ltx_td ltx_align_center\">13.0</td>\n<td class=\"ltx_td ltx_align_center\">9.3</td>\n<td class=\"ltx_td ltx_align_center\">27.4</td>\n<td class=\"ltx_td ltx_align_center\">11.3</td>\n<td class=\"ltx_td ltx_align_center\">2.69</td>\n<td class=\"ltx_td ltx_align_center\">9.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Gemini-2.5-Flash-audio<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">3.5</td>\n<td class=\"ltx_td ltx_align_center\">1.1</td>\n<td class=\"ltx_td ltx_align_center\">11.2</td>\n<td class=\"ltx_td ltx_align_center\">18.8</td>\n<td class=\"ltx_td ltx_align_center\">17.0</td>\n<td class=\"ltx_td ltx_align_center\">10.3</td>\n<td class=\"ltx_td ltx_align_center\">0.87</td>\n<td class=\"ltx_td ltx_align_center\">7.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Nova-Sonic</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n<td class=\"ltx_td ltx_align_center\">0.1</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n<td class=\"ltx_td ltx_align_center\">2.6</td>\n<td class=\"ltx_td ltx_align_center\">1.3</td>\n<td class=\"ltx_td ltx_align_center\">0.8</td>\n<td class=\"ltx_td ltx_align_center\">0.94</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"9\"><span class=\"ltx_text ltx_font_italic\">Open Voice Models</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Qwen2-Audio</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n<td class=\"ltx_td ltx_align_center\">0.4</td>\n<td class=\"ltx_td ltx_align_center\">4.4</td>\n<td class=\"ltx_td ltx_align_center\">0.2</td>\n<td class=\"ltx_td ltx_align_center\">2.1</td>\n<td class=\"ltx_td ltx_align_center\">1.4</td>\n<td class=\"ltx_td ltx_align_center\">1.26</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">UltraVox</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n<td class=\"ltx_td ltx_align_center\">0.2</td>\n<td class=\"ltx_td ltx_align_center\">1.2</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">26.6</span></td>\n<td class=\"ltx_td ltx_align_center\">1.4</td>\n<td class=\"ltx_td ltx_align_center\">5.9</td>\n<td class=\"ltx_td ltx_align_center\">3.42</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Audio Flamingo 3</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n<td class=\"ltx_td ltx_align_center\">0.3</td>\n<td class=\"ltx_td ltx_align_center\">3.1</td>\n<td class=\"ltx_td ltx_align_center\">3.8</td>\n<td class=\"ltx_td ltx_align_center\">1.5</td>\n<td class=\"ltx_td ltx_align_center\">1.7</td>\n<td class=\"ltx_td ltx_align_center\">2.40</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Audio Flamingo 3 (thinking)</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n<td class=\"ltx_td ltx_align_center\">0.4</td>\n<td class=\"ltx_td ltx_align_center\">4.4</td>\n<td class=\"ltx_td ltx_align_center\">1.8</td>\n<td class=\"ltx_td ltx_align_center\">1.1</td>\n<td class=\"ltx_td ltx_align_center\">1.5</td>\n<td class=\"ltx_td ltx_align_center\">15.14</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Phi-4-multimodal</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n<td class=\"ltx_td ltx_align_center\">0.5</td>\n<td class=\"ltx_td ltx_align_center\">1.2</td>\n<td class=\"ltx_td ltx_align_center\">12.0</td>\n<td class=\"ltx_td ltx_align_center\">2.6</td>\n<td class=\"ltx_td ltx_align_center\">3.3</td>\n<td class=\"ltx_td ltx_align_center\">2.22</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"9\"><span class=\"ltx_text ltx_font_italic\">End-to-End Voice Models</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Moshi</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n<td class=\"ltx_td ltx_align_center\">0.2</td>\n<td class=\"ltx_td ltx_align_center\">0.6</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n<td class=\"ltx_td ltx_align_center\">0.8</td>\n<td class=\"ltx_td ltx_align_center\">0.3</td>\n<td class=\"ltx_td ltx_align_center\">0.13</td>\n<td class=\"ltx_td ltx_align_center\">12.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Freeze-Omni</td>\n<td class=\"ltx_td ltx_align_center\">0.8</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n<td class=\"ltx_td ltx_align_center\">2.8</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n<td class=\"ltx_td ltx_align_center\">0.7</td>\n<td class=\"ltx_td ltx_align_center\">1.23</td>\n<td class=\"ltx_td ltx_align_center\">19.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Qwen2.5-Omni</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n<td class=\"ltx_td ltx_align_center\">0.1</td>\n<td class=\"ltx_td ltx_align_center\">1.9</td>\n<td class=\"ltx_td ltx_align_center\">1.4</td>\n<td class=\"ltx_td ltx_align_center\">1.0</td>\n<td class=\"ltx_td ltx_align_center\">0.9</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.06</span></td>\n<td class=\"ltx_td ltx_align_center\">19.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"9\"><span class=\"ltx_text ltx_font_italic\">Cascade Baseline</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">LiveAnswer<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;,&#8225;</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">59.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">13.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">31.7</span></td>\n<td class=\"ltx_td ltx_align_center\">0.2</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">31.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">27.0</span></td>\n<td class=\"ltx_td ltx_align_center\">10.50</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">7.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"9\"><span class=\"ltx_text ltx_font_italic\">Text-Only Upper Bounds</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">GPT-4o<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">10.4</td>\n<td class=\"ltx_td ltx_align_center\">0.8</td>\n<td class=\"ltx_td ltx_align_center\">21.7</td>\n<td class=\"ltx_td ltx_align_center\">12.2</td>\n<td class=\"ltx_td ltx_align_center\">37.5</td>\n<td class=\"ltx_td ltx_align_center\">16.5</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.04</span></td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">GPT-5<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup> (effort=low)</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">74.8</span></td>\n<td class=\"ltx_td ltx_align_center\">12.3</td>\n<td class=\"ltx_td ltx_align_center\">42.2</td>\n<td class=\"ltx_td ltx_align_center\">80.8</td>\n<td class=\"ltx_td ltx_align_center\">48.3</td>\n<td class=\"ltx_td ltx_align_center\">51.7</td>\n<td class=\"ltx_td ltx_align_center\">4.54</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">GPT-5<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup> (effort=high)</td>\n<td class=\"ltx_td ltx_align_center\">63.5</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">16.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">50.3</span></td>\n<td class=\"ltx_td ltx_align_center\">90.5</td>\n<td class=\"ltx_td ltx_align_center\">49.5</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">54.0</span></td>\n<td class=\"ltx_td ltx_align_center\">35.9</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Gemini-2.5-Pro<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">50.4</td>\n<td class=\"ltx_td ltx_align_center\">4.6</td>\n<td class=\"ltx_td ltx_align_center\">44.7</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">94.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">56.1</span></td>\n<td class=\"ltx_td ltx_align_center\">50.0</td>\n<td class=\"ltx_td ltx_align_center\">21.10</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">Gemini-2.5-Flash<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">37.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">38.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">86.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">31.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">39.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">26.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">N/A</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "flamingo",
            "text",
            "textonly",
            "byte",
            "equal",
            "avg",
            "math",
            "gemini25pro†",
            "accuracies",
            "streamingrealtime",
            "ultravox",
            "qwen25omni",
            "first",
            "track",
            "across",
            "voice",
            "gemini25flash†",
            "freezeomni",
            "baseline",
            "novasonic",
            "wer",
            "iii",
            "open",
            "qwen2audio",
            "web",
            "results",
            "gemini25flashaudio†",
            "timetofirstresponse",
            "ttfr",
            "model",
            "endtoend",
            "weight",
            "liveanswer†‡",
            "cascade",
            "nonstreaming",
            "evaluation",
            "enabled",
            "commercial",
            "denotes",
            "context",
            "token",
            "thinking",
            "bold",
            "apis",
            "vera",
            "moshi",
            "effortlow",
            "bounds",
            "underlined",
            "gpt4o†",
            "time",
            "gpt5†",
            "voicecascade",
            "science",
            "models",
            "factual",
            "tracks",
            "macroaveraged",
            "best",
            "phi4multimodal",
            "efforthigh",
            "search",
            "upper",
            "audio",
            "gptrealtime"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Our evaluation (table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.T3\" title=\"Table 3 &#8227; 5.1 What is the gap and how does it vary by task? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) reveals a stark <span class=\"ltx_text ltx_font_smallcaps\">VRG</span>: an average accuracy drop of 40.4 percentage points for voice models that widens dramatically on tasks requiring complex, multi-step reasoning.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Unless otherwise stated, gaps are computed against the text baseline (GPT-5, effort=low) while the text upper bound refers to GPT-5 (effort=high) and is shown as the dashed line in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</span></span></span>\nThis gap scales systematically with the complexity of the reasoning required.\nFor instance, while factual retrieval shows moderate degradation (GPT-5 text: 48.3% vs. GPT-realtime voice: 27.4%), the gap widens dramatically for tasks requiring multi-step reasoning, with mathematical reasoning exhibiting a near-total collapse in performance (GPT-5: 74.8% vs. GPT-realtime: 6.1%).\nThis suggests that certain tasks, such as the multi-hop synthesis required in our Web track, become particularly intractable under the constraints of a streaming voice interface.\nStatistical validation using McNemar&#8217;s test <cite class=\"ltx_cite ltx_citemacro_citep\">(McNemar, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib23\" title=\"\">1947</a>)</cite> confirms these differences are highly significant (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), as detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A5\" title=\"Appendix E Statistical Validation &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.\nThis pattern of differential failure extends universally across the diverse voice architectures we evaluated.\nThey consistently perform best on retrieval or short-answer tasks while failing on complex reasoning. GPT-realtime achieves its highest score on Factual questions (27.4%) but drops to 6.1% on Math.\nSome models exhibit extreme specialization; UltraVox, for example, maintains 26.6% accuracy on Context while scoring 0.0% on Math, suggesting an optimization for conversational continuity at the expense of deep reasoning.\nThis trend holds for Gemini&#8217;s audio model (18.8% on Context vs. 3.5% on Math) and open-source models like Phi-4-multimodal (12.0% on Context vs. 0.0% on Math).\nThis consistent pattern across 12 diverse voice systems demonstrates that the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> is not a model-specific artifact but a universal property of current voice technology, with the gap scaling systematically from moderate on simple retrieval tasks to severe on complex reasoning.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Voice Evaluation of Reasoning Ability (<span class=\"ltx_text ltx_font_smallcaps\">VERA</span>), a benchmark for evaluating <em class=\"ltx_emph ltx_font_italic\">reasoning</em> ability in voice-interactive systems under real-time conversational constraints.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> comprises 2,931 voice-native episodes derived from established text benchmarks and organized into five tracks (Math, Web, Science, Long-Context, Factual).\nEach item is adapted for speech interaction while preserving reasoning difficulty.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> enables direct text&#8211;voice comparison within model families and supports analysis of how architectural choices affect reliability.\nWe assess 12 contemporary voice systems alongside strong text baselines and observe large, consistent modality gaps: on competition mathematics a leading text model attains 74.8% accuracy while its voice counterpart reaches 6.1%; macro-averaged across tracks the best text models achieve 54.0% versus 11.3% for voice.\nLatency&#8211;accuracy analyses reveal a low-latency plateau, where fast voice systems cluster around <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>10% accuracy, while approaching text performance requires sacrificing real-time interaction.\nDiagnostic experiments indicate that common mitigations are insufficient.\nIncreasing &#8220;thinking time&#8221; yields negligible gains; a decoupled cascade that separates reasoning from narration improves accuracy but still falls well short of text and introduces characteristic grounding/consistency errors.\nFailure analyses further show distinct error signatures across native streaming, end-to-end, and cascade designs.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> provides a reproducible testbed and targeted diagnostics for architectures that decouple <em class=\"ltx_emph ltx_font_italic\">thinking</em> from <em class=\"ltx_emph ltx_font_italic\">speaking</em>, offering a principled way to measure progress toward real-time voice assistants that are both fluent and reliably reasoned.</p>\n\n",
                "matched_terms": [
                    "model",
                    "endtoend",
                    "voice",
                    "across",
                    "text",
                    "science",
                    "cascade",
                    "vera",
                    "evaluation",
                    "models",
                    "factual",
                    "tracks",
                    "macroaveraged",
                    "best",
                    "math",
                    "thinking",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a systematic evaluation of reasoning in today&#8217;s voice-interactive systems, documenting a significant and consistent performance degradation we term the Voice Reasoning Gap (<span class=\"ltx_text ltx_font_smallcaps\">VRG</span>). This gap is most pronounced on complex, multi-step reasoning tasks. For example, in our study, a leading voice assistant, GPT-realtime&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib27\" title=\"\">2024b</a>)</cite>, achieves 6.1% accuracy on mathematical problems, whereas a top-performing text model from the same developer, GPT-5&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib29\" title=\"\">2025b</a>)</cite>, achieves 74.8%. This 68.7-point difference is not an isolated finding but is representative of a broader pattern where models optimized for low-latency streaming show consistently lower performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "voice",
                    "text",
                    "models",
                    "evaluation",
                    "gptrealtime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The problem does not appear to be purely acoustic. Existing benchmarks show that current voice models are highly proficient at audio understanding, capable of transcribing speech with near-human accuracy and analyzing complex acoustic scenes&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib50\" title=\"\">2021</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib39\" title=\"\">2024a</a>)</cite>. While these capabilities confirm that the models can effectively &#8220;hear&#8221; a user&#8217;s request, they are separate from the cognitive processes required for general-purpose reasoning. We hypothesize that the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> is instead a consequence of a fundamental architectural tension: the design of real-time voice systems, which prioritizes an <span class=\"ltx_text ltx_font_italic\">irreversible, low-latency stream of audio</span>, is in direct conflict with the <span class=\"ltx_text ltx_font_italic\">iterative, revisable computation</span> that underpins complex reasoning in text-based models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate this hypothesis, we introduce the <span class=\"ltx_text ltx_font_bold\">Voice Evaluation of Reasoning Ability (<span class=\"ltx_text ltx_font_smallcaps\">VERA</span>)</span>, a benchmark designed to measure reasoning under real-time constraints. Our analysis with <span class=\"ltx_text ltx_font_smallcaps\">VERA</span>, summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, reveals a clear <span class=\"ltx_text ltx_font_italic\">latency-accuracy trade-off</span>. The data shows a <span class=\"ltx_text ltx_font_bold\">low-latency plateau</span>, where the fastest voice models remain shallow in their reasoning, and a <span class=\"ltx_text ltx_font_bold\">cascade lift, not parity</span>, where even a powerful text reasoner decoupled from the voice interface improves accuracy but still falls significantly short of its native text performance. Together, these patterns demonstrate that the ideal <span class=\"ltx_text ltx_font_italic\">fast-and-accurate upper-left corner of the frontier remains empty</span>, suggesting the gap is a systemic challenge for current architectures, not merely an efficiency issue. This work provides a framework for diagnosing these trade-offs, complementing (rather than replacing) existing audio-understanding evaluation. Our analysis uncovers distinct failure signatures tied to system architecture; for instance, native streaming models tend to produce fluent but incorrect responses, while decoupled cascades are more prone to grounding errors. The patterns we observe highlight key opportunities for improvement and suggest promising research directions. Our main contributions are:</p>\n\n",
                "matched_terms": [
                    "voice",
                    "text",
                    "models",
                    "cascade",
                    "evaluation",
                    "vera"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantifies and diagnoses the Voice Reasoning Gap.</span> We provide systematic measurements showing voice models achieve 42% lower accuracy on average, with gaps exceeding 68% on complex domains. Controlled experiments including cascade baselines demonstrate this gap persists even with perfect acoustic conditions and extended thinking time.</p>\n\n",
                "matched_terms": [
                    "time",
                    "voice",
                    "models",
                    "cascade",
                    "thinking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Characterizes distinct failure signatures tied to voice architectures.</span> Through analysis of 2,931 episodes, we provide the first systematic evidence showing that different voice system designs (e.g., native streaming vs. decoupled cascade) fail in predictably different ways, creating a diagnostic fingerprint for the underlying architectural trade-offs.</p>\n\n",
                "matched_terms": [
                    "cascade",
                    "voice",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Provides a unified evaluation framework for real-time systems.</span> <span class=\"ltx_text ltx_font_smallcaps\">VERA</span> enables fair comparison across heterogeneous voice architectures (native, cascade, and end-to-end) within a single evaluation protocol, a non-trivial orchestration that establishes a reproducible benchmark for measuring progress toward genuinely intelligent voice assistants.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code and data available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/linyueqian/VERA\" title=\"\">https://github.com/linyueqian/VERA</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "endtoend",
                    "voice",
                    "across",
                    "cascade",
                    "evaluation",
                    "vera"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing voice benchmarks, while valuable, have not evaluated the ability of models to perform general-purpose reasoning through a real-time conversational interface.\nInstead, prior work has focused on two distinct areas: a model&#8217;s ability to understand the acoustic signal itself, and its ability to manage conversational mechanics. Benchmarks like SUPERB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib50\" title=\"\">2021</a>)</cite>, AudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib39\" title=\"\">2024a</a>)</cite>, and even more recent ones like MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib32\" title=\"\">2024</a>)</cite> and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib21\" title=\"\">2025</a>)</cite>, evaluate <span class=\"ltx_text ltx_font_bold\">audio-content understanding, often with reasoning about sound</span>&#8212;tasks such as identifying events from sounds, analyzing acoustic scenes, or answering questions about the properties of the audio signal.\nSeparately, the spoken language understanding (SLU) and spoken-QA literature targets mapping speech to meaning, including intent and slot filling, dialog state tracking, and extractive or conversational QA, with representative corpora such as Spoken SQuAD, ODSQA, Spoken-CoQA, HeySQuAD, and the SLUE suite (Phase-1/2)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib15\" title=\"\">2018b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib14\" title=\"\">a</a>; You et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib51\" title=\"\">2022</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib46\" title=\"\">2023</a>; Shon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib34\" title=\"\">2022</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nThese datasets assess comprehension of recorded speech but generally lack explicit real-time constraints and do not provide text&#8211;versus&#8211;voice comparisons on reasoning problems.\nConcurrently, a separate line of work on full-duplex systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib30\" title=\"\">2025</a>; Arora et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib2\" title=\"\">2025</a>)</cite> has focused on the <span class=\"ltx_text ltx_font_bold\">mechanics of dialogue</span>, such as turn-taking and interruption handling, without evaluating the substantive reasoning that must occur within that conversation. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a comparative overview of representative benchmarks across these areas.</p>\n\n",
                "matched_terms": [
                    "models",
                    "voice",
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates (with a more comprehensive catalog in Appendix Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A1.T4\" title=\"Table 4 &#8227; Appendix A Previous Benchmarks &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), this focus on distinct capabilities has created a clear evaluation gap. The field measures whether a model can <em class=\"ltx_emph ltx_font_italic\">hear</em> (Audio Understanding), <em class=\"ltx_emph ltx_font_italic\">understand</em> spoken language, or <em class=\"ltx_emph ltx_font_italic\">handle</em> interaction mechanics (full-duplex/latency), but not whether it can <span class=\"ltx_text ltx_font_bold\">think on general problems while talking</span>.\nNo existing benchmark combines <span class=\"ltx_text ltx_font_bold\">(1) multi-step, general-purpose reasoning</span> with <span class=\"ltx_text ltx_font_bold\">(2) explicit real-time latency constraints</span> and <span class=\"ltx_text ltx_font_bold\">(3) a direct, cross-modal text&#8211;versus&#8211;voice comparison on identical tasks</span>.\nThis gap helps explain why the severe reasoning degradation we document has gone unquantified.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> is the first to occupy this intersection, providing a focused diagnostic tool for the trade-offs between conversational fluency and reasoning depth in modern voice systems.</p>\n\n",
                "matched_terms": [
                    "model",
                    "voice",
                    "evaluation",
                    "vera",
                    "audio",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We formalize the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> with a metric that we then operationalize for practical evaluation. For a distribution of reasoning tasks <math alttext=\"\\mathcal{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><annotation encoding=\"application/x-tex\">\\mathcal{T}</annotation></semantics></math>, we define the gap as the expected difference in accuracy between text and voice modalities:</p>\n\n",
                "matched_terms": [
                    "text",
                    "evaluation",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"P_{\\text{text}}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>P</mi><mtext>text</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P_{\\text{text}}(t)</annotation></semantics></math> and <math alttext=\"P_{\\text{voice}}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>P</mi><mtext>voice</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P_{\\text{voice}}(t)</annotation></semantics></math> represent the best achievable accuracy on task <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. In practice, we measure this by comparing top-performing models, using those from the same family where possible (e.g., GPT-5 vs. GPT-realtime). A crucial part of this framework is the text baseline; <em class=\"ltx_emph ltx_font_italic\">for this reference, we adopt accuracy-oriented text models rather than voice models with a text input</em>, as the latter remain architecturally optimized for low latency and would conflate modality with latency policy.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "text",
                    "models",
                    "best",
                    "baseline",
                    "gptrealtime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study provides a <span class=\"ltx_text ltx_font_bold\">diagnostic characterization</span> of the current voice systems&#8217; landscape, not a controlled experiment designed to prove causality. Because we evaluate heterogeneous commercial systems with different architectures and training objectives, <span class=\"ltx_text ltx_font_bold\">we cannot isolate the causal impact of modality alone</span>. Rather, our goal is to systematically document system performance and identify recurring, cross-model patterns that point toward underlying architectural challenges. The consistency of the gap we find across 12 systems strongly suggests that these challenges are fundamental and merit this investigation, for which we provide a reproducible benchmark.</p>\n\n",
                "matched_terms": [
                    "commercial",
                    "across",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The theoretical basis for the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> arises from the different operational dynamics of each interface. Current text-based generation is akin to <span class=\"ltx_text ltx_font_bold\">drafting</span>: models can explore multiple reasoning paths internally or use chain-of-thought to self-correct before committing to a final answer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib43\" title=\"\">2022</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib42\" title=\"\">2023b</a>)</cite>. This ability to &#8220;revise&#8221; is critical for complex problem-solving. In stark contrast, voice-native generation is a <span class=\"ltx_text ltx_font_bold\">live performance</span>. To maintain conversational fluency, models must begin generating an <em class=\"ltx_emph ltx_font_italic\">irreversible stream of audio</em> almost immediately, forcing a <span class=\"ltx_text ltx_font_italic\">streaming commitment</span> to an initial reasoning path that may be shallow or flawed. Once spoken, a token cannot be taken back, causing early missteps to cascade into unrecoverable errors. The model must divide its computational resources between the cognitive task of reasoning and the motor task of coherent speech synthesis, further constraining its problem-solving capacity.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "cascade",
                    "token",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This architectural asymmetry between revisable drafting and irreversible performance raises a series of critical diagnostic questions that guide our analysis. First, <span class=\"ltx_text ltx_font_bold\">what</span> is the magnitude of the gap, and how does it vary across different types of reasoning tasks? Second, <span class=\"ltx_text ltx_font_bold\">why</span> does this gap exist? Can it be attributed to simple engineering factors like insufficient thinking time or poor audio fidelity, or does it reflect a more fundamental limitation? Finally, <span class=\"ltx_text ltx_font_bold\">how</span> do these systems fail? Do different voice architectures produce systematically different error signatures? To answer these questions, <span class=\"ltx_text ltx_font_smallcaps\">VERA</span> is designed to enable controlled comparisons on identical reasoning tasks, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S3.F2\" title=\"Figure 2 &#8227; 3 The VERA Benchmark &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, while applying realistic conversational and latency constraints.</p>\n\n",
                "matched_terms": [
                    "time",
                    "voice",
                    "across",
                    "thinking",
                    "vera",
                    "audio",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Voice suitability filter.</span>\nFor each source question, a filtering agent screens for (i) <em class=\"ltx_emph ltx_font_italic\">visual dependence</em> (must not require diagrams/tables), (ii) <em class=\"ltx_emph ltx_font_italic\">audio memory load</em> (3&#8211;4 salient entities), (iii) <em class=\"ltx_emph ltx_font_italic\">multi-step structure</em> (interruptible reasoning), and (iv) <em class=\"ltx_emph ltx_font_italic\">articulatory feasibility</em> (clear tokenization for TTS). Items failing any criterion are excluded.</p>\n\n",
                "matched_terms": [
                    "iii",
                    "audio",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech generation.</span>\nValidated text episodes are rendered to 24kHz audio using Higgs-Audio v2 <cite class=\"ltx_cite ltx_citemacro_cite\">Boson AI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib3\" title=\"\">2025</a>)</cite>, which generates naturalistic speech with automatic variation in timbre, tone, and emotion based on textual content. This TTS system produces acoustically diverse outputs through its inherent voice variation, ensuring models are evaluated on reasoning rather than adaptation to specific acoustic patterns (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S3.SS3\" title=\"3.3 Dataset Composition &#8227; 3 The VERA Benchmark &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a> for diversity analysis).</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "audio",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">VERA</span> comprises 2,931 voice-optimized episodes that are systematically derived from five established benchmarks, with detailed statistics for each track presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S3.T2\" title=\"Table 2 &#8227; 3.3 Dataset Composition &#8227; 3 The VERA Benchmark &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "track",
                    "vera"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our benchmark is structured around five complementary tracks, each designed to isolate a distinct failure mode in voice-based reasoning. <span class=\"ltx_text ltx_font_bold\">Mathematical reasoning</span>, using 115 problems from the AIME math competition&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mathematical Association of America, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib22\" title=\"\">2025</a>)</cite>, tests solution coherence while speaking. <span class=\"ltx_text ltx_font_bold\">Web-grounded synthesis</span>, with 1,107 questions from the BrowseComp web-navigation benchmark&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib45\" title=\"\">2025</a>)</cite>, evaluates information integration under streaming constraints. <span class=\"ltx_text ltx_font_bold\">Scientific expertise</span>, drawn from 161 graduate-level GPQA Diamond questions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib31\" title=\"\">2023</a>)</cite>, probes knowledge access under the cognitive load of simultaneous speech generation. <span class=\"ltx_text ltx_font_bold\">Long-context memory</span>, using 548 MRCR episodes&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib28\" title=\"\">2025a</a>)</cite> with contexts up to 100K characters, examines state tracking during extended interactions. Finally, a crucial baseline of <span class=\"ltx_text ltx_font_bold\">Factual recall</span>, with 1,000 SimpleQA questions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib44\" title=\"\">2024</a>)</cite>, isolates architectural overhead from reasoning complexity.</p>\n\n",
                "matched_terms": [
                    "factual",
                    "math",
                    "tracks",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The creation of these 2,931 episodes involved a rigorous curation process that filtered approximately 22,000 source items to prioritize diagnostic clarity. Each adapted episode first achieved a mean quality score of 9.0, as assessed by an LLM validator, before being rendered to 24kHz audio using Higgs-Audio v2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Boson AI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib3\" title=\"\">2025</a>)</cite>. This TTS system is critical to the benchmark&#8217;s design, as it automatically varies timbre, tone, and emotion based on textual content to produce acoustically diverse speech. To ensure the final benchmark&#8217;s integrity, we validated both its semantic and acoustic properties. A manual audit of 200 episodes (6.8%) confirmed that semantic and logical structures were preserved, while an analysis of speaker embeddings using WeSpeaker&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib40\" title=\"\">2023a</a>)</cite> verified the acoustic diversity of the generated audio (<math alttext=\"\\mu=0.000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#956;</mi><mo>=</mo><mn>0.000</mn></mrow><annotation encoding=\"application/x-tex\">\\mu=0.000</annotation></semantics></math>, <math alttext=\"\\sigma=0.120\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><mi>&#963;</mi><mo>=</mo><mn>0.120</mn></mrow><annotation encoding=\"application/x-tex\">\\sigma=0.120</annotation></semantics></math>), confirming the absence of systemic acoustic bias.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Fidelity Assessment.</span> We evaluate generated speech using Word Error Rate (WER), comparing ASR transcripts against ground truth. Our LLM-based normalizer standardizes both the reference text and ASR\ntranscript to canonical mathematical notation (e.g., &#8220;f of sixteen equals fifty four&#8221; &#8594; &#8220;f(16) = 54&#8221;, &#8220;twenty twenty-four&#8221; &#8594; &#8220;2024&#8221;) before comparison. This normalization, with further examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A2\" title=\"Appendix B ASR Transcript Normalization &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>, ensures a fair comparison between mathematical expressions in written form and their spoken equivalents&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sproat &amp; Jaitly, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib37\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Accuracy Evaluation.</span> We assess task accuracy using an LLM-as-a-judge protocol&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib52\" title=\"\">2023</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib20\" title=\"\">2023</a>)</cite>. This approach is highly effective for <span class=\"ltx_text ltx_font_smallcaps\">VERA</span> because our benchmark tasks, while challenging, are designed to have <span class=\"ltx_text ltx_font_bold\">well-defined ground truth answers with minimal ambiguity</span>, making them suitable for reliable automated grading. We employ GPT-4o&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib26\" title=\"\">2024a</a>)</cite> as the grader, using the normalized ASR transcript for voice model outputs. Each prediction undergoes <span class=\"ltx_text ltx_font_bold\">three independent evaluations</span> to mitigate judgment stochasticity, with the final label (Correct, Incorrect, or Not Attempted) determined by majority vote.</p>\n\n",
                "matched_terms": [
                    "vera",
                    "model",
                    "voice",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Failure Analysis.</span> To understand error patterns systematically, we conduct detailed failure attribution on incorrect predictions using a comprehensive error taxonomy. Our analysis framework employs GPT-5 to classify failures across 16 error categories spanning knowledge errors (e.g., entity confusion, temporal errors), reasoning errors (e.g., computation mistakes, logical contradictions), and understanding errors (e.g., misinterpretation, off-target responses). For voice models specifically, the analysis distinguishes between transcription artifacts and genuine content errors, providing insights into whether failures stem from speech processing or core reasoning capabilities. This multi-label classification enables fine-grained understanding of model limitations and identifies systematic failure modes across different task types.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "voice",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Calibration.</span> To validate our LLM-based evaluation, we conducted human evaluation on 1,000 randomly sampled predictions across all tracks and models. GPT-4o&#8217;s judgments achieved 97.8% agreement with human evaluation (95% CI: 96.8-98.7%), ranging from perfect agreement on Math (100%) to 84.3% on Science where answers require more nuanced interpretation. Cross-vendor validation using Gemini-2.5-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Google Cloud, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib10\" title=\"\">2025a</a>)</cite> achieved 98.7% agreement with human evaluation and 98.1% with GPT-4o, confirming minimal vendor bias and consistent evaluation standards across judges. Detailed analyses are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A3\" title=\"Appendix C Human Evaluation and Judge Validation &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "science",
                    "models",
                    "evaluation",
                    "tracks",
                    "math"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To diagnose the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span>, we evaluate a comprehensive set of models on the <span class=\"ltx_text ltx_font_smallcaps\">VERA</span> benchmark. Our evaluation spans three categories of voice systems: <span class=\"ltx_text ltx_font_bold\">commercial realtime APIs</span> (GPT-realtime, Gemini-2.5-Flash-audio, Amazon Nova Sonic); <span class=\"ltx_text ltx_font_bold\">open voice models</span> (Qwen2-Audio, UltraVox, Audio Flamingo 3, Phi-4-multimodal); and <span class=\"ltx_text ltx_font_bold\">end-to-end architectures</span> that directly generate speech (Moshi, Freeze-Omni, Qwen2.5-Omni). Against these, we benchmark two critical references to isolate the source of the performance drop. First, a <span class=\"ltx_text ltx_font_bold\">text-only upper bound</span> (GPT-4o, GPT-5, Gemini-2.5 Pro/Flash) quantifies maximum achievable accuracy by isolating reasoning capacity from modality constraints. Second, we construct a sophisticated <span class=\"ltx_text ltx_font_bold\">cascade baseline</span>, <em class=\"ltx_emph ltx_font_italic\">LiveAnswer</em>, to simulate an architecture that separates deep reasoning from real-time narration. <em class=\"ltx_emph ltx_font_italic\">LiveAnswer</em> uses GPT-5 as a powerful core reasoner and a faster Llama-3.3-70B-Instruct as a narration synthesizer to convert the detailed reasoning into a concise, fluent spoken response, allowing us to test whether the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> persists even when thinking and speaking are decoupled. Full implementation details and citations for all models are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A4\" title=\"Appendix D Model Implementation Details &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "flamingo",
                    "textonly",
                    "ultravox",
                    "qwen25omni",
                    "first",
                    "audio",
                    "voice",
                    "baseline",
                    "open",
                    "qwen2audio",
                    "endtoend",
                    "cascade",
                    "evaluation",
                    "commercial",
                    "thinking",
                    "apis",
                    "moshi",
                    "vera",
                    "models",
                    "phi4multimodal",
                    "upper",
                    "freezeomni",
                    "gptrealtime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.F4\" title=\"Figure 4 &#8227; 5.1 What is the gap and how does it vary by task? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> further demonstrates this pattern for several model families.\nPanel (a) shows GPT-5 text maintaining robust multi-domain performance (54% radar chart coverage) while GPT-realtime voice achieves only 11% coverage, with moderate performance on Factual (27.4%) but severe weakness across reasoning tasks.\nPanel (b) confirms generalization to Gemini models, with text variants achieving 40-50% coverage versus 11% for voice.\nPanel (c) reveals that even diverse voice architectures&#8212;including an <em class=\"ltx_emph ltx_font_italic\">audio-encoder + LLM text-decoder</em> design (Qwen2-Audio), an <em class=\"ltx_emph ltx_font_italic\">end-to-end Thinker&#8211;Talker</em> model that jointly generates text and speech (Qwen2.5-Omni), and a Whisper-style encoder + LLM with on-demand reasoning (Audio Flamingo 3)&#8212;remain confined below 5% accuracy on reasoning tasks.\nThe variance within voice models (<math alttext=\"\\sigma^{2}=3.66\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><msup><mi>&#963;</mi><mn>2</mn></msup><mo>=</mo><mn>3.66</mn></mrow><annotation encoding=\"application/x-tex\">\\sigma^{2}=3.66</annotation></semantics></math> across Math scores) is 171&#215; smaller than between modalities (<math alttext=\"\\sigma^{2}=625.92\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><msup><mi>&#963;</mi><mn>2</mn></msup><mo>=</mo><mn>625.92</mn></mrow><annotation encoding=\"application/x-tex\">\\sigma^{2}=625.92</annotation></semantics></math>), confirming that architectural variations within the voice paradigm produce marginal improvements compared to the fundamental gap.\nThis pattern holds even for models featuring a &#8220;thinking mode,&#8221; which, as our analysis in Section 5.2 shows, fails to improve reasoning despite a significant increase in latency.</p>\n\n",
                "matched_terms": [
                    "model",
                    "endtoend",
                    "across",
                    "flamingo",
                    "text",
                    "voice",
                    "models",
                    "factual",
                    "math",
                    "qwen25omni",
                    "qwen2audio",
                    "audio",
                    "gptrealtime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our diagnostic experiments indicate the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> stems not from simple engineering limitations, but from a deeper architectural conflict between real-time streaming and complex reasoning.\nFirst, extended thinking time provides negligible benefit: Audio Flamingo 3&#8217;s thinking mode increases latency from 2.40s to 15.14s (a 530% increase) to allow internal deliberation before speaking, yet accuracy actually decreases from 1.7% to 1.5% overall while Context performance degrades from 3.8% to 1.8%.\nThe latency-accuracy frontier in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> confirms this pattern across all models, showing voice systems plateau below 10% accuracy regardless of response time, with no voice systems achieving both sub-1.5s latency and above-11% accuracy.\nSecond, the LiveAnswer cascade experiment isolates the modality penalty by using the same powerful GPT-5 model as our text upper bound.\nEven in this ideal setup, a persistent 15.7 percentage point gap remained on the Math track (59.1% vs. the text model&#8217;s 74.8%).\nThis drop is largely attributable to the narration synthesizer, which must translate the reasoner&#8217;s complex output into fluent speech periodically.\nThis translation process introduced logical inconsistencies and was particularly detrimental to tasks requiring exact string matching, causing a near-total failure on the Context track (0.2%).\nAs detailed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.F5\" title=\"Figure 5 &#8227; 5.2 Why does the gap exist? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the time-to-first-response for this system averages 10.5s, dominated by the Speech-to-Text step.\nThis demonstrates that even a sophisticated, decoupled architecture still cannot fully close the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span>, reinforcing the need for more fundamental architectural innovation to bridge the gap between deep reasoning and real-time narration.\nThird, output quality measurements confirm that speech synthesis is not the bottleneck: speech clarity does not determine success, as models across the WER spectrum from 7.9% (Gemini-2.5-Flash-Audio) to 19.8% (Freeze-Omni) show uniformly poor reasoning performance.\nCollectively, these diagnostic experiments demonstrate that the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> is not a simple engineering artifact that can be fixed by allocating more time, decoupling the architecture, or improving speech quality. The persistence of the gap across these conditions points instead to a fundamental constraint in how current streaming architectures support multi-step computation.</p>\n\n",
                "matched_terms": [
                    "track",
                    "timetofirstresponse",
                    "audio",
                    "model",
                    "time",
                    "across",
                    "flamingo",
                    "text",
                    "voice",
                    "cascade",
                    "models",
                    "context",
                    "math",
                    "thinking",
                    "wer",
                    "upper",
                    "freezeomni",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Voice models fail in systematically different ways tied to their architecture: native streaming models tend to fail by prioritizing fluent completion over accuracy, while decoupled cascade systems are more prone to internal logical contradictions.\nNative streaming models like GPT-realtime and Gemini-2.5-Flash-Audio show a strong bias towards completing their responses, even when incorrect.\nThey produce significantly fewer <span class=\"ltx_text ltx_font_smallcaps\">no_final_answer</span> and <span class=\"ltx_text ltx_font_smallcaps\">off_target</span> errors than the average, suggesting an architectural pressure to maintain conversational fluency at the cost of accuracy.\nThey are designed to avoid silence or abandonment, leading them to generate fluent continuations even when their underlying reasoning is flawed.\nCascade systems present an orthogonal failure profile: LiveAnswer shows strong positive deviations for <span class=\"ltx_text ltx_font_smallcaps\">unsupported_fact</span> (+0.27), <span class=\"ltx_text ltx_font_smallcaps\">off_target</span> (+0.31), and <span class=\"ltx_text ltx_font_smallcaps\">logical_contradiction</span> (+0.22), indicating systematic inconsistencies between reasoning and verbalization stages that manifest as factual grounding failures and logical incoherence.\nEnd-to-end architectures diverge maximally from baseline: Moshi exhibits extreme <span class=\"ltx_text ltx_font_smallcaps\">off_target</span> deviation (+0.52) with suppressed rates elsewhere, while Qwen2.5-Omni shows the inverse pattern with <span class=\"ltx_text ltx_font_smallcaps\">no_final_answer</span> (+0.36) but strong negative deviations for <span class=\"ltx_text ltx_font_smallcaps\">unsupported_fact</span> (-0.47), indicating task disengagement rather than incorrect completion.</p>\n\n",
                "matched_terms": [
                    "endtoend",
                    "voice",
                    "models",
                    "cascade",
                    "factual",
                    "baseline",
                    "qwen25omni",
                    "moshi",
                    "gptrealtime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The bimodal distribution of error signatures (completion-focused vs abandonment-focused) across architectures suggests that streaming audio generation imposes a binary constraint on failure modes: models either generate fluent but incorrect continuations or fail to engage, with no intermediate state that permits iterative refinement characteristic of text-based reasoning.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings indicate that achieving human-level reasoning in voice assistants will require architectural innovations beyond incremental improvements.\nThe convergent evidence from our analysis establishes that the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> appears not to be explained by the engineering factors we ablate, indicating architectural changes may be needed.\nThe 40.4 percentage point average gap resists all conventional solutions, single models show large performance differentials between retrieval and reasoning, and even architectural decoupling yields an irreducible 15.7-point penalty.\nThe systematic failure patterns in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.F6\" title=\"Figure 6 &#8227; 5.3 How do the models fail differently? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, particularly streaming commitment errors&#8212;manifesting primarily as <span class=\"ltx_text ltx_font_smallcaps\">off_target</span> and <span class=\"ltx_text ltx_font_smallcaps\">no_final_answer</span> deviations that <em class=\"ltx_emph ltx_font_italic\">vary by architecture</em> (underproduced for native voice, overproduced for cascades)&#8212;mechanistically explain why incremental improvements cannot bridge this gap.\nThese findings point toward our central design principle: architectures must decouple thinking from speaking through an editable internal state separate from the speech output buffer.\nThis principle suggests several research directions including asynchronous architectures <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib18\" title=\"\">2025c</a>)</cite> where backend reasoning models operate with higher latency while frontend verbalizers maintain conversational flow, and chunked reasoning with parallel processing <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib5\" title=\"\">2025</a>)</cite> where models use audio playback time to compute next reasoning steps.\nOur LiveAnswer analysis (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.F5\" title=\"Figure 5 &#8227; 5.2 Why does the gap exist? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) reveals specific engineering challenges: managing the latency-accuracy trade-off through streaming ASR with confidence-gated handoff and answer-first narration strategies, and ensuring cross-stage consistency to prevent the grounding failures (<span class=\"ltx_text ltx_font_smallcaps\">unsupported_fact</span> at +0.27) that arise when decoupling modules.\nAchieving human-like reasoning in voice assistants ultimately requires unique architectures that strategically combine pre-computation, parallel processing, and selective verbalization to deliver systems that are both deeply intelligent and naturally conversational.</p>\n\n",
                "matched_terms": [
                    "time",
                    "voice",
                    "models",
                    "thinking",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work systematically documents and diagnoses the Voice Reasoning Gap, a significant and consistent performance drop observed when current language models operate through a voice interface compared to text.\nUsing our purpose-built benchmark, <span class=\"ltx_text ltx_font_smallcaps\">VERA</span>, we provide the first quantitative characterization of this gap across a range of models and complex reasoning tasks.\nOur diagnostic experiments show that this performance degradation is not a simple engineering artifact, as it persists even when granting models extended thinking time, ensuring high audio fidelity, or employing a sophisticated cascade architecture that separates the reasoning core from audio I/O.\nInstead, our analysis suggests a fundamental tension between the architectural demands of low-latency streaming and the iterative, revisable computation required for deep reasoning.\nWe identified distinct failure signatures tied to different architectures, finding that native streaming models tend to fail by producing fluent but incorrect responses, while decoupled systems introduce grounding and consistency errors.\nThese findings indicate that bridging the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> will likely require a paradigm shift away from monolithic architectures toward novel systems that explicitly decouple reasoning from real-time narration.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> provides a critical diagnostic tool to guide and measure progress toward this goal, paving the way for voice assistants that are not only fluent but also genuinely intelligent.</p>\n\n",
                "matched_terms": [
                    "time",
                    "voice",
                    "across",
                    "text",
                    "models",
                    "cascade",
                    "thinking",
                    "vera",
                    "audio",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Organization</span> This Appendix provides comprehensive details on benchmark construction, evaluation methodology, and additional analyses not covered in the main paper. The sections are ordered following their introduction in the main text, with supplementary materials at the end. The document is organized as follows:</p>\n\n",
                "matched_terms": [
                    "text",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Commercial voice APIs</p>\n\n",
                "matched_terms": [
                    "commercial",
                    "apis",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Open voice models</p>\n\n",
                "matched_terms": [
                    "models",
                    "voice",
                    "open"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">End-to-end voice models</p>\n\n",
                "matched_terms": [
                    "models",
                    "endtoend",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Text-only upper bounds</p>\n\n",
                "matched_terms": [
                    "bounds",
                    "upper",
                    "textonly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LiveAnswer cascade baseline architecture</p>\n\n",
                "matched_terms": [
                    "cascade",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sampled 1,000 model outputs stratified across tracks (Math: 46, Web: 490, Science: 70, Factual: 394) for human validation. Each output was evaluated as correct or incorrect given the ground truth answer.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across",
                    "science",
                    "factual",
                    "tracks",
                    "math",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The near-perfect agreement on Math, Web, and Factual tracks reflects the objective nature of these tasks with clear correct answers. The lower but still strong agreement on Science (84.3-92.9%) appropriately captures the greater interpretive complexity in graduate-level scientific reasoning. These validation results confirm that our LLM-based evaluation provides reliable and consistent judgments aligned with human assessment.</p>\n\n",
                "matched_terms": [
                    "science",
                    "evaluation",
                    "factual",
                    "tracks",
                    "math",
                    "web",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Below we summarize the models evaluated in VERA. For proprietary systems, we treat them as black-box APIs and report only interface-level behavior (modality, streaming support, and how they are used in our pipeline). For open models, we cite the original papers when available.</p>\n\n",
                "matched_terms": [
                    "models",
                    "apis",
                    "vera",
                    "open"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GPT-realtime.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib27\" title=\"\">2024b</a>)</cite> A commercial, full-duplex voice model with streaming audio input and low-latency speech output. We use it as a native voice baseline: the model listens while speaking, produces incremental audio tokens, and has no separate text-reasoning stage exposed to the user. It serves as a representative of end-to-end, latency-optimized voice agents.</p>\n\n",
                "matched_terms": [
                    "endtoend",
                    "model",
                    "voice",
                    "commercial",
                    "baseline",
                    "audio",
                    "gptrealtime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Gemini-2.5-Flash-audio.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Google Cloud, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib10\" title=\"\">2025a</a>)</cite>A commercial, low-latency audio-capable model accessed through a streaming voice endpoint. We use it as a second native voice baseline emphasizing responsiveness over long-form reasoning. It supports real-time speech I/O with web search capability enabled; we treat it as a black box with default vendor settings.</p>\n\n",
                "matched_terms": [
                    "model",
                    "voice",
                    "enabled",
                    "commercial",
                    "baseline",
                    "search",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Nova-Sonic.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Amazon Web Services, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib1\" title=\"\">2025</a>)</cite> A commercial real-time voice system with streaming speech in/out. We include it to broaden the coverage of native, production-grade voice agents. We do not modify decoding parameters beyond the provider defaults.</p>\n\n",
                "matched_terms": [
                    "commercial",
                    "web",
                    "novasonic",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qwen2-Audio</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib6\" title=\"\">2024</a>)</cite>. A Large Audio-Language Model (LALM) that processes speech and text inputs to generate textual outputs. It demonstrates strong instruction-following over speech, sound, and music datasets, and provides an open baseline for voice understanding and mixed-modality dialogue.</p>\n\n",
                "matched_terms": [
                    "model",
                    "voice",
                    "text",
                    "baseline",
                    "open",
                    "qwen2audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Flamingo 3</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib9\" title=\"\">2025</a>)</cite>. An audio-language model that supports in-context learning, retrieval-augmented generation, and multi-turn dialogues over audio streams. We evaluate both its standard setting and a <em class=\"ltx_emph ltx_font_italic\">thinking mode</em> that allows extra internal compute before emitting final text.</p>\n\n",
                "matched_terms": [
                    "model",
                    "flamingo",
                    "text",
                    "thinking",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">UltraVox.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Fixie AI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib8\" title=\"\">2025</a>)</cite> An open-source voice assistant stack exposing streaming ASR <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> LLM <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> TTS in a single interface. We evaluate it in its default configuration to represent community voice agents optimized for interactivity rather than heavy-duty reasoning.</p>\n\n",
                "matched_terms": [
                    "ultravox",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phi-4-multimodal.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Microsoft, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib25\" title=\"\">2025</a>)</cite> A compact multimodal LLM that accepts text plus non-text inputs (including audio via a front-end encoder) and produces text outputs. We use it as a smaller-capacity open baseline to test whether compact models can sustain reasoning under voice constraints.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "text",
                    "models",
                    "phi4multimodal",
                    "baseline",
                    "open",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Moshi.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib7\" title=\"\">2024</a>)</cite> A real-time speech-in/speech-out model that directly maps audio to audio with minimal intermediate text exposure. We use it to probe the limits of ultra-low-latency architectures where most computation is spent on conversational fluidity.</p>\n\n",
                "matched_terms": [
                    "text",
                    "moshi",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Freeze-Omni.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib41\" title=\"\">2024b</a>)</cite> An omni-modal, streaming model operating with speech input and output. We include it as an additional end-to-end baseline to test whether architectural choices (single-tower vs. modular) affect reasoning under speech pressure.</p>\n\n",
                "matched_terms": [
                    "endtoend",
                    "model",
                    "baseline",
                    "freezeomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qwen2.5-Omni.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib47\" title=\"\">2025</a>)</cite> An omni model in the Qwen family that supports speech, text, and vision. We evaluate its native voice mode to compare omni-style training with audio-specialized training (cf. Qwen2-Audio).</p>\n\n",
                "matched_terms": [
                    "model",
                    "voice",
                    "text",
                    "qwen25omni",
                    "qwen2audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GPT-4o.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib26\" title=\"\">2024a</a>)</cite> A multimodal model evaluated in text-only mode with web search enabled.</p>\n\n",
                "matched_terms": [
                    "model",
                    "textonly",
                    "enabled",
                    "search",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GPT-5 (effort=low/high).</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib29\" title=\"\">2025b</a>)</cite> A reasoning model where &#8220;effort&#8221; denotes a higher decode-time compute budget (longer deliberation, slower first token). The high-effort setting allows for extended chain-of-thought reasoning at the cost of increased latency.</p>\n\n",
                "matched_terms": [
                    "denotes",
                    "model",
                    "first",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Gemini-2.5-Pro/Flash.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Google Cloud, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib11\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib10\" title=\"\">a</a>)</cite> Two text&#8209;only language models with web search enabled, providing alternative architectural approaches to reasoning at different capacity points.</p>\n\n",
                "matched_terms": [
                    "models",
                    "search",
                    "web",
                    "enabled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These systems receive the same tasks but interact purely via text, isolating reasoning capacity from voice constraints.</p>\n\n",
                "matched_terms": [
                    "text",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_typewriter\">LiveAnswer</span> system is a sophisticated cascade baseline designed to simulate an advanced voice architecture that decouples the computationally intensive process of deep reasoning from the user-facing task of real-time narration. The goal is to create a strong baseline that can &#8220;think&#8221; deeply without sacrificing conversational interactivity, allowing us to test if the VRG persists even when this architectural challenge is addressed. The system is composed of two primary logic modules, the <span class=\"ltx_text ltx_font_italic\">Core Reasoner</span> and the <span class=\"ltx_text ltx_font_italic\">Narration Synthesizer</span>, operating in concert.</p>\n\n",
                "matched_terms": [
                    "cascade",
                    "baseline",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Core Reasoner.</span> The first module is the <span class=\"ltx_text ltx_font_typewriter\">ProblemSolver</span>, which serves as the powerful but potentially slow cognitive core of the system. It is responsible for the actual problem-solving, leveraging <span class=\"ltx_text ltx_font_bold\">GPT-5</span> through its responses endpoint. This module is equipped with tools like web search and a code interpreter to handle complex, multi-hop reasoning tasks. Instead of generating a single, final text block, the solver produces a stream of structured &#8220;thoughts&#8221; that represent its internal state. This includes reasoning summaries, tool call invocations, and finally, the computed answer. These thoughts are not sent directly to the user but are pushed to the Narration Synthesizer via the <span class=\"ltx_text ltx_font_typewriter\">push_thought</span> method.</p>\n\n",
                "matched_terms": [
                    "text",
                    "search",
                    "web",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Incremental Updates:</span> As the Core Reasoner pushes new thoughts (e.g., updates from a web search), the synthesizer incorporates this new information into its ongoing narration. It includes logic to generate natural-sounding filler text (e.g., &#8220;I&#8217;m still thinking about this&#8230;&#8221;) if the Core Reasoner is taking a long time between thoughts, preventing awkward silences.</p>\n\n",
                "matched_terms": [
                    "time",
                    "text",
                    "thinking",
                    "search",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The full <span class=\"ltx_text ltx_font_typewriter\">LiveAnswer</span> pipeline operates as follows: (1) user speech is transcribed by <span class=\"ltx_text ltx_font_bold\">Azure Speech-to-Text</span>; (2) the text is sent to the <span class=\"ltx_text ltx_font_bold\">Core Reasoner</span> (GPT-5), which begins its detailed reasoning process; (3) in parallel, the <span class=\"ltx_text ltx_font_bold\">Narration Synthesizer</span> (Llama-3.3) generates an immediate, ongoing narration based on the stream of thoughts from the reasoner; (4) this narration is rendered into audio by <span class=\"ltx_text ltx_font_bold\">Azure Text-to-Speech</span>. This dual-model architecture directly tests the hypothesis that separating the &#8220;thinking&#8221; from the &#8220;speaking&#8221; can mitigate the Voice Reasoning Gap.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">WER Analysis.</span> We run ASR on model-generated speech and apply an LLM-based normalizer to canonicalize spoken math and notation before scoring.</p>\n\n",
                "matched_terms": [
                    "math",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Configuration Notes.</span> For all voice-native systems we enable streaming and full-duplex whenever supported by the provider. Unless otherwise stated, we do not allow web tools or retrieval beyond what the model natively exposes. Text upper bounds are evaluated with the same prompts and answer formats as their voice counterparts, differing only in modality and (for &#8220;effort=high&#8221;) decode-time budget.</p>\n\n",
                "matched_terms": [
                    "bounds",
                    "model",
                    "voice",
                    "text",
                    "upper",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note: Gaps calculated using macro-averaging (equal weight per track)</p>\n\n",
                "matched_terms": [
                    "track",
                    "equal",
                    "weight"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All primary comparisons show highly significant differences (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), confirming that the Voice Reasoning Gap is not due to measurement noise or random variation. The gap persists even when using identical text models with voice I/O wrappers (LiveAnswer), indicating that modality constraints rather than model capacity drive the performance degradation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>The Web track shows no significant difference in the LiveAnswer comparison (p = 0.636), likely due to low baseline performance (<math alttext=\"\\approx 12\\%\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>12</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 12\\%</annotation></semantics></math>) in both modalities.</span></span></span></p>\n\n",
                "matched_terms": [
                    "track",
                    "model",
                    "voice",
                    "text",
                    "models",
                    "baseline",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Note on anomalies:</span> The Web track shows no significant difference in the LiveAnswer comparison (<math alttext=\"p=0.636\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.636</mn></mrow><annotation encoding=\"application/x-tex\">p=0.636</annotation></semantics></math>), likely because both modalities struggle equally with multi-hop synthesis where base performance is low (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>12%). The Context track exhibits anomalously low LiveAnswer performance (0.2%), suggesting a possible system-specific failure that warrants investigation.</p>\n\n",
                "matched_terms": [
                    "track",
                    "context",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Source: 198 questions from GPQA Diamond subset\n<br class=\"ltx_break\"/>Domain distribution: Physics (61), Chemistry (52), Biology (48)\n<br class=\"ltx_break\"/>Excluded: 37 questions with visual dependencies (chemical structures, circuit schematics, complex derivations)\n<br class=\"ltx_break\"/>Retained: 161 questions\n<br class=\"ltx_break\"/>Performance baseline: PhD experts 65%, skilled non-experts with web access 34%\n<br class=\"ltx_break\"/>Notation adaptation: H<sub class=\"ltx_sub\">2</sub>SO<sub class=\"ltx_sub\">4</sub> verbalized as &#8220;H two S O four&#8221;</p>\n\n",
                "matched_terms": [
                    "web",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Retained: 1,000 episodes\n<br class=\"ltx_break\"/>Purpose: Control baseline to isolate voice interaction overhead</p>\n\n",
                "matched_terms": [
                    "baseline",
                    "voice"
                ]
            }
        ]
    },
    "A1.T4": {
        "source_file": "Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap",
        "caption": "Table 4: Voice benchmark comparison.",
        "body": "Test\n\n\nSamples",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Test</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Samples</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "test",
            "voice",
            "samples",
            "comparison",
            "benchmark"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates (with a more comprehensive catalog in Appendix Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A1.T4\" title=\"Table 4 &#8227; Appendix A Previous Benchmarks &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), this focus on distinct capabilities has created a clear evaluation gap. The field measures whether a model can <em class=\"ltx_emph ltx_font_italic\">hear</em> (Audio Understanding), <em class=\"ltx_emph ltx_font_italic\">understand</em> spoken language, or <em class=\"ltx_emph ltx_font_italic\">handle</em> interaction mechanics (full-duplex/latency), but not whether it can <span class=\"ltx_text ltx_font_bold\">think on general problems while talking</span>.\nNo existing benchmark combines <span class=\"ltx_text ltx_font_bold\">(1) multi-step, general-purpose reasoning</span> with <span class=\"ltx_text ltx_font_bold\">(2) explicit real-time latency constraints</span> and <span class=\"ltx_text ltx_font_bold\">(3) a direct, cross-modal text&#8211;versus&#8211;voice comparison on identical tasks</span>.\nThis gap helps explain why the severe reasoning degradation we document has gone unquantified.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> is the first to occupy this intersection, providing a focused diagnostic tool for the trade-offs between conversational fluency and reasoning depth in modern voice systems.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Voice Evaluation of Reasoning Ability (<span class=\"ltx_text ltx_font_smallcaps\">VERA</span>), a benchmark for evaluating <em class=\"ltx_emph ltx_font_italic\">reasoning</em> ability in voice-interactive systems under real-time conversational constraints.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> comprises 2,931 voice-native episodes derived from established text benchmarks and organized into five tracks (Math, Web, Science, Long-Context, Factual).\nEach item is adapted for speech interaction while preserving reasoning difficulty.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> enables direct text&#8211;voice comparison within model families and supports analysis of how architectural choices affect reliability.\nWe assess 12 contemporary voice systems alongside strong text baselines and observe large, consistent modality gaps: on competition mathematics a leading text model attains 74.8% accuracy while its voice counterpart reaches 6.1%; macro-averaged across tracks the best text models achieve 54.0% versus 11.3% for voice.\nLatency&#8211;accuracy analyses reveal a low-latency plateau, where fast voice systems cluster around <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>10% accuracy, while approaching text performance requires sacrificing real-time interaction.\nDiagnostic experiments indicate that common mitigations are insufficient.\nIncreasing &#8220;thinking time&#8221; yields negligible gains; a decoupled cascade that separates reasoning from narration improves accuracy but still falls well short of text and introduces characteristic grounding/consistency errors.\nFailure analyses further show distinct error signatures across native streaming, end-to-end, and cascade designs.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> provides a reproducible testbed and targeted diagnostics for architectures that decouple <em class=\"ltx_emph ltx_font_italic\">thinking</em> from <em class=\"ltx_emph ltx_font_italic\">speaking</em>, offering a principled way to measure progress toward real-time voice assistants that are both fluent and reliably reasoned.</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "benchmark",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate this hypothesis, we introduce the <span class=\"ltx_text ltx_font_bold\">Voice Evaluation of Reasoning Ability (<span class=\"ltx_text ltx_font_smallcaps\">VERA</span>)</span>, a benchmark designed to measure reasoning under real-time constraints. Our analysis with <span class=\"ltx_text ltx_font_smallcaps\">VERA</span>, summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, reveals a clear <span class=\"ltx_text ltx_font_italic\">latency-accuracy trade-off</span>. The data shows a <span class=\"ltx_text ltx_font_bold\">low-latency plateau</span>, where the fastest voice models remain shallow in their reasoning, and a <span class=\"ltx_text ltx_font_bold\">cascade lift, not parity</span>, where even a powerful text reasoner decoupled from the voice interface improves accuracy but still falls significantly short of its native text performance. Together, these patterns demonstrate that the ideal <span class=\"ltx_text ltx_font_italic\">fast-and-accurate upper-left corner of the frontier remains empty</span>, suggesting the gap is a systemic challenge for current architectures, not merely an efficiency issue. This work provides a framework for diagnosing these trade-offs, complementing (rather than replacing) existing audio-understanding evaluation. Our analysis uncovers distinct failure signatures tied to system architecture; for instance, native streaming models tend to produce fluent but incorrect responses, while decoupled cascades are more prone to grounding errors. The patterns we observe highlight key opportunities for improvement and suggest promising research directions. Our main contributions are:</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Provides a unified evaluation framework for real-time systems.</span> <span class=\"ltx_text ltx_font_smallcaps\">VERA</span> enables fair comparison across heterogeneous voice architectures (native, cascade, and end-to-end) within a single evaluation protocol, a non-trivial orchestration that establishes a reproducible benchmark for measuring progress toward genuinely intelligent voice assistants.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code and data available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/linyueqian/VERA\" title=\"\">https://github.com/linyueqian/VERA</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "comparison",
                    "benchmark",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study provides a <span class=\"ltx_text ltx_font_bold\">diagnostic characterization</span> of the current voice systems&#8217; landscape, not a controlled experiment designed to prove causality. Because we evaluate heterogeneous commercial systems with different architectures and training objectives, <span class=\"ltx_text ltx_font_bold\">we cannot isolate the causal impact of modality alone</span>. Rather, our goal is to systematically document system performance and identify recurring, cross-model patterns that point toward underlying architectural challenges. The consistency of the gap we find across 12 systems strongly suggests that these challenges are fundamental and merit this investigation, for which we provide a reproducible benchmark.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Accuracy Evaluation.</span> We assess task accuracy using an LLM-as-a-judge protocol&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib52\" title=\"\">2023</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib20\" title=\"\">2023</a>)</cite>. This approach is highly effective for <span class=\"ltx_text ltx_font_smallcaps\">VERA</span> because our benchmark tasks, while challenging, are designed to have <span class=\"ltx_text ltx_font_bold\">well-defined ground truth answers with minimal ambiguity</span>, making them suitable for reliable automated grading. We employ GPT-4o&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib26\" title=\"\">2024a</a>)</cite> as the grader, using the normalized ASR transcript for voice model outputs. Each prediction undergoes <span class=\"ltx_text ltx_font_bold\">three independent evaluations</span> to mitigate judgment stochasticity, with the final label (Correct, Incorrect, or Not Attempted) determined by majority vote.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To diagnose the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span>, we evaluate a comprehensive set of models on the <span class=\"ltx_text ltx_font_smallcaps\">VERA</span> benchmark. Our evaluation spans three categories of voice systems: <span class=\"ltx_text ltx_font_bold\">commercial realtime APIs</span> (GPT-realtime, Gemini-2.5-Flash-audio, Amazon Nova Sonic); <span class=\"ltx_text ltx_font_bold\">open voice models</span> (Qwen2-Audio, UltraVox, Audio Flamingo 3, Phi-4-multimodal); and <span class=\"ltx_text ltx_font_bold\">end-to-end architectures</span> that directly generate speech (Moshi, Freeze-Omni, Qwen2.5-Omni). Against these, we benchmark two critical references to isolate the source of the performance drop. First, a <span class=\"ltx_text ltx_font_bold\">text-only upper bound</span> (GPT-4o, GPT-5, Gemini-2.5 Pro/Flash) quantifies maximum achievable accuracy by isolating reasoning capacity from modality constraints. Second, we construct a sophisticated <span class=\"ltx_text ltx_font_bold\">cascade baseline</span>, <em class=\"ltx_emph ltx_font_italic\">LiveAnswer</em>, to simulate an architecture that separates deep reasoning from real-time narration. <em class=\"ltx_emph ltx_font_italic\">LiveAnswer</em> uses GPT-5 as a powerful core reasoner and a faster Llama-3.3-70B-Instruct as a narration synthesizer to convert the detailed reasoning into a concise, fluent spoken response, allowing us to test whether the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> persists even when thinking and speaking are decoupled. Full implementation details and citations for all models are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A4\" title=\"Appendix D Model Implementation Details &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "benchmark",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation (table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.T3\" title=\"Table 3 &#8227; 5.1 What is the gap and how does it vary by task? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) reveals a stark <span class=\"ltx_text ltx_font_smallcaps\">VRG</span>: an average accuracy drop of 40.4 percentage points for voice models that widens dramatically on tasks requiring complex, multi-step reasoning.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Unless otherwise stated, gaps are computed against the text baseline (GPT-5, effort=low) while the text upper bound refers to GPT-5 (effort=high) and is shown as the dashed line in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</span></span></span>\nThis gap scales systematically with the complexity of the reasoning required.\nFor instance, while factual retrieval shows moderate degradation (GPT-5 text: 48.3% vs. GPT-realtime voice: 27.4%), the gap widens dramatically for tasks requiring multi-step reasoning, with mathematical reasoning exhibiting a near-total collapse in performance (GPT-5: 74.8% vs. GPT-realtime: 6.1%).\nThis suggests that certain tasks, such as the multi-hop synthesis required in our Web track, become particularly intractable under the constraints of a streaming voice interface.\nStatistical validation using McNemar&#8217;s test <cite class=\"ltx_cite ltx_citemacro_citep\">(McNemar, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib23\" title=\"\">1947</a>)</cite> confirms these differences are highly significant (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), as detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A5\" title=\"Appendix E Statistical Validation &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.\nThis pattern of differential failure extends universally across the diverse voice architectures we evaluated.\nThey consistently perform best on retrieval or short-answer tasks while failing on complex reasoning. GPT-realtime achieves its highest score on Factual questions (27.4%) but drops to 6.1% on Math.\nSome models exhibit extreme specialization; UltraVox, for example, maintains 26.6% accuracy on Context while scoring 0.0% on Math, suggesting an optimization for conversational continuity at the expense of deep reasoning.\nThis trend holds for Gemini&#8217;s audio model (18.8% on Context vs. 3.5% on Math) and open-source models like Phi-4-multimodal (12.0% on Context vs. 0.0% on Math).\nThis consistent pattern across 12 diverse voice systems demonstrates that the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> is not a model-specific artifact but a universal property of current voice technology, with the gap scaling systematically from moderate on simple retrieval tasks to severe on complex reasoning.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work systematically documents and diagnoses the Voice Reasoning Gap, a significant and consistent performance drop observed when current language models operate through a voice interface compared to text.\nUsing our purpose-built benchmark, <span class=\"ltx_text ltx_font_smallcaps\">VERA</span>, we provide the first quantitative characterization of this gap across a range of models and complex reasoning tasks.\nOur diagnostic experiments show that this performance degradation is not a simple engineering artifact, as it persists even when granting models extended thinking time, ensuring high audio fidelity, or employing a sophisticated cascade architecture that separates the reasoning core from audio I/O.\nInstead, our analysis suggests a fundamental tension between the architectural demands of low-latency streaming and the iterative, revisable computation required for deep reasoning.\nWe identified distinct failure signatures tied to different architectures, finding that native streaming models tend to fail by producing fluent but incorrect responses, while decoupled systems introduce grounding and consistency errors.\nThese findings indicate that bridging the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> will likely require a paradigm shift away from monolithic architectures toward novel systems that explicitly decouple reasoning from real-time narration.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> provides a critical diagnostic tool to guide and measure progress toward this goal, paving the way for voice assistants that are not only fluent but also genuinely intelligent.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comprehensive comparison of voice benchmarks and their capabilities</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phi-4-multimodal.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Microsoft, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib25\" title=\"\">2025</a>)</cite> A compact multimodal LLM that accepts text plus non-text inputs (including audio via a front-end encoder) and produces text outputs. We use it as a smaller-capacity open baseline to test whether compact models can sustain reasoning under voice constraints.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_typewriter\">LiveAnswer</span> system is a sophisticated cascade baseline designed to simulate an advanced voice architecture that decouples the computationally intensive process of deep reasoning from the user-facing task of real-time narration. The goal is to create a strong baseline that can &#8220;think&#8221; deeply without sacrificing conversational interactivity, allowing us to test if the VRG persists even when this architectural challenge is addressed. The system is composed of two primary logic modules, the <span class=\"ltx_text ltx_font_italic\">Core Reasoner</span> and the <span class=\"ltx_text ltx_font_italic\">Narration Synthesizer</span>, operating in concert.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conducted comprehensive statistical testing to validate the robustness of the Voice Reasoning Gap. All comparisons use McNemar&#8217;s test for paired predictions, with confidence intervals estimated via bootstrap resampling (10,000 iterations).</p>\n\n",
                "matched_terms": [
                    "voice",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All primary comparisons show highly significant differences (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), confirming that the Voice Reasoning Gap is not due to measurement noise or random variation. The gap persists even when using identical text models with voice I/O wrappers (LiveAnswer), indicating that modality constraints rather than model capacity drive the performance degradation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>The Web track shows no significant difference in the LiveAnswer comparison (p = 0.636), likely due to low baseline performance (<math alttext=\"\\approx 12\\%\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>12</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 12\\%</annotation></semantics></math>) in both modalities.</span></span></span></p>\n\n",
                "matched_terms": [
                    "comparison",
                    "voice"
                ]
            }
        ]
    },
    "A2.T5": {
        "source_file": "Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap",
        "caption": "Table 5: Example normalizations applied by the LLM normalizer before WER computation",
        "body": "Input (ASR Output)\n\n\n\n\nNormalized Output\n\n\n\n\n\n\nP of x equals two x squared plus three x plus one\n\n\n\n\nP(x) = 2x² + 3x + 1\n\n\n\n\n\n\nf of sixteen equals fifty four\n\n\n\n\nf(16) = 54\n\n\n\n\n\n\nThe leading coefficient for Q of x is negative two\n\n\n\n\nThe leading coefficient for Q(x) is -2\n\n\n\n\n\n\ntwenty twenty four\n\n\n\n\n2024\n\n\n\n\n\n\nx plus y minus three\n\n\n\n\nx + y - 3\n\n\n\n\n\n\nthree point five\n\n\n\n\n3.5",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:190.8pt;\"><span class=\"ltx_text ltx_font_bold\">Input (ASR Output)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:190.8pt;\"><span class=\"ltx_text ltx_font_bold\">Normalized Output</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:190.8pt;\">P of x equals two x squared plus three x plus one</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:190.8pt;\">P(x) = 2x&#178; + 3x + 1</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:190.8pt;\">f of sixteen equals fifty four</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:190.8pt;\">f(16) = 54</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:190.8pt;\">The leading coefficient for Q of x is negative two</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:190.8pt;\">The leading coefficient for Q(x) is -2</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:190.8pt;\">twenty twenty four</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:190.8pt;\">2024</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:190.8pt;\">x plus y minus three</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:190.8pt;\">x + y - 3</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:190.8pt;\">three point five</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:190.8pt;\">3.5</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "llm",
            "equals",
            "output",
            "input",
            "2x²",
            "computation",
            "normalizations",
            "normalized",
            "plus",
            "leading",
            "example",
            "applied",
            "three",
            "one",
            "five",
            "twenty",
            "wer",
            "sixteen",
            "normalizer",
            "before",
            "fifty",
            "point",
            "four",
            "minus",
            "negative",
            "asr",
            "f16",
            "two",
            "squared",
            "coefficient"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Voice Evaluation of Reasoning Ability (<span class=\"ltx_text ltx_font_smallcaps\">VERA</span>), a benchmark for evaluating <em class=\"ltx_emph ltx_font_italic\">reasoning</em> ability in voice-interactive systems under real-time conversational constraints.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> comprises 2,931 voice-native episodes derived from established text benchmarks and organized into five tracks (Math, Web, Science, Long-Context, Factual).\nEach item is adapted for speech interaction while preserving reasoning difficulty.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> enables direct text&#8211;voice comparison within model families and supports analysis of how architectural choices affect reliability.\nWe assess 12 contemporary voice systems alongside strong text baselines and observe large, consistent modality gaps: on competition mathematics a leading text model attains 74.8% accuracy while its voice counterpart reaches 6.1%; macro-averaged across tracks the best text models achieve 54.0% versus 11.3% for voice.\nLatency&#8211;accuracy analyses reveal a low-latency plateau, where fast voice systems cluster around <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>10% accuracy, while approaching text performance requires sacrificing real-time interaction.\nDiagnostic experiments indicate that common mitigations are insufficient.\nIncreasing &#8220;thinking time&#8221; yields negligible gains; a decoupled cascade that separates reasoning from narration improves accuracy but still falls well short of text and introduces characteristic grounding/consistency errors.\nFailure analyses further show distinct error signatures across native streaming, end-to-end, and cascade designs.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> provides a reproducible testbed and targeted diagnostics for architectures that decouple <em class=\"ltx_emph ltx_font_italic\">thinking</em> from <em class=\"ltx_emph ltx_font_italic\">speaking</em>, offering a principled way to measure progress toward real-time voice assistants that are both fluent and reliably reasoned.</p>\n\n",
                "matched_terms": [
                    "five",
                    "leading"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a systematic evaluation of reasoning in today&#8217;s voice-interactive systems, documenting a significant and consistent performance degradation we term the Voice Reasoning Gap (<span class=\"ltx_text ltx_font_smallcaps\">VRG</span>). This gap is most pronounced on complex, multi-step reasoning tasks. For example, in our study, a leading voice assistant, GPT-realtime&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib27\" title=\"\">2024b</a>)</cite>, achieves 6.1% accuracy on mathematical problems, whereas a top-performing text model from the same developer, GPT-5&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib29\" title=\"\">2025b</a>)</cite>, achieves 74.8%. This 68.7-point difference is not an isolated finding but is representative of a broader pattern where models optimized for low-latency streaming show consistently lower performance.</p>\n\n",
                "matched_terms": [
                    "example",
                    "leading"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To scale beyond hand-authored items, we adapt established text benchmarks using a principled, multi-stage pipeline. This process is driven by a strong LLM ensemble with deterministic prompts and fixed roles to ensure reproducibility, preserving task semantics while rigorously enforcing voice-native constraints. The pipeline consists of four distinct stages:</p>\n\n",
                "matched_terms": [
                    "llm",
                    "four"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The creation of these 2,931 episodes involved a rigorous curation process that filtered approximately 22,000 source items to prioritize diagnostic clarity. Each adapted episode first achieved a mean quality score of 9.0, as assessed by an LLM validator, before being rendered to 24kHz audio using Higgs-Audio v2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Boson AI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib3\" title=\"\">2025</a>)</cite>. This TTS system is critical to the benchmark&#8217;s design, as it automatically varies timbre, tone, and emotion based on textual content to produce acoustically diverse speech. To ensure the final benchmark&#8217;s integrity, we validated both its semantic and acoustic properties. A manual audit of 200 episodes (6.8%) confirmed that semantic and logical structures were preserved, while an analysis of speaker embeddings using WeSpeaker&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib40\" title=\"\">2023a</a>)</cite> verified the acoustic diversity of the generated audio (<math alttext=\"\\mu=0.000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#956;</mi><mo>=</mo><mn>0.000</mn></mrow><annotation encoding=\"application/x-tex\">\\mu=0.000</annotation></semantics></math>, <math alttext=\"\\sigma=0.120\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><mi>&#963;</mi><mo>=</mo><mn>0.120</mn></mrow><annotation encoding=\"application/x-tex\">\\sigma=0.120</annotation></semantics></math>), confirming the absence of systemic acoustic bias.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "before"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Fidelity Assessment.</span> We evaluate generated speech using Word Error Rate (WER), comparing ASR transcripts against ground truth. Our LLM-based normalizer standardizes both the reference text and ASR\ntranscript to canonical mathematical notation (e.g., &#8220;f of sixteen equals fifty four&#8221; &#8594; &#8220;f(16) = 54&#8221;, &#8220;twenty twenty-four&#8221; &#8594; &#8220;2024&#8221;) before comparison. This normalization, with further examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A2\" title=\"Appendix B ASR Transcript Normalization &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>, ensures a fair comparison between mathematical expressions in written form and their spoken equivalents&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sproat &amp; Jaitly, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib37\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "normalizer",
                    "before",
                    "equals",
                    "fifty",
                    "wer",
                    "sixteen",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Accuracy Evaluation.</span> We assess task accuracy using an LLM-as-a-judge protocol&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib52\" title=\"\">2023</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib20\" title=\"\">2023</a>)</cite>. This approach is highly effective for <span class=\"ltx_text ltx_font_smallcaps\">VERA</span> because our benchmark tasks, while challenging, are designed to have <span class=\"ltx_text ltx_font_bold\">well-defined ground truth answers with minimal ambiguity</span>, making them suitable for reliable automated grading. We employ GPT-4o&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib26\" title=\"\">2024a</a>)</cite> as the grader, using the normalized ASR transcript for voice model outputs. Each prediction undergoes <span class=\"ltx_text ltx_font_bold\">three independent evaluations</span> to mitigate judgment stochasticity, with the final label (Correct, Incorrect, or Not Attempted) determined by majority vote.</p>\n\n",
                "matched_terms": [
                    "normalized",
                    "three",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To diagnose the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span>, we evaluate a comprehensive set of models on the <span class=\"ltx_text ltx_font_smallcaps\">VERA</span> benchmark. Our evaluation spans three categories of voice systems: <span class=\"ltx_text ltx_font_bold\">commercial realtime APIs</span> (GPT-realtime, Gemini-2.5-Flash-audio, Amazon Nova Sonic); <span class=\"ltx_text ltx_font_bold\">open voice models</span> (Qwen2-Audio, UltraVox, Audio Flamingo 3, Phi-4-multimodal); and <span class=\"ltx_text ltx_font_bold\">end-to-end architectures</span> that directly generate speech (Moshi, Freeze-Omni, Qwen2.5-Omni). Against these, we benchmark two critical references to isolate the source of the performance drop. First, a <span class=\"ltx_text ltx_font_bold\">text-only upper bound</span> (GPT-4o, GPT-5, Gemini-2.5 Pro/Flash) quantifies maximum achievable accuracy by isolating reasoning capacity from modality constraints. Second, we construct a sophisticated <span class=\"ltx_text ltx_font_bold\">cascade baseline</span>, <em class=\"ltx_emph ltx_font_italic\">LiveAnswer</em>, to simulate an architecture that separates deep reasoning from real-time narration. <em class=\"ltx_emph ltx_font_italic\">LiveAnswer</em> uses GPT-5 as a powerful core reasoner and a faster Llama-3.3-70B-Instruct as a narration synthesizer to convert the detailed reasoning into a concise, fluent spoken response, allowing us to test whether the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> persists even when thinking and speaking are decoupled. Full implementation details and citations for all models are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A4\" title=\"Appendix D Model Implementation Details &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "three",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our diagnostic experiments indicate the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> stems not from simple engineering limitations, but from a deeper architectural conflict between real-time streaming and complex reasoning.\nFirst, extended thinking time provides negligible benefit: Audio Flamingo 3&#8217;s thinking mode increases latency from 2.40s to 15.14s (a 530% increase) to allow internal deliberation before speaking, yet accuracy actually decreases from 1.7% to 1.5% overall while Context performance degrades from 3.8% to 1.8%.\nThe latency-accuracy frontier in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> confirms this pattern across all models, showing voice systems plateau below 10% accuracy regardless of response time, with no voice systems achieving both sub-1.5s latency and above-11% accuracy.\nSecond, the LiveAnswer cascade experiment isolates the modality penalty by using the same powerful GPT-5 model as our text upper bound.\nEven in this ideal setup, a persistent 15.7 percentage point gap remained on the Math track (59.1% vs. the text model&#8217;s 74.8%).\nThis drop is largely attributable to the narration synthesizer, which must translate the reasoner&#8217;s complex output into fluent speech periodically.\nThis translation process introduced logical inconsistencies and was particularly detrimental to tasks requiring exact string matching, causing a near-total failure on the Context track (0.2%).\nAs detailed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.F5\" title=\"Figure 5 &#8227; 5.2 Why does the gap exist? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the time-to-first-response for this system averages 10.5s, dominated by the Speech-to-Text step.\nThis demonstrates that even a sophisticated, decoupled architecture still cannot fully close the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span>, reinforcing the need for more fundamental architectural innovation to bridge the gap between deep reasoning and real-time narration.\nThird, output quality measurements confirm that speech synthesis is not the bottleneck: speech clarity does not determine success, as models across the WER spectrum from 7.9% (Gemini-2.5-Flash-Audio) to 19.8% (Freeze-Omni) show uniformly poor reasoning performance.\nCollectively, these diagnostic experiments demonstrate that the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> is not a simple engineering artifact that can be fixed by allocating more time, decoupling the architecture, or improving speech quality. The persistence of the gap across these conditions points instead to a fundamental constraint in how current streaming architectures support multi-step computation.</p>\n\n",
                "matched_terms": [
                    "before",
                    "output",
                    "computation",
                    "wer",
                    "point"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Voice models fail in systematically different ways tied to their architecture: native streaming models tend to fail by prioritizing fluent completion over accuracy, while decoupled cascade systems are more prone to internal logical contradictions.\nNative streaming models like GPT-realtime and Gemini-2.5-Flash-Audio show a strong bias towards completing their responses, even when incorrect.\nThey produce significantly fewer <span class=\"ltx_text ltx_font_smallcaps\">no_final_answer</span> and <span class=\"ltx_text ltx_font_smallcaps\">off_target</span> errors than the average, suggesting an architectural pressure to maintain conversational fluency at the cost of accuracy.\nThey are designed to avoid silence or abandonment, leading them to generate fluent continuations even when their underlying reasoning is flawed.\nCascade systems present an orthogonal failure profile: LiveAnswer shows strong positive deviations for <span class=\"ltx_text ltx_font_smallcaps\">unsupported_fact</span> (+0.27), <span class=\"ltx_text ltx_font_smallcaps\">off_target</span> (+0.31), and <span class=\"ltx_text ltx_font_smallcaps\">logical_contradiction</span> (+0.22), indicating systematic inconsistencies between reasoning and verbalization stages that manifest as factual grounding failures and logical incoherence.\nEnd-to-end architectures diverge maximally from baseline: Moshi exhibits extreme <span class=\"ltx_text ltx_font_smallcaps\">off_target</span> deviation (+0.52) with suppressed rates elsewhere, while Qwen2.5-Omni shows the inverse pattern with <span class=\"ltx_text ltx_font_smallcaps\">no_final_answer</span> (+0.36) but strong negative deviations for <span class=\"ltx_text ltx_font_smallcaps\">unsupported_fact</span> (-0.47), indicating task disengagement rather than incorrect completion.</p>\n\n",
                "matched_terms": [
                    "leading",
                    "negative"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings indicate that achieving human-level reasoning in voice assistants will require architectural innovations beyond incremental improvements.\nThe convergent evidence from our analysis establishes that the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> appears not to be explained by the engineering factors we ablate, indicating architectural changes may be needed.\nThe 40.4 percentage point average gap resists all conventional solutions, single models show large performance differentials between retrieval and reasoning, and even architectural decoupling yields an irreducible 15.7-point penalty.\nThe systematic failure patterns in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.F6\" title=\"Figure 6 &#8227; 5.3 How do the models fail differently? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, particularly streaming commitment errors&#8212;manifesting primarily as <span class=\"ltx_text ltx_font_smallcaps\">off_target</span> and <span class=\"ltx_text ltx_font_smallcaps\">no_final_answer</span> deviations that <em class=\"ltx_emph ltx_font_italic\">vary by architecture</em> (underproduced for native voice, overproduced for cascades)&#8212;mechanistically explain why incremental improvements cannot bridge this gap.\nThese findings point toward our central design principle: architectures must decouple thinking from speaking through an editable internal state separate from the speech output buffer.\nThis principle suggests several research directions including asynchronous architectures <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib18\" title=\"\">2025c</a>)</cite> where backend reasoning models operate with higher latency while frontend verbalizers maintain conversational flow, and chunked reasoning with parallel processing <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib5\" title=\"\">2025</a>)</cite> where models use audio playback time to compute next reasoning steps.\nOur LiveAnswer analysis (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.F5\" title=\"Figure 5 &#8227; 5.2 Why does the gap exist? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) reveals specific engineering challenges: managing the latency-accuracy trade-off through streaming ASR with confidence-gated handoff and answer-first narration strategies, and ensuring cross-stage consistency to prevent the grounding failures (<span class=\"ltx_text ltx_font_smallcaps\">unsupported_fact</span> at +0.27) that arise when decoupling modules.\nAchieving human-like reasoning in voice assistants ultimately requires unique architectures that strategically combine pre-computation, parallel processing, and selective verbalization to deliver systems that are both deeply intelligent and naturally conversational.</p>\n\n",
                "matched_terms": [
                    "point",
                    "output",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure fair comparison between spoken and written mathematical expressions, we employ an LLM-based normalizer that converts both ASR transcripts and reference texts to canonical mathematical notation before computing Word Error Rate (WER). This approach handles the complex variety of ways mathematical content can be verbalized.</p>\n\n",
                "matched_terms": [
                    "normalizer",
                    "asr",
                    "before",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This LLM-based normalization ensures that WER reflects genuine transcription errors rather than superficial formatting differences between spoken and written mathematical expressions. The same normalization is applied to both the ground truth and ASR output to maintain consistency. The full normalization prompt is available in our released code repository.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "applied",
                    "output",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GPT-realtime.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib27\" title=\"\">2024b</a>)</cite> A commercial, full-duplex voice model with streaming audio input and low-latency speech output. We use it as a native voice baseline: the model listens while speaking, produces incremental audio tokens, and has no separate text-reasoning stage exposed to the user. It serves as a representative of end-to-end, latency-optimized voice agents.</p>\n\n",
                "matched_terms": [
                    "output",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">UltraVox.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Fixie AI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib8\" title=\"\">2025</a>)</cite> An open-source voice assistant stack exposing streaming ASR <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> LLM <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> TTS in a single interface. We evaluate it in its default configuration to represent community voice agents optimized for interactivity rather than heavy-duty reasoning.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phi-4-multimodal.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Microsoft, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib25\" title=\"\">2025</a>)</cite> A compact multimodal LLM that accepts text plus non-text inputs (including audio via a front-end encoder) and produces text outputs. We use it as a smaller-capacity open baseline to test whether compact models can sustain reasoning under voice constraints.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "plus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Freeze-Omni.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib41\" title=\"\">2024b</a>)</cite> An omni-modal, streaming model operating with speech input and output. We include it as an additional end-to-end baseline to test whether architectural choices (single-tower vs. modular) affect reasoning under speech pressure.</p>\n\n",
                "matched_terms": [
                    "output",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">WER Analysis.</span> We run ASR on model-generated speech and apply an LLM-based normalizer to canonicalize spoken math and notation before scoring.</p>\n\n",
                "matched_terms": [
                    "normalizer",
                    "asr",
                    "before",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Source: 120 problems from AIME 2020-2025 (8 examination sittings)\n<br class=\"ltx_break\"/>Excluded: 5 problems requiring geometric diagrams or extensive symbolic manipulation\n<br class=\"ltx_break\"/>Retained: 115 problems\n<br class=\"ltx_break\"/>Key constraints: Integer answers in range [0, 999] for pronunciation clarity\n<br class=\"ltx_break\"/>Verbalization example: <math alttext=\"x^{2}+3x-2\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></mrow><mo>&#8722;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">x^{2}+3x-2</annotation></semantics></math> rendered as &#8220;x squared plus three x minus two&#8221;</p>\n\n",
                "matched_terms": [
                    "three",
                    "example",
                    "minus",
                    "squared",
                    "plus"
                ]
            }
        ]
    },
    "A3.T6": {
        "source_file": "Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap",
        "caption": "Table 6: Inter-annotator agreement validating GPT-4o as primary judge (n=1,000)",
        "body": "Track\nHuman-GPT-4o\nHuman-Gemini-2.5-Flash\nGPT-4o-Gemini-2.5-Flash\n\n\nMath\n100.0%\n100.0%\n100.0%\n\n\nWeb\n99.2%\n99.6%\n99.2%\n\n\nScience\n84.3%\n92.9%\n88.6%\n\n\nFactual\n98.2%\n98.5%\n98.2%\n\n\nOverall\n97.8%\n98.7%\n98.1%",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Track</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Human-GPT-4o</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Human-Gemini-2.5-Flash</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">GPT-4o-Gemini-2.5-Flash</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Math</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">100.0%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">100.0%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">100.0%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Web</td>\n<td class=\"ltx_td ltx_align_center\">99.2%</td>\n<td class=\"ltx_td ltx_align_center\">99.6%</td>\n<td class=\"ltx_td ltx_align_center\">99.2%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Science</td>\n<td class=\"ltx_td ltx_align_center\">84.3%</td>\n<td class=\"ltx_td ltx_align_center\">92.9%</td>\n<td class=\"ltx_td ltx_align_center\">88.6%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Factual</td>\n<td class=\"ltx_td ltx_align_center\">98.2%</td>\n<td class=\"ltx_td ltx_align_center\">98.5%</td>\n<td class=\"ltx_td ltx_align_center\">98.2%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">Overall</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">97.8%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">98.7%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">98.1%</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "track",
            "humangemini25flash",
            "overall",
            "humangpt4o",
            "science",
            "factual",
            "gpt4ogemini25flash",
            "math",
            "primary",
            "gpt4o",
            "n1000",
            "interannotator",
            "judge",
            "web",
            "agreement",
            "validating"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Voice Evaluation of Reasoning Ability (<span class=\"ltx_text ltx_font_smallcaps\">VERA</span>), a benchmark for evaluating <em class=\"ltx_emph ltx_font_italic\">reasoning</em> ability in voice-interactive systems under real-time conversational constraints.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> comprises 2,931 voice-native episodes derived from established text benchmarks and organized into five tracks (Math, Web, Science, Long-Context, Factual).\nEach item is adapted for speech interaction while preserving reasoning difficulty.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> enables direct text&#8211;voice comparison within model families and supports analysis of how architectural choices affect reliability.\nWe assess 12 contemporary voice systems alongside strong text baselines and observe large, consistent modality gaps: on competition mathematics a leading text model attains 74.8% accuracy while its voice counterpart reaches 6.1%; macro-averaged across tracks the best text models achieve 54.0% versus 11.3% for voice.\nLatency&#8211;accuracy analyses reveal a low-latency plateau, where fast voice systems cluster around <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>10% accuracy, while approaching text performance requires sacrificing real-time interaction.\nDiagnostic experiments indicate that common mitigations are insufficient.\nIncreasing &#8220;thinking time&#8221; yields negligible gains; a decoupled cascade that separates reasoning from narration improves accuracy but still falls well short of text and introduces characteristic grounding/consistency errors.\nFailure analyses further show distinct error signatures across native streaming, end-to-end, and cascade designs.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> provides a reproducible testbed and targeted diagnostics for architectures that decouple <em class=\"ltx_emph ltx_font_italic\">thinking</em> from <em class=\"ltx_emph ltx_font_italic\">speaking</em>, offering a principled way to measure progress toward real-time voice assistants that are both fluent and reliably reasoned.</p>\n\n",
                "matched_terms": [
                    "science",
                    "web",
                    "factual",
                    "math"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our benchmark is structured around five complementary tracks, each designed to isolate a distinct failure mode in voice-based reasoning. <span class=\"ltx_text ltx_font_bold\">Mathematical reasoning</span>, using 115 problems from the AIME math competition&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mathematical Association of America, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib22\" title=\"\">2025</a>)</cite>, tests solution coherence while speaking. <span class=\"ltx_text ltx_font_bold\">Web-grounded synthesis</span>, with 1,107 questions from the BrowseComp web-navigation benchmark&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib45\" title=\"\">2025</a>)</cite>, evaluates information integration under streaming constraints. <span class=\"ltx_text ltx_font_bold\">Scientific expertise</span>, drawn from 161 graduate-level GPQA Diamond questions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib31\" title=\"\">2023</a>)</cite>, probes knowledge access under the cognitive load of simultaneous speech generation. <span class=\"ltx_text ltx_font_bold\">Long-context memory</span>, using 548 MRCR episodes&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib28\" title=\"\">2025a</a>)</cite> with contexts up to 100K characters, examines state tracking during extended interactions. Finally, a crucial baseline of <span class=\"ltx_text ltx_font_bold\">Factual recall</span>, with 1,000 SimpleQA questions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib44\" title=\"\">2024</a>)</cite>, isolates architectural overhead from reasoning complexity.</p>\n\n",
                "matched_terms": [
                    "factual",
                    "math"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Calibration.</span> To validate our LLM-based evaluation, we conducted human evaluation on 1,000 randomly sampled predictions across all tracks and models. GPT-4o&#8217;s judgments achieved 97.8% agreement with human evaluation (95% CI: 96.8-98.7%), ranging from perfect agreement on Math (100%) to 84.3% on Science where answers require more nuanced interpretation. Cross-vendor validation using Gemini-2.5-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Google Cloud, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib10\" title=\"\">2025a</a>)</cite> achieved 98.7% agreement with human evaluation and 98.1% with GPT-4o, confirming minimal vendor bias and consistent evaluation standards across judges. Detailed analyses are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A3\" title=\"Appendix C Human Evaluation and Judge Validation &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "science",
                    "math",
                    "agreement",
                    "gpt4o"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation (table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.T3\" title=\"Table 3 &#8227; 5.1 What is the gap and how does it vary by task? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) reveals a stark <span class=\"ltx_text ltx_font_smallcaps\">VRG</span>: an average accuracy drop of 40.4 percentage points for voice models that widens dramatically on tasks requiring complex, multi-step reasoning.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Unless otherwise stated, gaps are computed against the text baseline (GPT-5, effort=low) while the text upper bound refers to GPT-5 (effort=high) and is shown as the dashed line in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</span></span></span>\nThis gap scales systematically with the complexity of the reasoning required.\nFor instance, while factual retrieval shows moderate degradation (GPT-5 text: 48.3% vs. GPT-realtime voice: 27.4%), the gap widens dramatically for tasks requiring multi-step reasoning, with mathematical reasoning exhibiting a near-total collapse in performance (GPT-5: 74.8% vs. GPT-realtime: 6.1%).\nThis suggests that certain tasks, such as the multi-hop synthesis required in our Web track, become particularly intractable under the constraints of a streaming voice interface.\nStatistical validation using McNemar&#8217;s test <cite class=\"ltx_cite ltx_citemacro_citep\">(McNemar, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib23\" title=\"\">1947</a>)</cite> confirms these differences are highly significant (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), as detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A5\" title=\"Appendix E Statistical Validation &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.\nThis pattern of differential failure extends universally across the diverse voice architectures we evaluated.\nThey consistently perform best on retrieval or short-answer tasks while failing on complex reasoning. GPT-realtime achieves its highest score on Factual questions (27.4%) but drops to 6.1% on Math.\nSome models exhibit extreme specialization; UltraVox, for example, maintains 26.6% accuracy on Context while scoring 0.0% on Math, suggesting an optimization for conversational continuity at the expense of deep reasoning.\nThis trend holds for Gemini&#8217;s audio model (18.8% on Context vs. 3.5% on Math) and open-source models like Phi-4-multimodal (12.0% on Context vs. 0.0% on Math).\nThis consistent pattern across 12 diverse voice systems demonstrates that the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> is not a model-specific artifact but a universal property of current voice technology, with the gap scaling systematically from moderate on simple retrieval tasks to severe on complex reasoning.</p>\n\n",
                "matched_terms": [
                    "track",
                    "web",
                    "factual",
                    "math"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.F4\" title=\"Figure 4 &#8227; 5.1 What is the gap and how does it vary by task? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> further demonstrates this pattern for several model families.\nPanel (a) shows GPT-5 text maintaining robust multi-domain performance (54% radar chart coverage) while GPT-realtime voice achieves only 11% coverage, with moderate performance on Factual (27.4%) but severe weakness across reasoning tasks.\nPanel (b) confirms generalization to Gemini models, with text variants achieving 40-50% coverage versus 11% for voice.\nPanel (c) reveals that even diverse voice architectures&#8212;including an <em class=\"ltx_emph ltx_font_italic\">audio-encoder + LLM text-decoder</em> design (Qwen2-Audio), an <em class=\"ltx_emph ltx_font_italic\">end-to-end Thinker&#8211;Talker</em> model that jointly generates text and speech (Qwen2.5-Omni), and a Whisper-style encoder + LLM with on-demand reasoning (Audio Flamingo 3)&#8212;remain confined below 5% accuracy on reasoning tasks.\nThe variance within voice models (<math alttext=\"\\sigma^{2}=3.66\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><msup><mi>&#963;</mi><mn>2</mn></msup><mo>=</mo><mn>3.66</mn></mrow><annotation encoding=\"application/x-tex\">\\sigma^{2}=3.66</annotation></semantics></math> across Math scores) is 171&#215; smaller than between modalities (<math alttext=\"\\sigma^{2}=625.92\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><msup><mi>&#963;</mi><mn>2</mn></msup><mo>=</mo><mn>625.92</mn></mrow><annotation encoding=\"application/x-tex\">\\sigma^{2}=625.92</annotation></semantics></math>), confirming that architectural variations within the voice paradigm produce marginal improvements compared to the fundamental gap.\nThis pattern holds even for models featuring a &#8220;thinking mode,&#8221; which, as our analysis in Section 5.2 shows, fails to improve reasoning despite a significant increase in latency.</p>\n\n",
                "matched_terms": [
                    "factual",
                    "math"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our diagnostic experiments indicate the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> stems not from simple engineering limitations, but from a deeper architectural conflict between real-time streaming and complex reasoning.\nFirst, extended thinking time provides negligible benefit: Audio Flamingo 3&#8217;s thinking mode increases latency from 2.40s to 15.14s (a 530% increase) to allow internal deliberation before speaking, yet accuracy actually decreases from 1.7% to 1.5% overall while Context performance degrades from 3.8% to 1.8%.\nThe latency-accuracy frontier in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> confirms this pattern across all models, showing voice systems plateau below 10% accuracy regardless of response time, with no voice systems achieving both sub-1.5s latency and above-11% accuracy.\nSecond, the LiveAnswer cascade experiment isolates the modality penalty by using the same powerful GPT-5 model as our text upper bound.\nEven in this ideal setup, a persistent 15.7 percentage point gap remained on the Math track (59.1% vs. the text model&#8217;s 74.8%).\nThis drop is largely attributable to the narration synthesizer, which must translate the reasoner&#8217;s complex output into fluent speech periodically.\nThis translation process introduced logical inconsistencies and was particularly detrimental to tasks requiring exact string matching, causing a near-total failure on the Context track (0.2%).\nAs detailed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.F5\" title=\"Figure 5 &#8227; 5.2 Why does the gap exist? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the time-to-first-response for this system averages 10.5s, dominated by the Speech-to-Text step.\nThis demonstrates that even a sophisticated, decoupled architecture still cannot fully close the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span>, reinforcing the need for more fundamental architectural innovation to bridge the gap between deep reasoning and real-time narration.\nThird, output quality measurements confirm that speech synthesis is not the bottleneck: speech clarity does not determine success, as models across the WER spectrum from 7.9% (Gemini-2.5-Flash-Audio) to 19.8% (Freeze-Omni) show uniformly poor reasoning performance.\nCollectively, these diagnostic experiments demonstrate that the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> is not a simple engineering artifact that can be fixed by allocating more time, decoupling the architecture, or improving speech quality. The persistence of the gap across these conditions points instead to a fundamental constraint in how current streaming architectures support multi-step computation.</p>\n\n",
                "matched_terms": [
                    "track",
                    "overall",
                    "math"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inter-annotator agreement analysis</p>\n\n",
                "matched_terms": [
                    "interannotator",
                    "agreement"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sampled 1,000 model outputs stratified across tracks (Math: 46, Web: 490, Science: 70, Factual: 394) for human validation. Each output was evaluated as correct or incorrect given the ground truth answer.</p>\n\n",
                "matched_terms": [
                    "science",
                    "web",
                    "factual",
                    "math"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The near-perfect agreement on Math, Web, and Factual tracks reflects the objective nature of these tasks with clear correct answers. The lower but still strong agreement on Science (84.3-92.9%) appropriately captures the greater interpretive complexity in graduate-level scientific reasoning. These validation results confirm that our LLM-based evaluation provides reliable and consistent judgments aligned with human assessment.</p>\n\n",
                "matched_terms": [
                    "science",
                    "factual",
                    "math",
                    "web",
                    "agreement"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GPT-4o.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib26\" title=\"\">2024a</a>)</cite> A multimodal model evaluated in text-only mode with web search enabled.</p>\n\n",
                "matched_terms": [
                    "web",
                    "gpt4o"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All primary comparisons show highly significant differences (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), confirming that the Voice Reasoning Gap is not due to measurement noise or random variation. The gap persists even when using identical text models with voice I/O wrappers (LiveAnswer), indicating that modality constraints rather than model capacity drive the performance degradation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>The Web track shows no significant difference in the LiveAnswer comparison (p = 0.636), likely due to low baseline performance (<math alttext=\"\\approx 12\\%\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>12</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 12\\%</annotation></semantics></math>) in both modalities.</span></span></span></p>\n\n",
                "matched_terms": [
                    "track",
                    "web",
                    "primary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Note on anomalies:</span> The Web track shows no significant difference in the LiveAnswer comparison (<math alttext=\"p=0.636\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.636</mn></mrow><annotation encoding=\"application/x-tex\">p=0.636</annotation></semantics></math>), likely because both modalities struggle equally with multi-hop synthesis where base performance is low (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>12%). The Context track exhibits anomalously low LiveAnswer performance (0.2%), suggesting a possible system-specific failure that warrants investigation.</p>\n\n",
                "matched_terms": [
                    "track",
                    "web"
                ]
            }
        ]
    },
    "A5.T7": {
        "source_file": "Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap",
        "caption": "Table 7: Statistical significance of voice-text performance gaps across key model comparisons",
        "body": "Comparison\nGap (%)\n95% CI\n\npp-value\nN\n\n\nPrimary comparison\n\n\nGPT-5 vs GPT-realtime\n40.4\n[37.7, 43.2]\n<0.001<0.001\n2,931\n\n\nControlled comparisons\n\n\nGPT-5 vs LiveAnswera\n\n24.7\n[22.2, 27.2]\n<0.001<0.001\n2,931\n\n\nGemini text vs voiceb\n\n39.7\n[37.0, 42.4]\n<0.001<0.001\n2,931",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Comparison</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Gap (%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">95% CI</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A5.T7.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">N</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"5\"><span class=\"ltx_text ltx_font_italic\">Primary comparison</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">GPT-5 vs GPT-realtime</td>\n<td class=\"ltx_td ltx_align_center\">40.4</td>\n<td class=\"ltx_td ltx_align_center\">[37.7, 43.2]</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A5.T7.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">&lt;0.001</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">2,931</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"5\"><span class=\"ltx_text ltx_font_italic\">Controlled comparisons</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">GPT-5 vs LiveAnswer<sup class=\"ltx_sup\">a</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">24.7</td>\n<td class=\"ltx_td ltx_align_center\">[22.2, 27.2]</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A5.T7.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">&lt;0.001</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">2,931</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Gemini text vs voice<sup class=\"ltx_sup\">b</sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">39.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">[37.0, 42.4]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A5.T7.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">&lt;0.001</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2,931</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "text",
            "voicetext",
            "significance",
            "gemini",
            "statistical",
            "gap",
            "gpt5",
            "across",
            "key",
            "ppvalue",
            "liveanswera",
            "comparisons",
            "model",
            "primary",
            "performance",
            "controlled",
            "gaps",
            "voiceb",
            "comparison",
            "gptrealtime"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Voice Evaluation of Reasoning Ability (<span class=\"ltx_text ltx_font_smallcaps\">VERA</span>), a benchmark for evaluating <em class=\"ltx_emph ltx_font_italic\">reasoning</em> ability in voice-interactive systems under real-time conversational constraints.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> comprises 2,931 voice-native episodes derived from established text benchmarks and organized into five tracks (Math, Web, Science, Long-Context, Factual).\nEach item is adapted for speech interaction while preserving reasoning difficulty.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> enables direct text&#8211;voice comparison within model families and supports analysis of how architectural choices affect reliability.\nWe assess 12 contemporary voice systems alongside strong text baselines and observe large, consistent modality gaps: on competition mathematics a leading text model attains 74.8% accuracy while its voice counterpart reaches 6.1%; macro-averaged across tracks the best text models achieve 54.0% versus 11.3% for voice.\nLatency&#8211;accuracy analyses reveal a low-latency plateau, where fast voice systems cluster around <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>10% accuracy, while approaching text performance requires sacrificing real-time interaction.\nDiagnostic experiments indicate that common mitigations are insufficient.\nIncreasing &#8220;thinking time&#8221; yields negligible gains; a decoupled cascade that separates reasoning from narration improves accuracy but still falls well short of text and introduces characteristic grounding/consistency errors.\nFailure analyses further show distinct error signatures across native streaming, end-to-end, and cascade designs.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> provides a reproducible testbed and targeted diagnostics for architectures that decouple <em class=\"ltx_emph ltx_font_italic\">thinking</em> from <em class=\"ltx_emph ltx_font_italic\">speaking</em>, offering a principled way to measure progress toward real-time voice assistants that are both fluent and reliably reasoned.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across",
                    "text",
                    "gaps",
                    "performance",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a systematic evaluation of reasoning in today&#8217;s voice-interactive systems, documenting a significant and consistent performance degradation we term the Voice Reasoning Gap (<span class=\"ltx_text ltx_font_smallcaps\">VRG</span>). This gap is most pronounced on complex, multi-step reasoning tasks. For example, in our study, a leading voice assistant, GPT-realtime&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib27\" title=\"\">2024b</a>)</cite>, achieves 6.1% accuracy on mathematical problems, whereas a top-performing text model from the same developer, GPT-5&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib29\" title=\"\">2025b</a>)</cite>, achieves 74.8%. This 68.7-point difference is not an isolated finding but is representative of a broader pattern where models optimized for low-latency streaming show consistently lower performance.</p>\n\n",
                "matched_terms": [
                    "gpt5",
                    "model",
                    "gap",
                    "text",
                    "performance",
                    "gptrealtime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate this hypothesis, we introduce the <span class=\"ltx_text ltx_font_bold\">Voice Evaluation of Reasoning Ability (<span class=\"ltx_text ltx_font_smallcaps\">VERA</span>)</span>, a benchmark designed to measure reasoning under real-time constraints. Our analysis with <span class=\"ltx_text ltx_font_smallcaps\">VERA</span>, summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, reveals a clear <span class=\"ltx_text ltx_font_italic\">latency-accuracy trade-off</span>. The data shows a <span class=\"ltx_text ltx_font_bold\">low-latency plateau</span>, where the fastest voice models remain shallow in their reasoning, and a <span class=\"ltx_text ltx_font_bold\">cascade lift, not parity</span>, where even a powerful text reasoner decoupled from the voice interface improves accuracy but still falls significantly short of its native text performance. Together, these patterns demonstrate that the ideal <span class=\"ltx_text ltx_font_italic\">fast-and-accurate upper-left corner of the frontier remains empty</span>, suggesting the gap is a systemic challenge for current architectures, not merely an efficiency issue. This work provides a framework for diagnosing these trade-offs, complementing (rather than replacing) existing audio-understanding evaluation. Our analysis uncovers distinct failure signatures tied to system architecture; for instance, native streaming models tend to produce fluent but incorrect responses, while decoupled cascades are more prone to grounding errors. The patterns we observe highlight key opportunities for improvement and suggest promising research directions. Our main contributions are:</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance",
                    "gap",
                    "key"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantifies and diagnoses the Voice Reasoning Gap.</span> We provide systematic measurements showing voice models achieve 42% lower accuracy on average, with gaps exceeding 68% on complex domains. Controlled experiments including cascade baselines demonstrate this gap persists even with perfect acoustic conditions and extended thinking time.</p>\n\n",
                "matched_terms": [
                    "gaps",
                    "controlled",
                    "gap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Provides a unified evaluation framework for real-time systems.</span> <span class=\"ltx_text ltx_font_smallcaps\">VERA</span> enables fair comparison across heterogeneous voice architectures (native, cascade, and end-to-end) within a single evaluation protocol, a non-trivial orchestration that establishes a reproducible benchmark for measuring progress toward genuinely intelligent voice assistants.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code and data available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/linyueqian/VERA\" title=\"\">https://github.com/linyueqian/VERA</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "comparison",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing voice benchmarks, while valuable, have not evaluated the ability of models to perform general-purpose reasoning through a real-time conversational interface.\nInstead, prior work has focused on two distinct areas: a model&#8217;s ability to understand the acoustic signal itself, and its ability to manage conversational mechanics. Benchmarks like SUPERB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib50\" title=\"\">2021</a>)</cite>, AudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib39\" title=\"\">2024a</a>)</cite>, and even more recent ones like MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib32\" title=\"\">2024</a>)</cite> and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib21\" title=\"\">2025</a>)</cite>, evaluate <span class=\"ltx_text ltx_font_bold\">audio-content understanding, often with reasoning about sound</span>&#8212;tasks such as identifying events from sounds, analyzing acoustic scenes, or answering questions about the properties of the audio signal.\nSeparately, the spoken language understanding (SLU) and spoken-QA literature targets mapping speech to meaning, including intent and slot filling, dialog state tracking, and extractive or conversational QA, with representative corpora such as Spoken SQuAD, ODSQA, Spoken-CoQA, HeySQuAD, and the SLUE suite (Phase-1/2)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib15\" title=\"\">2018b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib14\" title=\"\">a</a>; You et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib51\" title=\"\">2022</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib46\" title=\"\">2023</a>; Shon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib34\" title=\"\">2022</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nThese datasets assess comprehension of recorded speech but generally lack explicit real-time constraints and do not provide text&#8211;versus&#8211;voice comparisons on reasoning problems.\nConcurrently, a separate line of work on full-duplex systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib30\" title=\"\">2025</a>; Arora et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib2\" title=\"\">2025</a>)</cite> has focused on the <span class=\"ltx_text ltx_font_bold\">mechanics of dialogue</span>, such as turn-taking and interruption handling, without evaluating the substantive reasoning that must occur within that conversation. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a comparative overview of representative benchmarks across these areas.</p>\n\n",
                "matched_terms": [
                    "comparisons",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates (with a more comprehensive catalog in Appendix Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A1.T4\" title=\"Table 4 &#8227; Appendix A Previous Benchmarks &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), this focus on distinct capabilities has created a clear evaluation gap. The field measures whether a model can <em class=\"ltx_emph ltx_font_italic\">hear</em> (Audio Understanding), <em class=\"ltx_emph ltx_font_italic\">understand</em> spoken language, or <em class=\"ltx_emph ltx_font_italic\">handle</em> interaction mechanics (full-duplex/latency), but not whether it can <span class=\"ltx_text ltx_font_bold\">think on general problems while talking</span>.\nNo existing benchmark combines <span class=\"ltx_text ltx_font_bold\">(1) multi-step, general-purpose reasoning</span> with <span class=\"ltx_text ltx_font_bold\">(2) explicit real-time latency constraints</span> and <span class=\"ltx_text ltx_font_bold\">(3) a direct, cross-modal text&#8211;versus&#8211;voice comparison on identical tasks</span>.\nThis gap helps explain why the severe reasoning degradation we document has gone unquantified.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> is the first to occupy this intersection, providing a focused diagnostic tool for the trade-offs between conversational fluency and reasoning depth in modern voice systems.</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "model",
                    "gap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We formalize the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> with a metric that we then operationalize for practical evaluation. For a distribution of reasoning tasks <math alttext=\"\\mathcal{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><annotation encoding=\"application/x-tex\">\\mathcal{T}</annotation></semantics></math>, we define the gap as the expected difference in accuracy between text and voice modalities:</p>\n\n",
                "matched_terms": [
                    "text",
                    "gap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"P_{\\text{text}}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>P</mi><mtext>text</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P_{\\text{text}}(t)</annotation></semantics></math> and <math alttext=\"P_{\\text{voice}}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>P</mi><mtext>voice</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P_{\\text{voice}}(t)</annotation></semantics></math> represent the best achievable accuracy on task <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. In practice, we measure this by comparing top-performing models, using those from the same family where possible (e.g., GPT-5 vs. GPT-realtime). A crucial part of this framework is the text baseline; <em class=\"ltx_emph ltx_font_italic\">for this reference, we adopt accuracy-oriented text models rather than voice models with a text input</em>, as the latter remain architecturally optimized for low latency and would conflate modality with latency policy.</p>\n\n",
                "matched_terms": [
                    "text",
                    "gpt5",
                    "gptrealtime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study provides a <span class=\"ltx_text ltx_font_bold\">diagnostic characterization</span> of the current voice systems&#8217; landscape, not a controlled experiment designed to prove causality. Because we evaluate heterogeneous commercial systems with different architectures and training objectives, <span class=\"ltx_text ltx_font_bold\">we cannot isolate the causal impact of modality alone</span>. Rather, our goal is to systematically document system performance and identify recurring, cross-model patterns that point toward underlying architectural challenges. The consistency of the gap we find across 12 systems strongly suggests that these challenges are fundamental and merit this investigation, for which we provide a reproducible benchmark.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "controlled",
                    "gap",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The theoretical basis for the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> arises from the different operational dynamics of each interface. Current text-based generation is akin to <span class=\"ltx_text ltx_font_bold\">drafting</span>: models can explore multiple reasoning paths internally or use chain-of-thought to self-correct before committing to a final answer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib43\" title=\"\">2022</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib42\" title=\"\">2023b</a>)</cite>. This ability to &#8220;revise&#8221; is critical for complex problem-solving. In stark contrast, voice-native generation is a <span class=\"ltx_text ltx_font_bold\">live performance</span>. To maintain conversational fluency, models must begin generating an <em class=\"ltx_emph ltx_font_italic\">irreversible stream of audio</em> almost immediately, forcing a <span class=\"ltx_text ltx_font_italic\">streaming commitment</span> to an initial reasoning path that may be shallow or flawed. Once spoken, a token cannot be taken back, causing early missteps to cascade into unrecoverable errors. The model must divide its computational resources between the cognitive task of reasoning and the motor task of coherent speech synthesis, further constraining its problem-solving capacity.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This architectural asymmetry between revisable drafting and irreversible performance raises a series of critical diagnostic questions that guide our analysis. First, <span class=\"ltx_text ltx_font_bold\">what</span> is the magnitude of the gap, and how does it vary across different types of reasoning tasks? Second, <span class=\"ltx_text ltx_font_bold\">why</span> does this gap exist? Can it be attributed to simple engineering factors like insufficient thinking time or poor audio fidelity, or does it reflect a more fundamental limitation? Finally, <span class=\"ltx_text ltx_font_bold\">how</span> do these systems fail? Do different voice architectures produce systematically different error signatures? To answer these questions, <span class=\"ltx_text ltx_font_smallcaps\">VERA</span> is designed to enable controlled comparisons on identical reasoning tasks, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S3.F2\" title=\"Figure 2 &#8227; 3 The VERA Benchmark &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, while applying realistic conversational and latency constraints.</p>\n\n",
                "matched_terms": [
                    "comparisons",
                    "gap",
                    "across",
                    "performance",
                    "controlled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Fidelity Assessment.</span> We evaluate generated speech using Word Error Rate (WER), comparing ASR transcripts against ground truth. Our LLM-based normalizer standardizes both the reference text and ASR\ntranscript to canonical mathematical notation (e.g., &#8220;f of sixteen equals fifty four&#8221; &#8594; &#8220;f(16) = 54&#8221;, &#8220;twenty twenty-four&#8221; &#8594; &#8220;2024&#8221;) before comparison. This normalization, with further examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A2\" title=\"Appendix B ASR Transcript Normalization &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>, ensures a fair comparison between mathematical expressions in written form and their spoken equivalents&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sproat &amp; Jaitly, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib37\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Failure Analysis.</span> To understand error patterns systematically, we conduct detailed failure attribution on incorrect predictions using a comprehensive error taxonomy. Our analysis framework employs GPT-5 to classify failures across 16 error categories spanning knowledge errors (e.g., entity confusion, temporal errors), reasoning errors (e.g., computation mistakes, logical contradictions), and understanding errors (e.g., misinterpretation, off-target responses). For voice models specifically, the analysis distinguishes between transcription artifacts and genuine content errors, providing insights into whether failures stem from speech processing or core reasoning capabilities. This multi-label classification enables fine-grained understanding of model limitations and identifies systematic failure modes across different task types.</p>\n\n",
                "matched_terms": [
                    "gpt5",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To diagnose the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span>, we evaluate a comprehensive set of models on the <span class=\"ltx_text ltx_font_smallcaps\">VERA</span> benchmark. Our evaluation spans three categories of voice systems: <span class=\"ltx_text ltx_font_bold\">commercial realtime APIs</span> (GPT-realtime, Gemini-2.5-Flash-audio, Amazon Nova Sonic); <span class=\"ltx_text ltx_font_bold\">open voice models</span> (Qwen2-Audio, UltraVox, Audio Flamingo 3, Phi-4-multimodal); and <span class=\"ltx_text ltx_font_bold\">end-to-end architectures</span> that directly generate speech (Moshi, Freeze-Omni, Qwen2.5-Omni). Against these, we benchmark two critical references to isolate the source of the performance drop. First, a <span class=\"ltx_text ltx_font_bold\">text-only upper bound</span> (GPT-4o, GPT-5, Gemini-2.5 Pro/Flash) quantifies maximum achievable accuracy by isolating reasoning capacity from modality constraints. Second, we construct a sophisticated <span class=\"ltx_text ltx_font_bold\">cascade baseline</span>, <em class=\"ltx_emph ltx_font_italic\">LiveAnswer</em>, to simulate an architecture that separates deep reasoning from real-time narration. <em class=\"ltx_emph ltx_font_italic\">LiveAnswer</em> uses GPT-5 as a powerful core reasoner and a faster Llama-3.3-70B-Instruct as a narration synthesizer to convert the detailed reasoning into a concise, fluent spoken response, allowing us to test whether the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> persists even when thinking and speaking are decoupled. Full implementation details and citations for all models are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A4\" title=\"Appendix D Model Implementation Details &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "gpt5",
                    "gptrealtime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation (table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.T3\" title=\"Table 3 &#8227; 5.1 What is the gap and how does it vary by task? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) reveals a stark <span class=\"ltx_text ltx_font_smallcaps\">VRG</span>: an average accuracy drop of 40.4 percentage points for voice models that widens dramatically on tasks requiring complex, multi-step reasoning.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Unless otherwise stated, gaps are computed against the text baseline (GPT-5, effort=low) while the text upper bound refers to GPT-5 (effort=high) and is shown as the dashed line in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</span></span></span>\nThis gap scales systematically with the complexity of the reasoning required.\nFor instance, while factual retrieval shows moderate degradation (GPT-5 text: 48.3% vs. GPT-realtime voice: 27.4%), the gap widens dramatically for tasks requiring multi-step reasoning, with mathematical reasoning exhibiting a near-total collapse in performance (GPT-5: 74.8% vs. GPT-realtime: 6.1%).\nThis suggests that certain tasks, such as the multi-hop synthesis required in our Web track, become particularly intractable under the constraints of a streaming voice interface.\nStatistical validation using McNemar&#8217;s test <cite class=\"ltx_cite ltx_citemacro_citep\">(McNemar, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib23\" title=\"\">1947</a>)</cite> confirms these differences are highly significant (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), as detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A5\" title=\"Appendix E Statistical Validation &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.\nThis pattern of differential failure extends universally across the diverse voice architectures we evaluated.\nThey consistently perform best on retrieval or short-answer tasks while failing on complex reasoning. GPT-realtime achieves its highest score on Factual questions (27.4%) but drops to 6.1% on Math.\nSome models exhibit extreme specialization; UltraVox, for example, maintains 26.6% accuracy on Context while scoring 0.0% on Math, suggesting an optimization for conversational continuity at the expense of deep reasoning.\nThis trend holds for Gemini&#8217;s audio model (18.8% on Context vs. 3.5% on Math) and open-source models like Phi-4-multimodal (12.0% on Context vs. 0.0% on Math).\nThis consistent pattern across 12 diverse voice systems demonstrates that the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> is not a model-specific artifact but a universal property of current voice technology, with the gap scaling systematically from moderate on simple retrieval tasks to severe on complex reasoning.</p>\n\n",
                "matched_terms": [
                    "gpt5",
                    "statistical",
                    "gap",
                    "across",
                    "model",
                    "text",
                    "gaps",
                    "performance",
                    "gptrealtime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.F4\" title=\"Figure 4 &#8227; 5.1 What is the gap and how does it vary by task? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> further demonstrates this pattern for several model families.\nPanel (a) shows GPT-5 text maintaining robust multi-domain performance (54% radar chart coverage) while GPT-realtime voice achieves only 11% coverage, with moderate performance on Factual (27.4%) but severe weakness across reasoning tasks.\nPanel (b) confirms generalization to Gemini models, with text variants achieving 40-50% coverage versus 11% for voice.\nPanel (c) reveals that even diverse voice architectures&#8212;including an <em class=\"ltx_emph ltx_font_italic\">audio-encoder + LLM text-decoder</em> design (Qwen2-Audio), an <em class=\"ltx_emph ltx_font_italic\">end-to-end Thinker&#8211;Talker</em> model that jointly generates text and speech (Qwen2.5-Omni), and a Whisper-style encoder + LLM with on-demand reasoning (Audio Flamingo 3)&#8212;remain confined below 5% accuracy on reasoning tasks.\nThe variance within voice models (<math alttext=\"\\sigma^{2}=3.66\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><msup><mi>&#963;</mi><mn>2</mn></msup><mo>=</mo><mn>3.66</mn></mrow><annotation encoding=\"application/x-tex\">\\sigma^{2}=3.66</annotation></semantics></math> across Math scores) is 171&#215; smaller than between modalities (<math alttext=\"\\sigma^{2}=625.92\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><msup><mi>&#963;</mi><mn>2</mn></msup><mo>=</mo><mn>625.92</mn></mrow><annotation encoding=\"application/x-tex\">\\sigma^{2}=625.92</annotation></semantics></math>), confirming that architectural variations within the voice paradigm produce marginal improvements compared to the fundamental gap.\nThis pattern holds even for models featuring a &#8220;thinking mode,&#8221; which, as our analysis in Section 5.2 shows, fails to improve reasoning despite a significant increase in latency.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "gpt5",
                    "gap",
                    "model",
                    "across",
                    "text",
                    "performance",
                    "gptrealtime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our diagnostic experiments indicate the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> stems not from simple engineering limitations, but from a deeper architectural conflict between real-time streaming and complex reasoning.\nFirst, extended thinking time provides negligible benefit: Audio Flamingo 3&#8217;s thinking mode increases latency from 2.40s to 15.14s (a 530% increase) to allow internal deliberation before speaking, yet accuracy actually decreases from 1.7% to 1.5% overall while Context performance degrades from 3.8% to 1.8%.\nThe latency-accuracy frontier in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> confirms this pattern across all models, showing voice systems plateau below 10% accuracy regardless of response time, with no voice systems achieving both sub-1.5s latency and above-11% accuracy.\nSecond, the LiveAnswer cascade experiment isolates the modality penalty by using the same powerful GPT-5 model as our text upper bound.\nEven in this ideal setup, a persistent 15.7 percentage point gap remained on the Math track (59.1% vs. the text model&#8217;s 74.8%).\nThis drop is largely attributable to the narration synthesizer, which must translate the reasoner&#8217;s complex output into fluent speech periodically.\nThis translation process introduced logical inconsistencies and was particularly detrimental to tasks requiring exact string matching, causing a near-total failure on the Context track (0.2%).\nAs detailed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.F5\" title=\"Figure 5 &#8227; 5.2 Why does the gap exist? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the time-to-first-response for this system averages 10.5s, dominated by the Speech-to-Text step.\nThis demonstrates that even a sophisticated, decoupled architecture still cannot fully close the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span>, reinforcing the need for more fundamental architectural innovation to bridge the gap between deep reasoning and real-time narration.\nThird, output quality measurements confirm that speech synthesis is not the bottleneck: speech clarity does not determine success, as models across the WER spectrum from 7.9% (Gemini-2.5-Flash-Audio) to 19.8% (Freeze-Omni) show uniformly poor reasoning performance.\nCollectively, these diagnostic experiments demonstrate that the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> is not a simple engineering artifact that can be fixed by allocating more time, decoupling the architecture, or improving speech quality. The persistence of the gap across these conditions points instead to a fundamental constraint in how current streaming architectures support multi-step computation.</p>\n\n",
                "matched_terms": [
                    "gpt5",
                    "model",
                    "gap",
                    "across",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings indicate that achieving human-level reasoning in voice assistants will require architectural innovations beyond incremental improvements.\nThe convergent evidence from our analysis establishes that the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> appears not to be explained by the engineering factors we ablate, indicating architectural changes may be needed.\nThe 40.4 percentage point average gap resists all conventional solutions, single models show large performance differentials between retrieval and reasoning, and even architectural decoupling yields an irreducible 15.7-point penalty.\nThe systematic failure patterns in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.F6\" title=\"Figure 6 &#8227; 5.3 How do the models fail differently? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, particularly streaming commitment errors&#8212;manifesting primarily as <span class=\"ltx_text ltx_font_smallcaps\">off_target</span> and <span class=\"ltx_text ltx_font_smallcaps\">no_final_answer</span> deviations that <em class=\"ltx_emph ltx_font_italic\">vary by architecture</em> (underproduced for native voice, overproduced for cascades)&#8212;mechanistically explain why incremental improvements cannot bridge this gap.\nThese findings point toward our central design principle: architectures must decouple thinking from speaking through an editable internal state separate from the speech output buffer.\nThis principle suggests several research directions including asynchronous architectures <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib18\" title=\"\">2025c</a>)</cite> where backend reasoning models operate with higher latency while frontend verbalizers maintain conversational flow, and chunked reasoning with parallel processing <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib5\" title=\"\">2025</a>)</cite> where models use audio playback time to compute next reasoning steps.\nOur LiveAnswer analysis (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.F5\" title=\"Figure 5 &#8227; 5.2 Why does the gap exist? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) reveals specific engineering challenges: managing the latency-accuracy trade-off through streaming ASR with confidence-gated handoff and answer-first narration strategies, and ensuring cross-stage consistency to prevent the grounding failures (<span class=\"ltx_text ltx_font_smallcaps\">unsupported_fact</span> at +0.27) that arise when decoupling modules.\nAchieving human-like reasoning in voice assistants ultimately requires unique architectures that strategically combine pre-computation, parallel processing, and selective verbalization to deliver systems that are both deeply intelligent and naturally conversational.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "gap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work systematically documents and diagnoses the Voice Reasoning Gap, a significant and consistent performance drop observed when current language models operate through a voice interface compared to text.\nUsing our purpose-built benchmark, <span class=\"ltx_text ltx_font_smallcaps\">VERA</span>, we provide the first quantitative characterization of this gap across a range of models and complex reasoning tasks.\nOur diagnostic experiments show that this performance degradation is not a simple engineering artifact, as it persists even when granting models extended thinking time, ensuring high audio fidelity, or employing a sophisticated cascade architecture that separates the reasoning core from audio I/O.\nInstead, our analysis suggests a fundamental tension between the architectural demands of low-latency streaming and the iterative, revisable computation required for deep reasoning.\nWe identified distinct failure signatures tied to different architectures, finding that native streaming models tend to fail by producing fluent but incorrect responses, while decoupled systems introduce grounding and consistency errors.\nThese findings indicate that bridging the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> will likely require a paradigm shift away from monolithic architectures toward novel systems that explicitly decouple reasoning from real-time narration.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> provides a critical diagnostic tool to guide and measure progress toward this goal, paving the way for voice assistants that are not only fluent but also genuinely intelligent.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance",
                    "gap",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Significance testing of voice-text performance gaps</p>\n\n",
                "matched_terms": [
                    "voicetext",
                    "performance",
                    "gaps",
                    "significance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sampled 1,000 model outputs stratified across tracks (Math: 46, Web: 490, Science: 70, Factual: 394) for human validation. Each output was evaluated as correct or incorrect given the ground truth answer.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GPT-realtime.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib27\" title=\"\">2024b</a>)</cite> A commercial, full-duplex voice model with streaming audio input and low-latency speech output. We use it as a native voice baseline: the model listens while speaking, produces incremental audio tokens, and has no separate text-reasoning stage exposed to the user. It serves as a representative of end-to-end, latency-optimized voice agents.</p>\n\n",
                "matched_terms": [
                    "model",
                    "gptrealtime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qwen2-Audio</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib6\" title=\"\">2024</a>)</cite>. A Large Audio-Language Model (LALM) that processes speech and text inputs to generate textual outputs. It demonstrates strong instruction-following over speech, sound, and music datasets, and provides an open baseline for voice understanding and mixed-modality dialogue.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Flamingo 3</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib9\" title=\"\">2025</a>)</cite>. An audio-language model that supports in-context learning, retrieval-augmented generation, and multi-turn dialogues over audio streams. We evaluate both its standard setting and a <em class=\"ltx_emph ltx_font_italic\">thinking mode</em> that allows extra internal compute before emitting final text.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Moshi.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib7\" title=\"\">2024</a>)</cite> A real-time speech-in/speech-out model that directly maps audio to audio with minimal intermediate text exposure. We use it to probe the limits of ultra-low-latency architectures where most computation is spent on conversational fluidity.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qwen2.5-Omni.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib47\" title=\"\">2025</a>)</cite> An omni model in the Qwen family that supports speech, text, and vision. We evaluate its native voice mode to compare omni-style training with audio-specialized training (cf. Qwen2-Audio).</p>\n\n",
                "matched_terms": [
                    "text",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GPT-5 (effort=low/high).</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib29\" title=\"\">2025b</a>)</cite> A reasoning model where &#8220;effort&#8221; denotes a higher decode-time compute budget (longer deliberation, slower first token). The high-effort setting allows for extended chain-of-thought reasoning at the cost of increased latency.</p>\n\n",
                "matched_terms": [
                    "gpt5",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Core Reasoner.</span> The first module is the <span class=\"ltx_text ltx_font_typewriter\">ProblemSolver</span>, which serves as the powerful but potentially slow cognitive core of the system. It is responsible for the actual problem-solving, leveraging <span class=\"ltx_text ltx_font_bold\">GPT-5</span> through its responses endpoint. This module is equipped with tools like web search and a code interpreter to handle complex, multi-hop reasoning tasks. Instead of generating a single, final text block, the solver produces a stream of structured &#8220;thoughts&#8221; that represent its internal state. This includes reasoning summaries, tool call invocations, and finally, the computed answer. These thoughts are not sent directly to the user but are pushed to the Narration Synthesizer via the <span class=\"ltx_text ltx_font_typewriter\">push_thought</span> method.</p>\n\n",
                "matched_terms": [
                    "text",
                    "gpt5"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The full <span class=\"ltx_text ltx_font_typewriter\">LiveAnswer</span> pipeline operates as follows: (1) user speech is transcribed by <span class=\"ltx_text ltx_font_bold\">Azure Speech-to-Text</span>; (2) the text is sent to the <span class=\"ltx_text ltx_font_bold\">Core Reasoner</span> (GPT-5), which begins its detailed reasoning process; (3) in parallel, the <span class=\"ltx_text ltx_font_bold\">Narration Synthesizer</span> (Llama-3.3) generates an immediate, ongoing narration based on the stream of thoughts from the reasoner; (4) this narration is rendered into audio by <span class=\"ltx_text ltx_font_bold\">Azure Text-to-Speech</span>. This dual-model architecture directly tests the hypothesis that separating the &#8220;thinking&#8221; from the &#8220;speaking&#8221; can mitigate the Voice Reasoning Gap.</p>\n\n",
                "matched_terms": [
                    "text",
                    "gpt5",
                    "gap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Configuration Notes.</span> For all voice-native systems we enable streaming and full-duplex whenever supported by the provider. Unless otherwise stated, we do not allow web tools or retrieval beyond what the model natively exposes. Text upper bounds are evaluated with the same prompts and answer formats as their voice counterparts, differing only in modality and (for &#8220;effort=high&#8221;) decode-time budget.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conducted comprehensive statistical testing to validate the robustness of the Voice Reasoning Gap. All comparisons use McNemar&#8217;s test for paired predictions, with confidence intervals estimated via bootstrap resampling (10,000 iterations).</p>\n\n",
                "matched_terms": [
                    "comparisons",
                    "statistical",
                    "gap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All primary comparisons show highly significant differences (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), confirming that the Voice Reasoning Gap is not due to measurement noise or random variation. The gap persists even when using identical text models with voice I/O wrappers (LiveAnswer), indicating that modality constraints rather than model capacity drive the performance degradation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>The Web track shows no significant difference in the LiveAnswer comparison (p = 0.636), likely due to low baseline performance (<math alttext=\"\\approx 12\\%\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>12</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 12\\%</annotation></semantics></math>) in both modalities.</span></span></span></p>\n\n",
                "matched_terms": [
                    "comparisons",
                    "model",
                    "gap",
                    "text",
                    "primary",
                    "performance",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Note on anomalies:</span> The Web track shows no significant difference in the LiveAnswer comparison (<math alttext=\"p=0.636\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.636</mn></mrow><annotation encoding=\"application/x-tex\">p=0.636</annotation></semantics></math>), likely because both modalities struggle equally with multi-hop synthesis where base performance is low (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>12%). The Context track exhibits anomalously low LiveAnswer performance (0.2%), suggesting a possible system-specific failure that warrants investigation.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "comparison"
                ]
            }
        ]
    },
    "A5.T8": {
        "source_file": "Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap",
        "caption": "Table 8: Track-by-track statistical analysis for GPT-5 vs GPT-realtime comparison",
        "body": "Track\nN\nText Acc\nVoice Acc\nGap (%)\n\npp-value\n\n\nMath\n115\n74.8%\n6.1%\n68.7\n<0.001<0.001\n\n\nWeb\n1,107\n12.3%\n0.8%\n11.5\n<0.001<0.001\n\n\nScience\n161\n42.2%\n13.0%\n29.2\n<0.001<0.001\n\n\nContext\n548\n80.8%\n9.3%\n71.5\n<0.001<0.001\n\n\nFactual\n1,000\n48.3%\n27.4%\n20.9\n<0.001<0.001",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Track</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">N</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Text Acc</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Voice Acc</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Gap (%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A5.T8.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Math</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">115</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">74.8%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.1%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">68.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A5.T8.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">&lt;0.001</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Web</td>\n<td class=\"ltx_td ltx_align_center\">1,107</td>\n<td class=\"ltx_td ltx_align_center\">12.3%</td>\n<td class=\"ltx_td ltx_align_center\">0.8%</td>\n<td class=\"ltx_td ltx_align_center\">11.5</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A5.T8.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">&lt;0.001</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Science</td>\n<td class=\"ltx_td ltx_align_center\">161</td>\n<td class=\"ltx_td ltx_align_center\">42.2%</td>\n<td class=\"ltx_td ltx_align_center\">13.0%</td>\n<td class=\"ltx_td ltx_align_center\">29.2</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A5.T8.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">&lt;0.001</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Context</td>\n<td class=\"ltx_td ltx_align_center\">548</td>\n<td class=\"ltx_td ltx_align_center\">80.8%</td>\n<td class=\"ltx_td ltx_align_center\">9.3%</td>\n<td class=\"ltx_td ltx_align_center\">71.5</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A5.T8.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">&lt;0.001</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Factual</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1,000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">48.3%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">27.4%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">20.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A5.T8.m6\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">&lt;0.001</annotation></semantics></math></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "track",
            "acc",
            "gpt5",
            "statistical",
            "gap",
            "voice",
            "text",
            "science",
            "factual",
            "trackbytrack",
            "ppvalue",
            "analysis",
            "context",
            "math",
            "web",
            "comparison",
            "gptrealtime"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Voice Evaluation of Reasoning Ability (<span class=\"ltx_text ltx_font_smallcaps\">VERA</span>), a benchmark for evaluating <em class=\"ltx_emph ltx_font_italic\">reasoning</em> ability in voice-interactive systems under real-time conversational constraints.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> comprises 2,931 voice-native episodes derived from established text benchmarks and organized into five tracks (Math, Web, Science, Long-Context, Factual).\nEach item is adapted for speech interaction while preserving reasoning difficulty.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> enables direct text&#8211;voice comparison within model families and supports analysis of how architectural choices affect reliability.\nWe assess 12 contemporary voice systems alongside strong text baselines and observe large, consistent modality gaps: on competition mathematics a leading text model attains 74.8% accuracy while its voice counterpart reaches 6.1%; macro-averaged across tracks the best text models achieve 54.0% versus 11.3% for voice.\nLatency&#8211;accuracy analyses reveal a low-latency plateau, where fast voice systems cluster around <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>10% accuracy, while approaching text performance requires sacrificing real-time interaction.\nDiagnostic experiments indicate that common mitigations are insufficient.\nIncreasing &#8220;thinking time&#8221; yields negligible gains; a decoupled cascade that separates reasoning from narration improves accuracy but still falls well short of text and introduces characteristic grounding/consistency errors.\nFailure analyses further show distinct error signatures across native streaming, end-to-end, and cascade designs.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> provides a reproducible testbed and targeted diagnostics for architectures that decouple <em class=\"ltx_emph ltx_font_italic\">thinking</em> from <em class=\"ltx_emph ltx_font_italic\">speaking</em>, offering a principled way to measure progress toward real-time voice assistants that are both fluent and reliably reasoned.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "text",
                    "science",
                    "factual",
                    "analysis",
                    "math",
                    "web",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a systematic evaluation of reasoning in today&#8217;s voice-interactive systems, documenting a significant and consistent performance degradation we term the Voice Reasoning Gap (<span class=\"ltx_text ltx_font_smallcaps\">VRG</span>). This gap is most pronounced on complex, multi-step reasoning tasks. For example, in our study, a leading voice assistant, GPT-realtime&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib27\" title=\"\">2024b</a>)</cite>, achieves 6.1% accuracy on mathematical problems, whereas a top-performing text model from the same developer, GPT-5&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib29\" title=\"\">2025b</a>)</cite>, achieves 74.8%. This 68.7-point difference is not an isolated finding but is representative of a broader pattern where models optimized for low-latency streaming show consistently lower performance.</p>\n\n",
                "matched_terms": [
                    "gpt5",
                    "gap",
                    "voice",
                    "text",
                    "gptrealtime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate this hypothesis, we introduce the <span class=\"ltx_text ltx_font_bold\">Voice Evaluation of Reasoning Ability (<span class=\"ltx_text ltx_font_smallcaps\">VERA</span>)</span>, a benchmark designed to measure reasoning under real-time constraints. Our analysis with <span class=\"ltx_text ltx_font_smallcaps\">VERA</span>, summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, reveals a clear <span class=\"ltx_text ltx_font_italic\">latency-accuracy trade-off</span>. The data shows a <span class=\"ltx_text ltx_font_bold\">low-latency plateau</span>, where the fastest voice models remain shallow in their reasoning, and a <span class=\"ltx_text ltx_font_bold\">cascade lift, not parity</span>, where even a powerful text reasoner decoupled from the voice interface improves accuracy but still falls significantly short of its native text performance. Together, these patterns demonstrate that the ideal <span class=\"ltx_text ltx_font_italic\">fast-and-accurate upper-left corner of the frontier remains empty</span>, suggesting the gap is a systemic challenge for current architectures, not merely an efficiency issue. This work provides a framework for diagnosing these trade-offs, complementing (rather than replacing) existing audio-understanding evaluation. Our analysis uncovers distinct failure signatures tied to system architecture; for instance, native streaming models tend to produce fluent but incorrect responses, while decoupled cascades are more prone to grounding errors. The patterns we observe highlight key opportunities for improvement and suggest promising research directions. Our main contributions are:</p>\n\n",
                "matched_terms": [
                    "text",
                    "analysis",
                    "gap",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantifies and diagnoses the Voice Reasoning Gap.</span> We provide systematic measurements showing voice models achieve 42% lower accuracy on average, with gaps exceeding 68% on complex domains. Controlled experiments including cascade baselines demonstrate this gap persists even with perfect acoustic conditions and extended thinking time.</p>\n\n",
                "matched_terms": [
                    "gap",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Characterizes distinct failure signatures tied to voice architectures.</span> Through analysis of 2,931 episodes, we provide the first systematic evidence showing that different voice system designs (e.g., native streaming vs. decoupled cascade) fail in predictably different ways, creating a diagnostic fingerprint for the underlying architectural trade-offs.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Provides a unified evaluation framework for real-time systems.</span> <span class=\"ltx_text ltx_font_smallcaps\">VERA</span> enables fair comparison across heterogeneous voice architectures (native, cascade, and end-to-end) within a single evaluation protocol, a non-trivial orchestration that establishes a reproducible benchmark for measuring progress toward genuinely intelligent voice assistants.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code and data available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/linyueqian/VERA\" title=\"\">https://github.com/linyueqian/VERA</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "comparison",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates (with a more comprehensive catalog in Appendix Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A1.T4\" title=\"Table 4 &#8227; Appendix A Previous Benchmarks &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), this focus on distinct capabilities has created a clear evaluation gap. The field measures whether a model can <em class=\"ltx_emph ltx_font_italic\">hear</em> (Audio Understanding), <em class=\"ltx_emph ltx_font_italic\">understand</em> spoken language, or <em class=\"ltx_emph ltx_font_italic\">handle</em> interaction mechanics (full-duplex/latency), but not whether it can <span class=\"ltx_text ltx_font_bold\">think on general problems while talking</span>.\nNo existing benchmark combines <span class=\"ltx_text ltx_font_bold\">(1) multi-step, general-purpose reasoning</span> with <span class=\"ltx_text ltx_font_bold\">(2) explicit real-time latency constraints</span> and <span class=\"ltx_text ltx_font_bold\">(3) a direct, cross-modal text&#8211;versus&#8211;voice comparison on identical tasks</span>.\nThis gap helps explain why the severe reasoning degradation we document has gone unquantified.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> is the first to occupy this intersection, providing a focused diagnostic tool for the trade-offs between conversational fluency and reasoning depth in modern voice systems.</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "gap",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We formalize the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> with a metric that we then operationalize for practical evaluation. For a distribution of reasoning tasks <math alttext=\"\\mathcal{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><annotation encoding=\"application/x-tex\">\\mathcal{T}</annotation></semantics></math>, we define the gap as the expected difference in accuracy between text and voice modalities:</p>\n\n",
                "matched_terms": [
                    "text",
                    "gap",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"P_{\\text{text}}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>P</mi><mtext>text</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P_{\\text{text}}(t)</annotation></semantics></math> and <math alttext=\"P_{\\text{voice}}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>P</mi><mtext>voice</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P_{\\text{voice}}(t)</annotation></semantics></math> represent the best achievable accuracy on task <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. In practice, we measure this by comparing top-performing models, using those from the same family where possible (e.g., GPT-5 vs. GPT-realtime). A crucial part of this framework is the text baseline; <em class=\"ltx_emph ltx_font_italic\">for this reference, we adopt accuracy-oriented text models rather than voice models with a text input</em>, as the latter remain architecturally optimized for low latency and would conflate modality with latency policy.</p>\n\n",
                "matched_terms": [
                    "text",
                    "gpt5",
                    "voice",
                    "gptrealtime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study provides a <span class=\"ltx_text ltx_font_bold\">diagnostic characterization</span> of the current voice systems&#8217; landscape, not a controlled experiment designed to prove causality. Because we evaluate heterogeneous commercial systems with different architectures and training objectives, <span class=\"ltx_text ltx_font_bold\">we cannot isolate the causal impact of modality alone</span>. Rather, our goal is to systematically document system performance and identify recurring, cross-model patterns that point toward underlying architectural challenges. The consistency of the gap we find across 12 systems strongly suggests that these challenges are fundamental and merit this investigation, for which we provide a reproducible benchmark.</p>\n\n",
                "matched_terms": [
                    "gap",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This architectural asymmetry between revisable drafting and irreversible performance raises a series of critical diagnostic questions that guide our analysis. First, <span class=\"ltx_text ltx_font_bold\">what</span> is the magnitude of the gap, and how does it vary across different types of reasoning tasks? Second, <span class=\"ltx_text ltx_font_bold\">why</span> does this gap exist? Can it be attributed to simple engineering factors like insufficient thinking time or poor audio fidelity, or does it reflect a more fundamental limitation? Finally, <span class=\"ltx_text ltx_font_bold\">how</span> do these systems fail? Do different voice architectures produce systematically different error signatures? To answer these questions, <span class=\"ltx_text ltx_font_smallcaps\">VERA</span> is designed to enable controlled comparisons on identical reasoning tasks, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S3.F2\" title=\"Figure 2 &#8227; 3 The VERA Benchmark &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, while applying realistic conversational and latency constraints.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "gap",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech generation.</span>\nValidated text episodes are rendered to 24kHz audio using Higgs-Audio v2 <cite class=\"ltx_cite ltx_citemacro_cite\">Boson AI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib3\" title=\"\">2025</a>)</cite>, which generates naturalistic speech with automatic variation in timbre, tone, and emotion based on textual content. This TTS system produces acoustically diverse outputs through its inherent voice variation, ensuring models are evaluated on reasoning rather than adaptation to specific acoustic patterns (see Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S3.SS3\" title=\"3.3 Dataset Composition &#8227; 3 The VERA Benchmark &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a> for diversity analysis).</p>\n\n",
                "matched_terms": [
                    "text",
                    "analysis",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our benchmark is structured around five complementary tracks, each designed to isolate a distinct failure mode in voice-based reasoning. <span class=\"ltx_text ltx_font_bold\">Mathematical reasoning</span>, using 115 problems from the AIME math competition&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mathematical Association of America, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib22\" title=\"\">2025</a>)</cite>, tests solution coherence while speaking. <span class=\"ltx_text ltx_font_bold\">Web-grounded synthesis</span>, with 1,107 questions from the BrowseComp web-navigation benchmark&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib45\" title=\"\">2025</a>)</cite>, evaluates information integration under streaming constraints. <span class=\"ltx_text ltx_font_bold\">Scientific expertise</span>, drawn from 161 graduate-level GPQA Diamond questions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib31\" title=\"\">2023</a>)</cite>, probes knowledge access under the cognitive load of simultaneous speech generation. <span class=\"ltx_text ltx_font_bold\">Long-context memory</span>, using 548 MRCR episodes&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib28\" title=\"\">2025a</a>)</cite> with contexts up to 100K characters, examines state tracking during extended interactions. Finally, a crucial baseline of <span class=\"ltx_text ltx_font_bold\">Factual recall</span>, with 1,000 SimpleQA questions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib44\" title=\"\">2024</a>)</cite>, isolates architectural overhead from reasoning complexity.</p>\n\n",
                "matched_terms": [
                    "factual",
                    "math"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Fidelity Assessment.</span> We evaluate generated speech using Word Error Rate (WER), comparing ASR transcripts against ground truth. Our LLM-based normalizer standardizes both the reference text and ASR\ntranscript to canonical mathematical notation (e.g., &#8220;f of sixteen equals fifty four&#8221; &#8594; &#8220;f(16) = 54&#8221;, &#8220;twenty twenty-four&#8221; &#8594; &#8220;2024&#8221;) before comparison. This normalization, with further examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A2\" title=\"Appendix B ASR Transcript Normalization &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>, ensures a fair comparison between mathematical expressions in written form and their spoken equivalents&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sproat &amp; Jaitly, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib37\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Failure Analysis.</span> To understand error patterns systematically, we conduct detailed failure attribution on incorrect predictions using a comprehensive error taxonomy. Our analysis framework employs GPT-5 to classify failures across 16 error categories spanning knowledge errors (e.g., entity confusion, temporal errors), reasoning errors (e.g., computation mistakes, logical contradictions), and understanding errors (e.g., misinterpretation, off-target responses). For voice models specifically, the analysis distinguishes between transcription artifacts and genuine content errors, providing insights into whether failures stem from speech processing or core reasoning capabilities. This multi-label classification enables fine-grained understanding of model limitations and identifies systematic failure modes across different task types.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "gpt5",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Calibration.</span> To validate our LLM-based evaluation, we conducted human evaluation on 1,000 randomly sampled predictions across all tracks and models. GPT-4o&#8217;s judgments achieved 97.8% agreement with human evaluation (95% CI: 96.8-98.7%), ranging from perfect agreement on Math (100%) to 84.3% on Science where answers require more nuanced interpretation. Cross-vendor validation using Gemini-2.5-Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Google Cloud, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib10\" title=\"\">2025a</a>)</cite> achieved 98.7% agreement with human evaluation and 98.1% with GPT-4o, confirming minimal vendor bias and consistent evaluation standards across judges. Detailed analyses are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A3\" title=\"Appendix C Human Evaluation and Judge Validation &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "science",
                    "math"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To diagnose the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span>, we evaluate a comprehensive set of models on the <span class=\"ltx_text ltx_font_smallcaps\">VERA</span> benchmark. Our evaluation spans three categories of voice systems: <span class=\"ltx_text ltx_font_bold\">commercial realtime APIs</span> (GPT-realtime, Gemini-2.5-Flash-audio, Amazon Nova Sonic); <span class=\"ltx_text ltx_font_bold\">open voice models</span> (Qwen2-Audio, UltraVox, Audio Flamingo 3, Phi-4-multimodal); and <span class=\"ltx_text ltx_font_bold\">end-to-end architectures</span> that directly generate speech (Moshi, Freeze-Omni, Qwen2.5-Omni). Against these, we benchmark two critical references to isolate the source of the performance drop. First, a <span class=\"ltx_text ltx_font_bold\">text-only upper bound</span> (GPT-4o, GPT-5, Gemini-2.5 Pro/Flash) quantifies maximum achievable accuracy by isolating reasoning capacity from modality constraints. Second, we construct a sophisticated <span class=\"ltx_text ltx_font_bold\">cascade baseline</span>, <em class=\"ltx_emph ltx_font_italic\">LiveAnswer</em>, to simulate an architecture that separates deep reasoning from real-time narration. <em class=\"ltx_emph ltx_font_italic\">LiveAnswer</em> uses GPT-5 as a powerful core reasoner and a faster Llama-3.3-70B-Instruct as a narration synthesizer to convert the detailed reasoning into a concise, fluent spoken response, allowing us to test whether the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> persists even when thinking and speaking are decoupled. Full implementation details and citations for all models are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A4\" title=\"Appendix D Model Implementation Details &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "gpt5",
                    "voice",
                    "gptrealtime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation (table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.T3\" title=\"Table 3 &#8227; 5.1 What is the gap and how does it vary by task? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) reveals a stark <span class=\"ltx_text ltx_font_smallcaps\">VRG</span>: an average accuracy drop of 40.4 percentage points for voice models that widens dramatically on tasks requiring complex, multi-step reasoning.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Unless otherwise stated, gaps are computed against the text baseline (GPT-5, effort=low) while the text upper bound refers to GPT-5 (effort=high) and is shown as the dashed line in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</span></span></span>\nThis gap scales systematically with the complexity of the reasoning required.\nFor instance, while factual retrieval shows moderate degradation (GPT-5 text: 48.3% vs. GPT-realtime voice: 27.4%), the gap widens dramatically for tasks requiring multi-step reasoning, with mathematical reasoning exhibiting a near-total collapse in performance (GPT-5: 74.8% vs. GPT-realtime: 6.1%).\nThis suggests that certain tasks, such as the multi-hop synthesis required in our Web track, become particularly intractable under the constraints of a streaming voice interface.\nStatistical validation using McNemar&#8217;s test <cite class=\"ltx_cite ltx_citemacro_citep\">(McNemar, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib23\" title=\"\">1947</a>)</cite> confirms these differences are highly significant (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), as detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#A5\" title=\"Appendix E Statistical Validation &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.\nThis pattern of differential failure extends universally across the diverse voice architectures we evaluated.\nThey consistently perform best on retrieval or short-answer tasks while failing on complex reasoning. GPT-realtime achieves its highest score on Factual questions (27.4%) but drops to 6.1% on Math.\nSome models exhibit extreme specialization; UltraVox, for example, maintains 26.6% accuracy on Context while scoring 0.0% on Math, suggesting an optimization for conversational continuity at the expense of deep reasoning.\nThis trend holds for Gemini&#8217;s audio model (18.8% on Context vs. 3.5% on Math) and open-source models like Phi-4-multimodal (12.0% on Context vs. 0.0% on Math).\nThis consistent pattern across 12 diverse voice systems demonstrates that the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> is not a model-specific artifact but a universal property of current voice technology, with the gap scaling systematically from moderate on simple retrieval tasks to severe on complex reasoning.</p>\n\n",
                "matched_terms": [
                    "track",
                    "gpt5",
                    "gap",
                    "statistical",
                    "voice",
                    "text",
                    "factual",
                    "context",
                    "math",
                    "web",
                    "gptrealtime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.F4\" title=\"Figure 4 &#8227; 5.1 What is the gap and how does it vary by task? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> further demonstrates this pattern for several model families.\nPanel (a) shows GPT-5 text maintaining robust multi-domain performance (54% radar chart coverage) while GPT-realtime voice achieves only 11% coverage, with moderate performance on Factual (27.4%) but severe weakness across reasoning tasks.\nPanel (b) confirms generalization to Gemini models, with text variants achieving 40-50% coverage versus 11% for voice.\nPanel (c) reveals that even diverse voice architectures&#8212;including an <em class=\"ltx_emph ltx_font_italic\">audio-encoder + LLM text-decoder</em> design (Qwen2-Audio), an <em class=\"ltx_emph ltx_font_italic\">end-to-end Thinker&#8211;Talker</em> model that jointly generates text and speech (Qwen2.5-Omni), and a Whisper-style encoder + LLM with on-demand reasoning (Audio Flamingo 3)&#8212;remain confined below 5% accuracy on reasoning tasks.\nThe variance within voice models (<math alttext=\"\\sigma^{2}=3.66\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><msup><mi>&#963;</mi><mn>2</mn></msup><mo>=</mo><mn>3.66</mn></mrow><annotation encoding=\"application/x-tex\">\\sigma^{2}=3.66</annotation></semantics></math> across Math scores) is 171&#215; smaller than between modalities (<math alttext=\"\\sigma^{2}=625.92\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><msup><mi>&#963;</mi><mn>2</mn></msup><mo>=</mo><mn>625.92</mn></mrow><annotation encoding=\"application/x-tex\">\\sigma^{2}=625.92</annotation></semantics></math>), confirming that architectural variations within the voice paradigm produce marginal improvements compared to the fundamental gap.\nThis pattern holds even for models featuring a &#8220;thinking mode,&#8221; which, as our analysis in Section 5.2 shows, fails to improve reasoning despite a significant increase in latency.</p>\n\n",
                "matched_terms": [
                    "gpt5",
                    "gap",
                    "voice",
                    "text",
                    "factual",
                    "analysis",
                    "math",
                    "gptrealtime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our diagnostic experiments indicate the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> stems not from simple engineering limitations, but from a deeper architectural conflict between real-time streaming and complex reasoning.\nFirst, extended thinking time provides negligible benefit: Audio Flamingo 3&#8217;s thinking mode increases latency from 2.40s to 15.14s (a 530% increase) to allow internal deliberation before speaking, yet accuracy actually decreases from 1.7% to 1.5% overall while Context performance degrades from 3.8% to 1.8%.\nThe latency-accuracy frontier in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> confirms this pattern across all models, showing voice systems plateau below 10% accuracy regardless of response time, with no voice systems achieving both sub-1.5s latency and above-11% accuracy.\nSecond, the LiveAnswer cascade experiment isolates the modality penalty by using the same powerful GPT-5 model as our text upper bound.\nEven in this ideal setup, a persistent 15.7 percentage point gap remained on the Math track (59.1% vs. the text model&#8217;s 74.8%).\nThis drop is largely attributable to the narration synthesizer, which must translate the reasoner&#8217;s complex output into fluent speech periodically.\nThis translation process introduced logical inconsistencies and was particularly detrimental to tasks requiring exact string matching, causing a near-total failure on the Context track (0.2%).\nAs detailed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.F5\" title=\"Figure 5 &#8227; 5.2 Why does the gap exist? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the time-to-first-response for this system averages 10.5s, dominated by the Speech-to-Text step.\nThis demonstrates that even a sophisticated, decoupled architecture still cannot fully close the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span>, reinforcing the need for more fundamental architectural innovation to bridge the gap between deep reasoning and real-time narration.\nThird, output quality measurements confirm that speech synthesis is not the bottleneck: speech clarity does not determine success, as models across the WER spectrum from 7.9% (Gemini-2.5-Flash-Audio) to 19.8% (Freeze-Omni) show uniformly poor reasoning performance.\nCollectively, these diagnostic experiments demonstrate that the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> is not a simple engineering artifact that can be fixed by allocating more time, decoupling the architecture, or improving speech quality. The persistence of the gap across these conditions points instead to a fundamental constraint in how current streaming architectures support multi-step computation.</p>\n\n",
                "matched_terms": [
                    "track",
                    "gpt5",
                    "gap",
                    "voice",
                    "text",
                    "context",
                    "math"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Voice models fail in systematically different ways tied to their architecture: native streaming models tend to fail by prioritizing fluent completion over accuracy, while decoupled cascade systems are more prone to internal logical contradictions.\nNative streaming models like GPT-realtime and Gemini-2.5-Flash-Audio show a strong bias towards completing their responses, even when incorrect.\nThey produce significantly fewer <span class=\"ltx_text ltx_font_smallcaps\">no_final_answer</span> and <span class=\"ltx_text ltx_font_smallcaps\">off_target</span> errors than the average, suggesting an architectural pressure to maintain conversational fluency at the cost of accuracy.\nThey are designed to avoid silence or abandonment, leading them to generate fluent continuations even when their underlying reasoning is flawed.\nCascade systems present an orthogonal failure profile: LiveAnswer shows strong positive deviations for <span class=\"ltx_text ltx_font_smallcaps\">unsupported_fact</span> (+0.27), <span class=\"ltx_text ltx_font_smallcaps\">off_target</span> (+0.31), and <span class=\"ltx_text ltx_font_smallcaps\">logical_contradiction</span> (+0.22), indicating systematic inconsistencies between reasoning and verbalization stages that manifest as factual grounding failures and logical incoherence.\nEnd-to-end architectures diverge maximally from baseline: Moshi exhibits extreme <span class=\"ltx_text ltx_font_smallcaps\">off_target</span> deviation (+0.52) with suppressed rates elsewhere, while Qwen2.5-Omni shows the inverse pattern with <span class=\"ltx_text ltx_font_smallcaps\">no_final_answer</span> (+0.36) but strong negative deviations for <span class=\"ltx_text ltx_font_smallcaps\">unsupported_fact</span> (-0.47), indicating task disengagement rather than incorrect completion.</p>\n\n",
                "matched_terms": [
                    "factual",
                    "voice",
                    "gptrealtime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings indicate that achieving human-level reasoning in voice assistants will require architectural innovations beyond incremental improvements.\nThe convergent evidence from our analysis establishes that the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> appears not to be explained by the engineering factors we ablate, indicating architectural changes may be needed.\nThe 40.4 percentage point average gap resists all conventional solutions, single models show large performance differentials between retrieval and reasoning, and even architectural decoupling yields an irreducible 15.7-point penalty.\nThe systematic failure patterns in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.F6\" title=\"Figure 6 &#8227; 5.3 How do the models fail differently? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, particularly streaming commitment errors&#8212;manifesting primarily as <span class=\"ltx_text ltx_font_smallcaps\">off_target</span> and <span class=\"ltx_text ltx_font_smallcaps\">no_final_answer</span> deviations that <em class=\"ltx_emph ltx_font_italic\">vary by architecture</em> (underproduced for native voice, overproduced for cascades)&#8212;mechanistically explain why incremental improvements cannot bridge this gap.\nThese findings point toward our central design principle: architectures must decouple thinking from speaking through an editable internal state separate from the speech output buffer.\nThis principle suggests several research directions including asynchronous architectures <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib18\" title=\"\">2025c</a>)</cite> where backend reasoning models operate with higher latency while frontend verbalizers maintain conversational flow, and chunked reasoning with parallel processing <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib5\" title=\"\">2025</a>)</cite> where models use audio playback time to compute next reasoning steps.\nOur LiveAnswer analysis (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#S5.F5\" title=\"Figure 5 &#8227; 5.2 Why does the gap exist? &#8227; 5 Results and Analysis &#8227; Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) reveals specific engineering challenges: managing the latency-accuracy trade-off through streaming ASR with confidence-gated handoff and answer-first narration strategies, and ensuring cross-stage consistency to prevent the grounding failures (<span class=\"ltx_text ltx_font_smallcaps\">unsupported_fact</span> at +0.27) that arise when decoupling modules.\nAchieving human-like reasoning in voice assistants ultimately requires unique architectures that strategically combine pre-computation, parallel processing, and selective verbalization to deliver systems that are both deeply intelligent and naturally conversational.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "gap",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work systematically documents and diagnoses the Voice Reasoning Gap, a significant and consistent performance drop observed when current language models operate through a voice interface compared to text.\nUsing our purpose-built benchmark, <span class=\"ltx_text ltx_font_smallcaps\">VERA</span>, we provide the first quantitative characterization of this gap across a range of models and complex reasoning tasks.\nOur diagnostic experiments show that this performance degradation is not a simple engineering artifact, as it persists even when granting models extended thinking time, ensuring high audio fidelity, or employing a sophisticated cascade architecture that separates the reasoning core from audio I/O.\nInstead, our analysis suggests a fundamental tension between the architectural demands of low-latency streaming and the iterative, revisable computation required for deep reasoning.\nWe identified distinct failure signatures tied to different architectures, finding that native streaming models tend to fail by producing fluent but incorrect responses, while decoupled systems introduce grounding and consistency errors.\nThese findings indicate that bridging the <span class=\"ltx_text ltx_font_smallcaps\">VRG</span> will likely require a paradigm shift away from monolithic architectures toward novel systems that explicitly decouple reasoning from real-time narration.\n<span class=\"ltx_text ltx_font_smallcaps\">VERA</span> provides a critical diagnostic tool to guide and measure progress toward this goal, paving the way for voice assistants that are not only fluent but also genuinely intelligent.</p>\n\n",
                "matched_terms": [
                    "text",
                    "analysis",
                    "gap",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comprehensive comparison of voice benchmarks and their capabilities</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Track-by-track statistical analysis</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "statistical",
                    "trackbytrack"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sampled 1,000 model outputs stratified across tracks (Math: 46, Web: 490, Science: 70, Factual: 394) for human validation. Each output was evaluated as correct or incorrect given the ground truth answer.</p>\n\n",
                "matched_terms": [
                    "science",
                    "web",
                    "factual",
                    "math"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The near-perfect agreement on Math, Web, and Factual tracks reflects the objective nature of these tasks with clear correct answers. The lower but still strong agreement on Science (84.3-92.9%) appropriately captures the greater interpretive complexity in graduate-level scientific reasoning. These validation results confirm that our LLM-based evaluation provides reliable and consistent judgments aligned with human assessment.</p>\n\n",
                "matched_terms": [
                    "science",
                    "web",
                    "factual",
                    "math"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GPT-realtime.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib27\" title=\"\">2024b</a>)</cite> A commercial, full-duplex voice model with streaming audio input and low-latency speech output. We use it as a native voice baseline: the model listens while speaking, produces incremental audio tokens, and has no separate text-reasoning stage exposed to the user. It serves as a representative of end-to-end, latency-optimized voice agents.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "gptrealtime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Gemini-2.5-Flash-audio.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Google Cloud, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib10\" title=\"\">2025a</a>)</cite>A commercial, low-latency audio-capable model accessed through a streaming voice endpoint. We use it as a second native voice baseline emphasizing responsiveness over long-form reasoning. It supports real-time speech I/O with web search capability enabled; we treat it as a black box with default vendor settings.</p>\n\n",
                "matched_terms": [
                    "web",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Nova-Sonic.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Amazon Web Services, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib1\" title=\"\">2025</a>)</cite> A commercial real-time voice system with streaming speech in/out. We include it to broaden the coverage of native, production-grade voice agents. We do not modify decoding parameters beyond the provider defaults.</p>\n\n",
                "matched_terms": [
                    "web",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qwen2-Audio</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib6\" title=\"\">2024</a>)</cite>. A Large Audio-Language Model (LALM) that processes speech and text inputs to generate textual outputs. It demonstrates strong instruction-following over speech, sound, and music datasets, and provides an open baseline for voice understanding and mixed-modality dialogue.</p>\n\n",
                "matched_terms": [
                    "text",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phi-4-multimodal.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Microsoft, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib25\" title=\"\">2025</a>)</cite> A compact multimodal LLM that accepts text plus non-text inputs (including audio via a front-end encoder) and produces text outputs. We use it as a smaller-capacity open baseline to test whether compact models can sustain reasoning under voice constraints.</p>\n\n",
                "matched_terms": [
                    "text",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qwen2.5-Omni.</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26542v1#bib.bib47\" title=\"\">2025</a>)</cite> An omni model in the Qwen family that supports speech, text, and vision. We evaluate its native voice mode to compare omni-style training with audio-specialized training (cf. Qwen2-Audio).</p>\n\n",
                "matched_terms": [
                    "text",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These systems receive the same tasks but interact purely via text, isolating reasoning capacity from voice constraints.</p>\n\n",
                "matched_terms": [
                    "text",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Core Reasoner.</span> The first module is the <span class=\"ltx_text ltx_font_typewriter\">ProblemSolver</span>, which serves as the powerful but potentially slow cognitive core of the system. It is responsible for the actual problem-solving, leveraging <span class=\"ltx_text ltx_font_bold\">GPT-5</span> through its responses endpoint. This module is equipped with tools like web search and a code interpreter to handle complex, multi-hop reasoning tasks. Instead of generating a single, final text block, the solver produces a stream of structured &#8220;thoughts&#8221; that represent its internal state. This includes reasoning summaries, tool call invocations, and finally, the computed answer. These thoughts are not sent directly to the user but are pushed to the Narration Synthesizer via the <span class=\"ltx_text ltx_font_typewriter\">push_thought</span> method.</p>\n\n",
                "matched_terms": [
                    "text",
                    "web",
                    "gpt5"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Incremental Updates:</span> As the Core Reasoner pushes new thoughts (e.g., updates from a web search), the synthesizer incorporates this new information into its ongoing narration. It includes logic to generate natural-sounding filler text (e.g., &#8220;I&#8217;m still thinking about this&#8230;&#8221;) if the Core Reasoner is taking a long time between thoughts, preventing awkward silences.</p>\n\n",
                "matched_terms": [
                    "text",
                    "web"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The full <span class=\"ltx_text ltx_font_typewriter\">LiveAnswer</span> pipeline operates as follows: (1) user speech is transcribed by <span class=\"ltx_text ltx_font_bold\">Azure Speech-to-Text</span>; (2) the text is sent to the <span class=\"ltx_text ltx_font_bold\">Core Reasoner</span> (GPT-5), which begins its detailed reasoning process; (3) in parallel, the <span class=\"ltx_text ltx_font_bold\">Narration Synthesizer</span> (Llama-3.3) generates an immediate, ongoing narration based on the stream of thoughts from the reasoner; (4) this narration is rendered into audio by <span class=\"ltx_text ltx_font_bold\">Azure Text-to-Speech</span>. This dual-model architecture directly tests the hypothesis that separating the &#8220;thinking&#8221; from the &#8220;speaking&#8221; can mitigate the Voice Reasoning Gap.</p>\n\n",
                "matched_terms": [
                    "text",
                    "gpt5",
                    "gap",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">WER Analysis.</span> We run ASR on model-generated speech and apply an LLM-based normalizer to canonicalize spoken math and notation before scoring.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "math"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Configuration Notes.</span> For all voice-native systems we enable streaming and full-duplex whenever supported by the provider. Unless otherwise stated, we do not allow web tools or retrieval beyond what the model natively exposes. Text upper bounds are evaluated with the same prompts and answer formats as their voice counterparts, differing only in modality and (for &#8220;effort=high&#8221;) decode-time budget.</p>\n\n",
                "matched_terms": [
                    "text",
                    "web",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conducted comprehensive statistical testing to validate the robustness of the Voice Reasoning Gap. All comparisons use McNemar&#8217;s test for paired predictions, with confidence intervals estimated via bootstrap resampling (10,000 iterations).</p>\n\n",
                "matched_terms": [
                    "statistical",
                    "gap",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><sup class=\"ltx_sup\">a</sup>LiveAnswer uses GPT-5 for reasoning with voice I/O wrapper</p>\n\n",
                "matched_terms": [
                    "gpt5",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All primary comparisons show highly significant differences (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), confirming that the Voice Reasoning Gap is not due to measurement noise or random variation. The gap persists even when using identical text models with voice I/O wrappers (LiveAnswer), indicating that modality constraints rather than model capacity drive the performance degradation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>The Web track shows no significant difference in the LiveAnswer comparison (p = 0.636), likely due to low baseline performance (<math alttext=\"\\approx 12\\%\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>12</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 12\\%</annotation></semantics></math>) in both modalities.</span></span></span></p>\n\n",
                "matched_terms": [
                    "track",
                    "gap",
                    "voice",
                    "text",
                    "web",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Note on anomalies:</span> The Web track shows no significant difference in the LiveAnswer comparison (<math alttext=\"p=0.636\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.636</mn></mrow><annotation encoding=\"application/x-tex\">p=0.636</annotation></semantics></math>), likely because both modalities struggle equally with multi-hop synthesis where base performance is low (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>12%). The Context track exhibits anomalously low LiveAnswer performance (0.2%), suggesting a possible system-specific failure that warrants investigation.</p>\n\n",
                "matched_terms": [
                    "track",
                    "context",
                    "web",
                    "comparison"
                ]
            }
        ]
    }
}