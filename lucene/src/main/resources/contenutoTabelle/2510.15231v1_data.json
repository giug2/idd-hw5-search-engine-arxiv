{
    "S4.SS1.SSS0.Px1.tab1": {
        "caption": "Table 1: Training-free audio context extension performance (Accuracy). We compare extension methods under two strategies: stretching from the original 30s context and the empirically observed 2mins context. We see that stretching from the observed 2mins context proves significantly more effective. Comparing between the two top approaches: Whole and Partial YaRN, we find their performance to be closed, suggesting that preserving the positional information of text tokens might not be of utmost importance, at least in this MCQA setting of ours.",
        "body": "<table class=\"ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" colspan=\"2\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">YODAS2-MCQA</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1 min</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2 mins</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5 mins</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">10 mins</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"2\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>[gray]0.95 &#8194;&#8202;&#8194;&#8202;<span class=\"ltx_text ltx_font_bold\">GPT 4o</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">89.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">92.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">90.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">83.65</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" colspan=\"2\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>[gray]0.95 &#8194;&#8202;&#8194;&#8202;<span class=\"ltx_text ltx_font_bold\">Gemini 2.0 Flash</span>\n</th>\n<td class=\"ltx_td ltx_align_center\">94.70</td>\n<td class=\"ltx_td ltx_align_center\">95.86</td>\n<td class=\"ltx_td ltx_align_center\">92.80</td>\n<td class=\"ltx_td ltx_align_center\">89.87</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"2\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>[gray]0.95 &#8194;&#8202;&#8194;&#8202;<span class=\"ltx_text ltx_font_bold\">SALMONN</span>\n</th>\n<td class=\"ltx_td ltx_border_t\" colspan=\"4\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Vanilla</th>\n<td class=\"ltx_td ltx_align_center\">49.01</td>\n<td class=\"ltx_td ltx_align_center\">46.13</td>\n<td class=\"ltx_td ltx_align_center\">32.93</td>\n<td class=\"ltx_td ltx_align_center\">23.47</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" colspan=\"6\"><span class=\"ltx_text ltx_font_italic\">Stretching from 30s (original context)</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Whole PI</th>\n<td class=\"ltx_td ltx_align_center\">39.87 <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-9.14\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m1\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">9.14</mn></mrow><annotation encoding=\"application/x-tex\">-9.14</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">5.73 <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-40.40\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m2\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">40.40</mn></mrow><annotation encoding=\"application/x-tex\">-40.40</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.13 <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-32.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m3\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">32.80</mn></mrow><annotation encoding=\"application/x-tex\">-32.80</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">23.20 <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-0.27\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m4\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">0.27</mn></mrow><annotation encoding=\"application/x-tex\">-0.27</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Partial PI</th>\n<td class=\"ltx_td ltx_align_center\">50.07 <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+1.06\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m5\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">1.06</mn></mrow><annotation encoding=\"application/x-tex\">+1.06</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">45.60 <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-0.53\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m6\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">0.53</mn></mrow><annotation encoding=\"application/x-tex\">-0.53</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">12.40 <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-20.53\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m7\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">20.53</mn></mrow><annotation encoding=\"application/x-tex\">-20.53</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">1.20 <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-22.27\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m8\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">22.27</mn></mrow><annotation encoding=\"application/x-tex\">-22.27</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Whole YaRN</th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">54.70</span> <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+5.69\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m9\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">5.69</mn></mrow><annotation encoding=\"application/x-tex\">+5.69</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">59.87</span> <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+13.74\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m10\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">13.74</mn></mrow><annotation encoding=\"application/x-tex\">+13.74</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">47.20</span> <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+14.27\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m11\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">14.27</mn></mrow><annotation encoding=\"application/x-tex\">+14.27</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">30.13 <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+6.66\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m12\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">6.66</mn></mrow><annotation encoding=\"application/x-tex\">+6.66</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Partial YaRN</th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">57.35</span> <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+8.34\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m13\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">8.34</mn></mrow><annotation encoding=\"application/x-tex\">+8.34</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">59.60</span> <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+13.47\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m14\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">13.47</mn></mrow><annotation encoding=\"application/x-tex\">+13.47</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">38.53 <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+5.60\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m15\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">5.60</mn></mrow><annotation encoding=\"application/x-tex\">+5.60</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">32.93 <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+9.46\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m16\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">9.46</mn></mrow><annotation encoding=\"application/x-tex\">+9.46</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" colspan=\"6\"><span class=\"ltx_text ltx_font_italic\">Stretching from 2 mins (observed context)</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Whole PI</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#808080;\">49.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#808080;\">46.13</span></td>\n<td class=\"ltx_td ltx_align_center\">13.07 <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-19.86\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m17\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">19.86</mn></mrow><annotation encoding=\"application/x-tex\">-19.86</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">23.60 <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+0.13\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m18\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">0.13</mn></mrow><annotation encoding=\"application/x-tex\">+0.13</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Partial PI</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#808080;\">49.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#808080;\">46.13</span></td>\n<td class=\"ltx_td ltx_align_center\">28.27 <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-4.66\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m19\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">4.66</mn></mrow><annotation encoding=\"application/x-tex\">-4.66</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">4.13 <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-19.34\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m20\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">19.34</mn></mrow><annotation encoding=\"application/x-tex\">-19.34</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Whole YaRN</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#808080;\">49.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#808080;\">46.13</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">45.87</span> <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+12.94\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m21\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">12.94</mn></mrow><annotation encoding=\"application/x-tex\">+12.94</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">38.27</span> <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+14.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m22\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">14.80</mn></mrow><annotation encoding=\"application/x-tex\">+14.80</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Partial YaRN</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#808080;\">49.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#808080;\">46.13</span></td>\n<td class=\"ltx_td ltx_align_center\">41.47 <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+8.54\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m23\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">8.54</mn></mrow><annotation encoding=\"application/x-tex\">+8.54</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">33.45</span> <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+9.98\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m24\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">9.98</mn></mrow><annotation encoding=\"application/x-tex\">+9.98</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"2\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>[gray]0.95 &#8194;&#8202;&#8194;&#8202;<span class=\"ltx_text ltx_font_bold\">Qwen2-Audio 7B Instruct</span>\n</th>\n<td class=\"ltx_td ltx_border_t\" colspan=\"4\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Vanilla</th>\n<td class=\"ltx_td ltx_align_center\">72.71</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">75.87</span></td>\n<td class=\"ltx_td ltx_align_center\">55.33</td>\n<td class=\"ltx_td ltx_align_center\">22.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" colspan=\"6\"><span class=\"ltx_text ltx_font_italic\">Stretching from 30s (original context)</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Whole PI</th>\n<td class=\"ltx_td ltx_align_center\">64.50 <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-8.21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m25\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">8.21</mn></mrow><annotation encoding=\"application/x-tex\">-8.21</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">7.73 <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-68.14\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m26\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">68.14</mn></mrow><annotation encoding=\"application/x-tex\">-68.14</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">5.87 <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-49.46\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m27\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">49.46</mn></mrow><annotation encoding=\"application/x-tex\">-49.46</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">8.67 <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-13.33\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m28\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">13.33</mn></mrow><annotation encoding=\"application/x-tex\">-13.33</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Partial PI</th>\n<td class=\"ltx_td ltx_align_center\">72.45 <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-0.26\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m29\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">0.26</mn></mrow><annotation encoding=\"application/x-tex\">-0.26</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">67.73 <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-8.14\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m30\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">8.14</mn></mrow><annotation encoding=\"application/x-tex\">-8.14</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">40.13 <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-15.20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m31\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">15.20</mn></mrow><annotation encoding=\"application/x-tex\">-15.20</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">18.40 <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-3.60\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m32\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">3.60</mn></mrow><annotation encoding=\"application/x-tex\">-3.60</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Whole YaRN</th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">73.11</span> <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+0.40\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m33\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">0.40</mn></mrow><annotation encoding=\"application/x-tex\">+0.40</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">74.27</span> <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-1.60\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m34\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">1.60</mn></mrow><annotation encoding=\"application/x-tex\">-1.60</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">50.93 <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-4.40\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m35\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">4.40</mn></mrow><annotation encoding=\"application/x-tex\">-4.40</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">31.07 <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+9.07\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m36\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">9.07</mn></mrow><annotation encoding=\"application/x-tex\">+9.07</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Partial YaRN</th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">72.72</span> <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m37\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">0.01</mn></mrow><annotation encoding=\"application/x-tex\">+0.01</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">73.60 <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-2.27\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m38\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">2.27</mn></mrow><annotation encoding=\"application/x-tex\">-2.27</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">48.53 <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-6.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m39\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">6.80</mn></mrow><annotation encoding=\"application/x-tex\">-6.80</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">28.53 <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+6.53\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m40\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">6.53</mn></mrow><annotation encoding=\"application/x-tex\">+6.53</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" colspan=\"6\"><span class=\"ltx_text ltx_font_italic\">Stretching from 2 mins (observed context)</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Whole PI</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#808080;\">72.71</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#808080;\">75.87</span></td>\n<td class=\"ltx_td ltx_align_center\">46.93 <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-8.40\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m41\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">8.40</mn></mrow><annotation encoding=\"application/x-tex\">-8.40</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">21.20 <span class=\"ltx_text\" style=\"--ltx-fg-color:#E57373;\">(<math alttext=\"-0.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m42\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">&#8722;</mo><mn mathcolor=\"#E57373\" style=\"--ltx-fg-color:#E57373;\">0.80</mn></mrow><annotation encoding=\"application/x-tex\">-0.80</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Partial PI</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#808080;\">72.71</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#808080;\">75.87</span></td>\n<td class=\"ltx_td ltx_align_center\">59.60 <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+4.27\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m43\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">4.27</mn></mrow><annotation encoding=\"application/x-tex\">+4.27</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">45.73 <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+23.73\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m44\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">23.73</mn></mrow><annotation encoding=\"application/x-tex\">+23.73</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Whole YaRN</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#808080;\">72.71</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#808080;\">75.87</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">60.27</span> <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+4.94\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m45\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">4.94</mn></mrow><annotation encoding=\"application/x-tex\">+4.94</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">47.60</span> <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+25.60\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m46\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">25.60</mn></mrow><annotation encoding=\"application/x-tex\">+25.60</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_bb\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Partial YaRN</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#808080;\">72.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#808080;\">75.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\">60.40</span> <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+5.07\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m47\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">5.07</mn></mrow><annotation encoding=\"application/x-tex\">+5.07</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\">48.00</span> <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+26.00\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.m48\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">26.00</mn></mrow><annotation encoding=\"application/x-tex\">+26.00</annotation></semantics></math>)</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "compare",
            "proves",
            "their",
            "−68146814",
            "information",
            "from",
            "accuracy",
            "top",
            "tokens",
            "ours",
            "observed",
            "flash",
            "method",
            "−027027",
            "salmonn",
            "closed",
            "−227227",
            "whole",
            "not",
            "qwen2audio",
            "−360360",
            "audio",
            "context",
            "preserving",
            "least",
            "−814814",
            "text",
            "−080080",
            "−40404040",
            "−840840",
            "importance",
            "mins",
            "−20532053",
            "might",
            "find",
            "empirically",
            "setting",
            "−19861986",
            "effective",
            "−49464946",
            "strategies",
            "−053053",
            "trainingfree",
            "−15201520",
            "methods",
            "between",
            "performance",
            "−680680",
            "−026026",
            "see",
            "−13331333",
            "under",
            "gpt",
            "30s",
            "more",
            "rowcolorgray095",
            "significantly",
            "mcqa",
            "original",
            "yarn",
            "vanilla",
            "partial",
            "two",
            "instruct",
            "−440440",
            "suggesting",
            "comparing",
            "stretching",
            "min",
            "−19341934",
            "−821821",
            "−22272227",
            "2mins",
            "positional",
            "−160160",
            "gemini",
            "utmost",
            "−32803280",
            "extension",
            "−466466",
            "−914914",
            "yodas2mcqa",
            "approaches"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large Audio-Language Models (LALMs) are often constrained by short audio context windows, even when their text backbones support long contexts, limiting long-form audio understanding. Prior work has introduced context-extension methods (e.g. YaRN) on unimodal LLMs, yet their application to LALMs remains unexplored. First, building on RoPE-based context extension, we introduce <span class=\"ltx_text ltx_font_italic\">Partial YaRN</span>, a training-free, audio-only extension method that modifies only audio token positions, leaving text positions intact to preserve the base LLM&#8217;s text capabilities. Second, we propose <span class=\"ltx_text ltx_font_italic\">Virtual Longform Audio Training</span> (VLAT), a training strategy that extends Partial YaRN into a training-time positional augmentation. VLAT simulates diverse audio lengths during training, enabling generalization to inputs far longer than those seen in training and improving robustness for long-context audio understanding. Our experiments on SALMONN and Qwen2-Audio show that Partial YaRN outperforms the original models across wide range of settings, and VLAT training strategy provides substantial improvement, achieving strong performance on long audio of unseen lengths.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code is available at: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/scb-10x/partial-yarn\" title=\"\">https://github.com/scb-10x/partial-yarn</a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "positional",
                    "original",
                    "audio",
                    "their",
                    "context",
                    "trainingfree",
                    "extension",
                    "yarn",
                    "performance",
                    "method",
                    "methods",
                    "partial",
                    "text",
                    "salmonn"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">Extending Audio Context for Long-Form Understanding \n<br class=\"ltx_break\"/>in Large Audio-Language Models</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of Large Language Models (LLMs) has spurred multimodal extensions, notably large audio-language models (LALMs)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Also known by other names such as Speech-aware LLMs.</span></span></span> that pair an audio encoder with a text backbone. By aligning audio and text in a shared representation, LALMs can leverage the base LLM&#8217;s knowledge for complex audio understanding. However, practical use is constrained by short audio context windows: models such as SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib49\" title=\"\">2024</a>)</cite> and Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib5\" title=\"\">2024</a>)</cite> are typically trained on audio segments of 30s or less, and thus generalize poorly to longer inputs <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib19\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio",
                    "30s",
                    "context",
                    "text",
                    "salmonn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this limitation, we study the application of LLM context extension methods such as Positional Interpolation (PI) <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib4\" title=\"\">2023</a>)</cite> and YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> for audio context extension in LALMs. To our knowledge, the application of these techniques for this specific, modality-bound use case has not been systematically explored. Applying such whole-context techniques implicitly extends the audio context window as a side effect. However, as this straightforward approach alters the positional information of the entire sequence, including text tokens; it risks degrading the sophisticated language capability of the base LLM which was pretrained solely in text.</p>\n\n",
                "matched_terms": [
                    "positional",
                    "audio",
                    "context",
                    "text",
                    "extension",
                    "yarn",
                    "methods",
                    "not",
                    "tokens",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This potential drawback motivates a more targeted strategy. We introduce &#8220;<span class=\"ltx_text ltx_font_italic\">Partial YaRN</span>&#8221;, an audio-only context extension method that modifies only the audio tokens&#8217; positional encodings. This design enables a direct comparative study against the whole-context approaches, allowing us to investigate the tradeoff between preserving position encodings of the text modality and maintaining a globally uniform positional space.</p>\n\n",
                "matched_terms": [
                    "positional",
                    "audio",
                    "context",
                    "more",
                    "extension",
                    "preserving",
                    "method",
                    "between",
                    "text",
                    "approaches"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments on both training-free and fine-tuned audio context extensions reveal that extension-based methods outperforms original models. However, the best choice between whole-context and audio-only extensions is often model-dependent without a universally superior solution. This suggests that to build truly robust long-audio-context models, we must address the core problem of generalization during the training process. Therefore, we extend Partial YaRN into a novel fine-tuning technique called &#8220;<span class=\"ltx_text ltx_font_italic\">Virtual Longform Audio Training</span>&#8221; (VLAT). Acting as a <span class=\"ltx_text ltx_font_italic\">positional augmentation</span> technique, it simulates a wide range of audio lengths during fine-tuning, teaching the model to generalize beyond the lengths present in the training dataset. Through VLAT, we obtain excellent results on longform audio of unseen lengths. Our main contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "positional",
                    "original",
                    "audio",
                    "context",
                    "trainingfree",
                    "yarn",
                    "methods",
                    "partial",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_italic\">Partial YaRN</span>, a training-free audio-context extension for LALMs that preserves the base language model&#8217;s text capabilities by leaving text positions unaltered.</p>\n\n",
                "matched_terms": [
                    "trainingfree",
                    "extension",
                    "yarn",
                    "partial",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose Virtual Longform Audio Training (VLAT), a training strategy that applies Partial YaRN as positional augmentation to simulate diverse audio lengths during training, improving long-context generalization.</p>\n\n",
                "matched_terms": [
                    "positional",
                    "partial",
                    "audio",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a comparative study between existing context extension methods designed for unimodal LLMs and our proposed methods for LALMs, analyzing their performance trade-off in both training-free and fine-tuning settings.</p>\n\n",
                "matched_terms": [
                    "their",
                    "context",
                    "trainingfree",
                    "extension",
                    "methods",
                    "between",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As Transformer <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib52\" title=\"\">2017</a>)</cite> utilizes attention mechanism <cite class=\"ltx_cite ltx_citemacro_citep\">(Bahdanau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib2\" title=\"\">2014</a>)</cite> which is position-invariant, explicit positional information has to be provided for the models to differentiate tokens from different positions. Our study focuses on LALMs that use Rotary Positional Encoding (RoPE) <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib48\" title=\"\">2024</a>)</cite>, a widely adopted positional encoding.</p>\n\n",
                "matched_terms": [
                    "positional",
                    "from",
                    "tokens",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">RoPE formulates relative positional information through the rotation of query and key vectors. Namely, given a query <math alttext=\"\\boldsymbol{q_{m}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#119954;</mi><mi>&#119950;</mi></msub><annotation encoding=\"application/x-tex\">\\boldsymbol{q_{m}}</annotation></semantics></math> at position <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>, and key <math alttext=\"\\boldsymbol{k_{n}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#119948;</mi><mi>&#119951;</mi></msub><annotation encoding=\"application/x-tex\">\\boldsymbol{k_{n}}</annotation></semantics></math> at position <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>, we have:</p>\n\n",
                "matched_terms": [
                    "positional",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib4\" title=\"\">2023</a>)</cite> proposes that instead of extrapolating RoPE further to the region unfamiliar for the trained model, interpolation is done inside the pretrained context window. Specifically, let <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> be the original context length of the model, and <math alttext=\"L^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msup><mi>L</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">L^{\\prime}</annotation></semantics></math> be the target length to be extended to. Define <math alttext=\"s=\\frac{L^{\\prime}}{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo>=</mo><mfrac><msup><mi>L</mi><mo>&#8242;</mo></msup><mi>L</mi></mfrac></mrow><annotation encoding=\"application/x-tex\">s=\\frac{L^{\\prime}}{L}</annotation></semantics></math> as the extension factor. PI simply downscales the base rotational frequency by the extension factor: <math alttext=\"\\frac{\\theta_{i}}{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mfrac><msub><mi>&#952;</mi><mi>i</mi></msub><mi>s</mi></mfrac><annotation encoding=\"application/x-tex\">\\frac{\\theta_{i}}{s}</annotation></semantics></math>. For example, if originally each positional step is rotated by <math alttext=\"\\theta_{i}=10^{\\circ}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#952;</mi><mi>i</mi></msub><mo>=</mo><msup><mn>10</mn><mo>&#8728;</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\theta_{i}=10^{\\circ}</annotation></semantics></math>; to double the context length, the frequency (step size) can simply be halved to <math alttext=\"5^{\\circ}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msup><mn>5</mn><mo>&#8728;</mo></msup><annotation encoding=\"application/x-tex\">5^{\\circ}</annotation></semantics></math>. This way, the model will never have to attend to positions outside of its trained window; resulting in a more stable approach.</p>\n\n",
                "matched_terms": [
                    "positional",
                    "original",
                    "context",
                    "more",
                    "extension"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><cite class=\"ltx_cite ltx_citemacro_citet\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> identifies that interpolating every dimension equally leads to the loss of local distances and high frequency information. They propose to spread the interpolation pressure by partitioning RoPE dimensions into three frequency-based groups: (1) low frequency dimensions are interpolated, (2) high frequency dimensions are solely extrapolated without interpolation to preserve high frequency information, (3) dimensions in-between get a mix of both interpolation and extrapolation. Additionally, YaRN applies temperature scaling to the logits of attention softmax:</p>\n\n",
                "matched_terms": [
                    "yarn",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One broadly adopted paradigm of large audio-language models (LALMs) is the <span class=\"ltx_text ltx_font_italic\">unified input space</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib49\" title=\"\">2024</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib5\" title=\"\">2024</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib12\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib16\" title=\"\">2025</a>)</cite> which involves transforming audio inputs into the textual input space of a base LLM. By sharing the embedding space, the strong text capability and learned knowledge of the base LLM can be leveraged while simultaneously being augmented with audio understanding ability. There also exists other LALM paradigms such as <span class=\"ltx_text ltx_font_italic\">cross attention</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib25\" title=\"\">2024</a>; Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib14\" title=\"\">2025</a>)</cite> where the audio and text modalities are fused through cross-attention modules. This work focuses on models under the unified-input-space.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "under",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As with text context window in LLMs, LALMs also do not generalize well to audio with lengths much longer than the window seen during training (e.g. 7s for Pengi <cite class=\"ltx_cite ltx_citemacro_citep\">(Deshmukh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib10\" title=\"\">2023</a>)</cite>; 30s for Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib5\" title=\"\">2024</a>)</cite>). Concurrent to us, <cite class=\"ltx_cite ltx_citemacro_citet\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib19\" title=\"\">2025</a>)</cite> introduces FastLongSpeech, an efficient longform LALM that uses Iterative Fusion to compress long audio representation into compact forms. Our work, in contrast, investigates an orthogonal direction of adapting the model&#8217;s positional encodings rather than the audio embeddings, and therefore is generally applicable to the existing models without requiring costly retraining.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "positional",
                    "audio",
                    "30s",
                    "context",
                    "not",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To extend existing <span class=\"ltx_text ltx_font_italic\">unified input</span> LALMs to longer audio, we hypothesize that <em class=\"ltx_emph ltx_font_bold ltx_font_italic\">LALMs already possess a sufficient general understanding of audio and text, but the bottleneck is their unfamiliarity to audio positions beyond the range seen during audio-text training.</em><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>This differs from whole-context extension in unimodal LLMs, where the total sequence length exceeds the pre-trained context window, causing an out-of-distribution problem. Here, the total sequence (audio + text) usually still fits within the original window, making the core challenge one of adapting to an unfamiliar audio length and positions, rather than extrapolating to completely unseen positions.</span></span></span> This hypothesis suggests a training-free solution: manipulating the backbone LLM&#8217;s positional encoding. The goal is to remap positions of a long audio input into the model&#8217;s familiar audio range.</p>\n\n",
                "matched_terms": [
                    "positional",
                    "original",
                    "audio",
                    "their",
                    "context",
                    "trainingfree",
                    "extension",
                    "from",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we study the application of YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> to LALMs, as an example of whole-context extension methods. Since the audio window is a part of the total context window, performing whole-context extension extends the audio window as a byproduct. However, this alters positional information for all tokens, including text, risking the degradation of the base LLM, which was pretrained solely in text. Motivated by the aforementioned hypothesis, we propose <span class=\"ltx_text ltx_font_bold\">Partial YaRN</span>, an audio-only extension method designed to exclusively stretch the audio window. By leaving the text&#8217;s positional encodings unaltered, this approach aims to extend<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>The terms <span class=\"ltx_text ltx_font_italic\">extend</span> and <span class=\"ltx_text ltx_font_italic\">stretch</span> are used interchangeably.</span></span></span> audio context while preserving the base model&#8217;s text capability.</p>\n\n",
                "matched_terms": [
                    "positional",
                    "audio",
                    "context",
                    "extension",
                    "yarn",
                    "preserving",
                    "method",
                    "methods",
                    "partial",
                    "text",
                    "tokens",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by PI <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib4\" title=\"\">2023</a>)</cite> and YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> for whole-context extension in unimodal LLMs, we adapt the interpolation technique to LALMs by applying it exclusively to the audio region of the base language models.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "extension",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Concretely, assuming a single audio input, let <math alttext=\"L_{\\text{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>audio</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{audio}}</annotation></semantics></math> be the original audio context length, <math alttext=\"L_{\\text{audio}}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>L</mi><mtext>audio</mtext><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">L_{\\text{audio}}^{\\prime}</annotation></semantics></math> be the target length, and <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> be the position of the first audio token. We define the positional range <math alttext=\"[p,p+L_{\\text{audio}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>p</mi><mo>,</mo><mrow><mi>p</mi><mo>+</mo><msub><mi>L</mi><mtext>audio</mtext></msub></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">[p,p+L_{\\text{audio}})</annotation></semantics></math> as the <span class=\"ltx_text ltx_font_italic\">original audio context window</span>. It can be either a predefined location for audio input, or a dynamic region enclosed by special tokens (e.g. <span class=\"ltx_text ltx_font_typewriter\">&lt;speech&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;/speech&gt;</span>). We then apply our modified YaRN technique to exclusively stretch this region to the new length <math alttext=\"L_{\\text{audio}}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>L</mi><mtext>audio</mtext><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">L_{\\text{audio}}^{\\prime}</annotation></semantics></math>. This creates a partially stretched positional encoding:</p>\n\n",
                "matched_terms": [
                    "positional",
                    "original",
                    "audio",
                    "context",
                    "yarn",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, we only partition the RoPE dimensions into two frequency-based groups (instead of three as in the original YaRN formulation): a low frequency group that undergoes pure interpolation, and a high frequency group that undergoes pure extrapolation. The rationale for this design is twofold:</p>\n\n",
                "matched_terms": [
                    "yarn",
                    "original",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1) Consistency</span>: A two-group partition ensures a consistent and uniform positional encoding across the entire audio stream. YaRN&#8217;s original &#8220;in-between&#8221; group receives a mix of interpolation and extrapolation, which would cause these RoPE frequencies to not be extended to fit the entire audio. Our approach avoids this potential distortion.</p>\n\n",
                "matched_terms": [
                    "positional",
                    "audio",
                    "not",
                    "original"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2) Tuning Efficiency</span>: This simplification reduces the tuning space of the <span class=\"ltx_text ltx_font_italic\">cutoff</span> index from two coupled variables to a single parameter.</p>\n\n",
                "matched_terms": [
                    "from",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide empirical validation and a more in-depth discussion of the two-group partitioning in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S7.SS2\" title=\"7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a> and Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A2\" title=\"Appendix B Elaboration on Two-group Partitioning of Partial YaRN &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> respectively. See Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S0.F1\" title=\"Figure 1 &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> for an example depiction of Partial YaRN.</p>\n\n",
                "matched_terms": [
                    "see",
                    "more",
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Partial YaRN employs two hyperparameters:</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1) Cutoff Dimension Index</span>: This defines the boundary separating the RoPE dimensions into two groups. Dimensions equal or below this cutoff (low-frequency) are interpolated to stably cover longer audio context. Dimensions above it (high-frequency) are extrapolated to preserve local positional distances and high-frequency information. We default this to <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><mn>0</mn></math> (interpolate every dimension).</p>\n\n",
                "matched_terms": [
                    "positional",
                    "information",
                    "audio",
                    "context",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2) Attention Temperature</span>: This controls the sharpness of attention distribution within the audio context window. A higher temperature softens the distribution, preventing attention scores from collapsing to a few tokens over long sequences. We default this to <math alttext=\"1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mn>1.0</mn><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math> (no temperature scaling).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "from",
                    "context",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Instead of directly adding a new temperature term to the attention softmax, we follow <cite class=\"ltx_cite ltx_citemacro_citet\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> and integrate the attention temperature into magnitudes of RoPE&#8217;s signals. Particularly, define <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> and <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> as two positions within the extended audio context window, we have the attention softmax:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "context",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This reparameterization also handles attention between the unaltered text regions and the scaled audio regions. For instance, when a text query (unscaled) attends to an audio key (scaled by <math alttext=\"1/\\sqrt{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><msqrt><mi>t</mi></msqrt></mrow><annotation encoding=\"application/x-tex\">1/\\sqrt{t}</annotation></semantics></math>), the resulting logit naturally gets scaled by <math alttext=\"1/\\sqrt{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><msqrt><mi>t</mi></msqrt></mrow><annotation encoding=\"application/x-tex\">1/\\sqrt{t}</annotation></semantics></math>, creating a smoother temperature transition. Overall, we have temperature-scaled rotation matrices:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notice that under the default hyperparameter values, the audio-only extension reduces from YaRN to PI, we therefore specifically name Partial YaRN with default hyperparameters: <span class=\"ltx_text ltx_font_italic\">Partial PI</span>.</p>\n\n",
                "matched_terms": [
                    "under",
                    "yarn",
                    "from",
                    "extension",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We source audio from the English subset of YODAS2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib30\" title=\"\">2023b</a>)</cite>. We segment audio samples into non-overlapping segments of 1, 2, 5, and 10 minutes. For each segment, we generate five multiple-choice question-answering (MCQA) pairs using Gemini 2.0 Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib50\" title=\"\">2023</a>)</cite>, with four choices each. We design the generation prompt to ensure that each question focuses on different portions of the audio, and that they collectively cover the entire audio segment. The test set of each audio duration (1, 2, 5, and 10 minutes) has 750 QA pairs.</p>\n\n",
                "matched_terms": [
                    "mcqa",
                    "gemini",
                    "audio",
                    "flash",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We select two widely-used open-weights LALMs: SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib49\" title=\"\">2024</a>)</cite> and Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib5\" title=\"\">2024</a>)</cite>. Both models utilize Whisper-encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib42\" title=\"\">2023</a>)</cite> as audio encoder, which is limited to processing audio of up to 30s. We handle longer audio inputs by segmenting them into non-overlapping 30s chunks, where each chunk is encoded independently.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio",
                    "30s",
                    "two",
                    "salmonn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The number of audio representations differs considerably between these models. For each 30s chunk on 50Hz Whisper-encoder, SALMONN generates a sequence of 88 audio tokens (with Q-Former), whereas Qwen2-Audio produces a sequence of 750 tokens (with a 2x downsampling).</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio",
                    "30s",
                    "between",
                    "tokens",
                    "salmonn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate Partial PI (default hyperparameters) and Partial YaRN (tuned hyperparameters) against the following methods:</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1) Vanilla</span>: Unmodified base models. Long audio inputs are passed directly without any manipulation to their RoPE.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "their",
                    "vanilla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2) Whole Position Interpolation (PI)</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib4\" title=\"\">2023</a>)</cite>: Every RoPE dimension is uniformly interpolated across the whole context window. This serves as the primitive whole-context baseline.</p>\n\n",
                "matched_terms": [
                    "whole",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">3) Whole YaRN</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite>: Frequency-based RoPE interpolation with attention scaling, applied to the whole context window. This serves as the primary whole-context baseline.</p>\n\n",
                "matched_terms": [
                    "whole",
                    "yarn",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These three baselines require no tuning as Vanilla and Whole PI don&#8217;t utilize any hyperparameter, and YaRN has predefined cutoffs and a closed-form formula for obtaining temperature.</p>\n\n",
                "matched_terms": [
                    "yarn",
                    "whole",
                    "vanilla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also report performance of two proprietary models capable of long audio context: GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib37\" title=\"\">2024</a>)</cite> and Gemini 2.0 Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib50\" title=\"\">2023</a>)</cite>, in order to provide a broader perspective of LALMs and to validate the quality of our dataset<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>The model versions are <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini-2024-07-18</span> and <span class=\"ltx_text ltx_font_typewriter\">gemini-2.0-flash-001</span> respectively. Note that Gemini was also used for generating our YODAS2-MCQA dataset.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "audio",
                    "context",
                    "flash",
                    "yodas2mcqa",
                    "two",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we comparatively study the effect of whole-context and audio-only context extensions on both the training-free and fine-tuning settings for audio segments up to 10mins.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "context",
                    "trainingfree"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we extend the audio context length without training. We report the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S4.SS1.SSS0.Px1\" title=\"YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, showing that: no single method&#8212;whole-context or audio-only&#8212;is universally superior. Instead, the performance varies between the different models and the degree of extension. More importantly, we observe that <span class=\"ltx_text ltx_font_italic\">both models generally retain consistent performance up to 2mins</span>, suggesting that their innate audio context lengths are longer than just 30s, possibly a result from multi-audio training.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "their",
                    "30s",
                    "context",
                    "more",
                    "extension",
                    "from",
                    "between",
                    "2mins",
                    "suggesting",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When stretching from the 30s window, extension methods provide substantial performance gains compared to the Vanilla baseline. We observe that Partial YaRN shows the strongest performance on the 1min and 10mins settings, achieving 57.35% and 32.93% accuracies. On 2mins, both Whole and Partial YaRN perform similarly. However, Whole YaRN outperforms Partial YaRN at 5mins by a large margin. Possible reasons for this performance flip are (1) higher expressivity and less compression pressure from Whole YaRN&#8217;s additional &#8220;in-between&#8221; dimension group, outweighs the preservation of the base language capability in this 5mins setting, or (2) noise in the hyperparameter tuning process of Partial YaRN, which is an inherent drawback.</p>\n\n",
                "matched_terms": [
                    "30s",
                    "setting",
                    "stretching",
                    "extension",
                    "yarn",
                    "vanilla",
                    "from",
                    "methods",
                    "partial",
                    "whole",
                    "2mins",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to SALMONN, Qwen2-Audio demonstrates different characteristics. The Vanilla baseline is more robust, outperforming all stretch-from-30s methods at 2mins. Both Whole and Partial YaRN struggle to provide significant gains and even underperform the baseline at moderate lengths (2 and 5mins) when stretching from the original 30s context; however, they manage to attain considerable gains on the 10mins setting. This indicates that Qwen2-Audio likely possesses a strong audio-length generalization capability or longer innate audio context window. For this model, simply applying an extension method from the base context does not guarantee an improvement. This suggests that the model&#8217;s high intrinsic audio-length generalizability is superior to the extension methods that inevitably cram the positional information as a side effect.</p>\n\n",
                "matched_terms": [
                    "original",
                    "setting",
                    "yarn",
                    "vanilla",
                    "partial",
                    "whole",
                    "not",
                    "information",
                    "qwen2audio",
                    "audio",
                    "context",
                    "from",
                    "methods",
                    "stretching",
                    "2mins",
                    "positional",
                    "30s",
                    "more",
                    "extension",
                    "method",
                    "salmonn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As previously mentioned in the beginning of this section that we can observe both model&#8217;s innate audio contexts to be around 2mins instead of just 30s, here we experiment with extending from this <span class=\"ltx_text ltx_font_italic\">observed audio context of 2mins</span> to the target lengths. For both SALMONN and Qwen2-Audio, performance on the long audio improves substantially when the interpolation is anchored from a 2mins context instead of the original 30s. Notably on Qwen2-Audio, this strategy boosts Partial YaRN&#8217;s performance at 10mins from 28.53% to a 48.00%, an absolute improvement of almost 20%, yielding a final performance 26% higher than the Vanilla baseline. This finding suggests that determining the extension by observing the vanilla performance may be a more critical factor than the choice between whole-context or audio-only methods, especially on large extension ratios.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "original",
                    "audio",
                    "30s",
                    "context",
                    "methods",
                    "salmonn",
                    "observed",
                    "more",
                    "extension",
                    "vanilla",
                    "from",
                    "between",
                    "partial",
                    "2mins",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, we study training-based extension methods. Keeping other components frozen, we fine-tune the base language model of Qwen2-Audio using a <span class=\"ltx_text ltx_font_italic\">single epoch LoRA</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib20\" title=\"\">2022</a>)</cite> of rank 8 on the YODAS2-MCQA training set of the corresponding length. Two LoRA settings are explored: adapting only the query projection (q), and adapting query, key, value, and output projections (qkvo). Due to the high cost of tuning Partial YaRN&#8217;s hyperparameters during training, we only use the default configuration (Partial PI) for the audio-only method. Whole PI is omitted due to its poor performance as previously observed.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "observed",
                    "extension",
                    "method",
                    "methods",
                    "yodas2mcqa",
                    "partial",
                    "whole",
                    "two",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, showing that fine-tuning with a context extension method dramatically outperforms the vanilla fine-tuning baseline, especially at longer audio lengths. In the qkvo setting, both Whole YaRN (83.47%) and Partial PI (83.07%) achieve an absolute performance gain of around 19% over the Vanilla baseline (64.93%) at 10mins. This highlights the benefit of integrating a context extension method into the fine-tuning process for more better context extension.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "setting",
                    "context",
                    "more",
                    "extension",
                    "yarn",
                    "vanilla",
                    "method",
                    "partial",
                    "whole",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comparing the whole-context and audio-only approaches, we find that their performance is generally competitive. While Whole YaRN triumps when only adapting the query projection weights, suggesting it&#8217;s ease of adaptation. Their performance converges under standard LoRA practice which adapts qkvo. This result confirms that both whole-context and audio-only methods are highly effective strategies for fine-tuning.</p>\n\n",
                "matched_terms": [
                    "effective",
                    "comparing",
                    "find",
                    "strategies",
                    "their",
                    "under",
                    "yarn",
                    "methods",
                    "whole",
                    "approaches",
                    "suggesting",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our previous results show that while the training-free methods are beneficial, their effectiveness is often model and audio-length dependent. Based on this, we propose to fine-tune LALMs with Partial YaRN repurposed as a <span class=\"ltx_text ltx_font_italic\">positional augmentation</span> technique we call <span class=\"ltx_text ltx_font_italic\">\"Virtual Longform Audio Training\"</span> (VLAT).</p>\n\n",
                "matched_terms": [
                    "positional",
                    "audio",
                    "their",
                    "trainingfree",
                    "yarn",
                    "methods",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of VLAT is to simulate and expose models to audio context windows of diverse lengths during training. Let <math alttext=\"L_{\\text{data}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>data</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{data}}</annotation></semantics></math> be the actual length of a training audio sample. For each sample, we obtain a &#8220;virtual&#8221; source length, <math alttext=\"L_{\\text{virt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>virt</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{virt}}</annotation></semantics></math>, as the model&#8217;s default audio context length <span class=\"ltx_text ltx_font_italic\">times</span> a factor randomly sampled from the range [1, 5, 10, 15, 20, 25]x. We then apply Partial YaRN<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Whole YaRN is not used here, as we observe that it diverges quickly when training under the VLAT framework.</span></span></span> to stretch or compress the <math alttext=\"L_{\\text{virt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>virt</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{virt}}</annotation></semantics></math>-long positional window to <math alttext=\"L_{\\text{data}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>data</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{data}}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "positional",
                    "audio",
                    "context",
                    "under",
                    "yarn",
                    "from",
                    "partial",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For instance, consider an audio sample with <math alttext=\"L_{\\text{data}}=2\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mtext>data</mtext></msub><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">L_{\\text{data}}=2</annotation></semantics></math>mins, and we draw a <math alttext=\"L_{\\text{virt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>virt</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{virt}}</annotation></semantics></math> of 10mins, Partial YaRN will compress a 10mins context window to 2mins for this audio. This process effectively simulates the 2mins audio to be 10mins long virtually, thereby familiarizing the model to longer audio context and improving its ability to generalize to genuinely long audio at inference time.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "context",
                    "yarn",
                    "partial",
                    "2mins"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Randomizing positional encoding has previously been explored in <cite class=\"ltx_cite ltx_citemacro_citet\">Ruoss et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib44\" title=\"\">2023</a>)</cite>, where positional indices are randomly downsampled to let the model see larger positional values. Rather than using <math alttext=\"\\{1,2,\\dots,N\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{1,2,\\dots,N\\}</annotation></semantics></math> for an length-<math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> input, they use a randomly subsampled values such as <math alttext=\"\\{1,4,\\dots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>4</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{1,4,\\dots,L\\}</annotation></semantics></math> with <math alttext=\"L&gt;N\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m4\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>&gt;</mo><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">L&gt;N</annotation></semantics></math> instead. In contrast, VLAT differs in employing interpolation inside the context window to create a dense, <span class=\"ltx_text ltx_font_italic\">continuous</span> space of positions, unlike sparse integer subsampling. It is also bidirectional, teaching the model through both compressed and stretched contexts. Furthermore, VLAT is a targeted, modality-bound method that modifies only the audio tokens, preserving the base LLM&#8217;s original textual space.</p>\n\n",
                "matched_terms": [
                    "positional",
                    "original",
                    "see",
                    "audio",
                    "context",
                    "preserving",
                    "method",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune Qwen2-Audio on a 2mins YODAS2-MCQA training set<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>To prevent data leakage, the train/test splits were performed at the video level, ensuring no source material overlaps between the training set and any of the test sets.</span></span></span> using two methods: a standard Vanilla fine-tuning and VLAT. We employ 10-epoch qkvo LoRA for the adaptations. We then evaluate the models from both training methods under two different extensions at inference: Vanilla and Partial PI. See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.SS1\" title=\"E.1 Different VLAT&#8217;s Virtual Length Sampling Strategies &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">E.1</span></a> for additional detail on VLAT configuration.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "see",
                    "methods",
                    "under",
                    "vanilla",
                    "from",
                    "yodas2mcqa",
                    "between",
                    "partial",
                    "2mins",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S4.T2\" title=\"Table 2 &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, VLAT greatly improves generalization to unseen audio lengths. When evaluated with vanilla inference, the model trained with Virtual Longform shows a dramatic improvement on 10mins audio, increasing accuracy from 32.76% to 75.11%, closing in on the previous 1-epoch direct-fine-tuning result. This highlights the effectiveness of the training technique for training LALMs to generalize far beyond the audio lengths present in their training data, mitigating a key bottleneck in the development of robust long-audio models.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "their",
                    "vanilla",
                    "from",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, as can be seen in \"Partial PI Inference\" columns, VLAT&#8217;s benefit is complementary to inference-time extension. When Partial PI is applied during evaluation, the VLAT-trained model again outperforms its vanilla-trained counterpart, achieving the highest overall 10mins performance of 81.73%. This indicates that VLAT and inference-time extension are compatible strategies, and their combination yields the most robust long-context performance.</p>\n\n",
                "matched_terms": [
                    "strategies",
                    "their",
                    "extension",
                    "partial",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we conduct an ablation study to isolate and analyze the individual contributions of the two key components in Whole YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> and Partial YaRN: (1) frequency grouping and (2) attention temperature scaling. We note that by removing both of these components, the methods converge to Whole PI <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib4\" title=\"\">2023</a>)</cite> and Partial PI. The results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S7.SS1\" title=\"7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">7.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "yarn",
                    "methods",
                    "partial",
                    "whole",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can see that removing either the frequency grouping or the attention temperature generally leads to a large drop in performance, especially on higher extension ratios. Both the frequency partitioning and attention temperature scaling are crucial for robustly extending the audio context length of LALMs. Their combined application within the Whole and Partial YaRN frameworks generally yield the most effective and stable performance.</p>\n\n",
                "matched_terms": [
                    "effective",
                    "see",
                    "audio",
                    "their",
                    "context",
                    "extension",
                    "yarn",
                    "partial",
                    "whole",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the Partial YaRN method, we modify the original 3-group frequency partition from original YaRN to a 2-group method as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S3.SS1\" title=\"3.1 Methodology &#8227; 3 Partial YaRN &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> and Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A2\" title=\"Appendix B Elaboration on Two-group Partitioning of Partial YaRN &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. Here we conduct a direct empirical comparison between the 2-group and 3-group approaches.</p>\n\n",
                "matched_terms": [
                    "original",
                    "yarn",
                    "from",
                    "method",
                    "between",
                    "partial",
                    "approaches"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the 3-group baseline, we implement Partial YaRN using YaRN&#8217;s original partitioning scheme and its recommended hyperparameters. We compare this against our proposed 2-group Partial YaRN using the hyperparameters from our main experiments (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S5.SS1\" title=\"5.1 Training-free Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>). We present the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S7.T5\" title=\"Table 5 &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "compare",
                    "original",
                    "yarn",
                    "from",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We see that our 2-group partitioning broadly outperform 3-group in our modality-specific scenario. This is most evident with the Qwen2-Audio model, where the 3-group method suffers a large performance drop on 5mins and 10mins audio, with an accuracy gap of 28.53% and 12.00% respectively. Overall, this result validates the effectiveness and reliability of our proposed 2-group scheme used in Partial YaRN.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "see",
                    "audio",
                    "accuracy",
                    "yarn",
                    "method",
                    "partial",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work addressed the challenge of extending audio context windows of LALMs by studying the application of LLM&#8217;s whole-context extension methods, and proposing a training-free audio-only extension Partial YaRN and the VLAT training strategy. We demonstrate the effectiveness of extension-based methods over non-extension baselines across various long-audio settings. Leveraging our finding that SALMONN and Qwen2-Audio retain consistent performance up to 2mins audio window, we achieved an even stronger performance by extending from the 2mins window instead of 30s. Later, we showed that fine-tuning through VLAT helps the models to generalize to audio contexts far exceeding the length of the training data, offering a robust and data-efficient pathway to develop LALMs with better longform understanding. Future work could explore a similar modality-bound extension strategy for video models.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio",
                    "30s",
                    "context",
                    "trainingfree",
                    "salmonn",
                    "extension",
                    "yarn",
                    "from",
                    "methods",
                    "partial",
                    "2mins",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments are conducted exclusively on the multiple-choice question answering (MCQA) tasks. We made this choice to ensure a precise and unambiguous measurement of the model performance under different audio lengths. This mitigates the confounding variables inherent in evaluating open-ended generation, such as judging the semantic equivalence or stylistic alignment. However, a key tradeoff of this focused evaluation is its limited ability to assess the nuanced generative and language modeling capabilities of the base LLM. Consequently, while our results validate the model&#8217;s ability to retrieve information from audio, they do not fully test the importance of text preservation of the audio-only methods like Partial YaRN. Furthermore, our study does not assess performance across other long-audio task formats found in benchmarks such as <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib19\" title=\"\">2025</a>; Ahia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib1\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib16\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "mcqa",
                    "audio",
                    "methods",
                    "text",
                    "under",
                    "from",
                    "yarn",
                    "importance",
                    "partial",
                    "not",
                    "information",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike the original YaRN which has generally good predefined cutoff dimensions and a closed-form formula for determining the attention temperature, our proposed Partial YaRN requires tuning of these two hyperparameters, thereby introducing additional computational burden.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn",
                    "original",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">YC would like to thank Ekapol Chuangsuwanich for insightful discussion on the earlier version of this work. He also thanks Thanapat Trachu for valuable guidance on the illustration of Partial YaRN.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conceived along with the introduction of transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib52\" title=\"\">2017</a>)</cite>, the original <span class=\"ltx_text ltx_font_italic\">absolute positional encoding</span> utilizes sinusoidal waves of varying frequencies to uniquely represent absolute position of the tokens, and is incorporated into the model by adding directly into the input sequence. Later works <cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib11\" title=\"\">2019</a>; Lan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib26\" title=\"\">2020</a>; Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib7\" title=\"\">2020</a>)</cite> propose to use trainable vectors for representing absolute positional information, where token at each position is summed with a learned vector, allowing the models to learn the optimal positional encoding for themselves. However, it cannot extrapolate to sequences longer than the maximum length it was trained on.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Shaw et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib46\" title=\"\">2018</a>)</cite> introduces to encode the relative position of the token instead, where the relative distance between the query and key is captured and injected into every attention layer. <cite class=\"ltx_cite ltx_citemacro_citet\">Dai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib8\" title=\"\">2019</a>)</cite> modifies the decomposed form of the attention equation, and introduces additional trainable location-agnostic key vectors to distinguish between content-based and location-based querying behaviors. <cite class=\"ltx_cite ltx_citemacro_citet\">Raffel et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib43\" title=\"\">2020</a>)</cite> groups the relative distances into buckets, and associate each bucket with a learnable scalar for adding into the attention logits. Simplifying the encoding scheme and reducing trainable parameters. TUPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Ke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib24\" title=\"\">2021</a>)</cite> uses separate linear projections for the word and positional information. ALiBi <cite class=\"ltx_cite ltx_citemacro_citep\">(Press et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib41\" title=\"\">2022</a>)</cite> simply adds linear biases with a learnable slope into the attention scores, the bias grows linearly with the relative distance between the tokens. This method generally imposes a strong locality preference by encouraging models to attend more to the nearby tokens, while discouraging them from attending to tokens that are farther away. Several later works extend RoPE for multimodal modeling (e.g. Multimodal RoPE <cite class=\"ltx_cite ltx_citemacro_cite\">Bai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib3\" title=\"\">2025</a>)</cite>, VideoRoPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib53\" title=\"\">2025</a>)</cite>, VRoPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib31\" title=\"\">2025</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "positional",
                    "original",
                    "more",
                    "from",
                    "method",
                    "between",
                    "tokens",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Video input is similar to the audio modality in that they&#8217;re both naturally continuous and can be of arbitrary length. Many Large Video-Language Models (LVLMs), such as Video-LLaMA <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib58\" title=\"\">2023b</a>)</cite> and Video-ChatGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Maaz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib34\" title=\"\">2024</a>)</cite>, adopt architectural paradigm similar to the unified input space LALMs we study, where video frames are embedded and projected into a base LLM&#8217;s input space. However, the context length problem is even more exacerbated in the video domain due to the high dimensionality and large number of tokens needed to represent even short videos. To improve efficiency, the LVLM community has largely focused on input compression techniques like query aggregation <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib58\" title=\"\">2023b</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib28\" title=\"\">2023a</a>; Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib47\" title=\"\">2024</a>)</cite>, frame pooling <cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib33\" title=\"\">2023</a>; Maaz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib34\" title=\"\">2024</a>)</cite>, and feature merging <cite class=\"ltx_cite ltx_citemacro_citep\">(Weng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib54\" title=\"\">2024</a>)</cite>. These strategies primarily aim to reduce the number of tokens before they enter the LLM. Our work offers a complementary perspective: instead of altering the input, we manipulate the positional space within the model to accommodate a longer sequence. The principles of Partial YaRN and Virtual Longform Audio Training could therefore be potentially applicable to the video domain as well, hopefully providing an orthogonal path to achieving long-context understanding in synergy with existing compression methods.</p>\n\n",
                "matched_terms": [
                    "positional",
                    "audio",
                    "strategies",
                    "context",
                    "more",
                    "yarn",
                    "methods",
                    "partial",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The decision to simplify the original three-group frequency partition of YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> into a two-group system for Partial YaRN is a deliberate design choice aimed at ensuring positional consistency across the entire extended audio stream. The original YaRN formulation partitions RoPE dimensions into: 1) a low-frequency group that is purely interpolated, 2) a high-frequency group that is purely extrapolated, and 3) an \"in-between\" group that receives a mix of both, with a ramped interpolation factor.</p>\n\n",
                "matched_terms": [
                    "positional",
                    "original",
                    "audio",
                    "yarn",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While effective for whole-context extension, this \"in-between\" group becomes problematic in our partial, audio-only extension scenario. Consider a 2mins audio input, which requires a 4x extension of the model&#8217;s 30s original audio context.</p>\n\n",
                "matched_terms": [
                    "effective",
                    "original",
                    "audio",
                    "30s",
                    "context",
                    "extension",
                    "partial",
                    "2mins"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The low-frequency dimensions, responsible for quasi-absolute positioning, are fully interpolated by a 4x factor. This is crucial as it maps the entire 2mins audio into the model&#8217;s familiar positional range (original audio context).</p>\n\n",
                "matched_terms": [
                    "positional",
                    "original",
                    "audio",
                    "context",
                    "2mins"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The \"in-between\" dimensions, however, would receive interpolation factors between the 1x and 4x (e.g. 2x). This means that for these specific dimensions, the model&#8217;s original context window is stretched to cover only the first minute of the 2mins audio. The second minute would lie in an extrapolated, out-of-distribution region.</p>\n\n",
                "matched_terms": [
                    "original",
                    "audio",
                    "context",
                    "between",
                    "2mins"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This dimensional inconsistency risks creating a biased or distorted representation of the latter half of the audio. The model would receive conflicting signals about whether a token is \"inside\" or \"outside\" its familiar context.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By simplifying to a two-group partition, we circumvent this issue entirely. Every dimension is forced into a binary choice: either it is fully interpolated to cover the entire audio duration (low-frequency), or it is fully extrapolated to preserve local structure (high-frequency). This ensures that the model maintains a consistent positional understanding across all dimensions for the entirety of the audio input, avoiding the representational bias that partial coverage would introduce.</p>\n\n",
                "matched_terms": [
                    "positional",
                    "partial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S7.SS2\" title=\"7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a> for direct performance comparison between 2 and 3-group frequency partitioning versions of Partial YaRN.</p>\n\n",
                "matched_terms": [
                    "see",
                    "yarn",
                    "between",
                    "partial",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key implementation detail of our work is how we apply the positional stretching to only a subsection of the input sequence. While the original YaRN implementation modifies the RoPE frequency variables (<math alttext=\"\\boldsymbol{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><mi>&#120637;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{\\theta}</annotation></semantics></math>), this approach isn&#8217;t straightforward for a <span class=\"ltx_text ltx_font_italic\">partial</span> modification. Instead, our implementation of Partial YaRN directly manipulates the <span class=\"ltx_text ltx_font_typewriter\">position_ids</span> and multiply it with the unmodified frequencies. This section details the implementation our method.</p>\n\n",
                "matched_terms": [
                    "positional",
                    "original",
                    "stretching",
                    "yarn",
                    "method",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identify the Audio Region:</span> First, we identify the start and end indices of the audio tokens within the <span class=\"ltx_text ltx_font_typewriter\">position_ids</span> tensor. This creates three distinct segments: the leading text positions (unaltered), the audio positions (to be stretched), and the trailing text positions (unaltered relative to each other).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stretch the Audio Positions:</span> We replace the original sequence of integer positions for the audio segment with a new sequence generated via <span class=\"ltx_text ltx_font_typewriter\">torch.linspace</span>. For example, if the original audio positions were <span class=\"ltx_text ltx_font_typewriter\">[p, p+1, ..., p+base_L_audio-1]</span> and the new audio input has <span class=\"ltx_text ltx_font_typewriter\">L_audio</span> tokens, we generate <span class=\"ltx_text ltx_font_typewriter\">L_audio</span> new positions by interpolating within the original range: <span class=\"ltx_text ltx_font_typewriter\">torch.linspace(start=p, end=p+L_audio-1, steps=L_audio)</span>. This effectively &#8220;stretches&#8221; the familiar positional space to accommodate the longer audio input.</p>\n\n",
                "matched_terms": [
                    "positional",
                    "audio",
                    "tokens",
                    "original"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Concatenate:</span> The final <span class=\"ltx_text ltx_font_typewriter\">position_ids</span> tensor is constructed by concatenating the leading text positions, the new stretched audio positions, and the trailing text positions. This new <span class=\"ltx_text ltx_font_typewriter\">position_ids</span> is now ready to be use for subsequent construction of partially stretch RoPE signal for the relevant dimensions.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This implementation introduces negligible computational overhead in practice. The only additional computations are incurred during the construction of the <span class=\"ltx_text ltx_font_typewriter\">position_ids</span> tensor, which involves a few tensor slicing operations, a single <span class=\"ltx_text ltx_font_typewriter\">torch.linspace</span> call, and a final concatenation. These are highly optimized, vectorized operations that constitute a minuscule fraction of the total computation in a full forward pass. Crucially, this precomputation is a <span class=\"ltx_text ltx_ulem_uline\">one-time cost</span> per input sample, applied only during the initial processing of the prompt that contains the long audio. During the subsequent autoregressive generation steps, where tokens are generated one at a time, our method adds no overhead whatsoever. The model reverts to the standard process of incremental positional stepping, just as it would without any context extension. Therefore, the cost of enabling partial context extension is a small, fixed precomputation that does not impact the per-token generation latency.</p>\n\n",
                "matched_terms": [
                    "positional",
                    "audio",
                    "context",
                    "extension",
                    "method",
                    "partial",
                    "not",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recall that Partial YaRN has two hyperparameters, cutoff dimension and attention temperature. For the training-free experiment (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S5.SS1\" title=\"5.1 Training-free Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), we jointly sweep the cutoff dimension from <math alttext=\"\\{56,48,40,32,24,16,8\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>56</mn><mo>,</mo><mn>48</mn><mo>,</mo><mn>40</mn><mo>,</mo><mn>32</mn><mo>,</mo><mn>24</mn><mo>,</mo><mn>16</mn><mo>,</mo><mn>8</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{56,48,40,32,24,16,8\\}</annotation></semantics></math>, and the temperature from between 0.5 and 1.6 with a step size of 0.1. See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.SS2\" title=\"E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a> for tuning results and sensitivity.</p>\n\n",
                "matched_terms": [
                    "see",
                    "trainingfree",
                    "yarn",
                    "from",
                    "between",
                    "partial",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To obtain specific audio length, we chunk the long audio in the dataset to 1, 2, 5, and 10 minutes. Last, incomplete chunk of each long-audio is also included as well. Each of the chunk will be used to generate 5 MCQA pairs via Gemini 2.0 Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib50\" title=\"\">2023</a>)</cite>. We provide the full MCQA generation prompt in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.F7\" title=\"Figure 7 &#8227; Plausible explanation of this trend. &#8227; E.2.2 Sensitivity on SALMONN &#8227; E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mcqa",
                    "flash",
                    "gemini"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The effectiveness of Virtual Longform Audio Training (VLAT) depends on the strategy used to sample the virtual length during training. For generality, here we define the virtual length as a produce of true audio context length (i.e. 30s) and the <span class=\"ltx_text ltx_font_italic\">virtual factor</span>. In our main experiment Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S6\" title=\"6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we used a strategy of randomly sampling a factor from the range [1x, 5x, 10x, 15x, 20x, 25x]. To understand the sensitivity of our results to this choice, we compare our default strategy against four alternatives, training a Qwen2-Audio model on each strategy and evaluating on the 10mins audio task.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "compare",
                    "audio",
                    "30s",
                    "context",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Default</span>: Uniform random sampling from <math alttext=\"\\{1.0,5.0,10.0,15.0,20.0,25.0\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I1.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>1.0</mn><mo>,</mo><mn>5.0</mn><mo>,</mo><mn>10.0</mn><mo>,</mo><mn>15.0</mn><mo>,</mo><mn>20.0</mn><mo>,</mo><mn>25.0</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{1.0,5.0,10.0,15.0,20.0,25.0\\}</annotation></semantics></math>. This cover the audio context length of 30s (1.0x) to 12.5mins (25.0x).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "from",
                    "30s",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited Range</span>: Uniform random sampling from <math alttext=\"\\{1.0,5.0,10.0\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I1.i4.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>1.0</mn><mo>,</mo><mn>5.0</mn><mo>,</mo><mn>10.0</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{1.0,5.0,10.0\\}</annotation></semantics></math>. This factor set covers audio context length of at most 5mins (10.0x).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "from",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The experiment offers several insights into different VLAT sampling strategies: (1) <span class=\"ltx_text ltx_font_italic\">Fixed High-Factor</span> (always 20.0x) strategy performs the worst by a large margin, confirming that exposure to a variety of positional ranges is crucial for robust generalization, rather than training for just a single fixed target length. (2) <span class=\"ltx_text ltx_font_italic\">Default</span> strategy which uses a coarse set of 6 sampling points outperforms both the <span class=\"ltx_text ltx_font_italic\">Dense</span> (100 points) and <span class=\"ltx_text ltx_font_italic\">Very Dense</span> (1000 points) strategies. This suggests that finegrained sampling of virtual factors provides no additional benefit, and that a relatively small but diverse set of factors is sufficient for training length-robust models.</p>\n\n",
                "matched_terms": [
                    "positional",
                    "strategies"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The most surprising result comes from the <span class=\"ltx_text ltx_font_italic\">Limited Range</span> strategy, which achieves the highest performance across all evaluation lengths, despite its virtual lengths not extending beyond the 5mins mark (10.0x factor). While this demonstrates a powerful capacity for extrapolation, the mechanism behind this superior performance is not immediately clear. <span class=\"ltx_text ltx_ulem_uline\">It may suggest that randomization on extremely long virtual lengths could introduce instability to the training</span>, which could in turn explain why Whole YaRN diverged quickly under VLAT framework; or that the specific distribution of factors in this limited set is coincidentally optimal for our test data. We left the more in-depth study of this behavior as future work.</p>\n\n",
                "matched_terms": [
                    "more",
                    "under",
                    "yarn",
                    "from",
                    "whole",
                    "not",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here we analyze the effect and sensitivity of different Partial YaRN&#8217;s hyperparameter configurations. We report validation accuracies w.r.t. different cutoff dimension indices and attention temperatures, for the Qwen2-Audio and SALMONN models.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "partial",
                    "salmonn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.F3\" title=\"Figure 3 &#8227; Plausible explanation of this trend. &#8227; E.2.2 Sensitivity on SALMONN &#8227; E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> displays the hyperparameter sensitivity of Partial YaRN on Qwen2-Audio when extending from the original 30s audio context window. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.F4\" title=\"Figure 4 &#8227; Plausible explanation of this trend. &#8227; E.2.2 Sensitivity on SALMONN &#8227; E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the sensitivity when extending from the observed audio context window of 2mins.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "original",
                    "30s",
                    "audio",
                    "context",
                    "observed",
                    "yarn",
                    "from",
                    "partial",
                    "2mins"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe the trend of optimal hyperparameter configurations to move from the bottom left (high cutoff dimension index, temperature close to 1.0) toward the top right of the heatmaps (lower cutoff dimension index, higher temperature) as the audio length increases.</p>\n\n",
                "matched_terms": [
                    "top",
                    "audio",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This trend informs the following recommendation when using Partial YaRN. For moderate context extensions (e.g., 2x to 4x), it is advisable to start with a higher cutoff dimension index and a temperature value close to 1.0. For longer extensions, a lower cutoff dimension index with a considerably higher temperature is likely to yield better results.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the attention temperature controls the distribution of attention scores. In very long sequences with thousands of tokens, the standard softmax function can lead to an uneven attention score distribution, such as attending to only a few tokens while ignoring the vast majority of the context. Raising the temperature flattens this distribution, encouraging the model to maintain a broader attention pattern across the entire long audio stream and preventing \"attention collapse\".</p>\n\n",
                "matched_terms": [
                    "audio",
                    "tokens",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, the cutoff dimension index directly controls the \"compression pressure\" of the position encoding extension. A high cutoff index is expressive enough for shorter extensions while retaining majority of the original, detailed positional information. On the other hand, a lower cutoff index (higher compression) is required to map a much larger range of positions into the smaller familiar audio region.</p>\n\n",
                "matched_terms": [
                    "positional",
                    "original",
                    "audio",
                    "extension",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.F5\" title=\"Figure 5 &#8227; Plausible explanation of this trend. &#8227; E.2.2 Sensitivity on SALMONN &#8227; E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> displays the hyperparameter sensitivity of Partial YaRN on SALMONN when extending from the original 30s audio context window.</p>\n\n",
                "matched_terms": [
                    "original",
                    "30s",
                    "audio",
                    "context",
                    "yarn",
                    "from",
                    "partial",
                    "salmonn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For this model, we observe that the optimal hyperparameter sets are generally contained in the low temperature region (0.5 - 0.7), with the cutoff dimension index moving from low to higher ones as the audio length grows.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the observed trend, it&#8217;s advisable to set the attention temperature to <math alttext=\"0.6(\\pm 0.1)\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS2.SSS2.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>0.6</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>&#177;</mo><mn>0.1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">0.6(\\pm 0.1)</annotation></semantics></math>, adopt a low cutoff index for shorter audio length, and higher index for longer audio inputs.</p>\n\n",
                "matched_terms": [
                    "observed",
                    "audio",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The vast distinction between SALMONN and the previously discussed Qwen2-Audio lies in the their audio representation lengths&#8212;SALMONN only uses 88 tokens for every 30s audio vs. Qwen2-Audio&#8217;s 750 tokens. This, coupled with the model&#8217;s preference of low attention temperature, suggests that letting the model focuses on a relevant local region outweighs the need to spreading the attention distribution as in Qwen2-Audio.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio",
                    "their",
                    "30s",
                    "between",
                    "tokens",
                    "salmonn"
                ]
            }
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Virtual Longform Audio Training (VLAT) enables strong generalization to unseen audio lengths (Accuracy). We compare standard Vanilla fine-tuning and our proposed VLAT on Qwen2-Audio, evaluated with (Partial PI Inference column group) and without (Vanilla Inference column group) Partial PI during inference.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Vanilla Inference</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Partial PI Inference</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Training Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2 mins</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">5 mins</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">10 mins</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2 mins</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">5 mins</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">10 mins</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Vanilla Fine-tuning</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">97.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">89.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">96.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">89.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">75.56</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Virtual Longform</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">97.60 <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+0.00\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">0.00</mn></mrow><annotation encoding=\"application/x-tex\">+0.00</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">91.42</span> <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+2.35\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">2.35</mn></mrow><annotation encoding=\"application/x-tex\">+2.35</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">75.11</span> <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+42.35\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">42.35</mn></mrow><annotation encoding=\"application/x-tex\">+42.35</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">96.87</span> <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+0.47\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">0.47</mn></mrow><annotation encoding=\"application/x-tex\">+0.47</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">91.91</span> <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+2.13\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">2.13</mn></mrow><annotation encoding=\"application/x-tex\">+2.13</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">81.73</span> <span class=\"ltx_text\" style=\"--ltx-fg-color:#66B266;\">(<math alttext=\"+6.17\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mrow><mo mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">+</mo><mn mathcolor=\"#66B266\" style=\"--ltx-fg-color:#66B266;\">6.17</mn></mrow><annotation encoding=\"application/x-tex\">+6.17</annotation></semantics></math>)</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "evaluated",
            "compare",
            "column",
            "group",
            "vanilla",
            "inference",
            "partial",
            "lengths",
            "our",
            "qwen2audio",
            "audio",
            "strong",
            "longform",
            "training",
            "accuracy",
            "finetuning",
            "without",
            "vlat",
            "enables",
            "mins",
            "proposed",
            "virtual",
            "during",
            "standard",
            "unseen",
            "method",
            "generalization"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S4.T2\" title=\"Table 2 &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, VLAT greatly improves generalization to unseen audio lengths. When evaluated with vanilla inference, the model trained with Virtual Longform shows a dramatic improvement on 10mins audio, increasing accuracy from 32.76% to 75.11%, closing in on the previous 1-epoch direct-fine-tuning result. This highlights the effectiveness of the training technique for training LALMs to generalize far beyond the audio lengths present in their training data, mitigating a key bottleneck in the development of robust long-audio models.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large Audio-Language Models (LALMs) are often constrained by short audio context windows, even when their text backbones support long contexts, limiting long-form audio understanding. Prior work has introduced context-extension methods (e.g. YaRN) on unimodal LLMs, yet their application to LALMs remains unexplored. First, building on RoPE-based context extension, we introduce <span class=\"ltx_text ltx_font_italic\">Partial YaRN</span>, a training-free, audio-only extension method that modifies only audio token positions, leaving text positions intact to preserve the base LLM&#8217;s text capabilities. Second, we propose <span class=\"ltx_text ltx_font_italic\">Virtual Longform Audio Training</span> (VLAT), a training strategy that extends Partial YaRN into a training-time positional augmentation. VLAT simulates diverse audio lengths during training, enabling generalization to inputs far longer than those seen in training and improving robustness for long-context audio understanding. Our experiments on SALMONN and Qwen2-Audio show that Partial YaRN outperforms the original models across wide range of settings, and VLAT training strategy provides substantial improvement, achieving strong performance on long audio of unseen lengths.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code is available at: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/scb-10x/partial-yarn\" title=\"\">https://github.com/scb-10x/partial-yarn</a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "during",
                    "virtual",
                    "audio",
                    "strong",
                    "vlat",
                    "longform",
                    "unseen",
                    "training",
                    "method",
                    "generalization",
                    "partial",
                    "lengths",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">Extending Audio Context for Long-Form Understanding \n<br class=\"ltx_break\"/>in Large Audio-Language Models</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "longform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of Large Language Models (LLMs) has spurred multimodal extensions, notably large audio-language models (LALMs)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Also known by other names such as Speech-aware LLMs.</span></span></span> that pair an audio encoder with a text backbone. By aligning audio and text in a shared representation, LALMs can leverage the base LLM&#8217;s knowledge for complex audio understanding. However, practical use is constrained by short audio context windows: models such as SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib49\" title=\"\">2024</a>)</cite> and Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib5\" title=\"\">2024</a>)</cite> are typically trained on audio segments of 30s or less, and thus generalize poorly to longer inputs <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib19\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this limitation, we study the application of LLM context extension methods such as Positional Interpolation (PI) <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib4\" title=\"\">2023</a>)</cite> and YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> for audio context extension in LALMs. To our knowledge, the application of these techniques for this specific, modality-bound use case has not been systematically explored. Applying such whole-context techniques implicitly extends the audio context window as a side effect. However, as this straightforward approach alters the positional information of the entire sequence, including text tokens; it risks degrading the sophisticated language capability of the base LLM which was pretrained solely in text.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This potential drawback motivates a more targeted strategy. We introduce &#8220;<span class=\"ltx_text ltx_font_italic\">Partial YaRN</span>&#8221;, an audio-only context extension method that modifies only the audio tokens&#8217; positional encodings. This design enables a direct comparative study against the whole-context approaches, allowing us to investigate the tradeoff between preserving position encodings of the text modality and maintaining a globally uniform positional space.</p>\n\n",
                "matched_terms": [
                    "enables",
                    "audio",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments on both training-free and fine-tuned audio context extensions reveal that extension-based methods outperforms original models. However, the best choice between whole-context and audio-only extensions is often model-dependent without a universally superior solution. This suggests that to build truly robust long-audio-context models, we must address the core problem of generalization during the training process. Therefore, we extend Partial YaRN into a novel fine-tuning technique called &#8220;<span class=\"ltx_text ltx_font_italic\">Virtual Longform Audio Training</span>&#8221; (VLAT). Acting as a <span class=\"ltx_text ltx_font_italic\">positional augmentation</span> technique, it simulates a wide range of audio lengths during fine-tuning, teaching the model to generalize beyond the lengths present in the training dataset. Through VLAT, we obtain excellent results on longform audio of unseen lengths. Our main contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "during",
                    "without",
                    "audio",
                    "vlat",
                    "longform",
                    "unseen",
                    "training",
                    "generalization",
                    "partial",
                    "lengths",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose Virtual Longform Audio Training (VLAT), a training strategy that applies Partial YaRN as positional augmentation to simulate diverse audio lengths during training, improving long-context generalization.</p>\n\n",
                "matched_terms": [
                    "during",
                    "virtual",
                    "audio",
                    "vlat",
                    "longform",
                    "training",
                    "generalization",
                    "partial",
                    "lengths"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a comparative study between existing context extension methods designed for unimodal LLMs and our proposed methods for LALMs, analyzing their performance trade-off in both training-free and fine-tuning settings.</p>\n\n",
                "matched_terms": [
                    "proposed",
                    "finetuning",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Transformers are usually unable to generalize well past the context window length they have seen during the training (e.g. 2048 for LLaMA <cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib51\" title=\"\">2023</a>)</cite>, 8192 for Mistral <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib23\" title=\"\">2023</a>)</cite>). To permit longer context length, additional fine-tuning is often required.</p>\n\n",
                "matched_terms": [
                    "during",
                    "finetuning",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One broadly adopted paradigm of large audio-language models (LALMs) is the <span class=\"ltx_text ltx_font_italic\">unified input space</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib49\" title=\"\">2024</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib5\" title=\"\">2024</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib12\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib16\" title=\"\">2025</a>)</cite> which involves transforming audio inputs into the textual input space of a base LLM. By sharing the embedding space, the strong text capability and learned knowledge of the base LLM can be leveraged while simultaneously being augmented with audio understanding ability. There also exists other LALM paradigms such as <span class=\"ltx_text ltx_font_italic\">cross attention</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib25\" title=\"\">2024</a>; Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib14\" title=\"\">2025</a>)</cite> where the audio and text modalities are fused through cross-attention modules. This work focuses on models under the unified-input-space.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "strong"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As with text context window in LLMs, LALMs also do not generalize well to audio with lengths much longer than the window seen during training (e.g. 7s for Pengi <cite class=\"ltx_cite ltx_citemacro_citep\">(Deshmukh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib10\" title=\"\">2023</a>)</cite>; 30s for Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib5\" title=\"\">2024</a>)</cite>). Concurrent to us, <cite class=\"ltx_cite ltx_citemacro_citet\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib19\" title=\"\">2025</a>)</cite> introduces FastLongSpeech, an efficient longform LALM that uses Iterative Fusion to compress long audio representation into compact forms. Our work, in contrast, investigates an orthogonal direction of adapting the model&#8217;s positional encodings rather than the audio embeddings, and therefore is generally applicable to the existing models without requiring costly retraining.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "during",
                    "without",
                    "audio",
                    "longform",
                    "training",
                    "lengths",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To extend existing <span class=\"ltx_text ltx_font_italic\">unified input</span> LALMs to longer audio, we hypothesize that <em class=\"ltx_emph ltx_font_bold ltx_font_italic\">LALMs already possess a sufficient general understanding of audio and text, but the bottleneck is their unfamiliarity to audio positions beyond the range seen during audio-text training.</em><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>This differs from whole-context extension in unimodal LLMs, where the total sequence length exceeds the pre-trained context window, causing an out-of-distribution problem. Here, the total sequence (audio + text) usually still fits within the original window, making the core challenge one of adapting to an unfamiliar audio length and positions, rather than extrapolating to completely unseen positions.</span></span></span> This hypothesis suggests a training-free solution: manipulating the backbone LLM&#8217;s positional encoding. The goal is to remap positions of a long audio input into the model&#8217;s familiar audio range.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "unseen",
                    "during"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we study the application of YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> to LALMs, as an example of whole-context extension methods. Since the audio window is a part of the total context window, performing whole-context extension extends the audio window as a byproduct. However, this alters positional information for all tokens, including text, risking the degradation of the base LLM, which was pretrained solely in text. Motivated by the aforementioned hypothesis, we propose <span class=\"ltx_text ltx_font_bold\">Partial YaRN</span>, an audio-only extension method designed to exclusively stretch the audio window. By leaving the text&#8217;s positional encodings unaltered, this approach aims to extend<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>The terms <span class=\"ltx_text ltx_font_italic\">extend</span> and <span class=\"ltx_text ltx_font_italic\">stretch</span> are used interchangeably.</span></span></span> audio context while preserving the base model&#8217;s text capability.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "audio",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Concretely, assuming a single audio input, let <math alttext=\"L_{\\text{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>audio</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{audio}}</annotation></semantics></math> be the original audio context length, <math alttext=\"L_{\\text{audio}}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>L</mi><mtext>audio</mtext><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">L_{\\text{audio}}^{\\prime}</annotation></semantics></math> be the target length, and <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> be the position of the first audio token. We define the positional range <math alttext=\"[p,p+L_{\\text{audio}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>p</mi><mo>,</mo><mrow><mi>p</mi><mo>+</mo><msub><mi>L</mi><mtext>audio</mtext></msub></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">[p,p+L_{\\text{audio}})</annotation></semantics></math> as the <span class=\"ltx_text ltx_font_italic\">original audio context window</span>. It can be either a predefined location for audio input, or a dynamic region enclosed by special tokens (e.g. <span class=\"ltx_text ltx_font_typewriter\">&lt;speech&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;/speech&gt;</span>). We then apply our modified YaRN technique to exclusively stretch this region to the new length <math alttext=\"L_{\\text{audio}}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>L</mi><mtext>audio</mtext><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">L_{\\text{audio}}^{\\prime}</annotation></semantics></math>. This creates a partially stretched positional encoding:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1) Consistency</span>: A two-group partition ensures a consistent and uniform positional encoding across the entire audio stream. YaRN&#8217;s original &#8220;in-between&#8221; group receives a mix of interpolation and extrapolation, which would cause these RoPE frequencies to not be extended to fit the entire audio. Our approach avoids this potential distortion.</p>\n\n",
                "matched_terms": [
                    "group",
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the limited availability of length-tailored long audio datasets, we construct custom datasets with specific audio lengths.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "lengths"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We select two widely-used open-weights LALMs: SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib49\" title=\"\">2024</a>)</cite> and Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib5\" title=\"\">2024</a>)</cite>. Both models utilize Whisper-encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib42\" title=\"\">2023</a>)</cite> as audio encoder, which is limited to processing audio of up to 30s. We handle longer audio inputs by segmenting them into non-overlapping 30s chunks, where each chunk is encoded independently.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The number of audio representations differs considerably between these models. For each 30s chunk on 50Hz Whisper-encoder, SALMONN generates a sequence of 88 audio tokens (with Q-Former), whereas Qwen2-Audio produces a sequence of 750 tokens (with a 2x downsampling).</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1) Vanilla</span>: Unmodified base models. Long audio inputs are passed directly without any manipulation to their RoPE.</p>\n\n",
                "matched_terms": [
                    "without",
                    "audio",
                    "vanilla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also report performance of two proprietary models capable of long audio context: GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib37\" title=\"\">2024</a>)</cite> and Gemini 2.0 Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib50\" title=\"\">2023</a>)</cite>, in order to provide a broader perspective of LALMs and to validate the quality of our dataset<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>The model versions are <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini-2024-07-18</span> and <span class=\"ltx_text ltx_font_typewriter\">gemini-2.0-flash-001</span> respectively. Note that Gemini was also used for generating our YODAS2-MCQA dataset.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we comparatively study the effect of whole-context and audio-only context extensions on both the training-free and fine-tuning settings for audio segments up to 10mins.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we extend the audio context length without training. We report the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S4.SS1.SSS0.Px1\" title=\"YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, showing that: no single method&#8212;whole-context or audio-only&#8212;is universally superior. Instead, the performance varies between the different models and the degree of extension. More importantly, we observe that <span class=\"ltx_text ltx_font_italic\">both models generally retain consistent performance up to 2mins</span>, suggesting that their innate audio context lengths are longer than just 30s, possibly a result from multi-audio training.</p>\n\n",
                "matched_terms": [
                    "without",
                    "audio",
                    "lengths",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When stretching from the 30s window, extension methods provide substantial performance gains compared to the Vanilla baseline. We observe that Partial YaRN shows the strongest performance on the 1min and 10mins settings, achieving 57.35% and 32.93% accuracies. On 2mins, both Whole and Partial YaRN perform similarly. However, Whole YaRN outperforms Partial YaRN at 5mins by a large margin. Possible reasons for this performance flip are (1) higher expressivity and less compression pressure from Whole YaRN&#8217;s additional &#8220;in-between&#8221; dimension group, outweighs the preservation of the base language capability in this 5mins setting, or (2) noise in the hyperparameter tuning process of Partial YaRN, which is an inherent drawback.</p>\n\n",
                "matched_terms": [
                    "group",
                    "partial",
                    "vanilla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to SALMONN, Qwen2-Audio demonstrates different characteristics. The Vanilla baseline is more robust, outperforming all stretch-from-30s methods at 2mins. Both Whole and Partial YaRN struggle to provide significant gains and even underperform the baseline at moderate lengths (2 and 5mins) when stretching from the original 30s context; however, they manage to attain considerable gains on the 10mins setting. This indicates that Qwen2-Audio likely possesses a strong audio-length generalization capability or longer innate audio context window. For this model, simply applying an extension method from the base context does not guarantee an improvement. This suggests that the model&#8217;s high intrinsic audio-length generalizability is superior to the extension methods that inevitably cram the positional information as a side effect.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio",
                    "strong",
                    "vanilla",
                    "method",
                    "generalization",
                    "partial",
                    "lengths"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As previously mentioned in the beginning of this section that we can observe both model&#8217;s innate audio contexts to be around 2mins instead of just 30s, here we experiment with extending from this <span class=\"ltx_text ltx_font_italic\">observed audio context of 2mins</span> to the target lengths. For both SALMONN and Qwen2-Audio, performance on the long audio improves substantially when the interpolation is anchored from a 2mins context instead of the original 30s. Notably on Qwen2-Audio, this strategy boosts Partial YaRN&#8217;s performance at 10mins from 28.53% to a 48.00%, an absolute improvement of almost 20%, yielding a final performance 26% higher than the Vanilla baseline. This finding suggests that determining the extension by observing the vanilla performance may be a more critical factor than the choice between whole-context or audio-only methods, especially on large extension ratios.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio",
                    "vanilla",
                    "partial",
                    "lengths"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, we study training-based extension methods. Keeping other components frozen, we fine-tune the base language model of Qwen2-Audio using a <span class=\"ltx_text ltx_font_italic\">single epoch LoRA</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib20\" title=\"\">2022</a>)</cite> of rank 8 on the YODAS2-MCQA training set of the corresponding length. Two LoRA settings are explored: adapting only the query projection (q), and adapting query, key, value, and output projections (qkvo). Due to the high cost of tuning Partial YaRN&#8217;s hyperparameters during training, we only use the default configuration (Partial PI) for the audio-only method. Whole PI is omitted due to its poor performance as previously observed.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "during",
                    "training",
                    "method",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, showing that fine-tuning with a context extension method dramatically outperforms the vanilla fine-tuning baseline, especially at longer audio lengths. In the qkvo setting, both Whole YaRN (83.47%) and Partial PI (83.07%) achieve an absolute performance gain of around 19% over the Vanilla baseline (64.93%) at 10mins. This highlights the benefit of integrating a context extension method into the fine-tuning process for more better context extension.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "audio",
                    "vanilla",
                    "method",
                    "partial",
                    "lengths"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comparing the whole-context and audio-only approaches, we find that their performance is generally competitive. While Whole YaRN triumps when only adapting the query projection weights, suggesting it&#8217;s ease of adaptation. Their performance converges under standard LoRA practice which adapts qkvo. This result confirms that both whole-context and audio-only methods are highly effective strategies for fine-tuning.</p>\n\n",
                "matched_terms": [
                    "standard",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our previous results show that while the training-free methods are beneficial, their effectiveness is often model and audio-length dependent. Based on this, we propose to fine-tune LALMs with Partial YaRN repurposed as a <span class=\"ltx_text ltx_font_italic\">positional augmentation</span> technique we call <span class=\"ltx_text ltx_font_italic\">\"Virtual Longform Audio Training\"</span> (VLAT).</p>\n\n",
                "matched_terms": [
                    "virtual",
                    "audio",
                    "vlat",
                    "longform",
                    "training",
                    "partial",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of VLAT is to simulate and expose models to audio context windows of diverse lengths during training. Let <math alttext=\"L_{\\text{data}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>data</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{data}}</annotation></semantics></math> be the actual length of a training audio sample. For each sample, we obtain a &#8220;virtual&#8221; source length, <math alttext=\"L_{\\text{virt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>virt</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{virt}}</annotation></semantics></math>, as the model&#8217;s default audio context length <span class=\"ltx_text ltx_font_italic\">times</span> a factor randomly sampled from the range [1, 5, 10, 15, 20, 25]x. We then apply Partial YaRN<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Whole YaRN is not used here, as we observe that it diverges quickly when training under the VLAT framework.</span></span></span> to stretch or compress the <math alttext=\"L_{\\text{virt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>virt</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{virt}}</annotation></semantics></math>-long positional window to <math alttext=\"L_{\\text{data}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>data</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{data}}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "during",
                    "audio",
                    "vlat",
                    "training",
                    "partial",
                    "lengths"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For instance, consider an audio sample with <math alttext=\"L_{\\text{data}}=2\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mtext>data</mtext></msub><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">L_{\\text{data}}=2</annotation></semantics></math>mins, and we draw a <math alttext=\"L_{\\text{virt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>virt</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{virt}}</annotation></semantics></math> of 10mins, Partial YaRN will compress a 10mins context window to 2mins for this audio. This process effectively simulates the 2mins audio to be 10mins long virtually, thereby familiarizing the model to longer audio context and improving its ability to generalize to genuinely long audio at inference time.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "audio",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Randomizing positional encoding has previously been explored in <cite class=\"ltx_cite ltx_citemacro_citet\">Ruoss et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib44\" title=\"\">2023</a>)</cite>, where positional indices are randomly downsampled to let the model see larger positional values. Rather than using <math alttext=\"\\{1,2,\\dots,N\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{1,2,\\dots,N\\}</annotation></semantics></math> for an length-<math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> input, they use a randomly subsampled values such as <math alttext=\"\\{1,4,\\dots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>4</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{1,4,\\dots,L\\}</annotation></semantics></math> with <math alttext=\"L&gt;N\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m4\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>&gt;</mo><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">L&gt;N</annotation></semantics></math> instead. In contrast, VLAT differs in employing interpolation inside the context window to create a dense, <span class=\"ltx_text ltx_font_italic\">continuous</span> space of positions, unlike sparse integer subsampling. It is also bidirectional, teaching the model through both compressed and stretched contexts. Furthermore, VLAT is a targeted, modality-bound method that modifies only the audio tokens, preserving the base LLM&#8217;s original textual space.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "method",
                    "vlat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune Qwen2-Audio on a 2mins YODAS2-MCQA training set<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>To prevent data leakage, the train/test splits were performed at the video level, ensuring no source material overlaps between the training set and any of the test sets.</span></span></span> using two methods: a standard Vanilla fine-tuning and VLAT. We employ 10-epoch qkvo LoRA for the adaptations. We then evaluate the models from both training methods under two different extensions at inference: Vanilla and Partial PI. See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.SS1\" title=\"E.1 Different VLAT&#8217;s Virtual Length Sampling Strategies &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">E.1</span></a> for additional detail on VLAT configuration.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "finetuning",
                    "standard",
                    "vlat",
                    "inference",
                    "vanilla",
                    "training",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, as can be seen in \"Partial PI Inference\" columns, VLAT&#8217;s benefit is complementary to inference-time extension. When Partial PI is applied during evaluation, the VLAT-trained model again outperforms its vanilla-trained counterpart, achieving the highest overall 10mins performance of 81.73%. This indicates that VLAT and inference-time extension are compatible strategies, and their combination yields the most robust long-context performance.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "inference",
                    "during",
                    "vlat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can see that removing either the frequency grouping or the attention temperature generally leads to a large drop in performance, especially on higher extension ratios. Both the frequency partitioning and attention temperature scaling are crucial for robustly extending the audio context length of LALMs. Their combined application within the Whole and Partial YaRN frameworks generally yield the most effective and stable performance.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the Partial YaRN method, we modify the original 3-group frequency partition from original YaRN to a 2-group method as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S3.SS1\" title=\"3.1 Methodology &#8227; 3 Partial YaRN &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> and Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A2\" title=\"Appendix B Elaboration on Two-group Partitioning of Partial YaRN &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. Here we conduct a direct empirical comparison between the 2-group and 3-group approaches.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the 3-group baseline, we implement Partial YaRN using YaRN&#8217;s original partitioning scheme and its recommended hyperparameters. We compare this against our proposed 2-group Partial YaRN using the hyperparameters from our main experiments (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S5.SS1\" title=\"5.1 Training-free Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>). We present the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S7.T5\" title=\"Table 5 &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "proposed",
                    "compare",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We see that our 2-group partitioning broadly outperform 3-group in our modality-specific scenario. This is most evident with the Qwen2-Audio model, where the 3-group method suffers a large performance drop on 5mins and 10mins audio, with an accuracy gap of 28.53% and 12.00% respectively. Overall, this result validates the effectiveness and reliability of our proposed 2-group scheme used in Partial YaRN.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "proposed",
                    "audio",
                    "accuracy",
                    "method",
                    "partial",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work addressed the challenge of extending audio context windows of LALMs by studying the application of LLM&#8217;s whole-context extension methods, and proposing a training-free audio-only extension Partial YaRN and the VLAT training strategy. We demonstrate the effectiveness of extension-based methods over non-extension baselines across various long-audio settings. Leveraging our finding that SALMONN and Qwen2-Audio retain consistent performance up to 2mins audio window, we achieved an even stronger performance by extending from the 2mins window instead of 30s. Later, we showed that fine-tuning through VLAT helps the models to generalize to audio contexts far exceeding the length of the training data, offering a robust and data-efficient pathway to develop LALMs with better longform understanding. Future work could explore a similar modality-bound extension strategy for video models.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "finetuning",
                    "audio",
                    "vlat",
                    "longform",
                    "training",
                    "partial",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments are conducted exclusively on the multiple-choice question answering (MCQA) tasks. We made this choice to ensure a precise and unambiguous measurement of the model performance under different audio lengths. This mitigates the confounding variables inherent in evaluating open-ended generation, such as judging the semantic equivalence or stylistic alignment. However, a key tradeoff of this focused evaluation is its limited ability to assess the nuanced generative and language modeling capabilities of the base LLM. Consequently, while our results validate the model&#8217;s ability to retrieve information from audio, they do not fully test the importance of text preservation of the audio-only methods like Partial YaRN. Furthermore, our study does not assess performance across other long-audio task formats found in benchmarks such as <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib19\" title=\"\">2025</a>; Ahia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib1\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib16\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "lengths",
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike the original YaRN which has generally good predefined cutoff dimensions and a closed-form formula for determining the attention temperature, our proposed Partial YaRN requires tuning of these two hyperparameters, thereby introducing additional computational burden.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "proposed",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conceived along with the introduction of transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib52\" title=\"\">2017</a>)</cite>, the original <span class=\"ltx_text ltx_font_italic\">absolute positional encoding</span> utilizes sinusoidal waves of varying frequencies to uniquely represent absolute position of the tokens, and is incorporated into the model by adding directly into the input sequence. Later works <cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib11\" title=\"\">2019</a>; Lan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib26\" title=\"\">2020</a>; Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib7\" title=\"\">2020</a>)</cite> propose to use trainable vectors for representing absolute positional information, where token at each position is summed with a learned vector, allowing the models to learn the optimal positional encoding for themselves. However, it cannot extrapolate to sequences longer than the maximum length it was trained on.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Shaw et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib46\" title=\"\">2018</a>)</cite> introduces to encode the relative position of the token instead, where the relative distance between the query and key is captured and injected into every attention layer. <cite class=\"ltx_cite ltx_citemacro_citet\">Dai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib8\" title=\"\">2019</a>)</cite> modifies the decomposed form of the attention equation, and introduces additional trainable location-agnostic key vectors to distinguish between content-based and location-based querying behaviors. <cite class=\"ltx_cite ltx_citemacro_citet\">Raffel et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib43\" title=\"\">2020</a>)</cite> groups the relative distances into buckets, and associate each bucket with a learnable scalar for adding into the attention logits. Simplifying the encoding scheme and reducing trainable parameters. TUPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Ke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib24\" title=\"\">2021</a>)</cite> uses separate linear projections for the word and positional information. ALiBi <cite class=\"ltx_cite ltx_citemacro_citep\">(Press et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib41\" title=\"\">2022</a>)</cite> simply adds linear biases with a learnable slope into the attention scores, the bias grows linearly with the relative distance between the tokens. This method generally imposes a strong locality preference by encouraging models to attend more to the nearby tokens, while discouraging them from attending to tokens that are farther away. Several later works extend RoPE for multimodal modeling (e.g. Multimodal RoPE <cite class=\"ltx_cite ltx_citemacro_cite\">Bai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib3\" title=\"\">2025</a>)</cite>, VideoRoPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib53\" title=\"\">2025</a>)</cite>, VRoPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib31\" title=\"\">2025</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "strong",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Video input is similar to the audio modality in that they&#8217;re both naturally continuous and can be of arbitrary length. Many Large Video-Language Models (LVLMs), such as Video-LLaMA <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib58\" title=\"\">2023b</a>)</cite> and Video-ChatGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Maaz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib34\" title=\"\">2024</a>)</cite>, adopt architectural paradigm similar to the unified input space LALMs we study, where video frames are embedded and projected into a base LLM&#8217;s input space. However, the context length problem is even more exacerbated in the video domain due to the high dimensionality and large number of tokens needed to represent even short videos. To improve efficiency, the LVLM community has largely focused on input compression techniques like query aggregation <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib58\" title=\"\">2023b</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib28\" title=\"\">2023a</a>; Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib47\" title=\"\">2024</a>)</cite>, frame pooling <cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib33\" title=\"\">2023</a>; Maaz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib34\" title=\"\">2024</a>)</cite>, and feature merging <cite class=\"ltx_cite ltx_citemacro_citep\">(Weng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib54\" title=\"\">2024</a>)</cite>. These strategies primarily aim to reduce the number of tokens before they enter the LLM. Our work offers a complementary perspective: instead of altering the input, we manipulate the positional space within the model to accommodate a longer sequence. The principles of Partial YaRN and Virtual Longform Audio Training could therefore be potentially applicable to the video domain as well, hopefully providing an orthogonal path to achieving long-context understanding in synergy with existing compression methods.</p>\n\n",
                "matched_terms": [
                    "virtual",
                    "audio",
                    "longform",
                    "training",
                    "partial",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The decision to simplify the original three-group frequency partition of YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> into a two-group system for Partial YaRN is a deliberate design choice aimed at ensuring positional consistency across the entire extended audio stream. The original YaRN formulation partitions RoPE dimensions into: 1) a low-frequency group that is purely interpolated, 2) a high-frequency group that is purely extrapolated, and 3) an \"in-between\" group that receives a mix of both, with a ramped interpolation factor.</p>\n\n",
                "matched_terms": [
                    "group",
                    "partial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While effective for whole-context extension, this \"in-between\" group becomes problematic in our partial, audio-only extension scenario. Consider a 2mins audio input, which requires a 4x extension of the model&#8217;s 30s original audio context.</p>\n\n",
                "matched_terms": [
                    "group",
                    "partial",
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By simplifying to a two-group partition, we circumvent this issue entirely. Every dimension is forced into a binary choice: either it is fully interpolated to cover the entire audio duration (low-frequency), or it is fully extrapolated to preserve local structure (high-frequency). This ensures that the model maintains a consistent positional understanding across all dimensions for the entirety of the audio input, avoiding the representational bias that partial coverage would introduce.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key implementation detail of our work is how we apply the positional stretching to only a subsection of the input sequence. While the original YaRN implementation modifies the RoPE frequency variables (<math alttext=\"\\boldsymbol{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><mi>&#120637;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{\\theta}</annotation></semantics></math>), this approach isn&#8217;t straightforward for a <span class=\"ltx_text ltx_font_italic\">partial</span> modification. Instead, our implementation of Partial YaRN directly manipulates the <span class=\"ltx_text ltx_font_typewriter\">position_ids</span> and multiply it with the unmodified frequencies. This section details the implementation our method.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "method",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In most Transformer architectures, including the models used in this work, the RoPE mechanism computes embeddings based on an input tensor called <span class=\"ltx_text ltx_font_typewriter\">position_ids</span>. This tensor is simply a sequence of integers representing the absolute position of each token (i.e. <span class=\"ltx_text ltx_font_typewriter\">[0, 1, 2, ..., N-1]</span>). Our method leverages this by modifying a section this tensor. The process is as follows:</p>\n\n",
                "matched_terms": [
                    "method",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This implementation introduces negligible computational overhead in practice. The only additional computations are incurred during the construction of the <span class=\"ltx_text ltx_font_typewriter\">position_ids</span> tensor, which involves a few tensor slicing operations, a single <span class=\"ltx_text ltx_font_typewriter\">torch.linspace</span> call, and a final concatenation. These are highly optimized, vectorized operations that constitute a minuscule fraction of the total computation in a full forward pass. Crucially, this precomputation is a <span class=\"ltx_text ltx_ulem_uline\">one-time cost</span> per input sample, applied only during the initial processing of the prompt that contains the long audio. During the subsequent autoregressive generation steps, where tokens are generated one at a time, our method adds no overhead whatsoever. The model reverts to the standard process of incremental positional stepping, just as it would without any context extension. Therefore, the cost of enabling partial context extension is a small, fixed precomputation that does not impact the per-token generation latency.</p>\n\n",
                "matched_terms": [
                    "during",
                    "without",
                    "audio",
                    "standard",
                    "method",
                    "partial",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The effectiveness of Virtual Longform Audio Training (VLAT) depends on the strategy used to sample the virtual length during training. For generality, here we define the virtual length as a produce of true audio context length (i.e. 30s) and the <span class=\"ltx_text ltx_font_italic\">virtual factor</span>. In our main experiment Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S6\" title=\"6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we used a strategy of randomly sampling a factor from the range [1x, 5x, 10x, 15x, 20x, 25x]. To understand the sensitivity of our results to this choice, we compare our default strategy against four alternatives, training a Qwen2-Audio model on each strategy and evaluating on the 10mins audio task.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "compare",
                    "virtual",
                    "during",
                    "audio",
                    "vlat",
                    "longform",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The experiment offers several insights into different VLAT sampling strategies: (1) <span class=\"ltx_text ltx_font_italic\">Fixed High-Factor</span> (always 20.0x) strategy performs the worst by a large margin, confirming that exposure to a variety of positional ranges is crucial for robust generalization, rather than training for just a single fixed target length. (2) <span class=\"ltx_text ltx_font_italic\">Default</span> strategy which uses a coarse set of 6 sampling points outperforms both the <span class=\"ltx_text ltx_font_italic\">Dense</span> (100 points) and <span class=\"ltx_text ltx_font_italic\">Very Dense</span> (1000 points) strategies. This suggests that finegrained sampling of virtual factors provides no additional benefit, and that a relatively small but diverse set of factors is sufficient for training length-robust models.</p>\n\n",
                "matched_terms": [
                    "generalization",
                    "training",
                    "virtual",
                    "vlat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The most surprising result comes from the <span class=\"ltx_text ltx_font_italic\">Limited Range</span> strategy, which achieves the highest performance across all evaluation lengths, despite its virtual lengths not extending beyond the 5mins mark (10.0x factor). While this demonstrates a powerful capacity for extrapolation, the mechanism behind this superior performance is not immediately clear. <span class=\"ltx_text ltx_ulem_uline\">It may suggest that randomization on extremely long virtual lengths could introduce instability to the training</span>, which could in turn explain why Whole YaRN diverged quickly under VLAT framework; or that the specific distribution of factors in this limited set is coincidentally optimal for our test data. We left the more in-depth study of this behavior as future work.</p>\n\n",
                "matched_terms": [
                    "virtual",
                    "vlat",
                    "training",
                    "lengths",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given this uncertainty, we retained the <span class=\"ltx_text ltx_font_italic\">Default</span> strategy for our main experiments in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S6\" title=\"6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Its factor range intuitively and completely covers the lengths seen during evaluation, making it a more principled and safer choice.</p>\n\n",
                "matched_terms": [
                    "lengths",
                    "during",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here we analyze the effect and sensitivity of different Partial YaRN&#8217;s hyperparameter configurations. We report validation accuracies w.r.t. different cutoff dimension indices and attention temperatures, for the Qwen2-Audio and SALMONN models.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.F3\" title=\"Figure 3 &#8227; Plausible explanation of this trend. &#8227; E.2.2 Sensitivity on SALMONN &#8227; E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> displays the hyperparameter sensitivity of Partial YaRN on Qwen2-Audio when extending from the original 30s audio context window. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.F4\" title=\"Figure 4 &#8227; Plausible explanation of this trend. &#8227; E.2.2 Sensitivity on SALMONN &#8227; E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the sensitivity when extending from the observed audio context window of 2mins.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the attention temperature controls the distribution of attention scores. In very long sequences with thousands of tokens, the standard softmax function can lead to an uneven attention score distribution, such as attending to only a few tokens while ignoring the vast majority of the context. Raising the temperature flattens this distribution, encouraging the model to maintain a broader attention pattern across the entire long audio stream and preventing \"attention collapse\".</p>\n\n",
                "matched_terms": [
                    "audio",
                    "standard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.F5\" title=\"Figure 5 &#8227; Plausible explanation of this trend. &#8227; E.2.2 Sensitivity on SALMONN &#8227; E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> displays the hyperparameter sensitivity of Partial YaRN on SALMONN when extending from the original 30s audio context window.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The vast distinction between SALMONN and the previously discussed Qwen2-Audio lies in the their audio representation lengths&#8212;SALMONN only uses 88 tokens for every 30s audio vs. Qwen2-Audio&#8217;s 750 tokens. This, coupled with the model&#8217;s preference of low attention temperature, suggests that letting the model focuses on a relevant local region outweighs the need to spreading the attention distribution as in Qwen2-Audio.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio"
                ]
            }
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Finetuned audio context extension performance (Accuracy). We finetune Qwen2-Audio on YODAS2-MCQA, under two LoRA settings: adapting only q, and adapting qkvo. Each model is finetuned and evaluated using the same extension method.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Adapt</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">2 mins</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">5 mins</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">10 mins</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\"><span class=\"ltx_text ltx_font_bold\">q</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Vanilla</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">95.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">78.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">21.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Whole YaRN</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">96.53</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">88.53</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">73.73</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Partial PI</th>\n<td class=\"ltx_td ltx_align_center\">96.13</td>\n<td class=\"ltx_td ltx_align_center\">83.73</td>\n<td class=\"ltx_td ltx_align_center\">69.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" rowspan=\"3\"><span class=\"ltx_text ltx_font_bold\">qkvo</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Vanilla</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">96.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">85.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">64.93</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Whole YaRN</th>\n<td class=\"ltx_td ltx_align_center\">97.47</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">93.60</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">83.47</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Partial PI</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">98.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">93.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">83.07</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "evaluated",
            "finetuned",
            "settings",
            "each",
            "vanilla",
            "yarn",
            "adapting",
            "partial",
            "whole",
            "two",
            "qwen2audio",
            "audio",
            "context",
            "adapt",
            "accuracy",
            "lora",
            "performance",
            "finetune",
            "qkvo",
            "under",
            "only",
            "mins",
            "same",
            "model",
            "extension",
            "method",
            "yodas2mcqa"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, showing that fine-tuning with a context extension method dramatically outperforms the vanilla fine-tuning baseline, especially at longer audio lengths. In the qkvo setting, both Whole YaRN (83.47%) and Partial PI (83.07%) achieve an absolute performance gain of around 19% over the Vanilla baseline (64.93%) at 10mins. This highlights the benefit of integrating a context extension method into the fine-tuning process for more better context extension.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large Audio-Language Models (LALMs) are often constrained by short audio context windows, even when their text backbones support long contexts, limiting long-form audio understanding. Prior work has introduced context-extension methods (e.g. YaRN) on unimodal LLMs, yet their application to LALMs remains unexplored. First, building on RoPE-based context extension, we introduce <span class=\"ltx_text ltx_font_italic\">Partial YaRN</span>, a training-free, audio-only extension method that modifies only audio token positions, leaving text positions intact to preserve the base LLM&#8217;s text capabilities. Second, we propose <span class=\"ltx_text ltx_font_italic\">Virtual Longform Audio Training</span> (VLAT), a training strategy that extends Partial YaRN into a training-time positional augmentation. VLAT simulates diverse audio lengths during training, enabling generalization to inputs far longer than those seen in training and improving robustness for long-context audio understanding. Our experiments on SALMONN and Qwen2-Audio show that Partial YaRN outperforms the original models across wide range of settings, and VLAT training strategy provides substantial improvement, achieving strong performance on long audio of unseen lengths.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code is available at: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/scb-10x/partial-yarn\" title=\"\">https://github.com/scb-10x/partial-yarn</a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio",
                    "context",
                    "settings",
                    "yarn",
                    "extension",
                    "method",
                    "partial",
                    "only",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">Extending Audio Context for Long-Form Understanding \n<br class=\"ltx_break\"/>in Large Audio-Language Models</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of Large Language Models (LLMs) has spurred multimodal extensions, notably large audio-language models (LALMs)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Also known by other names such as Speech-aware LLMs.</span></span></span> that pair an audio encoder with a text backbone. By aligning audio and text in a shared representation, LALMs can leverage the base LLM&#8217;s knowledge for complex audio understanding. However, practical use is constrained by short audio context windows: models such as SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib49\" title=\"\">2024</a>)</cite> and Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib5\" title=\"\">2024</a>)</cite> are typically trained on audio segments of 30s or less, and thus generalize poorly to longer inputs <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib19\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this limitation, we study the application of LLM context extension methods such as Positional Interpolation (PI) <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib4\" title=\"\">2023</a>)</cite> and YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> for audio context extension in LALMs. To our knowledge, the application of these techniques for this specific, modality-bound use case has not been systematically explored. Applying such whole-context techniques implicitly extends the audio context window as a side effect. However, as this straightforward approach alters the positional information of the entire sequence, including text tokens; it risks degrading the sophisticated language capability of the base LLM which was pretrained solely in text.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "extension",
                    "yarn",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This potential drawback motivates a more targeted strategy. We introduce &#8220;<span class=\"ltx_text ltx_font_italic\">Partial YaRN</span>&#8221;, an audio-only context extension method that modifies only the audio tokens&#8217; positional encodings. This design enables a direct comparative study against the whole-context approaches, allowing us to investigate the tradeoff between preserving position encodings of the text modality and maintaining a globally uniform positional space.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "context",
                    "extension",
                    "method",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments on both training-free and fine-tuned audio context extensions reveal that extension-based methods outperforms original models. However, the best choice between whole-context and audio-only extensions is often model-dependent without a universally superior solution. This suggests that to build truly robust long-audio-context models, we must address the core problem of generalization during the training process. Therefore, we extend Partial YaRN into a novel fine-tuning technique called &#8220;<span class=\"ltx_text ltx_font_italic\">Virtual Longform Audio Training</span>&#8221; (VLAT). Acting as a <span class=\"ltx_text ltx_font_italic\">positional augmentation</span> technique, it simulates a wide range of audio lengths during fine-tuning, teaching the model to generalize beyond the lengths present in the training dataset. Through VLAT, we obtain excellent results on longform audio of unseen lengths. Our main contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "finetuned",
                    "audio",
                    "model",
                    "context",
                    "yarn",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_italic\">Partial YaRN</span>, a training-free audio-context extension for LALMs that preserves the base language model&#8217;s text capabilities by leaving text positions unaltered.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "extension",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose Virtual Longform Audio Training (VLAT), a training strategy that applies Partial YaRN as positional augmentation to simulate diverse audio lengths during training, improving long-context generalization.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "audio",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a comparative study between existing context extension methods designed for unimodal LLMs and our proposed methods for LALMs, analyzing their performance trade-off in both training-free and fine-tuning settings.</p>\n\n",
                "matched_terms": [
                    "extension",
                    "settings",
                    "context",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib4\" title=\"\">2023</a>)</cite> proposes that instead of extrapolating RoPE further to the region unfamiliar for the trained model, interpolation is done inside the pretrained context window. Specifically, let <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> be the original context length of the model, and <math alttext=\"L^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msup><mi>L</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">L^{\\prime}</annotation></semantics></math> be the target length to be extended to. Define <math alttext=\"s=\\frac{L^{\\prime}}{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo>=</mo><mfrac><msup><mi>L</mi><mo>&#8242;</mo></msup><mi>L</mi></mfrac></mrow><annotation encoding=\"application/x-tex\">s=\\frac{L^{\\prime}}{L}</annotation></semantics></math> as the extension factor. PI simply downscales the base rotational frequency by the extension factor: <math alttext=\"\\frac{\\theta_{i}}{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mfrac><msub><mi>&#952;</mi><mi>i</mi></msub><mi>s</mi></mfrac><annotation encoding=\"application/x-tex\">\\frac{\\theta_{i}}{s}</annotation></semantics></math>. For example, if originally each positional step is rotated by <math alttext=\"\\theta_{i}=10^{\\circ}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#952;</mi><mi>i</mi></msub><mo>=</mo><msup><mn>10</mn><mo>&#8728;</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\theta_{i}=10^{\\circ}</annotation></semantics></math>; to double the context length, the frequency (step size) can simply be halved to <math alttext=\"5^{\\circ}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msup><mn>5</mn><mo>&#8728;</mo></msup><annotation encoding=\"application/x-tex\">5^{\\circ}</annotation></semantics></math>. This way, the model will never have to attend to positions outside of its trained window; resulting in a more stable approach.</p>\n\n",
                "matched_terms": [
                    "model",
                    "extension",
                    "each",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One broadly adopted paradigm of large audio-language models (LALMs) is the <span class=\"ltx_text ltx_font_italic\">unified input space</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib49\" title=\"\">2024</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib5\" title=\"\">2024</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib12\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib16\" title=\"\">2025</a>)</cite> which involves transforming audio inputs into the textual input space of a base LLM. By sharing the embedding space, the strong text capability and learned knowledge of the base LLM can be leveraged while simultaneously being augmented with audio understanding ability. There also exists other LALM paradigms such as <span class=\"ltx_text ltx_font_italic\">cross attention</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib25\" title=\"\">2024</a>; Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib14\" title=\"\">2025</a>)</cite> where the audio and text modalities are fused through cross-attention modules. This work focuses on models under the unified-input-space.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "under"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As with text context window in LLMs, LALMs also do not generalize well to audio with lengths much longer than the window seen during training (e.g. 7s for Pengi <cite class=\"ltx_cite ltx_citemacro_citep\">(Deshmukh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib10\" title=\"\">2023</a>)</cite>; 30s for Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib5\" title=\"\">2024</a>)</cite>). Concurrent to us, <cite class=\"ltx_cite ltx_citemacro_citet\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib19\" title=\"\">2025</a>)</cite> introduces FastLongSpeech, an efficient longform LALM that uses Iterative Fusion to compress long audio representation into compact forms. Our work, in contrast, investigates an orthogonal direction of adapting the model&#8217;s positional encodings rather than the audio embeddings, and therefore is generally applicable to the existing models without requiring costly retraining.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio",
                    "context",
                    "adapting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To extend existing <span class=\"ltx_text ltx_font_italic\">unified input</span> LALMs to longer audio, we hypothesize that <em class=\"ltx_emph ltx_font_bold ltx_font_italic\">LALMs already possess a sufficient general understanding of audio and text, but the bottleneck is their unfamiliarity to audio positions beyond the range seen during audio-text training.</em><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>This differs from whole-context extension in unimodal LLMs, where the total sequence length exceeds the pre-trained context window, causing an out-of-distribution problem. Here, the total sequence (audio + text) usually still fits within the original window, making the core challenge one of adapting to an unfamiliar audio length and positions, rather than extrapolating to completely unseen positions.</span></span></span> This hypothesis suggests a training-free solution: manipulating the backbone LLM&#8217;s positional encoding. The goal is to remap positions of a long audio input into the model&#8217;s familiar audio range.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "extension",
                    "context",
                    "adapting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we study the application of YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> to LALMs, as an example of whole-context extension methods. Since the audio window is a part of the total context window, performing whole-context extension extends the audio window as a byproduct. However, this alters positional information for all tokens, including text, risking the degradation of the base LLM, which was pretrained solely in text. Motivated by the aforementioned hypothesis, we propose <span class=\"ltx_text ltx_font_bold\">Partial YaRN</span>, an audio-only extension method designed to exclusively stretch the audio window. By leaving the text&#8217;s positional encodings unaltered, this approach aims to extend<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>The terms <span class=\"ltx_text ltx_font_italic\">extend</span> and <span class=\"ltx_text ltx_font_italic\">stretch</span> are used interchangeably.</span></span></span> audio context while preserving the base model&#8217;s text capability.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "context",
                    "extension",
                    "yarn",
                    "method",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by PI <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib4\" title=\"\">2023</a>)</cite> and YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> for whole-context extension in unimodal LLMs, we adapt the interpolation technique to LALMs by applying it exclusively to the audio region of the base language models.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "adapt",
                    "extension",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Concretely, assuming a single audio input, let <math alttext=\"L_{\\text{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>audio</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{audio}}</annotation></semantics></math> be the original audio context length, <math alttext=\"L_{\\text{audio}}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>L</mi><mtext>audio</mtext><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">L_{\\text{audio}}^{\\prime}</annotation></semantics></math> be the target length, and <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> be the position of the first audio token. We define the positional range <math alttext=\"[p,p+L_{\\text{audio}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>p</mi><mo>,</mo><mrow><mi>p</mi><mo>+</mo><msub><mi>L</mi><mtext>audio</mtext></msub></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">[p,p+L_{\\text{audio}})</annotation></semantics></math> as the <span class=\"ltx_text ltx_font_italic\">original audio context window</span>. It can be either a predefined location for audio input, or a dynamic region enclosed by special tokens (e.g. <span class=\"ltx_text ltx_font_typewriter\">&lt;speech&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;/speech&gt;</span>). We then apply our modified YaRN technique to exclusively stretch this region to the new length <math alttext=\"L_{\\text{audio}}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>L</mi><mtext>audio</mtext><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">L_{\\text{audio}}^{\\prime}</annotation></semantics></math>. This creates a partially stretched positional encoding:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "yarn",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, we only partition the RoPE dimensions into two frequency-based groups (instead of three as in the original YaRN formulation): a low frequency group that undergoes pure interpolation, and a high frequency group that undergoes pure extrapolation. The rationale for this design is twofold:</p>\n\n",
                "matched_terms": [
                    "only",
                    "yarn",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide empirical validation and a more in-depth discussion of the two-group partitioning in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S7.SS2\" title=\"7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a> and Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A2\" title=\"Appendix B Elaboration on Two-group Partitioning of Partial YaRN &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> respectively. See Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S0.F1\" title=\"Figure 1 &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> for an example depiction of Partial YaRN.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Partial YaRN employs two hyperparameters:</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1) Cutoff Dimension Index</span>: This defines the boundary separating the RoPE dimensions into two groups. Dimensions equal or below this cutoff (low-frequency) are interpolated to stably cover longer audio context. Dimensions above it (high-frequency) are extrapolated to preserve local positional distances and high-frequency information. We default this to <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><mn>0</mn></math> (interpolate every dimension).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "context",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2) Attention Temperature</span>: This controls the sharpness of attention distribution within the audio context window. A higher temperature softens the distribution, preventing attention scores from collapsing to a few tokens over long sequences. We default this to <math alttext=\"1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mn>1.0</mn><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math> (no temperature scaling).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Instead of directly adding a new temperature term to the attention softmax, we follow <cite class=\"ltx_cite ltx_citemacro_citet\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> and integrate the attention temperature into magnitudes of RoPE&#8217;s signals. Particularly, define <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> and <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> as two positions within the extended audio context window, we have the attention softmax:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "context",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Then, we invert <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p5.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> into the two rotation matrices by scaling them with <math alttext=\"1/\\sqrt{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p5.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><msqrt><mi>t</mi></msqrt></mrow><annotation encoding=\"application/x-tex\">1/\\sqrt{t}</annotation></semantics></math> each:</p>\n\n",
                "matched_terms": [
                    "each",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notice that under the default hyperparameter values, the audio-only extension reduces from YaRN to PI, we therefore specifically name Partial YaRN with default hyperparameters: <span class=\"ltx_text ltx_font_italic\">Partial PI</span>.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "under",
                    "yarn",
                    "extension"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We source audio from the English subset of YODAS2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib30\" title=\"\">2023b</a>)</cite>. We segment audio samples into non-overlapping segments of 1, 2, 5, and 10 minutes. For each segment, we generate five multiple-choice question-answering (MCQA) pairs using Gemini 2.0 Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib50\" title=\"\">2023</a>)</cite>, with four choices each. We design the generation prompt to ensure that each question focuses on different portions of the audio, and that they collectively cover the entire audio segment. The test set of each audio duration (1, 2, 5, and 10 minutes) has 750 QA pairs.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We select two widely-used open-weights LALMs: SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib49\" title=\"\">2024</a>)</cite> and Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib5\" title=\"\">2024</a>)</cite>. Both models utilize Whisper-encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib42\" title=\"\">2023</a>)</cite> as audio encoder, which is limited to processing audio of up to 30s. We handle longer audio inputs by segmenting them into non-overlapping 30s chunks, where each chunk is encoded independently.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio",
                    "each",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The number of audio representations differs considerably between these models. For each 30s chunk on 50Hz Whisper-encoder, SALMONN generates a sequence of 88 audio tokens (with Q-Former), whereas Qwen2-Audio produces a sequence of 750 tokens (with a 2x downsampling).</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate Partial PI (default hyperparameters) and Partial YaRN (tuned hyperparameters) against the following methods:</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1) Vanilla</span>: Unmodified base models. Long audio inputs are passed directly without any manipulation to their RoPE.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "vanilla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2) Whole Position Interpolation (PI)</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib4\" title=\"\">2023</a>)</cite>: Every RoPE dimension is uniformly interpolated across the whole context window. This serves as the primitive whole-context baseline.</p>\n\n",
                "matched_terms": [
                    "whole",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">3) Whole YaRN</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite>: Frequency-based RoPE interpolation with attention scaling, applied to the whole context window. This serves as the primary whole-context baseline.</p>\n\n",
                "matched_terms": [
                    "whole",
                    "yarn",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These three baselines require no tuning as Vanilla and Whole PI don&#8217;t utilize any hyperparameter, and YaRN has predefined cutoffs and a closed-form formula for obtaining temperature.</p>\n\n",
                "matched_terms": [
                    "yarn",
                    "whole",
                    "vanilla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also report performance of two proprietary models capable of long audio context: GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib37\" title=\"\">2024</a>)</cite> and Gemini 2.0 Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib50\" title=\"\">2023</a>)</cite>, in order to provide a broader perspective of LALMs and to validate the quality of our dataset<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>The model versions are <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini-2024-07-18</span> and <span class=\"ltx_text ltx_font_typewriter\">gemini-2.0-flash-001</span> respectively. Note that Gemini was also used for generating our YODAS2-MCQA dataset.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "context",
                    "yodas2mcqa",
                    "two",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we comparatively study the effect of whole-context and audio-only context extensions on both the training-free and fine-tuning settings for audio segments up to 10mins.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "settings",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we extend the audio context length without training. We report the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S4.SS1.SSS0.Px1\" title=\"YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, showing that: no single method&#8212;whole-context or audio-only&#8212;is universally superior. Instead, the performance varies between the different models and the degree of extension. More importantly, we observe that <span class=\"ltx_text ltx_font_italic\">both models generally retain consistent performance up to 2mins</span>, suggesting that their innate audio context lengths are longer than just 30s, possibly a result from multi-audio training.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "extension",
                    "context",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When stretching from the 30s window, extension methods provide substantial performance gains compared to the Vanilla baseline. We observe that Partial YaRN shows the strongest performance on the 1min and 10mins settings, achieving 57.35% and 32.93% accuracies. On 2mins, both Whole and Partial YaRN perform similarly. However, Whole YaRN outperforms Partial YaRN at 5mins by a large margin. Possible reasons for this performance flip are (1) higher expressivity and less compression pressure from Whole YaRN&#8217;s additional &#8220;in-between&#8221; dimension group, outweighs the preservation of the base language capability in this 5mins setting, or (2) noise in the hyperparameter tuning process of Partial YaRN, which is an inherent drawback.</p>\n\n",
                "matched_terms": [
                    "settings",
                    "yarn",
                    "vanilla",
                    "extension",
                    "partial",
                    "whole",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to SALMONN, Qwen2-Audio demonstrates different characteristics. The Vanilla baseline is more robust, outperforming all stretch-from-30s methods at 2mins. Both Whole and Partial YaRN struggle to provide significant gains and even underperform the baseline at moderate lengths (2 and 5mins) when stretching from the original 30s context; however, they manage to attain considerable gains on the 10mins setting. This indicates that Qwen2-Audio likely possesses a strong audio-length generalization capability or longer innate audio context window. For this model, simply applying an extension method from the base context does not guarantee an improvement. This suggests that the model&#8217;s high intrinsic audio-length generalizability is superior to the extension methods that inevitably cram the positional information as a side effect.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio",
                    "model",
                    "context",
                    "extension",
                    "yarn",
                    "vanilla",
                    "method",
                    "partial",
                    "whole"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As previously mentioned in the beginning of this section that we can observe both model&#8217;s innate audio contexts to be around 2mins instead of just 30s, here we experiment with extending from this <span class=\"ltx_text ltx_font_italic\">observed audio context of 2mins</span> to the target lengths. For both SALMONN and Qwen2-Audio, performance on the long audio improves substantially when the interpolation is anchored from a 2mins context instead of the original 30s. Notably on Qwen2-Audio, this strategy boosts Partial YaRN&#8217;s performance at 10mins from 28.53% to a 48.00%, an absolute improvement of almost 20%, yielding a final performance 26% higher than the Vanilla baseline. This finding suggests that determining the extension by observing the vanilla performance may be a more critical factor than the choice between whole-context or audio-only methods, especially on large extension ratios.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio",
                    "context",
                    "extension",
                    "vanilla",
                    "partial",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, we study training-based extension methods. Keeping other components frozen, we fine-tune the base language model of Qwen2-Audio using a <span class=\"ltx_text ltx_font_italic\">single epoch LoRA</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib20\" title=\"\">2022</a>)</cite> of rank 8 on the YODAS2-MCQA training set of the corresponding length. Two LoRA settings are explored: adapting only the query projection (q), and adapting query, key, value, and output projections (qkvo). Due to the high cost of tuning Partial YaRN&#8217;s hyperparameters during training, we only use the default configuration (Partial PI) for the audio-only method. Whole PI is omitted due to its poor performance as previously observed.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "finetune",
                    "qkvo",
                    "model",
                    "settings",
                    "extension",
                    "method",
                    "adapting",
                    "yodas2mcqa",
                    "partial",
                    "whole",
                    "only",
                    "lora",
                    "two",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comparing the whole-context and audio-only approaches, we find that their performance is generally competitive. While Whole YaRN triumps when only adapting the query projection weights, suggesting it&#8217;s ease of adaptation. Their performance converges under standard LoRA practice which adapts qkvo. This result confirms that both whole-context and audio-only methods are highly effective strategies for fine-tuning.</p>\n\n",
                "matched_terms": [
                    "qkvo",
                    "under",
                    "yarn",
                    "adapting",
                    "whole",
                    "only",
                    "lora",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our previous results show that while the training-free methods are beneficial, their effectiveness is often model and audio-length dependent. Based on this, we propose to fine-tune LALMs with Partial YaRN repurposed as a <span class=\"ltx_text ltx_font_italic\">positional augmentation</span> technique we call <span class=\"ltx_text ltx_font_italic\">\"Virtual Longform Audio Training\"</span> (VLAT).</p>\n\n",
                "matched_terms": [
                    "finetune",
                    "audio",
                    "model",
                    "yarn",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of VLAT is to simulate and expose models to audio context windows of diverse lengths during training. Let <math alttext=\"L_{\\text{data}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>data</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{data}}</annotation></semantics></math> be the actual length of a training audio sample. For each sample, we obtain a &#8220;virtual&#8221; source length, <math alttext=\"L_{\\text{virt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>virt</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{virt}}</annotation></semantics></math>, as the model&#8217;s default audio context length <span class=\"ltx_text ltx_font_italic\">times</span> a factor randomly sampled from the range [1, 5, 10, 15, 20, 25]x. We then apply Partial YaRN<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Whole YaRN is not used here, as we observe that it diverges quickly when training under the VLAT framework.</span></span></span> to stretch or compress the <math alttext=\"L_{\\text{virt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>virt</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{virt}}</annotation></semantics></math>-long positional window to <math alttext=\"L_{\\text{data}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>data</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{data}}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "context",
                    "under",
                    "yarn",
                    "each",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For instance, consider an audio sample with <math alttext=\"L_{\\text{data}}=2\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mtext>data</mtext></msub><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">L_{\\text{data}}=2</annotation></semantics></math>mins, and we draw a <math alttext=\"L_{\\text{virt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>virt</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{virt}}</annotation></semantics></math> of 10mins, Partial YaRN will compress a 10mins context window to 2mins for this audio. This process effectively simulates the 2mins audio to be 10mins long virtually, thereby familiarizing the model to longer audio context and improving its ability to generalize to genuinely long audio at inference time.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "context",
                    "yarn",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Randomizing positional encoding has previously been explored in <cite class=\"ltx_cite ltx_citemacro_citet\">Ruoss et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib44\" title=\"\">2023</a>)</cite>, where positional indices are randomly downsampled to let the model see larger positional values. Rather than using <math alttext=\"\\{1,2,\\dots,N\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{1,2,\\dots,N\\}</annotation></semantics></math> for an length-<math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> input, they use a randomly subsampled values such as <math alttext=\"\\{1,4,\\dots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>4</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{1,4,\\dots,L\\}</annotation></semantics></math> with <math alttext=\"L&gt;N\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m4\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>&gt;</mo><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">L&gt;N</annotation></semantics></math> instead. In contrast, VLAT differs in employing interpolation inside the context window to create a dense, <span class=\"ltx_text ltx_font_italic\">continuous</span> space of positions, unlike sparse integer subsampling. It is also bidirectional, teaching the model through both compressed and stretched contexts. Furthermore, VLAT is a targeted, modality-bound method that modifies only the audio tokens, preserving the base LLM&#8217;s original textual space.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "context",
                    "method",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune Qwen2-Audio on a 2mins YODAS2-MCQA training set<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>To prevent data leakage, the train/test splits were performed at the video level, ensuring no source material overlaps between the training set and any of the test sets.</span></span></span> using two methods: a standard Vanilla fine-tuning and VLAT. We employ 10-epoch qkvo LoRA for the adaptations. We then evaluate the models from both training methods under two different extensions at inference: Vanilla and Partial PI. See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.SS1\" title=\"E.1 Different VLAT&#8217;s Virtual Length Sampling Strategies &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">E.1</span></a> for additional detail on VLAT configuration.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "finetune",
                    "qkvo",
                    "under",
                    "vanilla",
                    "yodas2mcqa",
                    "partial",
                    "lora",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S4.T2\" title=\"Table 2 &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, VLAT greatly improves generalization to unseen audio lengths. When evaluated with vanilla inference, the model trained with Virtual Longform shows a dramatic improvement on 10mins audio, increasing accuracy from 32.76% to 75.11%, closing in on the previous 1-epoch direct-fine-tuning result. This highlights the effectiveness of the training technique for training LALMs to generalize far beyond the audio lengths present in their training data, mitigating a key bottleneck in the development of robust long-audio models.</p>\n\n",
                "matched_terms": [
                    "evaluated",
                    "audio",
                    "model",
                    "vanilla",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, as can be seen in \"Partial PI Inference\" columns, VLAT&#8217;s benefit is complementary to inference-time extension. When Partial PI is applied during evaluation, the VLAT-trained model again outperforms its vanilla-trained counterpart, achieving the highest overall 10mins performance of 81.73%. This indicates that VLAT and inference-time extension are compatible strategies, and their combination yields the most robust long-context performance.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "extension",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we conduct an ablation study to isolate and analyze the individual contributions of the two key components in Whole YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> and Partial YaRN: (1) frequency grouping and (2) attention temperature scaling. We note that by removing both of these components, the methods converge to Whole PI <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib4\" title=\"\">2023</a>)</cite> and Partial PI. The results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S7.SS1\" title=\"7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">7.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "whole",
                    "yarn",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can see that removing either the frequency grouping or the attention temperature generally leads to a large drop in performance, especially on higher extension ratios. Both the frequency partitioning and attention temperature scaling are crucial for robustly extending the audio context length of LALMs. Their combined application within the Whole and Partial YaRN frameworks generally yield the most effective and stable performance.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "context",
                    "extension",
                    "yarn",
                    "partial",
                    "whole",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the Partial YaRN method, we modify the original 3-group frequency partition from original YaRN to a 2-group method as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S3.SS1\" title=\"3.1 Methodology &#8227; 3 Partial YaRN &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> and Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A2\" title=\"Appendix B Elaboration on Two-group Partitioning of Partial YaRN &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. Here we conduct a direct empirical comparison between the 2-group and 3-group approaches.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the 3-group baseline, we implement Partial YaRN using YaRN&#8217;s original partitioning scheme and its recommended hyperparameters. We compare this against our proposed 2-group Partial YaRN using the hyperparameters from our main experiments (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S5.SS1\" title=\"5.1 Training-free Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>). We present the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S7.T5\" title=\"Table 5 &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We see that our 2-group partitioning broadly outperform 3-group in our modality-specific scenario. This is most evident with the Qwen2-Audio model, where the 3-group method suffers a large performance drop on 5mins and 10mins audio, with an accuracy gap of 28.53% and 12.00% respectively. Overall, this result validates the effectiveness and reliability of our proposed 2-group scheme used in Partial YaRN.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio",
                    "model",
                    "accuracy",
                    "yarn",
                    "method",
                    "partial",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work addressed the challenge of extending audio context windows of LALMs by studying the application of LLM&#8217;s whole-context extension methods, and proposing a training-free audio-only extension Partial YaRN and the VLAT training strategy. We demonstrate the effectiveness of extension-based methods over non-extension baselines across various long-audio settings. Leveraging our finding that SALMONN and Qwen2-Audio retain consistent performance up to 2mins audio window, we achieved an even stronger performance by extending from the 2mins window instead of 30s. Later, we showed that fine-tuning through VLAT helps the models to generalize to audio contexts far exceeding the length of the training data, offering a robust and data-efficient pathway to develop LALMs with better longform understanding. Future work could explore a similar modality-bound extension strategy for video models.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio",
                    "context",
                    "settings",
                    "yarn",
                    "extension",
                    "partial",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments are conducted exclusively on the multiple-choice question answering (MCQA) tasks. We made this choice to ensure a precise and unambiguous measurement of the model performance under different audio lengths. This mitigates the confounding variables inherent in evaluating open-ended generation, such as judging the semantic equivalence or stylistic alignment. However, a key tradeoff of this focused evaluation is its limited ability to assess the nuanced generative and language modeling capabilities of the base LLM. Consequently, while our results validate the model&#8217;s ability to retrieve information from audio, they do not fully test the importance of text preservation of the audio-only methods like Partial YaRN. Furthermore, our study does not assess performance across other long-audio task formats found in benchmarks such as <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib19\" title=\"\">2025</a>; Ahia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib1\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib16\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "under",
                    "yarn",
                    "partial",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike the original YaRN which has generally good predefined cutoff dimensions and a closed-form formula for determining the attention temperature, our proposed Partial YaRN requires tuning of these two hyperparameters, thereby introducing additional computational burden.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">YC would like to thank Ekapol Chuangsuwanich for insightful discussion on the earlier version of this work. He also thanks Thanapat Trachu for valuable guidance on the illustration of Partial YaRN.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conceived along with the introduction of transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib52\" title=\"\">2017</a>)</cite>, the original <span class=\"ltx_text ltx_font_italic\">absolute positional encoding</span> utilizes sinusoidal waves of varying frequencies to uniquely represent absolute position of the tokens, and is incorporated into the model by adding directly into the input sequence. Later works <cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib11\" title=\"\">2019</a>; Lan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib26\" title=\"\">2020</a>; Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib7\" title=\"\">2020</a>)</cite> propose to use trainable vectors for representing absolute positional information, where token at each position is summed with a learned vector, allowing the models to learn the optimal positional encoding for themselves. However, it cannot extrapolate to sequences longer than the maximum length it was trained on.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Shaw et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib46\" title=\"\">2018</a>)</cite> introduces to encode the relative position of the token instead, where the relative distance between the query and key is captured and injected into every attention layer. <cite class=\"ltx_cite ltx_citemacro_citet\">Dai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib8\" title=\"\">2019</a>)</cite> modifies the decomposed form of the attention equation, and introduces additional trainable location-agnostic key vectors to distinguish between content-based and location-based querying behaviors. <cite class=\"ltx_cite ltx_citemacro_citet\">Raffel et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib43\" title=\"\">2020</a>)</cite> groups the relative distances into buckets, and associate each bucket with a learnable scalar for adding into the attention logits. Simplifying the encoding scheme and reducing trainable parameters. TUPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Ke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib24\" title=\"\">2021</a>)</cite> uses separate linear projections for the word and positional information. ALiBi <cite class=\"ltx_cite ltx_citemacro_citep\">(Press et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib41\" title=\"\">2022</a>)</cite> simply adds linear biases with a learnable slope into the attention scores, the bias grows linearly with the relative distance between the tokens. This method generally imposes a strong locality preference by encouraging models to attend more to the nearby tokens, while discouraging them from attending to tokens that are farther away. Several later works extend RoPE for multimodal modeling (e.g. Multimodal RoPE <cite class=\"ltx_cite ltx_citemacro_cite\">Bai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib3\" title=\"\">2025</a>)</cite>, VideoRoPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib53\" title=\"\">2025</a>)</cite>, VRoPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib31\" title=\"\">2025</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "model",
                    "each",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Video input is similar to the audio modality in that they&#8217;re both naturally continuous and can be of arbitrary length. Many Large Video-Language Models (LVLMs), such as Video-LLaMA <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib58\" title=\"\">2023b</a>)</cite> and Video-ChatGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Maaz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib34\" title=\"\">2024</a>)</cite>, adopt architectural paradigm similar to the unified input space LALMs we study, where video frames are embedded and projected into a base LLM&#8217;s input space. However, the context length problem is even more exacerbated in the video domain due to the high dimensionality and large number of tokens needed to represent even short videos. To improve efficiency, the LVLM community has largely focused on input compression techniques like query aggregation <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib58\" title=\"\">2023b</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib28\" title=\"\">2023a</a>; Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib47\" title=\"\">2024</a>)</cite>, frame pooling <cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib33\" title=\"\">2023</a>; Maaz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib34\" title=\"\">2024</a>)</cite>, and feature merging <cite class=\"ltx_cite ltx_citemacro_citep\">(Weng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib54\" title=\"\">2024</a>)</cite>. These strategies primarily aim to reduce the number of tokens before they enter the LLM. Our work offers a complementary perspective: instead of altering the input, we manipulate the positional space within the model to accommodate a longer sequence. The principles of Partial YaRN and Virtual Longform Audio Training could therefore be potentially applicable to the video domain as well, hopefully providing an orthogonal path to achieving long-context understanding in synergy with existing compression methods.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "context",
                    "yarn",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The decision to simplify the original three-group frequency partition of YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> into a two-group system for Partial YaRN is a deliberate design choice aimed at ensuring positional consistency across the entire extended audio stream. The original YaRN formulation partitions RoPE dimensions into: 1) a low-frequency group that is purely interpolated, 2) a high-frequency group that is purely extrapolated, and 3) an \"in-between\" group that receives a mix of both, with a ramped interpolation factor.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "audio",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While effective for whole-context extension, this \"in-between\" group becomes problematic in our partial, audio-only extension scenario. Consider a 2mins audio input, which requires a 4x extension of the model&#8217;s 30s original audio context.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "audio",
                    "context",
                    "extension"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The low-frequency dimensions, responsible for quasi-absolute positioning, are fully interpolated by a 4x factor. This is crucial as it maps the entire 2mins audio into the model&#8217;s familiar positional range (original audio context).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The \"in-between\" dimensions, however, would receive interpolation factors between the 1x and 4x (e.g. 2x). This means that for these specific dimensions, the model&#8217;s original context window is stretched to cover only the first minute of the 2mins audio. The second minute would lie in an extrapolated, out-of-distribution region.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "only",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This dimensional inconsistency risks creating a biased or distorted representation of the latter half of the audio. The model would receive conflicting signals about whether a token is \"inside\" or \"outside\" its familiar context.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By simplifying to a two-group partition, we circumvent this issue entirely. Every dimension is forced into a binary choice: either it is fully interpolated to cover the entire audio duration (low-frequency), or it is fully extrapolated to preserve local structure (high-frequency). This ensures that the model maintains a consistent positional understanding across all dimensions for the entirety of the audio input, avoiding the representational bias that partial coverage would introduce.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S7.SS2\" title=\"7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a> for direct performance comparison between 2 and 3-group frequency partitioning versions of Partial YaRN.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key implementation detail of our work is how we apply the positional stretching to only a subsection of the input sequence. While the original YaRN implementation modifies the RoPE frequency variables (<math alttext=\"\\boldsymbol{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><mi>&#120637;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{\\theta}</annotation></semantics></math>), this approach isn&#8217;t straightforward for a <span class=\"ltx_text ltx_font_italic\">partial</span> modification. Instead, our implementation of Partial YaRN directly manipulates the <span class=\"ltx_text ltx_font_typewriter\">position_ids</span> and multiply it with the unmodified frequencies. This section details the implementation our method.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "only",
                    "yarn",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In most Transformer architectures, including the models used in this work, the RoPE mechanism computes embeddings based on an input tensor called <span class=\"ltx_text ltx_font_typewriter\">position_ids</span>. This tensor is simply a sequence of integers representing the absolute position of each token (i.e. <span class=\"ltx_text ltx_font_typewriter\">[0, 1, 2, ..., N-1]</span>). Our method leverages this by modifying a section this tensor. The process is as follows:</p>\n\n",
                "matched_terms": [
                    "each",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identify the Audio Region:</span> First, we identify the start and end indices of the audio tokens within the <span class=\"ltx_text ltx_font_typewriter\">position_ids</span> tensor. This creates three distinct segments: the leading text positions (unaltered), the audio positions (to be stretched), and the trailing text positions (unaltered relative to each other).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This implementation introduces negligible computational overhead in practice. The only additional computations are incurred during the construction of the <span class=\"ltx_text ltx_font_typewriter\">position_ids</span> tensor, which involves a few tensor slicing operations, a single <span class=\"ltx_text ltx_font_typewriter\">torch.linspace</span> call, and a final concatenation. These are highly optimized, vectorized operations that constitute a minuscule fraction of the total computation in a full forward pass. Crucially, this precomputation is a <span class=\"ltx_text ltx_ulem_uline\">one-time cost</span> per input sample, applied only during the initial processing of the prompt that contains the long audio. During the subsequent autoregressive generation steps, where tokens are generated one at a time, our method adds no overhead whatsoever. The model reverts to the standard process of incremental positional stepping, just as it would without any context extension. Therefore, the cost of enabling partial context extension is a small, fixed precomputation that does not impact the per-token generation latency.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "context",
                    "extension",
                    "method",
                    "partial",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recall that Partial YaRN has two hyperparameters, cutoff dimension and attention temperature. For the training-free experiment (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S5.SS1\" title=\"5.1 Training-free Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), we jointly sweep the cutoff dimension from <math alttext=\"\\{56,48,40,32,24,16,8\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>56</mn><mo>,</mo><mn>48</mn><mo>,</mo><mn>40</mn><mo>,</mo><mn>32</mn><mo>,</mo><mn>24</mn><mo>,</mo><mn>16</mn><mo>,</mo><mn>8</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{56,48,40,32,24,16,8\\}</annotation></semantics></math>, and the temperature from between 0.5 and 1.6 with a step size of 0.1. See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.SS2\" title=\"E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a> for tuning results and sensitivity.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To obtain specific audio length, we chunk the long audio in the dataset to 1, 2, 5, and 10 minutes. Last, incomplete chunk of each long-audio is also included as well. Each of the chunk will be used to generate 5 MCQA pairs via Gemini 2.0 Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib50\" title=\"\">2023</a>)</cite>. We provide the full MCQA generation prompt in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.F7\" title=\"Figure 7 &#8227; Plausible explanation of this trend. &#8227; E.2.2 Sensitivity on SALMONN &#8227; E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The effectiveness of Virtual Longform Audio Training (VLAT) depends on the strategy used to sample the virtual length during training. For generality, here we define the virtual length as a produce of true audio context length (i.e. 30s) and the <span class=\"ltx_text ltx_font_italic\">virtual factor</span>. In our main experiment Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S6\" title=\"6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we used a strategy of randomly sampling a factor from the range [1x, 5x, 10x, 15x, 20x, 25x]. To understand the sensitivity of our results to this choice, we compare our default strategy against four alternatives, training a Qwen2-Audio model on each strategy and evaluating on the 10mins audio task.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio",
                    "model",
                    "context",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Default</span>: Uniform random sampling from <math alttext=\"\\{1.0,5.0,10.0,15.0,20.0,25.0\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I1.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>1.0</mn><mo>,</mo><mn>5.0</mn><mo>,</mo><mn>10.0</mn><mo>,</mo><mn>15.0</mn><mo>,</mo><mn>20.0</mn><mo>,</mo><mn>25.0</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{1.0,5.0,10.0,15.0,20.0,25.0\\}</annotation></semantics></math>. This cover the audio context length of 30s (1.0x) to 12.5mins (25.0x).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited Range</span>: Uniform random sampling from <math alttext=\"\\{1.0,5.0,10.0\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I1.i4.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>1.0</mn><mo>,</mo><mn>5.0</mn><mo>,</mo><mn>10.0</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{1.0,5.0,10.0\\}</annotation></semantics></math>. This factor set covers audio context length of at most 5mins (10.0x).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The most surprising result comes from the <span class=\"ltx_text ltx_font_italic\">Limited Range</span> strategy, which achieves the highest performance across all evaluation lengths, despite its virtual lengths not extending beyond the 5mins mark (10.0x factor). While this demonstrates a powerful capacity for extrapolation, the mechanism behind this superior performance is not immediately clear. <span class=\"ltx_text ltx_ulem_uline\">It may suggest that randomization on extremely long virtual lengths could introduce instability to the training</span>, which could in turn explain why Whole YaRN diverged quickly under VLAT framework; or that the specific distribution of factors in this limited set is coincidentally optimal for our test data. We left the more in-depth study of this behavior as future work.</p>\n\n",
                "matched_terms": [
                    "whole",
                    "under",
                    "yarn",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here we analyze the effect and sensitivity of different Partial YaRN&#8217;s hyperparameter configurations. We report validation accuracies w.r.t. different cutoff dimension indices and attention temperatures, for the Qwen2-Audio and SALMONN models.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.F3\" title=\"Figure 3 &#8227; Plausible explanation of this trend. &#8227; E.2.2 Sensitivity on SALMONN &#8227; E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> displays the hyperparameter sensitivity of Partial YaRN on Qwen2-Audio when extending from the original 30s audio context window. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.F4\" title=\"Figure 4 &#8227; Plausible explanation of this trend. &#8227; E.2.2 Sensitivity on SALMONN &#8227; E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the sensitivity when extending from the observed audio context window of 2mins.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio",
                    "context",
                    "yarn",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This trend informs the following recommendation when using Partial YaRN. For moderate context extensions (e.g., 2x to 4x), it is advisable to start with a higher cutoff dimension index and a temperature value close to 1.0. For longer extensions, a lower cutoff dimension index with a considerably higher temperature is likely to yield better results.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the attention temperature controls the distribution of attention scores. In very long sequences with thousands of tokens, the standard softmax function can lead to an uneven attention score distribution, such as attending to only a few tokens while ignoring the vast majority of the context. Raising the temperature flattens this distribution, encouraging the model to maintain a broader attention pattern across the entire long audio stream and preventing \"attention collapse\".</p>\n\n",
                "matched_terms": [
                    "audio",
                    "only",
                    "model",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, the cutoff dimension index directly controls the \"compression pressure\" of the position encoding extension. A high cutoff index is expressive enough for shorter extensions while retaining majority of the original, detailed positional information. On the other hand, a lower cutoff index (higher compression) is required to map a much larger range of positions into the smaller familiar audio region.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "extension"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.F5\" title=\"Figure 5 &#8227; Plausible explanation of this trend. &#8227; E.2.2 Sensitivity on SALMONN &#8227; E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> displays the hyperparameter sensitivity of Partial YaRN on SALMONN when extending from the original 30s audio context window.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "partial",
                    "yarn",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For this model, we observe that the optimal hyperparameter sets are generally contained in the low temperature region (0.5 - 0.7), with the cutoff dimension index moving from low to higher ones as the audio length grows.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The vast distinction between SALMONN and the previously discussed Qwen2-Audio lies in the their audio representation lengths&#8212;SALMONN only uses 88 tokens for every 30s audio vs. Qwen2-Audio&#8217;s 750 tokens. This, coupled with the model&#8217;s preference of low attention temperature, suggests that letting the model focuses on a relevant local region outweighs the need to spreading the attention distribution as in Qwen2-Audio.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "audio",
                    "only",
                    "model"
                ]
            }
        ]
    },
    "S7.SS1.tab1": {
        "caption": "Table 4: Ablation of frequency grouping and attention temperature components (Accuracy). Removing either the frequency grouping or the attention temperature leads to a large drop in performance.",
        "body": "<table class=\"ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">Partial</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">Whole</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">2 mins</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">10 mins</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">2 mins</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">10 mins</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"5\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>[gray]0.95 &#8196;&#8196;<span class=\"ltx_text ltx_font_bold\">SALMONN</span>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">YaRN</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">59.60</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">32.93</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">59.87</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">30.13</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">- Freq grouping</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">56.93</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">4.53</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">23.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">29.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">- Attn temp</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">48.13</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">23.87</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">41.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">23.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">PI</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">45.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">1.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">5.37</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">23.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"5\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">\n<span class=\"ltx_ERROR undefined\">\\arrayrulecolor</span>black!70\n\n<span class=\"ltx_ERROR undefined\">\\arrayrulecolor</span>black\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>[gray]0.95 &#8196;&#8196;  &#8196;&#8196;&#8196;<span class=\"ltx_text ltx_font_bold\">Qwen2-Audio</span>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">YaRN</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">73.60</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">28.53</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">73.47</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">25.33</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">- Freq grouping</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">66.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">28.13</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">17.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">9.87</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">- Attn temp</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">73.47</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">18.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">75.07</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">22.80</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">PI</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">67.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">18.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">7.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">8.67</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "attention",
            "drop",
            "yarn",
            "arrayrulecolorblack70",
            "partial",
            "whole",
            "attn",
            "grouping",
            "qwen2audio",
            "ablation",
            "frequency",
            "leads",
            "accuracy",
            "performance",
            "removing",
            "arrayrulecolorblack",
            "temperature",
            "mins",
            "components",
            "freq",
            "large",
            "rowcolorgray095",
            "temp",
            "either",
            "method",
            "salmonn"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large Audio-Language Models (LALMs) are often constrained by short audio context windows, even when their text backbones support long contexts, limiting long-form audio understanding. Prior work has introduced context-extension methods (e.g. YaRN) on unimodal LLMs, yet their application to LALMs remains unexplored. First, building on RoPE-based context extension, we introduce <span class=\"ltx_text ltx_font_italic\">Partial YaRN</span>, a training-free, audio-only extension method that modifies only audio token positions, leaving text positions intact to preserve the base LLM&#8217;s text capabilities. Second, we propose <span class=\"ltx_text ltx_font_italic\">Virtual Longform Audio Training</span> (VLAT), a training strategy that extends Partial YaRN into a training-time positional augmentation. VLAT simulates diverse audio lengths during training, enabling generalization to inputs far longer than those seen in training and improving robustness for long-context audio understanding. Our experiments on SALMONN and Qwen2-Audio show that Partial YaRN outperforms the original models across wide range of settings, and VLAT training strategy provides substantial improvement, achieving strong performance on long audio of unseen lengths.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code is available at: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/scb-10x/partial-yarn\" title=\"\">https://github.com/scb-10x/partial-yarn</a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "large",
                    "salmonn",
                    "yarn",
                    "method",
                    "partial",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of Large Language Models (LLMs) has spurred multimodal extensions, notably large audio-language models (LALMs)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Also known by other names such as Speech-aware LLMs.</span></span></span> that pair an audio encoder with a text backbone. By aligning audio and text in a shared representation, LALMs can leverage the base LLM&#8217;s knowledge for complex audio understanding. However, practical use is constrained by short audio context windows: models such as SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib49\" title=\"\">2024</a>)</cite> and Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib5\" title=\"\">2024</a>)</cite> are typically trained on audio segments of 30s or less, and thus generalize poorly to longer inputs <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib19\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "large",
                    "salmonn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments on both training-free and fine-tuned audio context extensions reveal that extension-based methods outperforms original models. However, the best choice between whole-context and audio-only extensions is often model-dependent without a universally superior solution. This suggests that to build truly robust long-audio-context models, we must address the core problem of generalization during the training process. Therefore, we extend Partial YaRN into a novel fine-tuning technique called &#8220;<span class=\"ltx_text ltx_font_italic\">Virtual Longform Audio Training</span>&#8221; (VLAT). Acting as a <span class=\"ltx_text ltx_font_italic\">positional augmentation</span> technique, it simulates a wide range of audio lengths during fine-tuning, teaching the model to generalize beyond the lengths present in the training dataset. Through VLAT, we obtain excellent results on longform audio of unseen lengths. Our main contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_italic\">Partial YaRN</span>, a training-free audio-context extension for LALMs that preserves the base language model&#8217;s text capabilities by leaving text positions unaltered.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose Virtual Longform Audio Training (VLAT), a training strategy that applies Partial YaRN as positional augmentation to simulate diverse audio lengths during training, improving long-context generalization.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><cite class=\"ltx_cite ltx_citemacro_citet\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> identifies that interpolating every dimension equally leads to the loss of local distances and high frequency information. They propose to spread the interpolation pressure by partitioning RoPE dimensions into three frequency-based groups: (1) low frequency dimensions are interpolated, (2) high frequency dimensions are solely extrapolated without interpolation to preserve high frequency information, (3) dimensions in-between get a mix of both interpolation and extrapolation. Additionally, YaRN applies temperature scaling to the logits of attention softmax:</p>\n\n",
                "matched_terms": [
                    "temperature",
                    "attention",
                    "frequency",
                    "yarn",
                    "leads"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> is the attention temperature.</p>\n\n",
                "matched_terms": [
                    "attention",
                    "temperature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One broadly adopted paradigm of large audio-language models (LALMs) is the <span class=\"ltx_text ltx_font_italic\">unified input space</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib49\" title=\"\">2024</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib5\" title=\"\">2024</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib12\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib16\" title=\"\">2025</a>)</cite> which involves transforming audio inputs into the textual input space of a base LLM. By sharing the embedding space, the strong text capability and learned knowledge of the base LLM can be leveraged while simultaneously being augmented with audio understanding ability. There also exists other LALM paradigms such as <span class=\"ltx_text ltx_font_italic\">cross attention</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib25\" title=\"\">2024</a>; Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib14\" title=\"\">2025</a>)</cite> where the audio and text modalities are fused through cross-attention modules. This work focuses on models under the unified-input-space.</p>\n\n",
                "matched_terms": [
                    "attention",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we study the application of YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> to LALMs, as an example of whole-context extension methods. Since the audio window is a part of the total context window, performing whole-context extension extends the audio window as a byproduct. However, this alters positional information for all tokens, including text, risking the degradation of the base LLM, which was pretrained solely in text. Motivated by the aforementioned hypothesis, we propose <span class=\"ltx_text ltx_font_bold\">Partial YaRN</span>, an audio-only extension method designed to exclusively stretch the audio window. By leaving the text&#8217;s positional encodings unaltered, this approach aims to extend<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>The terms <span class=\"ltx_text ltx_font_italic\">extend</span> and <span class=\"ltx_text ltx_font_italic\">stretch</span> are used interchangeably.</span></span></span> audio context while preserving the base model&#8217;s text capability.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Concretely, assuming a single audio input, let <math alttext=\"L_{\\text{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>audio</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{audio}}</annotation></semantics></math> be the original audio context length, <math alttext=\"L_{\\text{audio}}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>L</mi><mtext>audio</mtext><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">L_{\\text{audio}}^{\\prime}</annotation></semantics></math> be the target length, and <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> be the position of the first audio token. We define the positional range <math alttext=\"[p,p+L_{\\text{audio}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>p</mi><mo>,</mo><mrow><mi>p</mi><mo>+</mo><msub><mi>L</mi><mtext>audio</mtext></msub></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">[p,p+L_{\\text{audio}})</annotation></semantics></math> as the <span class=\"ltx_text ltx_font_italic\">original audio context window</span>. It can be either a predefined location for audio input, or a dynamic region enclosed by special tokens (e.g. <span class=\"ltx_text ltx_font_typewriter\">&lt;speech&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;/speech&gt;</span>). We then apply our modified YaRN technique to exclusively stretch this region to the new length <math alttext=\"L_{\\text{audio}}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>L</mi><mtext>audio</mtext><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">L_{\\text{audio}}^{\\prime}</annotation></semantics></math>. This creates a partially stretched positional encoding:</p>\n\n",
                "matched_terms": [
                    "either",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, we only partition the RoPE dimensions into two frequency-based groups (instead of three as in the original YaRN formulation): a low frequency group that undergoes pure interpolation, and a high frequency group that undergoes pure extrapolation. The rationale for this design is twofold:</p>\n\n",
                "matched_terms": [
                    "frequency",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide empirical validation and a more in-depth discussion of the two-group partitioning in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S7.SS2\" title=\"7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a> and Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A2\" title=\"Appendix B Elaboration on Two-group Partitioning of Partial YaRN &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> respectively. See Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S0.F1\" title=\"Figure 1 &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> for an example depiction of Partial YaRN.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Partial YaRN employs two hyperparameters:</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2) Attention Temperature</span>: This controls the sharpness of attention distribution within the audio context window. A higher temperature softens the distribution, preventing attention scores from collapsing to a few tokens over long sequences. We default this to <math alttext=\"1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mn>1.0</mn><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math> (no temperature scaling).</p>\n\n",
                "matched_terms": [
                    "attention",
                    "temperature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Instead of directly adding a new temperature term to the attention softmax, we follow <cite class=\"ltx_cite ltx_citemacro_citet\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> and integrate the attention temperature into magnitudes of RoPE&#8217;s signals. Particularly, define <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> and <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> as two positions within the extended audio context window, we have the attention softmax:</p>\n\n",
                "matched_terms": [
                    "attention",
                    "temperature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This reparameterization also handles attention between the unaltered text regions and the scaled audio regions. For instance, when a text query (unscaled) attends to an audio key (scaled by <math alttext=\"1/\\sqrt{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><msqrt><mi>t</mi></msqrt></mrow><annotation encoding=\"application/x-tex\">1/\\sqrt{t}</annotation></semantics></math>), the resulting logit naturally gets scaled by <math alttext=\"1/\\sqrt{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><msqrt><mi>t</mi></msqrt></mrow><annotation encoding=\"application/x-tex\">1/\\sqrt{t}</annotation></semantics></math>, creating a smoother temperature transition. Overall, we have temperature-scaled rotation matrices:</p>\n\n",
                "matched_terms": [
                    "attention",
                    "temperature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notice that under the default hyperparameter values, the audio-only extension reduces from YaRN to PI, we therefore specifically name Partial YaRN with default hyperparameters: <span class=\"ltx_text ltx_font_italic\">Partial PI</span>.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We select two widely-used open-weights LALMs: SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib49\" title=\"\">2024</a>)</cite> and Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib5\" title=\"\">2024</a>)</cite>. Both models utilize Whisper-encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib42\" title=\"\">2023</a>)</cite> as audio encoder, which is limited to processing audio of up to 30s. We handle longer audio inputs by segmenting them into non-overlapping 30s chunks, where each chunk is encoded independently.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "salmonn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The number of audio representations differs considerably between these models. For each 30s chunk on 50Hz Whisper-encoder, SALMONN generates a sequence of 88 audio tokens (with Q-Former), whereas Qwen2-Audio produces a sequence of 750 tokens (with a 2x downsampling).</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "salmonn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate Partial PI (default hyperparameters) and Partial YaRN (tuned hyperparameters) against the following methods:</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">3) Whole YaRN</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite>: Frequency-based RoPE interpolation with attention scaling, applied to the whole context window. This serves as the primary whole-context baseline.</p>\n\n",
                "matched_terms": [
                    "attention",
                    "whole",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These three baselines require no tuning as Vanilla and Whole PI don&#8217;t utilize any hyperparameter, and YaRN has predefined cutoffs and a closed-form formula for obtaining temperature.</p>\n\n",
                "matched_terms": [
                    "whole",
                    "yarn",
                    "temperature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When stretching from the 30s window, extension methods provide substantial performance gains compared to the Vanilla baseline. We observe that Partial YaRN shows the strongest performance on the 1min and 10mins settings, achieving 57.35% and 32.93% accuracies. On 2mins, both Whole and Partial YaRN perform similarly. However, Whole YaRN outperforms Partial YaRN at 5mins by a large margin. Possible reasons for this performance flip are (1) higher expressivity and less compression pressure from Whole YaRN&#8217;s additional &#8220;in-between&#8221; dimension group, outweighs the preservation of the base language capability in this 5mins setting, or (2) noise in the hyperparameter tuning process of Partial YaRN, which is an inherent drawback.</p>\n\n",
                "matched_terms": [
                    "large",
                    "yarn",
                    "partial",
                    "whole",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to SALMONN, Qwen2-Audio demonstrates different characteristics. The Vanilla baseline is more robust, outperforming all stretch-from-30s methods at 2mins. Both Whole and Partial YaRN struggle to provide significant gains and even underperform the baseline at moderate lengths (2 and 5mins) when stretching from the original 30s context; however, they manage to attain considerable gains on the 10mins setting. This indicates that Qwen2-Audio likely possesses a strong audio-length generalization capability or longer innate audio context window. For this model, simply applying an extension method from the base context does not guarantee an improvement. This suggests that the model&#8217;s high intrinsic audio-length generalizability is superior to the extension methods that inevitably cram the positional information as a side effect.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "yarn",
                    "method",
                    "partial",
                    "whole",
                    "salmonn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As previously mentioned in the beginning of this section that we can observe both model&#8217;s innate audio contexts to be around 2mins instead of just 30s, here we experiment with extending from this <span class=\"ltx_text ltx_font_italic\">observed audio context of 2mins</span> to the target lengths. For both SALMONN and Qwen2-Audio, performance on the long audio improves substantially when the interpolation is anchored from a 2mins context instead of the original 30s. Notably on Qwen2-Audio, this strategy boosts Partial YaRN&#8217;s performance at 10mins from 28.53% to a 48.00%, an absolute improvement of almost 20%, yielding a final performance 26% higher than the Vanilla baseline. This finding suggests that determining the extension by observing the vanilla performance may be a more critical factor than the choice between whole-context or audio-only methods, especially on large extension ratios.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "large",
                    "performance",
                    "partial",
                    "salmonn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, we study training-based extension methods. Keeping other components frozen, we fine-tune the base language model of Qwen2-Audio using a <span class=\"ltx_text ltx_font_italic\">single epoch LoRA</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib20\" title=\"\">2022</a>)</cite> of rank 8 on the YODAS2-MCQA training set of the corresponding length. Two LoRA settings are explored: adapting only the query projection (q), and adapting query, key, value, and output projections (qkvo). Due to the high cost of tuning Partial YaRN&#8217;s hyperparameters during training, we only use the default configuration (Partial PI) for the audio-only method. Whole PI is omitted due to its poor performance as previously observed.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "components",
                    "method",
                    "partial",
                    "whole",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, showing that fine-tuning with a context extension method dramatically outperforms the vanilla fine-tuning baseline, especially at longer audio lengths. In the qkvo setting, both Whole YaRN (83.47%) and Partial PI (83.07%) achieve an absolute performance gain of around 19% over the Vanilla baseline (64.93%) at 10mins. This highlights the benefit of integrating a context extension method into the fine-tuning process for more better context extension.</p>\n\n",
                "matched_terms": [
                    "yarn",
                    "method",
                    "partial",
                    "whole",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comparing the whole-context and audio-only approaches, we find that their performance is generally competitive. While Whole YaRN triumps when only adapting the query projection weights, suggesting it&#8217;s ease of adaptation. Their performance converges under standard LoRA practice which adapts qkvo. This result confirms that both whole-context and audio-only methods are highly effective strategies for fine-tuning.</p>\n\n",
                "matched_terms": [
                    "whole",
                    "yarn",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our previous results show that while the training-free methods are beneficial, their effectiveness is often model and audio-length dependent. Based on this, we propose to fine-tune LALMs with Partial YaRN repurposed as a <span class=\"ltx_text ltx_font_italic\">positional augmentation</span> technique we call <span class=\"ltx_text ltx_font_italic\">\"Virtual Longform Audio Training\"</span> (VLAT).</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of VLAT is to simulate and expose models to audio context windows of diverse lengths during training. Let <math alttext=\"L_{\\text{data}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>data</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{data}}</annotation></semantics></math> be the actual length of a training audio sample. For each sample, we obtain a &#8220;virtual&#8221; source length, <math alttext=\"L_{\\text{virt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>virt</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{virt}}</annotation></semantics></math>, as the model&#8217;s default audio context length <span class=\"ltx_text ltx_font_italic\">times</span> a factor randomly sampled from the range [1, 5, 10, 15, 20, 25]x. We then apply Partial YaRN<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Whole YaRN is not used here, as we observe that it diverges quickly when training under the VLAT framework.</span></span></span> to stretch or compress the <math alttext=\"L_{\\text{virt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>virt</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{virt}}</annotation></semantics></math>-long positional window to <math alttext=\"L_{\\text{data}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>data</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{data}}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For instance, consider an audio sample with <math alttext=\"L_{\\text{data}}=2\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mtext>data</mtext></msub><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">L_{\\text{data}}=2</annotation></semantics></math>mins, and we draw a <math alttext=\"L_{\\text{virt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>virt</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{virt}}</annotation></semantics></math> of 10mins, Partial YaRN will compress a 10mins context window to 2mins for this audio. This process effectively simulates the 2mins audio to be 10mins long virtually, thereby familiarizing the model to longer audio context and improving its ability to generalize to genuinely long audio at inference time.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune Qwen2-Audio on a 2mins YODAS2-MCQA training set<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>To prevent data leakage, the train/test splits were performed at the video level, ensuring no source material overlaps between the training set and any of the test sets.</span></span></span> using two methods: a standard Vanilla fine-tuning and VLAT. We employ 10-epoch qkvo LoRA for the adaptations. We then evaluate the models from both training methods under two different extensions at inference: Vanilla and Partial PI. See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.SS1\" title=\"E.1 Different VLAT&#8217;s Virtual Length Sampling Strategies &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">E.1</span></a> for additional detail on VLAT configuration.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, as can be seen in \"Partial PI Inference\" columns, VLAT&#8217;s benefit is complementary to inference-time extension. When Partial PI is applied during evaluation, the VLAT-trained model again outperforms its vanilla-trained counterpart, achieving the highest overall 10mins performance of 81.73%. This indicates that VLAT and inference-time extension are compatible strategies, and their combination yields the most robust long-context performance.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we conduct an ablation study to isolate and analyze the individual contributions of the two key components in Whole YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> and Partial YaRN: (1) frequency grouping and (2) attention temperature scaling. We note that by removing both of these components, the methods converge to Whole PI <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib4\" title=\"\">2023</a>)</cite> and Partial PI. The results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S7.SS1\" title=\"7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">7.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "components",
                    "removing",
                    "ablation",
                    "attention",
                    "temperature",
                    "frequency",
                    "yarn",
                    "partial",
                    "whole",
                    "grouping"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can see that removing either the frequency grouping or the attention temperature generally leads to a large drop in performance, especially on higher extension ratios. Both the frequency partitioning and attention temperature scaling are crucial for robustly extending the audio context length of LALMs. Their combined application within the Whole and Partial YaRN frameworks generally yield the most effective and stable performance.</p>\n\n",
                "matched_terms": [
                    "removing",
                    "large",
                    "temperature",
                    "attention",
                    "drop",
                    "frequency",
                    "yarn",
                    "either",
                    "leads",
                    "partial",
                    "whole",
                    "grouping",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the Partial YaRN method, we modify the original 3-group frequency partition from original YaRN to a 2-group method as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S3.SS1\" title=\"3.1 Methodology &#8227; 3 Partial YaRN &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> and Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A2\" title=\"Appendix B Elaboration on Two-group Partitioning of Partial YaRN &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. Here we conduct a direct empirical comparison between the 2-group and 3-group approaches.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "frequency",
                    "yarn",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the 3-group baseline, we implement Partial YaRN using YaRN&#8217;s original partitioning scheme and its recommended hyperparameters. We compare this against our proposed 2-group Partial YaRN using the hyperparameters from our main experiments (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S5.SS1\" title=\"5.1 Training-free Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>). We present the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S7.T5\" title=\"Table 5 &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We see that our 2-group partitioning broadly outperform 3-group in our modality-specific scenario. This is most evident with the Qwen2-Audio model, where the 3-group method suffers a large performance drop on 5mins and 10mins audio, with an accuracy gap of 28.53% and 12.00% respectively. Overall, this result validates the effectiveness and reliability of our proposed 2-group scheme used in Partial YaRN.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "large",
                    "accuracy",
                    "drop",
                    "yarn",
                    "method",
                    "partial",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work addressed the challenge of extending audio context windows of LALMs by studying the application of LLM&#8217;s whole-context extension methods, and proposing a training-free audio-only extension Partial YaRN and the VLAT training strategy. We demonstrate the effectiveness of extension-based methods over non-extension baselines across various long-audio settings. Leveraging our finding that SALMONN and Qwen2-Audio retain consistent performance up to 2mins audio window, we achieved an even stronger performance by extending from the 2mins window instead of 30s. Later, we showed that fine-tuning through VLAT helps the models to generalize to audio contexts far exceeding the length of the training data, offering a robust and data-efficient pathway to develop LALMs with better longform understanding. Future work could explore a similar modality-bound extension strategy for video models.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "yarn",
                    "performance",
                    "partial",
                    "salmonn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments are conducted exclusively on the multiple-choice question answering (MCQA) tasks. We made this choice to ensure a precise and unambiguous measurement of the model performance under different audio lengths. This mitigates the confounding variables inherent in evaluating open-ended generation, such as judging the semantic equivalence or stylistic alignment. However, a key tradeoff of this focused evaluation is its limited ability to assess the nuanced generative and language modeling capabilities of the base LLM. Consequently, while our results validate the model&#8217;s ability to retrieve information from audio, they do not fully test the importance of text preservation of the audio-only methods like Partial YaRN. Furthermore, our study does not assess performance across other long-audio task formats found in benchmarks such as <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib19\" title=\"\">2025</a>; Ahia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib1\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib16\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike the original YaRN which has generally good predefined cutoff dimensions and a closed-form formula for determining the attention temperature, our proposed Partial YaRN requires tuning of these two hyperparameters, thereby introducing additional computational burden.</p>\n\n",
                "matched_terms": [
                    "attention",
                    "partial",
                    "yarn",
                    "temperature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">YC would like to thank Ekapol Chuangsuwanich for insightful discussion on the earlier version of this work. He also thanks Thanapat Trachu for valuable guidance on the illustration of Partial YaRN.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conceived along with the introduction of transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib52\" title=\"\">2017</a>)</cite>, the original <span class=\"ltx_text ltx_font_italic\">absolute positional encoding</span> utilizes sinusoidal waves of varying frequencies to uniquely represent absolute position of the tokens, and is incorporated into the model by adding directly into the input sequence. Later works <cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib11\" title=\"\">2019</a>; Lan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib26\" title=\"\">2020</a>; Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib7\" title=\"\">2020</a>)</cite> propose to use trainable vectors for representing absolute positional information, where token at each position is summed with a learned vector, allowing the models to learn the optimal positional encoding for themselves. However, it cannot extrapolate to sequences longer than the maximum length it was trained on.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Shaw et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib46\" title=\"\">2018</a>)</cite> introduces to encode the relative position of the token instead, where the relative distance between the query and key is captured and injected into every attention layer. <cite class=\"ltx_cite ltx_citemacro_citet\">Dai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib8\" title=\"\">2019</a>)</cite> modifies the decomposed form of the attention equation, and introduces additional trainable location-agnostic key vectors to distinguish between content-based and location-based querying behaviors. <cite class=\"ltx_cite ltx_citemacro_citet\">Raffel et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib43\" title=\"\">2020</a>)</cite> groups the relative distances into buckets, and associate each bucket with a learnable scalar for adding into the attention logits. Simplifying the encoding scheme and reducing trainable parameters. TUPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Ke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib24\" title=\"\">2021</a>)</cite> uses separate linear projections for the word and positional information. ALiBi <cite class=\"ltx_cite ltx_citemacro_citep\">(Press et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib41\" title=\"\">2022</a>)</cite> simply adds linear biases with a learnable slope into the attention scores, the bias grows linearly with the relative distance between the tokens. This method generally imposes a strong locality preference by encouraging models to attend more to the nearby tokens, while discouraging them from attending to tokens that are farther away. Several later works extend RoPE for multimodal modeling (e.g. Multimodal RoPE <cite class=\"ltx_cite ltx_citemacro_cite\">Bai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib3\" title=\"\">2025</a>)</cite>, VideoRoPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib53\" title=\"\">2025</a>)</cite>, VRoPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib31\" title=\"\">2025</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "attention",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Video input is similar to the audio modality in that they&#8217;re both naturally continuous and can be of arbitrary length. Many Large Video-Language Models (LVLMs), such as Video-LLaMA <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib58\" title=\"\">2023b</a>)</cite> and Video-ChatGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Maaz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib34\" title=\"\">2024</a>)</cite>, adopt architectural paradigm similar to the unified input space LALMs we study, where video frames are embedded and projected into a base LLM&#8217;s input space. However, the context length problem is even more exacerbated in the video domain due to the high dimensionality and large number of tokens needed to represent even short videos. To improve efficiency, the LVLM community has largely focused on input compression techniques like query aggregation <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib58\" title=\"\">2023b</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib28\" title=\"\">2023a</a>; Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib47\" title=\"\">2024</a>)</cite>, frame pooling <cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib33\" title=\"\">2023</a>; Maaz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib34\" title=\"\">2024</a>)</cite>, and feature merging <cite class=\"ltx_cite ltx_citemacro_citep\">(Weng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib54\" title=\"\">2024</a>)</cite>. These strategies primarily aim to reduce the number of tokens before they enter the LLM. Our work offers a complementary perspective: instead of altering the input, we manipulate the positional space within the model to accommodate a longer sequence. The principles of Partial YaRN and Virtual Longform Audio Training could therefore be potentially applicable to the video domain as well, hopefully providing an orthogonal path to achieving long-context understanding in synergy with existing compression methods.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The decision to simplify the original three-group frequency partition of YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> into a two-group system for Partial YaRN is a deliberate design choice aimed at ensuring positional consistency across the entire extended audio stream. The original YaRN formulation partitions RoPE dimensions into: 1) a low-frequency group that is purely interpolated, 2) a high-frequency group that is purely extrapolated, and 3) an \"in-between\" group that receives a mix of both, with a ramped interpolation factor.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "frequency",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By simplifying to a two-group partition, we circumvent this issue entirely. Every dimension is forced into a binary choice: either it is fully interpolated to cover the entire audio duration (low-frequency), or it is fully extrapolated to preserve local structure (high-frequency). This ensures that the model maintains a consistent positional understanding across all dimensions for the entirety of the audio input, avoiding the representational bias that partial coverage would introduce.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "either"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S7.SS2\" title=\"7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a> for direct performance comparison between 2 and 3-group frequency partitioning versions of Partial YaRN.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "frequency",
                    "yarn",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key implementation detail of our work is how we apply the positional stretching to only a subsection of the input sequence. While the original YaRN implementation modifies the RoPE frequency variables (<math alttext=\"\\boldsymbol{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><mi>&#120637;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{\\theta}</annotation></semantics></math>), this approach isn&#8217;t straightforward for a <span class=\"ltx_text ltx_font_italic\">partial</span> modification. Instead, our implementation of Partial YaRN directly manipulates the <span class=\"ltx_text ltx_font_typewriter\">position_ids</span> and multiply it with the unmodified frequencies. This section details the implementation our method.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "frequency",
                    "yarn",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This implementation introduces negligible computational overhead in practice. The only additional computations are incurred during the construction of the <span class=\"ltx_text ltx_font_typewriter\">position_ids</span> tensor, which involves a few tensor slicing operations, a single <span class=\"ltx_text ltx_font_typewriter\">torch.linspace</span> call, and a final concatenation. These are highly optimized, vectorized operations that constitute a minuscule fraction of the total computation in a full forward pass. Crucially, this precomputation is a <span class=\"ltx_text ltx_ulem_uline\">one-time cost</span> per input sample, applied only during the initial processing of the prompt that contains the long audio. During the subsequent autoregressive generation steps, where tokens are generated one at a time, our method adds no overhead whatsoever. The model reverts to the standard process of incremental positional stepping, just as it would without any context extension. Therefore, the cost of enabling partial context extension is a small, fixed precomputation that does not impact the per-token generation latency.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recall that Partial YaRN has two hyperparameters, cutoff dimension and attention temperature. For the training-free experiment (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S5.SS1\" title=\"5.1 Training-free Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), we jointly sweep the cutoff dimension from <math alttext=\"\\{56,48,40,32,24,16,8\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>56</mn><mo>,</mo><mn>48</mn><mo>,</mo><mn>40</mn><mo>,</mo><mn>32</mn><mo>,</mo><mn>24</mn><mo>,</mo><mn>16</mn><mo>,</mo><mn>8</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{56,48,40,32,24,16,8\\}</annotation></semantics></math>, and the temperature from between 0.5 and 1.6 with a step size of 0.1. See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.SS2\" title=\"E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a> for tuning results and sensitivity.</p>\n\n",
                "matched_terms": [
                    "attention",
                    "partial",
                    "yarn",
                    "temperature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The most surprising result comes from the <span class=\"ltx_text ltx_font_italic\">Limited Range</span> strategy, which achieves the highest performance across all evaluation lengths, despite its virtual lengths not extending beyond the 5mins mark (10.0x factor). While this demonstrates a powerful capacity for extrapolation, the mechanism behind this superior performance is not immediately clear. <span class=\"ltx_text ltx_ulem_uline\">It may suggest that randomization on extremely long virtual lengths could introduce instability to the training</span>, which could in turn explain why Whole YaRN diverged quickly under VLAT framework; or that the specific distribution of factors in this limited set is coincidentally optimal for our test data. We left the more in-depth study of this behavior as future work.</p>\n\n",
                "matched_terms": [
                    "whole",
                    "yarn",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here we analyze the effect and sensitivity of different Partial YaRN&#8217;s hyperparameter configurations. We report validation accuracies w.r.t. different cutoff dimension indices and attention temperatures, for the Qwen2-Audio and SALMONN models.</p>\n\n",
                "matched_terms": [
                    "attention",
                    "partial",
                    "qwen2audio",
                    "salmonn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.F3\" title=\"Figure 3 &#8227; Plausible explanation of this trend. &#8227; E.2.2 Sensitivity on SALMONN &#8227; E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> displays the hyperparameter sensitivity of Partial YaRN on Qwen2-Audio when extending from the original 30s audio context window. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.F4\" title=\"Figure 4 &#8227; Plausible explanation of this trend. &#8227; E.2.2 Sensitivity on SALMONN &#8227; E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the sensitivity when extending from the observed audio context window of 2mins.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This trend informs the following recommendation when using Partial YaRN. For moderate context extensions (e.g., 2x to 4x), it is advisable to start with a higher cutoff dimension index and a temperature value close to 1.0. For longer extensions, a lower cutoff dimension index with a considerably higher temperature is likely to yield better results.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn",
                    "temperature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the attention temperature controls the distribution of attention scores. In very long sequences with thousands of tokens, the standard softmax function can lead to an uneven attention score distribution, such as attending to only a few tokens while ignoring the vast majority of the context. Raising the temperature flattens this distribution, encouraging the model to maintain a broader attention pattern across the entire long audio stream and preventing \"attention collapse\".</p>\n\n",
                "matched_terms": [
                    "attention",
                    "temperature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.F5\" title=\"Figure 5 &#8227; Plausible explanation of this trend. &#8227; E.2.2 Sensitivity on SALMONN &#8227; E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> displays the hyperparameter sensitivity of Partial YaRN on SALMONN when extending from the original 30s audio context window.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn",
                    "salmonn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the observed trend, it&#8217;s advisable to set the attention temperature to <math alttext=\"0.6(\\pm 0.1)\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS2.SSS2.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>0.6</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>&#177;</mo><mn>0.1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">0.6(\\pm 0.1)</annotation></semantics></math>, adopt a low cutoff index for shorter audio length, and higher index for longer audio inputs.</p>\n\n",
                "matched_terms": [
                    "attention",
                    "temperature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The vast distinction between SALMONN and the previously discussed Qwen2-Audio lies in the their audio representation lengths&#8212;SALMONN only uses 88 tokens for every 30s audio vs. Qwen2-Audio&#8217;s 750 tokens. This, coupled with the model&#8217;s preference of low attention temperature, suggests that letting the model focuses on a relevant local region outweighs the need to spreading the attention distribution as in Qwen2-Audio.</p>\n\n",
                "matched_terms": [
                    "attention",
                    "salmonn",
                    "qwen2audio",
                    "temperature"
                ]
            }
        ]
    },
    "S7.T5": {
        "caption": "Table 5: Performance Comparison of Two-Group vs. Three-Group Partitioning (Accuracy). We compare our Partial YaRN with 2-group frequency partitioning used throughout the work, against a 3-group version.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" colspan=\"2\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">YODAS2-MCQA</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">Freq Grouping</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">2 mins</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">5 mins</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">10 mins</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">SALMONN</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">2-Group</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">59.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">38.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">32.93</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">3-Group</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">56.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">41.87</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">25.46</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" rowspan=\"2\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">Qwen2-Audio</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">2-Group</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">73.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">48.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">28.53</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">3-Group</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">72.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">20.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">16.53</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "compare",
            "yarn",
            "partial",
            "comparison",
            "grouping",
            "our",
            "qwen2audio",
            "2group",
            "against",
            "frequency",
            "accuracy",
            "used",
            "performance",
            "3group",
            "twogroup",
            "mins",
            "version",
            "partitioning",
            "model",
            "freq",
            "work",
            "throughout",
            "threegroup",
            "yodas2mcqa",
            "salmonn"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">For the 3-group baseline, we implement Partial YaRN using YaRN&#8217;s original partitioning scheme and its recommended hyperparameters. We compare this against our proposed 2-group Partial YaRN using the hyperparameters from our main experiments (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S5.SS1\" title=\"5.1 Training-free Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>). We present the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S7.T5\" title=\"Table 5 &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large Audio-Language Models (LALMs) are often constrained by short audio context windows, even when their text backbones support long contexts, limiting long-form audio understanding. Prior work has introduced context-extension methods (e.g. YaRN) on unimodal LLMs, yet their application to LALMs remains unexplored. First, building on RoPE-based context extension, we introduce <span class=\"ltx_text ltx_font_italic\">Partial YaRN</span>, a training-free, audio-only extension method that modifies only audio token positions, leaving text positions intact to preserve the base LLM&#8217;s text capabilities. Second, we propose <span class=\"ltx_text ltx_font_italic\">Virtual Longform Audio Training</span> (VLAT), a training strategy that extends Partial YaRN into a training-time positional augmentation. VLAT simulates diverse audio lengths during training, enabling generalization to inputs far longer than those seen in training and improving robustness for long-context audio understanding. Our experiments on SALMONN and Qwen2-Audio show that Partial YaRN outperforms the original models across wide range of settings, and VLAT training strategy provides substantial improvement, achieving strong performance on long audio of unseen lengths.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code is available at: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/scb-10x/partial-yarn\" title=\"\">https://github.com/scb-10x/partial-yarn</a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "our",
                    "salmonn",
                    "work",
                    "yarn",
                    "partial",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of Large Language Models (LLMs) has spurred multimodal extensions, notably large audio-language models (LALMs)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Also known by other names such as Speech-aware LLMs.</span></span></span> that pair an audio encoder with a text backbone. By aligning audio and text in a shared representation, LALMs can leverage the base LLM&#8217;s knowledge for complex audio understanding. However, practical use is constrained by short audio context windows: models such as SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib49\" title=\"\">2024</a>)</cite> and Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib5\" title=\"\">2024</a>)</cite> are typically trained on audio segments of 30s or less, and thus generalize poorly to longer inputs <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib19\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "salmonn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this limitation, we study the application of LLM context extension methods such as Positional Interpolation (PI) <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib4\" title=\"\">2023</a>)</cite> and YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> for audio context extension in LALMs. To our knowledge, the application of these techniques for this specific, modality-bound use case has not been systematically explored. Applying such whole-context techniques implicitly extends the audio context window as a side effect. However, as this straightforward approach alters the positional information of the entire sequence, including text tokens; it risks degrading the sophisticated language capability of the base LLM which was pretrained solely in text.</p>\n\n",
                "matched_terms": [
                    "yarn",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments on both training-free and fine-tuned audio context extensions reveal that extension-based methods outperforms original models. However, the best choice between whole-context and audio-only extensions is often model-dependent without a universally superior solution. This suggests that to build truly robust long-audio-context models, we must address the core problem of generalization during the training process. Therefore, we extend Partial YaRN into a novel fine-tuning technique called &#8220;<span class=\"ltx_text ltx_font_italic\">Virtual Longform Audio Training</span>&#8221; (VLAT). Acting as a <span class=\"ltx_text ltx_font_italic\">positional augmentation</span> technique, it simulates a wide range of audio lengths during fine-tuning, teaching the model to generalize beyond the lengths present in the training dataset. Through VLAT, we obtain excellent results on longform audio of unseen lengths. Our main contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "model",
                    "partial",
                    "yarn",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_italic\">Partial YaRN</span>, a training-free audio-context extension for LALMs that preserves the base language model&#8217;s text capabilities by leaving text positions unaltered.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose Virtual Longform Audio Training (VLAT), a training strategy that applies Partial YaRN as positional augmentation to simulate diverse audio lengths during training, improving long-context generalization.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a comparative study between existing context extension methods designed for unimodal LLMs and our proposed methods for LALMs, analyzing their performance trade-off in both training-free and fine-tuning settings.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib4\" title=\"\">2023</a>)</cite> proposes that instead of extrapolating RoPE further to the region unfamiliar for the trained model, interpolation is done inside the pretrained context window. Specifically, let <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> be the original context length of the model, and <math alttext=\"L^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msup><mi>L</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">L^{\\prime}</annotation></semantics></math> be the target length to be extended to. Define <math alttext=\"s=\\frac{L^{\\prime}}{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo>=</mo><mfrac><msup><mi>L</mi><mo>&#8242;</mo></msup><mi>L</mi></mfrac></mrow><annotation encoding=\"application/x-tex\">s=\\frac{L^{\\prime}}{L}</annotation></semantics></math> as the extension factor. PI simply downscales the base rotational frequency by the extension factor: <math alttext=\"\\frac{\\theta_{i}}{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mfrac><msub><mi>&#952;</mi><mi>i</mi></msub><mi>s</mi></mfrac><annotation encoding=\"application/x-tex\">\\frac{\\theta_{i}}{s}</annotation></semantics></math>. For example, if originally each positional step is rotated by <math alttext=\"\\theta_{i}=10^{\\circ}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#952;</mi><mi>i</mi></msub><mo>=</mo><msup><mn>10</mn><mo>&#8728;</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\theta_{i}=10^{\\circ}</annotation></semantics></math>; to double the context length, the frequency (step size) can simply be halved to <math alttext=\"5^{\\circ}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msup><mn>5</mn><mo>&#8728;</mo></msup><annotation encoding=\"application/x-tex\">5^{\\circ}</annotation></semantics></math>. This way, the model will never have to attend to positions outside of its trained window; resulting in a more stable approach.</p>\n\n",
                "matched_terms": [
                    "frequency",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><cite class=\"ltx_cite ltx_citemacro_citet\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> identifies that interpolating every dimension equally leads to the loss of local distances and high frequency information. They propose to spread the interpolation pressure by partitioning RoPE dimensions into three frequency-based groups: (1) low frequency dimensions are interpolated, (2) high frequency dimensions are solely extrapolated without interpolation to preserve high frequency information, (3) dimensions in-between get a mix of both interpolation and extrapolation. Additionally, YaRN applies temperature scaling to the logits of attention softmax:</p>\n\n",
                "matched_terms": [
                    "partitioning",
                    "frequency",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As with text context window in LLMs, LALMs also do not generalize well to audio with lengths much longer than the window seen during training (e.g. 7s for Pengi <cite class=\"ltx_cite ltx_citemacro_citep\">(Deshmukh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib10\" title=\"\">2023</a>)</cite>; 30s for Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib5\" title=\"\">2024</a>)</cite>). Concurrent to us, <cite class=\"ltx_cite ltx_citemacro_citet\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib19\" title=\"\">2025</a>)</cite> introduces FastLongSpeech, an efficient longform LALM that uses Iterative Fusion to compress long audio representation into compact forms. Our work, in contrast, investigates an orthogonal direction of adapting the model&#8217;s positional encodings rather than the audio embeddings, and therefore is generally applicable to the existing models without requiring costly retraining.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "work",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we study the application of YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> to LALMs, as an example of whole-context extension methods. Since the audio window is a part of the total context window, performing whole-context extension extends the audio window as a byproduct. However, this alters positional information for all tokens, including text, risking the degradation of the base LLM, which was pretrained solely in text. Motivated by the aforementioned hypothesis, we propose <span class=\"ltx_text ltx_font_bold\">Partial YaRN</span>, an audio-only extension method designed to exclusively stretch the audio window. By leaving the text&#8217;s positional encodings unaltered, this approach aims to extend<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>The terms <span class=\"ltx_text ltx_font_italic\">extend</span> and <span class=\"ltx_text ltx_font_italic\">stretch</span> are used interchangeably.</span></span></span> audio context while preserving the base model&#8217;s text capability.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "used",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Concretely, assuming a single audio input, let <math alttext=\"L_{\\text{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>audio</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{audio}}</annotation></semantics></math> be the original audio context length, <math alttext=\"L_{\\text{audio}}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>L</mi><mtext>audio</mtext><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">L_{\\text{audio}}^{\\prime}</annotation></semantics></math> be the target length, and <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> be the position of the first audio token. We define the positional range <math alttext=\"[p,p+L_{\\text{audio}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>p</mi><mo>,</mo><mrow><mi>p</mi><mo>+</mo><msub><mi>L</mi><mtext>audio</mtext></msub></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">[p,p+L_{\\text{audio}})</annotation></semantics></math> as the <span class=\"ltx_text ltx_font_italic\">original audio context window</span>. It can be either a predefined location for audio input, or a dynamic region enclosed by special tokens (e.g. <span class=\"ltx_text ltx_font_typewriter\">&lt;speech&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;/speech&gt;</span>). We then apply our modified YaRN technique to exclusively stretch this region to the new length <math alttext=\"L_{\\text{audio}}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>L</mi><mtext>audio</mtext><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">L_{\\text{audio}}^{\\prime}</annotation></semantics></math>. This creates a partially stretched positional encoding:</p>\n\n",
                "matched_terms": [
                    "yarn",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, we only partition the RoPE dimensions into two frequency-based groups (instead of three as in the original YaRN formulation): a low frequency group that undergoes pure interpolation, and a high frequency group that undergoes pure extrapolation. The rationale for this design is twofold:</p>\n\n",
                "matched_terms": [
                    "frequency",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1) Consistency</span>: A two-group partition ensures a consistent and uniform positional encoding across the entire audio stream. YaRN&#8217;s original &#8220;in-between&#8221; group receives a mix of interpolation and extrapolation, which would cause these RoPE frequencies to not be extended to fit the entire audio. Our approach avoids this potential distortion.</p>\n\n",
                "matched_terms": [
                    "twogroup",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide empirical validation and a more in-depth discussion of the two-group partitioning in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S7.SS2\" title=\"7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a> and Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A2\" title=\"Appendix B Elaboration on Two-group Partitioning of Partial YaRN &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> respectively. See Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S0.F1\" title=\"Figure 1 &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> for an example depiction of Partial YaRN.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "partitioning",
                    "yarn",
                    "twogroup"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Partial YaRN employs two hyperparameters:</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notice that under the default hyperparameter values, the audio-only extension reduces from YaRN to PI, we therefore specifically name Partial YaRN with default hyperparameters: <span class=\"ltx_text ltx_font_italic\">Partial PI</span>.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We select two widely-used open-weights LALMs: SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib49\" title=\"\">2024</a>)</cite> and Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib5\" title=\"\">2024</a>)</cite>. Both models utilize Whisper-encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib42\" title=\"\">2023</a>)</cite> as audio encoder, which is limited to processing audio of up to 30s. We handle longer audio inputs by segmenting them into non-overlapping 30s chunks, where each chunk is encoded independently.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "salmonn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The number of audio representations differs considerably between these models. For each 30s chunk on 50Hz Whisper-encoder, SALMONN generates a sequence of 88 audio tokens (with Q-Former), whereas Qwen2-Audio produces a sequence of 750 tokens (with a 2x downsampling).</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "salmonn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate Partial PI (default hyperparameters) and Partial YaRN (tuned hyperparameters) against the following methods:</p>\n\n",
                "matched_terms": [
                    "against",
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also report performance of two proprietary models capable of long audio context: GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib37\" title=\"\">2024</a>)</cite> and Gemini 2.0 Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib50\" title=\"\">2023</a>)</cite>, in order to provide a broader perspective of LALMs and to validate the quality of our dataset<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>The model versions are <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini-2024-07-18</span> and <span class=\"ltx_text ltx_font_typewriter\">gemini-2.0-flash-001</span> respectively. Note that Gemini was also used for generating our YODAS2-MCQA dataset.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "used",
                    "yodas2mcqa",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When stretching from the 30s window, extension methods provide substantial performance gains compared to the Vanilla baseline. We observe that Partial YaRN shows the strongest performance on the 1min and 10mins settings, achieving 57.35% and 32.93% accuracies. On 2mins, both Whole and Partial YaRN perform similarly. However, Whole YaRN outperforms Partial YaRN at 5mins by a large margin. Possible reasons for this performance flip are (1) higher expressivity and less compression pressure from Whole YaRN&#8217;s additional &#8220;in-between&#8221; dimension group, outweighs the preservation of the base language capability in this 5mins setting, or (2) noise in the hyperparameter tuning process of Partial YaRN, which is an inherent drawback.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to SALMONN, Qwen2-Audio demonstrates different characteristics. The Vanilla baseline is more robust, outperforming all stretch-from-30s methods at 2mins. Both Whole and Partial YaRN struggle to provide significant gains and even underperform the baseline at moderate lengths (2 and 5mins) when stretching from the original 30s context; however, they manage to attain considerable gains on the 10mins setting. This indicates that Qwen2-Audio likely possesses a strong audio-length generalization capability or longer innate audio context window. For this model, simply applying an extension method from the base context does not guarantee an improvement. This suggests that the model&#8217;s high intrinsic audio-length generalizability is superior to the extension methods that inevitably cram the positional information as a side effect.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "model",
                    "yarn",
                    "partial",
                    "salmonn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As previously mentioned in the beginning of this section that we can observe both model&#8217;s innate audio contexts to be around 2mins instead of just 30s, here we experiment with extending from this <span class=\"ltx_text ltx_font_italic\">observed audio context of 2mins</span> to the target lengths. For both SALMONN and Qwen2-Audio, performance on the long audio improves substantially when the interpolation is anchored from a 2mins context instead of the original 30s. Notably on Qwen2-Audio, this strategy boosts Partial YaRN&#8217;s performance at 10mins from 28.53% to a 48.00%, an absolute improvement of almost 20%, yielding a final performance 26% higher than the Vanilla baseline. This finding suggests that determining the extension by observing the vanilla performance may be a more critical factor than the choice between whole-context or audio-only methods, especially on large extension ratios.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "partial",
                    "salmonn",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, we study training-based extension methods. Keeping other components frozen, we fine-tune the base language model of Qwen2-Audio using a <span class=\"ltx_text ltx_font_italic\">single epoch LoRA</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib20\" title=\"\">2022</a>)</cite> of rank 8 on the YODAS2-MCQA training set of the corresponding length. Two LoRA settings are explored: adapting only the query projection (q), and adapting query, key, value, and output projections (qkvo). Due to the high cost of tuning Partial YaRN&#8217;s hyperparameters during training, we only use the default configuration (Partial PI) for the audio-only method. Whole PI is omitted due to its poor performance as previously observed.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "model",
                    "partial",
                    "yodas2mcqa",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, showing that fine-tuning with a context extension method dramatically outperforms the vanilla fine-tuning baseline, especially at longer audio lengths. In the qkvo setting, both Whole YaRN (83.47%) and Partial PI (83.07%) achieve an absolute performance gain of around 19% over the Vanilla baseline (64.93%) at 10mins. This highlights the benefit of integrating a context extension method into the fine-tuning process for more better context extension.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comparing the whole-context and audio-only approaches, we find that their performance is generally competitive. While Whole YaRN triumps when only adapting the query projection weights, suggesting it&#8217;s ease of adaptation. Their performance converges under standard LoRA practice which adapts qkvo. This result confirms that both whole-context and audio-only methods are highly effective strategies for fine-tuning.</p>\n\n",
                "matched_terms": [
                    "yarn",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our previous results show that while the training-free methods are beneficial, their effectiveness is often model and audio-length dependent. Based on this, we propose to fine-tune LALMs with Partial YaRN repurposed as a <span class=\"ltx_text ltx_font_italic\">positional augmentation</span> technique we call <span class=\"ltx_text ltx_font_italic\">\"Virtual Longform Audio Training\"</span> (VLAT).</p>\n\n",
                "matched_terms": [
                    "model",
                    "partial",
                    "yarn",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of VLAT is to simulate and expose models to audio context windows of diverse lengths during training. Let <math alttext=\"L_{\\text{data}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>data</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{data}}</annotation></semantics></math> be the actual length of a training audio sample. For each sample, we obtain a &#8220;virtual&#8221; source length, <math alttext=\"L_{\\text{virt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>virt</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{virt}}</annotation></semantics></math>, as the model&#8217;s default audio context length <span class=\"ltx_text ltx_font_italic\">times</span> a factor randomly sampled from the range [1, 5, 10, 15, 20, 25]x. We then apply Partial YaRN<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Whole YaRN is not used here, as we observe that it diverges quickly when training under the VLAT framework.</span></span></span> to stretch or compress the <math alttext=\"L_{\\text{virt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>virt</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{virt}}</annotation></semantics></math>-long positional window to <math alttext=\"L_{\\text{data}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>data</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{data}}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "used",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For instance, consider an audio sample with <math alttext=\"L_{\\text{data}}=2\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mtext>data</mtext></msub><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">L_{\\text{data}}=2</annotation></semantics></math>mins, and we draw a <math alttext=\"L_{\\text{virt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>virt</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{virt}}</annotation></semantics></math> of 10mins, Partial YaRN will compress a 10mins context window to 2mins for this audio. This process effectively simulates the 2mins audio to be 10mins long virtually, thereby familiarizing the model to longer audio context and improving its ability to generalize to genuinely long audio at inference time.</p>\n\n",
                "matched_terms": [
                    "model",
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune Qwen2-Audio on a 2mins YODAS2-MCQA training set<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>To prevent data leakage, the train/test splits were performed at the video level, ensuring no source material overlaps between the training set and any of the test sets.</span></span></span> using two methods: a standard Vanilla fine-tuning and VLAT. We employ 10-epoch qkvo LoRA for the adaptations. We then evaluate the models from both training methods under two different extensions at inference: Vanilla and Partial PI. See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.SS1\" title=\"E.1 Different VLAT&#8217;s Virtual Length Sampling Strategies &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">E.1</span></a> for additional detail on VLAT configuration.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "partial",
                    "yodas2mcqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S4.T2\" title=\"Table 2 &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, VLAT greatly improves generalization to unseen audio lengths. When evaluated with vanilla inference, the model trained with Virtual Longform shows a dramatic improvement on 10mins audio, increasing accuracy from 32.76% to 75.11%, closing in on the previous 1-epoch direct-fine-tuning result. This highlights the effectiveness of the training technique for training LALMs to generalize far beyond the audio lengths present in their training data, mitigating a key bottleneck in the development of robust long-audio models.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, as can be seen in \"Partial PI Inference\" columns, VLAT&#8217;s benefit is complementary to inference-time extension. When Partial PI is applied during evaluation, the VLAT-trained model again outperforms its vanilla-trained counterpart, achieving the highest overall 10mins performance of 81.73%. This indicates that VLAT and inference-time extension are compatible strategies, and their combination yields the most robust long-context performance.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we conduct an ablation study to isolate and analyze the individual contributions of the two key components in Whole YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> and Partial YaRN: (1) frequency grouping and (2) attention temperature scaling. We note that by removing both of these components, the methods converge to Whole PI <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib4\" title=\"\">2023</a>)</cite> and Partial PI. The results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S7.SS1\" title=\"7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">7.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "frequency",
                    "yarn",
                    "grouping"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can see that removing either the frequency grouping or the attention temperature generally leads to a large drop in performance, especially on higher extension ratios. Both the frequency partitioning and attention temperature scaling are crucial for robustly extending the audio context length of LALMs. Their combined application within the Whole and Partial YaRN frameworks generally yield the most effective and stable performance.</p>\n\n",
                "matched_terms": [
                    "partitioning",
                    "frequency",
                    "yarn",
                    "partial",
                    "grouping",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the Partial YaRN method, we modify the original 3-group frequency partition from original YaRN to a 2-group method as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S3.SS1\" title=\"3.1 Methodology &#8227; 3 Partial YaRN &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> and Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A2\" title=\"Appendix B Elaboration on Two-group Partitioning of Partial YaRN &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. Here we conduct a direct empirical comparison between the 2-group and 3-group approaches.</p>\n\n",
                "matched_terms": [
                    "3group",
                    "2group",
                    "frequency",
                    "yarn",
                    "partial",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We see that our 2-group partitioning broadly outperform 3-group in our modality-specific scenario. This is most evident with the Qwen2-Audio model, where the 3-group method suffers a large performance drop on 5mins and 10mins audio, with an accuracy gap of 28.53% and 12.00% respectively. Overall, this result validates the effectiveness and reliability of our proposed 2-group scheme used in Partial YaRN.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "3group",
                    "2group",
                    "partitioning",
                    "model",
                    "accuracy",
                    "yarn",
                    "performance",
                    "partial",
                    "used",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work addressed the challenge of extending audio context windows of LALMs by studying the application of LLM&#8217;s whole-context extension methods, and proposing a training-free audio-only extension Partial YaRN and the VLAT training strategy. We demonstrate the effectiveness of extension-based methods over non-extension baselines across various long-audio settings. Leveraging our finding that SALMONN and Qwen2-Audio retain consistent performance up to 2mins audio window, we achieved an even stronger performance by extending from the 2mins window instead of 30s. Later, we showed that fine-tuning through VLAT helps the models to generalize to audio contexts far exceeding the length of the training data, offering a robust and data-efficient pathway to develop LALMs with better longform understanding. Future work could explore a similar modality-bound extension strategy for video models.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "our",
                    "salmonn",
                    "work",
                    "yarn",
                    "partial",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments are conducted exclusively on the multiple-choice question answering (MCQA) tasks. We made this choice to ensure a precise and unambiguous measurement of the model performance under different audio lengths. This mitigates the confounding variables inherent in evaluating open-ended generation, such as judging the semantic equivalence or stylistic alignment. However, a key tradeoff of this focused evaluation is its limited ability to assess the nuanced generative and language modeling capabilities of the base LLM. Consequently, while our results validate the model&#8217;s ability to retrieve information from audio, they do not fully test the importance of text preservation of the audio-only methods like Partial YaRN. Furthermore, our study does not assess performance across other long-audio task formats found in benchmarks such as <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib19\" title=\"\">2025</a>; Ahia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib1\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib16\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "yarn",
                    "partial",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike the original YaRN which has generally good predefined cutoff dimensions and a closed-form formula for determining the attention temperature, our proposed Partial YaRN requires tuning of these two hyperparameters, thereby introducing additional computational burden.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">YC would like to thank Ekapol Chuangsuwanich for insightful discussion on the earlier version of this work. He also thanks Thanapat Trachu for valuable guidance on the illustration of Partial YaRN.</p>\n\n",
                "matched_terms": [
                    "work",
                    "partial",
                    "version",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Video input is similar to the audio modality in that they&#8217;re both naturally continuous and can be of arbitrary length. Many Large Video-Language Models (LVLMs), such as Video-LLaMA <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib58\" title=\"\">2023b</a>)</cite> and Video-ChatGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Maaz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib34\" title=\"\">2024</a>)</cite>, adopt architectural paradigm similar to the unified input space LALMs we study, where video frames are embedded and projected into a base LLM&#8217;s input space. However, the context length problem is even more exacerbated in the video domain due to the high dimensionality and large number of tokens needed to represent even short videos. To improve efficiency, the LVLM community has largely focused on input compression techniques like query aggregation <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib58\" title=\"\">2023b</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib28\" title=\"\">2023a</a>; Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib47\" title=\"\">2024</a>)</cite>, frame pooling <cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib33\" title=\"\">2023</a>; Maaz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib34\" title=\"\">2024</a>)</cite>, and feature merging <cite class=\"ltx_cite ltx_citemacro_citep\">(Weng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib54\" title=\"\">2024</a>)</cite>. These strategies primarily aim to reduce the number of tokens before they enter the LLM. Our work offers a complementary perspective: instead of altering the input, we manipulate the positional space within the model to accommodate a longer sequence. The principles of Partial YaRN and Virtual Longform Audio Training could therefore be potentially applicable to the video domain as well, hopefully providing an orthogonal path to achieving long-context understanding in synergy with existing compression methods.</p>\n\n",
                "matched_terms": [
                    "model",
                    "work",
                    "yarn",
                    "partial",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The decision to simplify the original three-group frequency partition of YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> into a two-group system for Partial YaRN is a deliberate design choice aimed at ensuring positional consistency across the entire extended audio stream. The original YaRN formulation partitions RoPE dimensions into: 1) a low-frequency group that is purely interpolated, 2) a high-frequency group that is purely extrapolated, and 3) an \"in-between\" group that receives a mix of both, with a ramped interpolation factor.</p>\n\n",
                "matched_terms": [
                    "twogroup",
                    "threegroup",
                    "frequency",
                    "yarn",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While effective for whole-context extension, this \"in-between\" group becomes problematic in our partial, audio-only extension scenario. Consider a 2mins audio input, which requires a 4x extension of the model&#8217;s 30s original audio context.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By simplifying to a two-group partition, we circumvent this issue entirely. Every dimension is forced into a binary choice: either it is fully interpolated to cover the entire audio duration (low-frequency), or it is fully extrapolated to preserve local structure (high-frequency). This ensures that the model maintains a consistent positional understanding across all dimensions for the entirety of the audio input, avoiding the representational bias that partial coverage would introduce.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "model",
                    "twogroup"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S7.SS2\" title=\"7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a> for direct performance comparison between 2 and 3-group frequency partitioning versions of Partial YaRN.</p>\n\n",
                "matched_terms": [
                    "3group",
                    "partitioning",
                    "frequency",
                    "yarn",
                    "partial",
                    "comparison",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key implementation detail of our work is how we apply the positional stretching to only a subsection of the input sequence. While the original YaRN implementation modifies the RoPE frequency variables (<math alttext=\"\\boldsymbol{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><mi>&#120637;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{\\theta}</annotation></semantics></math>), this approach isn&#8217;t straightforward for a <span class=\"ltx_text ltx_font_italic\">partial</span> modification. Instead, our implementation of Partial YaRN directly manipulates the <span class=\"ltx_text ltx_font_typewriter\">position_ids</span> and multiply it with the unmodified frequencies. This section details the implementation our method.</p>\n\n",
                "matched_terms": [
                    "work",
                    "frequency",
                    "yarn",
                    "partial",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In most Transformer architectures, including the models used in this work, the RoPE mechanism computes embeddings based on an input tensor called <span class=\"ltx_text ltx_font_typewriter\">position_ids</span>. This tensor is simply a sequence of integers representing the absolute position of each token (i.e. <span class=\"ltx_text ltx_font_typewriter\">[0, 1, 2, ..., N-1]</span>). Our method leverages this by modifying a section this tensor. The process is as follows:</p>\n\n",
                "matched_terms": [
                    "work",
                    "used",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This implementation introduces negligible computational overhead in practice. The only additional computations are incurred during the construction of the <span class=\"ltx_text ltx_font_typewriter\">position_ids</span> tensor, which involves a few tensor slicing operations, a single <span class=\"ltx_text ltx_font_typewriter\">torch.linspace</span> call, and a final concatenation. These are highly optimized, vectorized operations that constitute a minuscule fraction of the total computation in a full forward pass. Crucially, this precomputation is a <span class=\"ltx_text ltx_ulem_uline\">one-time cost</span> per input sample, applied only during the initial processing of the prompt that contains the long audio. During the subsequent autoregressive generation steps, where tokens are generated one at a time, our method adds no overhead whatsoever. The model reverts to the standard process of incremental positional stepping, just as it would without any context extension. Therefore, the cost of enabling partial context extension is a small, fixed precomputation that does not impact the per-token generation latency.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recall that Partial YaRN has two hyperparameters, cutoff dimension and attention temperature. For the training-free experiment (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S5.SS1\" title=\"5.1 Training-free Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), we jointly sweep the cutoff dimension from <math alttext=\"\\{56,48,40,32,24,16,8\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>56</mn><mo>,</mo><mn>48</mn><mo>,</mo><mn>40</mn><mo>,</mo><mn>32</mn><mo>,</mo><mn>24</mn><mo>,</mo><mn>16</mn><mo>,</mo><mn>8</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{56,48,40,32,24,16,8\\}</annotation></semantics></math>, and the temperature from between 0.5 and 1.6 with a step size of 0.1. See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.SS2\" title=\"E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a> for tuning results and sensitivity.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The effectiveness of Virtual Longform Audio Training (VLAT) depends on the strategy used to sample the virtual length during training. For generality, here we define the virtual length as a produce of true audio context length (i.e. 30s) and the <span class=\"ltx_text ltx_font_italic\">virtual factor</span>. In our main experiment Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S6\" title=\"6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we used a strategy of randomly sampling a factor from the range [1x, 5x, 10x, 15x, 20x, 25x]. To understand the sensitivity of our results to this choice, we compare our default strategy against four alternatives, training a Qwen2-Audio model on each strategy and evaluating on the 10mins audio task.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "compare",
                    "against",
                    "model",
                    "used",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The most surprising result comes from the <span class=\"ltx_text ltx_font_italic\">Limited Range</span> strategy, which achieves the highest performance across all evaluation lengths, despite its virtual lengths not extending beyond the 5mins mark (10.0x factor). While this demonstrates a powerful capacity for extrapolation, the mechanism behind this superior performance is not immediately clear. <span class=\"ltx_text ltx_ulem_uline\">It may suggest that randomization on extremely long virtual lengths could introduce instability to the training</span>, which could in turn explain why Whole YaRN diverged quickly under VLAT framework; or that the specific distribution of factors in this limited set is coincidentally optimal for our test data. We left the more in-depth study of this behavior as future work.</p>\n\n",
                "matched_terms": [
                    "work",
                    "yarn",
                    "performance",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here we analyze the effect and sensitivity of different Partial YaRN&#8217;s hyperparameter configurations. We report validation accuracies w.r.t. different cutoff dimension indices and attention temperatures, for the Qwen2-Audio and SALMONN models.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "partial",
                    "salmonn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.F3\" title=\"Figure 3 &#8227; Plausible explanation of this trend. &#8227; E.2.2 Sensitivity on SALMONN &#8227; E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> displays the hyperparameter sensitivity of Partial YaRN on Qwen2-Audio when extending from the original 30s audio context window. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.F4\" title=\"Figure 4 &#8227; Plausible explanation of this trend. &#8227; E.2.2 Sensitivity on SALMONN &#8227; E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the sensitivity when extending from the observed audio context window of 2mins.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This trend informs the following recommendation when using Partial YaRN. For moderate context extensions (e.g., 2x to 4x), it is advisable to start with a higher cutoff dimension index and a temperature value close to 1.0. For longer extensions, a lower cutoff dimension index with a considerably higher temperature is likely to yield better results.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.F5\" title=\"Figure 5 &#8227; Plausible explanation of this trend. &#8227; E.2.2 Sensitivity on SALMONN &#8227; E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> displays the hyperparameter sensitivity of Partial YaRN on SALMONN when extending from the original 30s audio context window.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "yarn",
                    "salmonn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The vast distinction between SALMONN and the previously discussed Qwen2-Audio lies in the their audio representation lengths&#8212;SALMONN only uses 88 tokens for every 30s audio vs. Qwen2-Audio&#8217;s 750 tokens. This, coupled with the model&#8217;s preference of low attention temperature, suggests that letting the model focuses on a relevant local region outweighs the need to spreading the attention distribution as in Qwen2-Audio.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "model",
                    "salmonn"
                ]
            }
        ]
    },
    "A5.T6": {
        "caption": "Table 6: VLAT performance with different sampling strategies. All models are trained on 2mins audio and evaluated with Partial PI inference. The results are averaged across three random seeds.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">YODAS2-MCQA</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Virtual Length Strategy</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2 mins</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">5 mins</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">10 mins</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Default</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_ulem_uline\">96.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_ulem_uline\">91.91</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_ulem_uline\">81.73</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">100 Sampling Points</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">95.91</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">89.25</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">79.78</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1000 Sampling Points</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">96.18</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">87.69</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">79.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Limited Range</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">98.00</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">93.55</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">89.20</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Fixed to 20.0x</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">89.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">68.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">50.31</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "evaluated",
            "strategy",
            "length",
            "inference",
            "seeds",
            "partial",
            "sampling",
            "strategies",
            "audio",
            "default",
            "range",
            "trained",
            "performance",
            "200x",
            "across",
            "averaged",
            "vlat",
            "results",
            "mins",
            "2mins",
            "random",
            "points",
            "fixed",
            "virtual",
            "three",
            "all",
            "limited",
            "different",
            "yodas2mcqa"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We report the result in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.T6\" title=\"Table 6 &#8227; E.1 Different VLAT&#8217;s Virtual Length Sampling Strategies &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large Audio-Language Models (LALMs) are often constrained by short audio context windows, even when their text backbones support long contexts, limiting long-form audio understanding. Prior work has introduced context-extension methods (e.g. YaRN) on unimodal LLMs, yet their application to LALMs remains unexplored. First, building on RoPE-based context extension, we introduce <span class=\"ltx_text ltx_font_italic\">Partial YaRN</span>, a training-free, audio-only extension method that modifies only audio token positions, leaving text positions intact to preserve the base LLM&#8217;s text capabilities. Second, we propose <span class=\"ltx_text ltx_font_italic\">Virtual Longform Audio Training</span> (VLAT), a training strategy that extends Partial YaRN into a training-time positional augmentation. VLAT simulates diverse audio lengths during training, enabling generalization to inputs far longer than those seen in training and improving robustness for long-context audio understanding. Our experiments on SALMONN and Qwen2-Audio show that Partial YaRN outperforms the original models across wide range of settings, and VLAT training strategy provides substantial improvement, achieving strong performance on long audio of unseen lengths.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code is available at: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/scb-10x/partial-yarn\" title=\"\">https://github.com/scb-10x/partial-yarn</a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "virtual",
                    "audio",
                    "strategy",
                    "vlat",
                    "partial",
                    "range",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">Extending Audio Context for Long-Form Understanding \n<br class=\"ltx_break\"/>in Large Audio-Language Models</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of Large Language Models (LLMs) has spurred multimodal extensions, notably large audio-language models (LALMs)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Also known by other names such as Speech-aware LLMs.</span></span></span> that pair an audio encoder with a text backbone. By aligning audio and text in a shared representation, LALMs can leverage the base LLM&#8217;s knowledge for complex audio understanding. However, practical use is constrained by short audio context windows: models such as SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib49\" title=\"\">2024</a>)</cite> and Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib5\" title=\"\">2024</a>)</cite> are typically trained on audio segments of 30s or less, and thus generalize poorly to longer inputs <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib19\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This potential drawback motivates a more targeted strategy. We introduce &#8220;<span class=\"ltx_text ltx_font_italic\">Partial YaRN</span>&#8221;, an audio-only context extension method that modifies only the audio tokens&#8217; positional encodings. This design enables a direct comparative study against the whole-context approaches, allowing us to investigate the tradeoff between preserving position encodings of the text modality and maintaining a globally uniform positional space.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "strategy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments on both training-free and fine-tuned audio context extensions reveal that extension-based methods outperforms original models. However, the best choice between whole-context and audio-only extensions is often model-dependent without a universally superior solution. This suggests that to build truly robust long-audio-context models, we must address the core problem of generalization during the training process. Therefore, we extend Partial YaRN into a novel fine-tuning technique called &#8220;<span class=\"ltx_text ltx_font_italic\">Virtual Longform Audio Training</span>&#8221; (VLAT). Acting as a <span class=\"ltx_text ltx_font_italic\">positional augmentation</span> technique, it simulates a wide range of audio lengths during fine-tuning, teaching the model to generalize beyond the lengths present in the training dataset. Through VLAT, we obtain excellent results on longform audio of unseen lengths. Our main contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "vlat",
                    "results",
                    "partial",
                    "range"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose Virtual Longform Audio Training (VLAT), a training strategy that applies Partial YaRN as positional augmentation to simulate diverse audio lengths during training, improving long-context generalization.</p>\n\n",
                "matched_terms": [
                    "virtual",
                    "audio",
                    "strategy",
                    "vlat",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As Transformer <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib52\" title=\"\">2017</a>)</cite> utilizes attention mechanism <cite class=\"ltx_cite ltx_citemacro_citep\">(Bahdanau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib2\" title=\"\">2014</a>)</cite> which is position-invariant, explicit positional information has to be provided for the models to differentiate tokens from different positions. Our study focuses on LALMs that use Rotary Positional Encoding (RoPE) <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib48\" title=\"\">2024</a>)</cite>, a widely adopted positional encoding.</p>\n\n",
                "matched_terms": [
                    "models",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib4\" title=\"\">2023</a>)</cite> proposes that instead of extrapolating RoPE further to the region unfamiliar for the trained model, interpolation is done inside the pretrained context window. Specifically, let <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> be the original context length of the model, and <math alttext=\"L^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msup><mi>L</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">L^{\\prime}</annotation></semantics></math> be the target length to be extended to. Define <math alttext=\"s=\\frac{L^{\\prime}}{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo>=</mo><mfrac><msup><mi>L</mi><mo>&#8242;</mo></msup><mi>L</mi></mfrac></mrow><annotation encoding=\"application/x-tex\">s=\\frac{L^{\\prime}}{L}</annotation></semantics></math> as the extension factor. PI simply downscales the base rotational frequency by the extension factor: <math alttext=\"\\frac{\\theta_{i}}{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mfrac><msub><mi>&#952;</mi><mi>i</mi></msub><mi>s</mi></mfrac><annotation encoding=\"application/x-tex\">\\frac{\\theta_{i}}{s}</annotation></semantics></math>. For example, if originally each positional step is rotated by <math alttext=\"\\theta_{i}=10^{\\circ}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#952;</mi><mi>i</mi></msub><mo>=</mo><msup><mn>10</mn><mo>&#8728;</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\theta_{i}=10^{\\circ}</annotation></semantics></math>; to double the context length, the frequency (step size) can simply be halved to <math alttext=\"5^{\\circ}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msup><mn>5</mn><mo>&#8728;</mo></msup><annotation encoding=\"application/x-tex\">5^{\\circ}</annotation></semantics></math>. This way, the model will never have to attend to positions outside of its trained window; resulting in a more stable approach.</p>\n\n",
                "matched_terms": [
                    "length",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One broadly adopted paradigm of large audio-language models (LALMs) is the <span class=\"ltx_text ltx_font_italic\">unified input space</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib49\" title=\"\">2024</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib5\" title=\"\">2024</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib12\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib16\" title=\"\">2025</a>)</cite> which involves transforming audio inputs into the textual input space of a base LLM. By sharing the embedding space, the strong text capability and learned knowledge of the base LLM can be leveraged while simultaneously being augmented with audio understanding ability. There also exists other LALM paradigms such as <span class=\"ltx_text ltx_font_italic\">cross attention</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib25\" title=\"\">2024</a>; Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib14\" title=\"\">2025</a>)</cite> where the audio and text modalities are fused through cross-attention modules. This work focuses on models under the unified-input-space.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As with text context window in LLMs, LALMs also do not generalize well to audio with lengths much longer than the window seen during training (e.g. 7s for Pengi <cite class=\"ltx_cite ltx_citemacro_citep\">(Deshmukh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib10\" title=\"\">2023</a>)</cite>; 30s for Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib5\" title=\"\">2024</a>)</cite>). Concurrent to us, <cite class=\"ltx_cite ltx_citemacro_citet\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib19\" title=\"\">2025</a>)</cite> introduces FastLongSpeech, an efficient longform LALM that uses Iterative Fusion to compress long audio representation into compact forms. Our work, in contrast, investigates an orthogonal direction of adapting the model&#8217;s positional encodings rather than the audio embeddings, and therefore is generally applicable to the existing models without requiring costly retraining.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To extend existing <span class=\"ltx_text ltx_font_italic\">unified input</span> LALMs to longer audio, we hypothesize that <em class=\"ltx_emph ltx_font_bold ltx_font_italic\">LALMs already possess a sufficient general understanding of audio and text, but the bottleneck is their unfamiliarity to audio positions beyond the range seen during audio-text training.</em><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>This differs from whole-context extension in unimodal LLMs, where the total sequence length exceeds the pre-trained context window, causing an out-of-distribution problem. Here, the total sequence (audio + text) usually still fits within the original window, making the core challenge one of adapting to an unfamiliar audio length and positions, rather than extrapolating to completely unseen positions.</span></span></span> This hypothesis suggests a training-free solution: manipulating the backbone LLM&#8217;s positional encoding. The goal is to remap positions of a long audio input into the model&#8217;s familiar audio range.</p>\n\n",
                "matched_terms": [
                    "length",
                    "audio",
                    "range"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we study the application of YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> to LALMs, as an example of whole-context extension methods. Since the audio window is a part of the total context window, performing whole-context extension extends the audio window as a byproduct. However, this alters positional information for all tokens, including text, risking the degradation of the base LLM, which was pretrained solely in text. Motivated by the aforementioned hypothesis, we propose <span class=\"ltx_text ltx_font_bold\">Partial YaRN</span>, an audio-only extension method designed to exclusively stretch the audio window. By leaving the text&#8217;s positional encodings unaltered, this approach aims to extend<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>The terms <span class=\"ltx_text ltx_font_italic\">extend</span> and <span class=\"ltx_text ltx_font_italic\">stretch</span> are used interchangeably.</span></span></span> audio context while preserving the base model&#8217;s text capability.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "audio",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by PI <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib4\" title=\"\">2023</a>)</cite> and YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> for whole-context extension in unimodal LLMs, we adapt the interpolation technique to LALMs by applying it exclusively to the audio region of the base language models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Concretely, assuming a single audio input, let <math alttext=\"L_{\\text{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>audio</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{audio}}</annotation></semantics></math> be the original audio context length, <math alttext=\"L_{\\text{audio}}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>L</mi><mtext>audio</mtext><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">L_{\\text{audio}}^{\\prime}</annotation></semantics></math> be the target length, and <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> be the position of the first audio token. We define the positional range <math alttext=\"[p,p+L_{\\text{audio}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>p</mi><mo>,</mo><mrow><mi>p</mi><mo>+</mo><msub><mi>L</mi><mtext>audio</mtext></msub></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">[p,p+L_{\\text{audio}})</annotation></semantics></math> as the <span class=\"ltx_text ltx_font_italic\">original audio context window</span>. It can be either a predefined location for audio input, or a dynamic region enclosed by special tokens (e.g. <span class=\"ltx_text ltx_font_typewriter\">&lt;speech&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;/speech&gt;</span>). We then apply our modified YaRN technique to exclusively stretch this region to the new length <math alttext=\"L_{\\text{audio}}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>L</mi><mtext>audio</mtext><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">L_{\\text{audio}}^{\\prime}</annotation></semantics></math>. This creates a partially stretched positional encoding:</p>\n\n",
                "matched_terms": [
                    "length",
                    "audio",
                    "range"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1) Consistency</span>: A two-group partition ensures a consistent and uniform positional encoding across the entire audio stream. YaRN&#8217;s original &#8220;in-between&#8221; group receives a mix of interpolation and extrapolation, which would cause these RoPE frequencies to not be extended to fit the entire audio. Our approach avoids this potential distortion.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1) Cutoff Dimension Index</span>: This defines the boundary separating the RoPE dimensions into two groups. Dimensions equal or below this cutoff (low-frequency) are interpolated to stably cover longer audio context. Dimensions above it (high-frequency) are extrapolated to preserve local positional distances and high-frequency information. We default this to <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><mn>0</mn></math> (interpolate every dimension).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "default"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2) Attention Temperature</span>: This controls the sharpness of attention distribution within the audio context window. A higher temperature softens the distribution, preventing attention scores from collapsing to a few tokens over long sequences. We default this to <math alttext=\"1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mn>1.0</mn><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math> (no temperature scaling).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "default"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notice that under the default hyperparameter values, the audio-only extension reduces from YaRN to PI, we therefore specifically name Partial YaRN with default hyperparameters: <span class=\"ltx_text ltx_font_italic\">Partial PI</span>.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "default"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the limited availability of length-tailored long audio datasets, we construct custom datasets with specific audio lengths.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "limited"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We source audio from the English subset of YODAS2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib30\" title=\"\">2023b</a>)</cite>. We segment audio samples into non-overlapping segments of 1, 2, 5, and 10 minutes. For each segment, we generate five multiple-choice question-answering (MCQA) pairs using Gemini 2.0 Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib50\" title=\"\">2023</a>)</cite>, with four choices each. We design the generation prompt to ensure that each question focuses on different portions of the audio, and that they collectively cover the entire audio segment. The test set of each audio duration (1, 2, 5, and 10 minutes) has 750 QA pairs.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We select two widely-used open-weights LALMs: SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib49\" title=\"\">2024</a>)</cite> and Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib5\" title=\"\">2024</a>)</cite>. Both models utilize Whisper-encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib42\" title=\"\">2023</a>)</cite> as audio encoder, which is limited to processing audio of up to 30s. We handle longer audio inputs by segmenting them into non-overlapping 30s chunks, where each chunk is encoded independently.</p>\n\n",
                "matched_terms": [
                    "models",
                    "limited",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The number of audio representations differs considerably between these models. For each 30s chunk on 50Hz Whisper-encoder, SALMONN generates a sequence of 88 audio tokens (with Q-Former), whereas Qwen2-Audio produces a sequence of 750 tokens (with a 2x downsampling).</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate Partial PI (default hyperparameters) and Partial YaRN (tuned hyperparameters) against the following methods:</p>\n\n",
                "matched_terms": [
                    "partial",
                    "default"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1) Vanilla</span>: Unmodified base models. Long audio inputs are passed directly without any manipulation to their RoPE.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also report performance of two proprietary models capable of long audio context: GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib37\" title=\"\">2024</a>)</cite> and Gemini 2.0 Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib50\" title=\"\">2023</a>)</cite>, in order to provide a broader perspective of LALMs and to validate the quality of our dataset<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>The model versions are <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini-2024-07-18</span> and <span class=\"ltx_text ltx_font_typewriter\">gemini-2.0-flash-001</span> respectively. Note that Gemini was also used for generating our YODAS2-MCQA dataset.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "yodas2mcqa",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we extend the audio context length without training. We report the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S4.SS1.SSS0.Px1\" title=\"YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, showing that: no single method&#8212;whole-context or audio-only&#8212;is universally superior. Instead, the performance varies between the different models and the degree of extension. More importantly, we observe that <span class=\"ltx_text ltx_font_italic\">both models generally retain consistent performance up to 2mins</span>, suggesting that their innate audio context lengths are longer than just 30s, possibly a result from multi-audio training.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "length",
                    "results",
                    "different",
                    "2mins",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When stretching from the 30s window, extension methods provide substantial performance gains compared to the Vanilla baseline. We observe that Partial YaRN shows the strongest performance on the 1min and 10mins settings, achieving 57.35% and 32.93% accuracies. On 2mins, both Whole and Partial YaRN perform similarly. However, Whole YaRN outperforms Partial YaRN at 5mins by a large margin. Possible reasons for this performance flip are (1) higher expressivity and less compression pressure from Whole YaRN&#8217;s additional &#8220;in-between&#8221; dimension group, outweighs the preservation of the base language capability in this 5mins setting, or (2) noise in the hyperparameter tuning process of Partial YaRN, which is an inherent drawback.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "2mins",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to SALMONN, Qwen2-Audio demonstrates different characteristics. The Vanilla baseline is more robust, outperforming all stretch-from-30s methods at 2mins. Both Whole and Partial YaRN struggle to provide significant gains and even underperform the baseline at moderate lengths (2 and 5mins) when stretching from the original 30s context; however, they manage to attain considerable gains on the 10mins setting. This indicates that Qwen2-Audio likely possesses a strong audio-length generalization capability or longer innate audio context window. For this model, simply applying an extension method from the base context does not guarantee an improvement. This suggests that the model&#8217;s high intrinsic audio-length generalizability is superior to the extension methods that inevitably cram the positional information as a side effect.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "all",
                    "partial",
                    "different",
                    "2mins"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As previously mentioned in the beginning of this section that we can observe both model&#8217;s innate audio contexts to be around 2mins instead of just 30s, here we experiment with extending from this <span class=\"ltx_text ltx_font_italic\">observed audio context of 2mins</span> to the target lengths. For both SALMONN and Qwen2-Audio, performance on the long audio improves substantially when the interpolation is anchored from a 2mins context instead of the original 30s. Notably on Qwen2-Audio, this strategy boosts Partial YaRN&#8217;s performance at 10mins from 28.53% to a 48.00%, an absolute improvement of almost 20%, yielding a final performance 26% higher than the Vanilla baseline. This finding suggests that determining the extension by observing the vanilla performance may be a more critical factor than the choice between whole-context or audio-only methods, especially on large extension ratios.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "strategy",
                    "partial",
                    "2mins",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, we study training-based extension methods. Keeping other components frozen, we fine-tune the base language model of Qwen2-Audio using a <span class=\"ltx_text ltx_font_italic\">single epoch LoRA</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib20\" title=\"\">2022</a>)</cite> of rank 8 on the YODAS2-MCQA training set of the corresponding length. Two LoRA settings are explored: adapting only the query projection (q), and adapting query, key, value, and output projections (qkvo). Due to the high cost of tuning Partial YaRN&#8217;s hyperparameters during training, we only use the default configuration (Partial PI) for the audio-only method. Whole PI is omitted due to its poor performance as previously observed.</p>\n\n",
                "matched_terms": [
                    "length",
                    "default",
                    "partial",
                    "yodas2mcqa",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, showing that fine-tuning with a context extension method dramatically outperforms the vanilla fine-tuning baseline, especially at longer audio lengths. In the qkvo setting, both Whole YaRN (83.47%) and Partial PI (83.07%) achieve an absolute performance gain of around 19% over the Vanilla baseline (64.93%) at 10mins. This highlights the benefit of integrating a context extension method into the fine-tuning process for more better context extension.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "audio",
                    "results",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comparing the whole-context and audio-only approaches, we find that their performance is generally competitive. While Whole YaRN triumps when only adapting the query projection weights, suggesting it&#8217;s ease of adaptation. Their performance converges under standard LoRA practice which adapts qkvo. This result confirms that both whole-context and audio-only methods are highly effective strategies for fine-tuning.</p>\n\n",
                "matched_terms": [
                    "strategies",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our previous results show that while the training-free methods are beneficial, their effectiveness is often model and audio-length dependent. Based on this, we propose to fine-tune LALMs with Partial YaRN repurposed as a <span class=\"ltx_text ltx_font_italic\">positional augmentation</span> technique we call <span class=\"ltx_text ltx_font_italic\">\"Virtual Longform Audio Training\"</span> (VLAT).</p>\n\n",
                "matched_terms": [
                    "virtual",
                    "audio",
                    "vlat",
                    "results",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of VLAT is to simulate and expose models to audio context windows of diverse lengths during training. Let <math alttext=\"L_{\\text{data}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>data</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{data}}</annotation></semantics></math> be the actual length of a training audio sample. For each sample, we obtain a &#8220;virtual&#8221; source length, <math alttext=\"L_{\\text{virt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>virt</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{virt}}</annotation></semantics></math>, as the model&#8217;s default audio context length <span class=\"ltx_text ltx_font_italic\">times</span> a factor randomly sampled from the range [1, 5, 10, 15, 20, 25]x. We then apply Partial YaRN<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Whole YaRN is not used here, as we observe that it diverges quickly when training under the VLAT framework.</span></span></span> to stretch or compress the <math alttext=\"L_{\\text{virt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>virt</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{virt}}</annotation></semantics></math>-long positional window to <math alttext=\"L_{\\text{data}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>data</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{data}}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "vlat",
                    "length",
                    "default",
                    "partial",
                    "range"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For instance, consider an audio sample with <math alttext=\"L_{\\text{data}}=2\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mtext>data</mtext></msub><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">L_{\\text{data}}=2</annotation></semantics></math>mins, and we draw a <math alttext=\"L_{\\text{virt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>virt</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{virt}}</annotation></semantics></math> of 10mins, Partial YaRN will compress a 10mins context window to 2mins for this audio. This process effectively simulates the 2mins audio to be 10mins long virtually, thereby familiarizing the model to longer audio context and improving its ability to generalize to genuinely long audio at inference time.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "audio",
                    "inference",
                    "2mins"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Randomizing positional encoding has previously been explored in <cite class=\"ltx_cite ltx_citemacro_citet\">Ruoss et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib44\" title=\"\">2023</a>)</cite>, where positional indices are randomly downsampled to let the model see larger positional values. Rather than using <math alttext=\"\\{1,2,\\dots,N\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{1,2,\\dots,N\\}</annotation></semantics></math> for an length-<math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> input, they use a randomly subsampled values such as <math alttext=\"\\{1,4,\\dots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>4</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{1,4,\\dots,L\\}</annotation></semantics></math> with <math alttext=\"L&gt;N\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m4\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>&gt;</mo><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">L&gt;N</annotation></semantics></math> instead. In contrast, VLAT differs in employing interpolation inside the context window to create a dense, <span class=\"ltx_text ltx_font_italic\">continuous</span> space of positions, unlike sparse integer subsampling. It is also bidirectional, teaching the model through both compressed and stretched contexts. Furthermore, VLAT is a targeted, modality-bound method that modifies only the audio tokens, preserving the base LLM&#8217;s original textual space.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "vlat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune Qwen2-Audio on a 2mins YODAS2-MCQA training set<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>To prevent data leakage, the train/test splits were performed at the video level, ensuring no source material overlaps between the training set and any of the test sets.</span></span></span> using two methods: a standard Vanilla fine-tuning and VLAT. We employ 10-epoch qkvo LoRA for the adaptations. We then evaluate the models from both training methods under two different extensions at inference: Vanilla and Partial PI. See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.SS1\" title=\"E.1 Different VLAT&#8217;s Virtual Length Sampling Strategies &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">E.1</span></a> for additional detail on VLAT configuration.</p>\n\n",
                "matched_terms": [
                    "models",
                    "vlat",
                    "inference",
                    "partial",
                    "different",
                    "2mins",
                    "yodas2mcqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S4.T2\" title=\"Table 2 &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, VLAT greatly improves generalization to unseen audio lengths. When evaluated with vanilla inference, the model trained with Virtual Longform shows a dramatic improvement on 10mins audio, increasing accuracy from 32.76% to 75.11%, closing in on the previous 1-epoch direct-fine-tuning result. This highlights the effectiveness of the training technique for training LALMs to generalize far beyond the audio lengths present in their training data, mitigating a key bottleneck in the development of robust long-audio models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "evaluated",
                    "virtual",
                    "audio",
                    "vlat",
                    "inference",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, as can be seen in \"Partial PI Inference\" columns, VLAT&#8217;s benefit is complementary to inference-time extension. When Partial PI is applied during evaluation, the VLAT-trained model again outperforms its vanilla-trained counterpart, achieving the highest overall 10mins performance of 81.73%. This indicates that VLAT and inference-time extension are compatible strategies, and their combination yields the most robust long-context performance.</p>\n\n",
                "matched_terms": [
                    "strategies",
                    "vlat",
                    "inference",
                    "partial",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we conduct an ablation study to isolate and analyze the individual contributions of the two key components in Whole YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> and Partial YaRN: (1) frequency grouping and (2) attention temperature scaling. We note that by removing both of these components, the methods converge to Whole PI <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib4\" title=\"\">2023</a>)</cite> and Partial PI. The results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S7.SS1\" title=\"7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">7.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can see that removing either the frequency grouping or the attention temperature generally leads to a large drop in performance, especially on higher extension ratios. Both the frequency partitioning and attention temperature scaling are crucial for robustly extending the audio context length of LALMs. Their combined application within the Whole and Partial YaRN frameworks generally yield the most effective and stable performance.</p>\n\n",
                "matched_terms": [
                    "length",
                    "partial",
                    "audio",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the 3-group baseline, we implement Partial YaRN using YaRN&#8217;s original partitioning scheme and its recommended hyperparameters. We compare this against our proposed 2-group Partial YaRN using the hyperparameters from our main experiments (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S5.SS1\" title=\"5.1 Training-free Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>). We present the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S7.T5\" title=\"Table 5 &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We see that our 2-group partitioning broadly outperform 3-group in our modality-specific scenario. This is most evident with the Qwen2-Audio model, where the 3-group method suffers a large performance drop on 5mins and 10mins audio, with an accuracy gap of 28.53% and 12.00% respectively. Overall, this result validates the effectiveness and reliability of our proposed 2-group scheme used in Partial YaRN.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "audio",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work addressed the challenge of extending audio context windows of LALMs by studying the application of LLM&#8217;s whole-context extension methods, and proposing a training-free audio-only extension Partial YaRN and the VLAT training strategy. We demonstrate the effectiveness of extension-based methods over non-extension baselines across various long-audio settings. Leveraging our finding that SALMONN and Qwen2-Audio retain consistent performance up to 2mins audio window, we achieved an even stronger performance by extending from the 2mins window instead of 30s. Later, we showed that fine-tuning through VLAT helps the models to generalize to audio contexts far exceeding the length of the training data, offering a robust and data-efficient pathway to develop LALMs with better longform understanding. Future work could explore a similar modality-bound extension strategy for video models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "audio",
                    "strategy",
                    "vlat",
                    "length",
                    "partial",
                    "2mins",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments are conducted exclusively on the multiple-choice question answering (MCQA) tasks. We made this choice to ensure a precise and unambiguous measurement of the model performance under different audio lengths. This mitigates the confounding variables inherent in evaluating open-ended generation, such as judging the semantic equivalence or stylistic alignment. However, a key tradeoff of this focused evaluation is its limited ability to assess the nuanced generative and language modeling capabilities of the base LLM. Consequently, while our results validate the model&#8217;s ability to retrieve information from audio, they do not fully test the importance of text preservation of the audio-only methods like Partial YaRN. Furthermore, our study does not assess performance across other long-audio task formats found in benchmarks such as <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib19\" title=\"\">2025</a>; Ahia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib1\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib16\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "audio",
                    "limited",
                    "results",
                    "partial",
                    "different",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conceived along with the introduction of transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib52\" title=\"\">2017</a>)</cite>, the original <span class=\"ltx_text ltx_font_italic\">absolute positional encoding</span> utilizes sinusoidal waves of varying frequencies to uniquely represent absolute position of the tokens, and is incorporated into the model by adding directly into the input sequence. Later works <cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib11\" title=\"\">2019</a>; Lan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib26\" title=\"\">2020</a>; Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib7\" title=\"\">2020</a>)</cite> propose to use trainable vectors for representing absolute positional information, where token at each position is summed with a learned vector, allowing the models to learn the optimal positional encoding for themselves. However, it cannot extrapolate to sequences longer than the maximum length it was trained on.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Shaw et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib46\" title=\"\">2018</a>)</cite> introduces to encode the relative position of the token instead, where the relative distance between the query and key is captured and injected into every attention layer. <cite class=\"ltx_cite ltx_citemacro_citet\">Dai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib8\" title=\"\">2019</a>)</cite> modifies the decomposed form of the attention equation, and introduces additional trainable location-agnostic key vectors to distinguish between content-based and location-based querying behaviors. <cite class=\"ltx_cite ltx_citemacro_citet\">Raffel et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib43\" title=\"\">2020</a>)</cite> groups the relative distances into buckets, and associate each bucket with a learnable scalar for adding into the attention logits. Simplifying the encoding scheme and reducing trainable parameters. TUPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Ke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib24\" title=\"\">2021</a>)</cite> uses separate linear projections for the word and positional information. ALiBi <cite class=\"ltx_cite ltx_citemacro_citep\">(Press et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib41\" title=\"\">2022</a>)</cite> simply adds linear biases with a learnable slope into the attention scores, the bias grows linearly with the relative distance between the tokens. This method generally imposes a strong locality preference by encouraging models to attend more to the nearby tokens, while discouraging them from attending to tokens that are farther away. Several later works extend RoPE for multimodal modeling (e.g. Multimodal RoPE <cite class=\"ltx_cite ltx_citemacro_cite\">Bai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib3\" title=\"\">2025</a>)</cite>, VideoRoPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib53\" title=\"\">2025</a>)</cite>, VRoPE <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib31\" title=\"\">2025</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "length",
                    "models",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Video input is similar to the audio modality in that they&#8217;re both naturally continuous and can be of arbitrary length. Many Large Video-Language Models (LVLMs), such as Video-LLaMA <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib58\" title=\"\">2023b</a>)</cite> and Video-ChatGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Maaz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib34\" title=\"\">2024</a>)</cite>, adopt architectural paradigm similar to the unified input space LALMs we study, where video frames are embedded and projected into a base LLM&#8217;s input space. However, the context length problem is even more exacerbated in the video domain due to the high dimensionality and large number of tokens needed to represent even short videos. To improve efficiency, the LVLM community has largely focused on input compression techniques like query aggregation <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib58\" title=\"\">2023b</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib28\" title=\"\">2023a</a>; Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib47\" title=\"\">2024</a>)</cite>, frame pooling <cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib33\" title=\"\">2023</a>; Maaz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib34\" title=\"\">2024</a>)</cite>, and feature merging <cite class=\"ltx_cite ltx_citemacro_citep\">(Weng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib54\" title=\"\">2024</a>)</cite>. These strategies primarily aim to reduce the number of tokens before they enter the LLM. Our work offers a complementary perspective: instead of altering the input, we manipulate the positional space within the model to accommodate a longer sequence. The principles of Partial YaRN and Virtual Longform Audio Training could therefore be potentially applicable to the video domain as well, hopefully providing an orthogonal path to achieving long-context understanding in synergy with existing compression methods.</p>\n\n",
                "matched_terms": [
                    "models",
                    "virtual",
                    "audio",
                    "strategies",
                    "length",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The decision to simplify the original three-group frequency partition of YaRN <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib40\" title=\"\">2024</a>)</cite> into a two-group system for Partial YaRN is a deliberate design choice aimed at ensuring positional consistency across the entire extended audio stream. The original YaRN formulation partitions RoPE dimensions into: 1) a low-frequency group that is purely interpolated, 2) a high-frequency group that is purely extrapolated, and 3) an \"in-between\" group that receives a mix of both, with a ramped interpolation factor.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "across",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While effective for whole-context extension, this \"in-between\" group becomes problematic in our partial, audio-only extension scenario. Consider a 2mins audio input, which requires a 4x extension of the model&#8217;s 30s original audio context.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "audio",
                    "2mins"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The low-frequency dimensions, responsible for quasi-absolute positioning, are fully interpolated by a 4x factor. This is crucial as it maps the entire 2mins audio into the model&#8217;s familiar positional range (original audio context).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "range",
                    "2mins"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The \"in-between\" dimensions, however, would receive interpolation factors between the 1x and 4x (e.g. 2x). This means that for these specific dimensions, the model&#8217;s original context window is stretched to cover only the first minute of the 2mins audio. The second minute would lie in an extrapolated, out-of-distribution region.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "2mins"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By simplifying to a two-group partition, we circumvent this issue entirely. Every dimension is forced into a binary choice: either it is fully interpolated to cover the entire audio duration (low-frequency), or it is fully extrapolated to preserve local structure (high-frequency). This ensures that the model maintains a consistent positional understanding across all dimensions for the entirety of the audio input, avoiding the representational bias that partial coverage would introduce.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "across",
                    "audio",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S7.SS2\" title=\"7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a> for direct performance comparison between 2 and 3-group frequency partitioning versions of Partial YaRN.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identify the Audio Region:</span> First, we identify the start and end indices of the audio tokens within the <span class=\"ltx_text ltx_font_typewriter\">position_ids</span> tensor. This creates three distinct segments: the leading text positions (unaltered), the audio positions (to be stretched), and the trailing text positions (unaltered relative to each other).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "three"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stretch the Audio Positions:</span> We replace the original sequence of integer positions for the audio segment with a new sequence generated via <span class=\"ltx_text ltx_font_typewriter\">torch.linspace</span>. For example, if the original audio positions were <span class=\"ltx_text ltx_font_typewriter\">[p, p+1, ..., p+base_L_audio-1]</span> and the new audio input has <span class=\"ltx_text ltx_font_typewriter\">L_audio</span> tokens, we generate <span class=\"ltx_text ltx_font_typewriter\">L_audio</span> new positions by interpolating within the original range: <span class=\"ltx_text ltx_font_typewriter\">torch.linspace(start=p, end=p+L_audio-1, steps=L_audio)</span>. This effectively &#8220;stretches&#8221; the familiar positional space to accommodate the longer audio input.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "range"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This implementation introduces negligible computational overhead in practice. The only additional computations are incurred during the construction of the <span class=\"ltx_text ltx_font_typewriter\">position_ids</span> tensor, which involves a few tensor slicing operations, a single <span class=\"ltx_text ltx_font_typewriter\">torch.linspace</span> call, and a final concatenation. These are highly optimized, vectorized operations that constitute a minuscule fraction of the total computation in a full forward pass. Crucially, this precomputation is a <span class=\"ltx_text ltx_ulem_uline\">one-time cost</span> per input sample, applied only during the initial processing of the prompt that contains the long audio. During the subsequent autoregressive generation steps, where tokens are generated one at a time, our method adds no overhead whatsoever. The model reverts to the standard process of incremental positional stepping, just as it would without any context extension. Therefore, the cost of enabling partial context extension is a small, fixed precomputation that does not impact the per-token generation latency.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "audio",
                    "fixed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use LoRA <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib20\" title=\"\">2022</a>)</cite> of rank 8 for fine-tuning through HuggingFace&#8217;s PEFT library <cite class=\"ltx_cite ltx_citemacro_citep\">(Mangrulkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib36\" title=\"\">2022</a>)</cite>. With alpha set to 16, and a small LoRA dropout of 0.5%. Models are trained with AdamW optimizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov and Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib32\" title=\"\">2019</a>)</cite>, learning rate is set to 5e-5, a batch size of 8 samples, and gradient norm clipped to 1.0 <cite class=\"ltx_cite ltx_citemacro_citep\">(Pascanu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib38\" title=\"\">2013</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recall that Partial YaRN has two hyperparameters, cutoff dimension and attention temperature. For the training-free experiment (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S5.SS1\" title=\"5.1 Training-free Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), we jointly sweep the cutoff dimension from <math alttext=\"\\{56,48,40,32,24,16,8\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>56</mn><mo>,</mo><mn>48</mn><mo>,</mo><mn>40</mn><mo>,</mo><mn>32</mn><mo>,</mo><mn>24</mn><mo>,</mo><mn>16</mn><mo>,</mo><mn>8</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{56,48,40,32,24,16,8\\}</annotation></semantics></math>, and the temperature from between 0.5 and 1.6 with a step size of 0.1. See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.SS2\" title=\"E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a> for tuning results and sensitivity.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To obtain specific audio length, we chunk the long audio in the dataset to 1, 2, 5, and 10 minutes. Last, incomplete chunk of each long-audio is also included as well. Each of the chunk will be used to generate 5 MCQA pairs via Gemini 2.0 Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#bib.bib50\" title=\"\">2023</a>)</cite>. We provide the full MCQA generation prompt in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.F7\" title=\"Figure 7 &#8227; Plausible explanation of this trend. &#8227; E.2.2 Sensitivity on SALMONN &#8227; E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "length",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The effectiveness of Virtual Longform Audio Training (VLAT) depends on the strategy used to sample the virtual length during training. For generality, here we define the virtual length as a produce of true audio context length (i.e. 30s) and the <span class=\"ltx_text ltx_font_italic\">virtual factor</span>. In our main experiment Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S6\" title=\"6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we used a strategy of randomly sampling a factor from the range [1x, 5x, 10x, 15x, 20x, 25x]. To understand the sensitivity of our results to this choice, we compare our default strategy against four alternatives, training a Qwen2-Audio model on each strategy and evaluating on the 10mins audio task.</p>\n\n",
                "matched_terms": [
                    "virtual",
                    "sampling",
                    "audio",
                    "strategy",
                    "vlat",
                    "length",
                    "default",
                    "results",
                    "range"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The virtual factor sampling strategies are:</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "strategies",
                    "virtual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Default</span>: Uniform random sampling from <math alttext=\"\\{1.0,5.0,10.0,15.0,20.0,25.0\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I1.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>1.0</mn><mo>,</mo><mn>5.0</mn><mo>,</mo><mn>10.0</mn><mo>,</mo><mn>15.0</mn><mo>,</mo><mn>20.0</mn><mo>,</mo><mn>25.0</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{1.0,5.0,10.0,15.0,20.0,25.0\\}</annotation></semantics></math>. This cover the audio context length of 30s (1.0x) to 12.5mins (25.0x).</p>\n\n",
                "matched_terms": [
                    "random",
                    "sampling",
                    "audio",
                    "length",
                    "default"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dense Sampling</span>: Random sampling from a discrete set of 100 uniformly spaced points within the default strategy&#8217;s range.</p>\n\n",
                "matched_terms": [
                    "random",
                    "points",
                    "sampling",
                    "default",
                    "range"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Very Dense Sampling</span>: Random sampling from a discrete set of 1000 uniformly spaced points within the default strategy&#8217;s range.</p>\n\n",
                "matched_terms": [
                    "random",
                    "points",
                    "sampling",
                    "default",
                    "range"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited Range</span>: Uniform random sampling from <math alttext=\"\\{1.0,5.0,10.0\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I1.i4.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>1.0</mn><mo>,</mo><mn>5.0</mn><mo>,</mo><mn>10.0</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{1.0,5.0,10.0\\}</annotation></semantics></math>. This factor set covers audio context length of at most 5mins (10.0x).</p>\n\n",
                "matched_terms": [
                    "random",
                    "sampling",
                    "audio",
                    "length",
                    "limited",
                    "range"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fixed High-Factor</span>: No randomization; the scale factor is always fixed at 20.0x.</p>\n\n",
                "matched_terms": [
                    "200x",
                    "fixed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The experiment offers several insights into different VLAT sampling strategies: (1) <span class=\"ltx_text ltx_font_italic\">Fixed High-Factor</span> (always 20.0x) strategy performs the worst by a large margin, confirming that exposure to a variety of positional ranges is crucial for robust generalization, rather than training for just a single fixed target length. (2) <span class=\"ltx_text ltx_font_italic\">Default</span> strategy which uses a coarse set of 6 sampling points outperforms both the <span class=\"ltx_text ltx_font_italic\">Dense</span> (100 points) and <span class=\"ltx_text ltx_font_italic\">Very Dense</span> (1000 points) strategies. This suggests that finegrained sampling of virtual factors provides no additional benefit, and that a relatively small but diverse set of factors is sufficient for training length-robust models.</p>\n\n",
                "matched_terms": [
                    "200x",
                    "models",
                    "points",
                    "fixed",
                    "virtual",
                    "sampling",
                    "strategies",
                    "strategy",
                    "vlat",
                    "length",
                    "default",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The most surprising result comes from the <span class=\"ltx_text ltx_font_italic\">Limited Range</span> strategy, which achieves the highest performance across all evaluation lengths, despite its virtual lengths not extending beyond the 5mins mark (10.0x factor). While this demonstrates a powerful capacity for extrapolation, the mechanism behind this superior performance is not immediately clear. <span class=\"ltx_text ltx_ulem_uline\">It may suggest that randomization on extremely long virtual lengths could introduce instability to the training</span>, which could in turn explain why Whole YaRN diverged quickly under VLAT framework; or that the specific distribution of factors in this limited set is coincidentally optimal for our test data. We left the more in-depth study of this behavior as future work.</p>\n\n",
                "matched_terms": [
                    "across",
                    "virtual",
                    "strategy",
                    "vlat",
                    "all",
                    "limited",
                    "range",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given this uncertainty, we retained the <span class=\"ltx_text ltx_font_italic\">Default</span> strategy for our main experiments in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#S6\" title=\"6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Its factor range intuitively and completely covers the lengths seen during evaluation, making it a more principled and safer choice.</p>\n\n",
                "matched_terms": [
                    "range",
                    "default",
                    "strategy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here we analyze the effect and sensitivity of different Partial YaRN&#8217;s hyperparameter configurations. We report validation accuracies w.r.t. different cutoff dimension indices and attention temperatures, for the Qwen2-Audio and SALMONN models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "partial",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.F3\" title=\"Figure 3 &#8227; Plausible explanation of this trend. &#8227; E.2.2 Sensitivity on SALMONN &#8227; E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> displays the hyperparameter sensitivity of Partial YaRN on Qwen2-Audio when extending from the original 30s audio context window. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.F4\" title=\"Figure 4 &#8227; Plausible explanation of this trend. &#8227; E.2.2 Sensitivity on SALMONN &#8227; E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the sensitivity when extending from the observed audio context window of 2mins.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "partial",
                    "2mins"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe the trend of optimal hyperparameter configurations to move from the bottom left (high cutoff dimension index, temperature close to 1.0) toward the top right of the heatmaps (lower cutoff dimension index, higher temperature) as the audio length increases.</p>\n\n",
                "matched_terms": [
                    "length",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This trend informs the following recommendation when using Partial YaRN. For moderate context extensions (e.g., 2x to 4x), it is advisable to start with a higher cutoff dimension index and a temperature value close to 1.0. For longer extensions, a lower cutoff dimension index with a considerably higher temperature is likely to yield better results.</p>\n\n",
                "matched_terms": [
                    "partial",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the attention temperature controls the distribution of attention scores. In very long sequences with thousands of tokens, the standard softmax function can lead to an uneven attention score distribution, such as attending to only a few tokens while ignoring the vast majority of the context. Raising the temperature flattens this distribution, encouraging the model to maintain a broader attention pattern across the entire long audio stream and preventing \"attention collapse\".</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, the cutoff dimension index directly controls the \"compression pressure\" of the position encoding extension. A high cutoff index is expressive enough for shorter extensions while retaining majority of the original, detailed positional information. On the other hand, a lower cutoff index (higher compression) is required to map a much larger range of positions into the smaller familiar audio region.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "range"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15231v1#A5.F5\" title=\"Figure 5 &#8227; Plausible explanation of this trend. &#8227; E.2.2 Sensitivity on SALMONN &#8227; E.2 Hyperparameter Sensitivity of Partial YaRN &#8227; Appendix E Further Experimental Results &#8227; Acknowledgments &#8227; Hyperparameter tuning burden. &#8227; Limitations &#8227; 8 Conclusion &#8227; 7.2 Partial YaRN with Three-group Frequency Partitioning &#8227; 7.1 Ablation Study &#8227; 7 Analysis &#8227; 6.3 Results &#8227; 6 Virtual Longform Audio Training: Generalizing Beyond the Training Data &#8227; 5.2 Fine-tuning for Audio Context Extension &#8227; 5 Main Results &#8227; Reference models. &#8227; 4.3 Baseline Methods &#8227; 4.2 Models &#8227; YODAS2-MCQA &#8227; 4.1 Data &#8227; 4 Experimental Setup &#8227; Extending Audio Context for Long-Form Understanding in Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> displays the hyperparameter sensitivity of Partial YaRN on SALMONN when extending from the original 30s audio context window.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "partial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For this model, we observe that the optimal hyperparameter sets are generally contained in the low temperature region (0.5 - 0.7), with the cutoff dimension index moving from low to higher ones as the audio length grows.</p>\n\n",
                "matched_terms": [
                    "length",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the observed trend, it&#8217;s advisable to set the attention temperature to <math alttext=\"0.6(\\pm 0.1)\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS2.SSS2.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>0.6</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>&#177;</mo><mn>0.1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">0.6(\\pm 0.1)</annotation></semantics></math>, adopt a low cutoff index for shorter audio length, and higher index for longer audio inputs.</p>\n\n",
                "matched_terms": [
                    "length",
                    "audio"
                ]
            }
        ]
    }
}