{
    "S3.T1": {
        "source_file": "End-to-end Speech Recognition with similar length speech and text",
        "caption": "TABLE I: The overall result on AISHELL-1 with CER.",
        "body": "Models\nCondition\nTest\nDrop ratio\n\n\n\n\nConformer[20]\n\nAll\n4.75\n/\n\n\nB0 (baseline)\nAll\n4.58\n/\n\n\nEfficient Conformer[21]\n\nAll\n4.64\n/\n\n\nE1 (KFSA)\nK\n4.58\n/\n\n\nE2 (KFDS)\n[-1, +1] + K\n4.52\n65%\n\n\nE3 (KFDS with TIL)\nK\n4.65\n87%\n\n\nE4 (KFDS with AXE)\nK\n4.49\n\n87%\n\n\nE5 (KFDS without Loss)\nK\n89.83\n87%",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Models</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Condition</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Test</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Drop ratio</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Conformer<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib20\" title=\"\">20</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">All</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">4.75</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">/</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">B0 (baseline)</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">All</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">4.58</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">/</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Efficient Conformer<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib21\" title=\"\">21</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">All</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">4.64</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">/</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">E1 (KFSA)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">K</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">4.58</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">/</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">E2 (KFDS)</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">[-1, +1] + K</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">4.52</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">65%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">E3 (KFDS with TIL)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">K</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">4.65</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">87%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">E4 (KFDS with AXE)</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">K</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\">4.49</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<span class=\"ltx_text ltx_font_bold\">87</span>%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">E5 (KFDS without Loss)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">K</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">89.83</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">87%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "overall",
            "axe",
            "conformer20",
            "loss",
            "aishell1",
            "test",
            "drop",
            "all",
            "kfds",
            "baseline",
            "efficient",
            "result",
            "without",
            "til",
            "ratio",
            "cer",
            "kfsa",
            "models",
            "condition",
            "conformer21"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">TABLE&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#S3.T1\" title=\"TABLE I &#8227; III-B Overall Results &#8227; III EXPERIMENTS &#8227; End-to-end Speech Recognition with similar length speech and text\"><span class=\"ltx_text ltx_ref_tag\">I</span></a> shows the overall character error rate (CER) on the AISHELL-1 test set. During decoding, CTC prefix beam search is used to generate N-best candidates first and then rescored using a Transformer decoder. We only report the final results here after the Transformer decoder rescore. In Table I, the result of the vanilla Conformer model was from <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib20\" title=\"\">20</a>]</cite>. However, there is no intermediate CTC during model training in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib20\" title=\"\">20</a>]</cite>. For a fair comparison, we add intermediate CTC loss during training of B0 model, which is our baseline model with 4.58% CER. We also compare our methods with Efficient Conformer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib21\" title=\"\">21</a>]</cite>, which downsampled feature sequences uniformly.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The mismatch of speech length and text length poses a challenge in automatic speech recognition (ASR). In previous research, various approaches have been employed to align text with speech, including the utilization of Connectionist Temporal Classification (CTC). In earlier work, a key frame mechanism (KFDS) was introduced, utilizing intermediate CTC outputs to guide downsampling and preserve keyframes, but traditional methods (CTC) failed to align speech and text appropriately when downsampling speech to a text-similar length. In this paper, we focus on speech recognition in those cases where the length of speech aligns closely with that of the corresponding text. To address this issue, we introduce two methods for alignment: a) Time Independence Loss (TIL) and b) Aligned Cross Entropy (AXE) Loss, which is based on edit distance. To enhance the information on keyframes, we incorporate frame fusion by applying weights and summing the keyframe with its context 2 frames. Experimental results on AISHELL-1 and AISHELL-2 dataset subsets show that the proposed methods outperform the previous work and achieve a reduction of at least 86% in the number of frames.</p>\n\n",
                "matched_terms": [
                    "axe",
                    "til",
                    "kfds",
                    "loss",
                    "aishell1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, our work is based on KFDS. We only retain key frames and downsample them to a length similar to the text. However, a new problem arises, that is, CTC loss cannot be applied if the speech length is similar to the text length. Therefore, we introduce the Length Similarity Loss (LSL) to address this issue. LSL comprises two implementation methods: one is the Time Independence Loss (TIL), which removes temporal information from both the input and output. The other is the Aligned Cross-Entropy (AXE) loss, which relies on edit distance alignment to synchronize the input and output before computing the cross-entropy loss. Our work is akin to CIF&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib15\" title=\"\">15</a>]</cite> in terms of achieving alignment between input and output. However, a notable difference is that CIF does not incorporate speech downsampling.</p>\n\n",
                "matched_terms": [
                    "til",
                    "kfds",
                    "loss",
                    "axe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we focus on ASR with the aim of downsampling speech to a length similar to the corresponding text. Experimental results on the AISHELL-1 dataset and AISHELL-2 dataset subsets demonstrate the effectiveness of our proposed methods, achieving a Character Error Rate (CER) comparable to the baseline. Furthermore, our approach performs comparably to previous methods that utilized CTC loss. Meantime, substantial reduction in computational complexity is achieved, while reducing the number of frames by at least 86%.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "aishell1",
                    "baseline",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ KFDS to downsample the length of speech to be similar to the text and subsequently explore speech recognition in this context. We will briefly introduce the key frame-based self-attention (KFSA) and the down-sampling process based on the key frame mechanism (KFDS). As shown in the dashed box at the bottom right of Fig. 1, the key frame mechanism selects key frame using the non-blank frame sequence generated by the intermediate CTC loss to remove duplicates and blank frames&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib11\" title=\"\">11</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "kfsa",
                    "kfds",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The KFSA mechanism utilizes previously generated keyframes to reduce the self-attention mechanism module. The KFDS process, as illustrated in Fig. 1, involves down-sampling frames guided by key frames and preserving the frames corresponding to these key frames.</p>\n\n",
                "matched_terms": [
                    "kfds",
                    "kfsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, in prior research, the final output of the encoder was determined using the CTC loss. When we use the KFDS mechanism to downsample speech to a length similar to the text, the speech and text are mapped to the same feature space, and the previous method (CTC) is not suitable for calculating losses.</p>\n\n",
                "matched_terms": [
                    "kfds",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This subsection, we introduce the LSL to deal with it. We introduce two LSL functions to address this issue from two distinct angles. Firstly, we employ a time information removal method (TIL) to compute the loss between the model&#8217;s predictions and the ground truth values. Secondly, we align these elements by adjusting the distance before calculating the CE loss (AXE loss).</p>\n\n",
                "matched_terms": [
                    "til",
                    "loss",
                    "axe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, after frame-downsampling by the KFDS mechanism, the length of the output of the encoder is similar to the target text. However, their corresponding relationship cannot be established directly. The calculation of loss is prevented due to the absence of chronological order and one-to-one correspondence. Consequently, the temporal information was removed, and all information sets were consolidated into a single output denoted as <math alttext=\"Y^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><msup><mi>Y</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">Y^{\\prime}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "kfds",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The AXE loss was initially proposed to solve the problem of excessive penalty for the change of output word order when using CE loss in machine translation<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib16\" title=\"\">16</a>]</cite>. The final prediction sequence of our encoder is very close to the target sequence in length, but there may be insertion and deletion errors, which is unsuitable for directly using CE loss, so the AXE loss function is introduced to calculate the loss.\nOur goal is to find a monotonic alignment between <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math> and <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math> that will minimize the CE loss, and thus focusing the penalty on lexical errors (predicting the wrong token) rather than positional errors (predicting the right token in the wrong place). We define an alignment <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> to be a function that maps target positions to prediction positions, i.e. <math alttext=\"\\{1,\\ldots,L\\}\\to\\{1,\\ldots,T\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m4\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">&#8594;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\{1,\\ldots,L\\}\\to\\{1,\\ldots,T\\}</annotation></semantics></math>. The AXE loss can be depicted with the following Eq.7&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib16\" title=\"\">16</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "loss",
                    "axe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate model training convergence, a three-stage training process is proposed. The training process of our proposed model is depicted in Fig.4. In the first stage, a base Conformer model is trained to utilize intermediate CTC loss to obtain peak information. Subsequently, in the second stage, the KFDS method with key frame and context 2 frames ([-1,+1]+K), initializes the model using the Stage 1 model init for downsampling speech length. In the third stage, to achieve further downsampling, LS loss is adopted, and the model is initialized using the Stage 2 model.</p>\n\n",
                "matched_terms": [
                    "kfds",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this subsection, we verify our proposed KFDS-based Conformer encoder-decoder network on the open-source datasets: AISHELL-1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib17\" title=\"\">17</a>]</cite> and AISHELL-2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib18\" title=\"\">18</a>]</cite> subsets. In all our experiments, 80-dimensional log Mel-filter bank (Fbank) features are extracted from a 25ms window with a 10ms frame shift. SpecAugment is used as acoustic feature augmentation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib19\" title=\"\">19</a>]</cite>. To conduct modeling on the AiSHELL-1, a vocabulary consisting of 5234 labels that incorporate Chinese characters and other special characters is employed.</p>\n\n",
                "matched_terms": [
                    "all",
                    "aishell1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As for training details, we follow the training recipes provided by Wenet. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The recipe on AISHELL-1 is publicly available in <span class=\"ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self\">https://github.com/wenet-e2e/wenet/tree/main/examples/aishell/s0/conf</span>;</span></span></span> It will be easy for others to reproduce our experiments. Note that, in order to obtain a better initial intermediate CTC guidance, KFSA and KFDS are introduced after the first <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> normal training epochs. <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is 40 for AISHELL-1 experiments.</p>\n\n",
                "matched_terms": [
                    "aishell1",
                    "kfds",
                    "kfsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Model E1 is trained using the previous KFSA mechanism with &#8220;+ K&#8221;, which means all other key frames are used during attention calculation. E1 is trained using key frames only and obtains 4.58% 9.CER, which also proves that key frames contain more helpful information and are crucial for attention mechanisms. E2 is trained using the KFDS mechanism with local temporal context widths 1 ([-1, +1] + K). Compared with the KFSA-based models E1, the KFDS-based models obtain lower, 4.52% CER and less computational complexity. E1 and E2 are both the previous key frame mechanism models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "all",
                    "kfds",
                    "cer",
                    "kfsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Models E3 and E4 are trained using the specified KFDS mechanism, which uses only keyframes to guide the downsampling of speech frames. Both E3 and E4 adopt extreme downsampling, which reduces the speech frame to the length of the deduplicated non-blank label sequence predicted by the intermediate CTC loss. The E3 and E4 obtain 4.65% and 4.49% CER respectively. E3&#8217;s performance surpasses the vanilla Conformer and is comparable with B0 and E&#64256;icient Conformer. However, it performed wose compared to E1 using the KFSA mechanism and E2 using 3 frames of KFDS. This is likely because this method ignores the order information of the text, which is important in speech recognition. However, this method is also a tradeoff one for calculating the loss when the length of a speech frame sequence is close to that of a text sequence.</p>\n\n",
                "matched_terms": [
                    "models",
                    "kfds",
                    "loss",
                    "cer",
                    "kfsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Especially, E4 obtained 4.49% CER, which surpasses the previous best results with 0.03% absolute CER. This is a very interesting result. When using the KFDS mechanism, E4 with less information actually performs better than E2, indicating that the key frame mechanism has indeed learned enough key information. Meanwhile, compared to E3, the absolute CER decreased by 0.16%, indicating that the time information in the speech frame is very important for predicting the final text sequence. E5 employed an extreme downsampling strategy without computing the loss for the output of the second encoder. Consequently, it achieved a CER of 89.83%, leading to performance degradation. This underscores the crucial necessity of calculating the loss for the encoder 2 output. Additionally, our approach achieves an 87% reduction in frames on the AISHELL-1 dataset, surpassing the previous reduction of 65%.</p>\n\n",
                "matched_terms": [
                    "without",
                    "result",
                    "kfds",
                    "loss",
                    "cer",
                    "aishell1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To verify the effectiveness of our proposed method, we conducted experiments on subsets of the AISHELL-2 dataset. TABLE&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#S3.T2\" title=\"TABLE II &#8227; III-B1 Results On Aishell-1 &#8227; III-B Overall Results &#8227; III EXPERIMENTS &#8227; End-to-end Speech Recognition with similar length speech and text\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> presents the overall CER on the AISHELL-2 test set. The baseline B0 model with intermediate CTC loss achieves a CER of 8.30%, while the vanilla Conformer has a CER of 8.40%, demonstrating that the intermediate CTC loss method enhances model performance. In addition, the Efficiency Conformer model achieves a CER of 8.42%, performing slightly worse than the first two baseline models but with reduced computational complexity. The KFSA model reaches a CER of 8.21%, and the KFDS model achieves 8.18%, demonstrating the effectiveness of the keyframe mechanism. For the proposed method in this study, TIL yields a CER of 8.76%, the lowest performance among all models, attributed to information loss; temporal information is removed after feature downsampling. The AXE loss model, with a CER of 8.43%, performs worse than both the B0 baseline model and the Efficiency Conformer but achieves substantial downsampling by discarding 86% of the frames. Lastly, when the loss is not computed after the second encoder that performs downsampling, the model&#8217;s CER increases significantly to 87.52%, indicating a substantial degradation in performance. This aligns with the results observed on the AISHELL-1 dataset, underscoring the importance of calculating the loss at this stage.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "test",
                    "models",
                    "all",
                    "axe",
                    "til",
                    "kfds",
                    "baseline",
                    "loss",
                    "cer",
                    "aishell1",
                    "kfsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first explored whether frame fusion is necessary for our experiment and which fusion method is optimal. Note that all models in this experiment were trained through pseudo-one-hot with KL loss. As shown in TABLE&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#S3.T3\" title=\"TABLE III &#8227; III-B3 Frame fusion &#8227; III-B Overall Results &#8227; III EXPERIMENTS &#8227; End-to-end Speech Recognition with similar length speech and text\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>, E6 is a KFDS that only uses keyframes without any frame fusion, but it achieved the worst result 4.81%, which is worse than the E2 of the original KFDS mechanism and even worse than the performance of the three baseline models. By fusing 3 frames into 1 frame, E7 achieved a poor CER effect of 4.71% using an additional network for frame fusion, compared to 4.65% using weighted summation for E8. Finally, we attempted to fuse 5 frames into 1 frame and obtained a CER of 4.74% using a weighted sum method, indicating that the effect of fusing more frames would also degrade, possibly due to the inclusion of information unrelated to the current keyframe.</p>\n\n",
                "matched_terms": [
                    "models",
                    "all",
                    "result",
                    "without",
                    "kfds",
                    "baseline",
                    "loss",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Matching the length of a speech with the length of its target text presents a significant challenge in the field of ASR. In this paper, we investigate ASR within the context of downsampling speech to match the length of the text using KFDS. In response to this challenge, we introduce two novel alignment techniques: a)TIL and b) AXE Loss. To enhance the content of key frames, we implement a frame fusion approach, incorporating weights and summing the key frame with its contextual two frames. Our proposed methodology has exhibited results that superior to the baseline in extensive experiments conducted on the AISHELL-1 dataset. Furthermore, our approach achieves a substantial reduction of at least 87% in the number of frames. Further experiments on a subset of AISHELL-2 reinforce the effectiveness and robustness of our proposed method.</p>\n\n",
                "matched_terms": [
                    "axe",
                    "kfds",
                    "baseline",
                    "loss",
                    "aishell1"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "End-to-end Speech Recognition with similar length speech and text",
        "caption": "TABLE II: The overall result on AISHELL-2 subsets with CER.",
        "body": "Models\nCondition\nTest\nDrop ratio\n\n\n\n\nConformer[20]\n\nAll\n8.40\n/\n\n\nB0 (baseline)\nAll\n8.30\n/\n\n\nEfficient Conformer[21]\n\nAll\n8.42\n/\n\n\nE1 (KFSA)\nK\n8.21\n/\n\n\nE2 (KFDS)\n[-1, +1] + K\n8.18\n58%\n\n\nE3 (KFDS with TIL)\nK\n8.76\n86%\n\n\nE4 (KFDS with AXE)\nK\n8.43\n\n86%\n\n\nE5 (KFDS without Loss)\nK\n87.52\n86%",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Models</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Condition</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Test</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Drop ratio</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Conformer<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib20\" title=\"\">20</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">All</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">8.40</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">/</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">B0 (baseline)</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">All</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">8.30</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">/</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Efficient Conformer<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib21\" title=\"\">21</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">All</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">8.42</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">/</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">E1 (KFSA)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">K</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">8.21</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">/</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">E2 (KFDS)</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">[-1, +1] + K</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\">8.18</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">58%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">E3 (KFDS with TIL)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">K</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">8.76</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">86%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">E4 (KFDS with AXE)</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">K</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">8.43</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<span class=\"ltx_text ltx_font_bold\">86</span>%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">E5 (KFDS without Loss)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">K</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">87.52</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">86%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "overall",
            "aishell2",
            "axe",
            "conformer20",
            "loss",
            "test",
            "drop",
            "all",
            "kfds",
            "baseline",
            "efficient",
            "subsets",
            "result",
            "without",
            "til",
            "ratio",
            "cer",
            "kfsa",
            "models",
            "condition",
            "conformer21"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To verify the effectiveness of our proposed method, we conducted experiments on subsets of the AISHELL-2 dataset. TABLE&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#S3.T2\" title=\"TABLE II &#8227; III-B1 Results On Aishell-1 &#8227; III-B Overall Results &#8227; III EXPERIMENTS &#8227; End-to-end Speech Recognition with similar length speech and text\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> presents the overall CER on the AISHELL-2 test set. The baseline B0 model with intermediate CTC loss achieves a CER of 8.30%, while the vanilla Conformer has a CER of 8.40%, demonstrating that the intermediate CTC loss method enhances model performance. In addition, the Efficiency Conformer model achieves a CER of 8.42%, performing slightly worse than the first two baseline models but with reduced computational complexity. The KFSA model reaches a CER of 8.21%, and the KFDS model achieves 8.18%, demonstrating the effectiveness of the keyframe mechanism. For the proposed method in this study, TIL yields a CER of 8.76%, the lowest performance among all models, attributed to information loss; temporal information is removed after feature downsampling. The AXE loss model, with a CER of 8.43%, performs worse than both the B0 baseline model and the Efficiency Conformer but achieves substantial downsampling by discarding 86% of the frames. Lastly, when the loss is not computed after the second encoder that performs downsampling, the model&#8217;s CER increases significantly to 87.52%, indicating a substantial degradation in performance. This aligns with the results observed on the AISHELL-1 dataset, underscoring the importance of calculating the loss at this stage.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The mismatch of speech length and text length poses a challenge in automatic speech recognition (ASR). In previous research, various approaches have been employed to align text with speech, including the utilization of Connectionist Temporal Classification (CTC). In earlier work, a key frame mechanism (KFDS) was introduced, utilizing intermediate CTC outputs to guide downsampling and preserve keyframes, but traditional methods (CTC) failed to align speech and text appropriately when downsampling speech to a text-similar length. In this paper, we focus on speech recognition in those cases where the length of speech aligns closely with that of the corresponding text. To address this issue, we introduce two methods for alignment: a) Time Independence Loss (TIL) and b) Aligned Cross Entropy (AXE) Loss, which is based on edit distance. To enhance the information on keyframes, we incorporate frame fusion by applying weights and summing the keyframe with its context 2 frames. Experimental results on AISHELL-1 and AISHELL-2 dataset subsets show that the proposed methods outperform the previous work and achieve a reduction of at least 86% in the number of frames.</p>\n\n",
                "matched_terms": [
                    "aishell2",
                    "subsets",
                    "axe",
                    "til",
                    "kfds",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, our work is based on KFDS. We only retain key frames and downsample them to a length similar to the text. However, a new problem arises, that is, CTC loss cannot be applied if the speech length is similar to the text length. Therefore, we introduce the Length Similarity Loss (LSL) to address this issue. LSL comprises two implementation methods: one is the Time Independence Loss (TIL), which removes temporal information from both the input and output. The other is the Aligned Cross-Entropy (AXE) loss, which relies on edit distance alignment to synchronize the input and output before computing the cross-entropy loss. Our work is akin to CIF&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib15\" title=\"\">15</a>]</cite> in terms of achieving alignment between input and output. However, a notable difference is that CIF does not incorporate speech downsampling.</p>\n\n",
                "matched_terms": [
                    "til",
                    "kfds",
                    "loss",
                    "axe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we focus on ASR with the aim of downsampling speech to a length similar to the corresponding text. Experimental results on the AISHELL-1 dataset and AISHELL-2 dataset subsets demonstrate the effectiveness of our proposed methods, achieving a Character Error Rate (CER) comparable to the baseline. Furthermore, our approach performs comparably to previous methods that utilized CTC loss. Meantime, substantial reduction in computational complexity is achieved, while reducing the number of frames by at least 86%.</p>\n\n",
                "matched_terms": [
                    "aishell2",
                    "subsets",
                    "baseline",
                    "loss",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ KFDS to downsample the length of speech to be similar to the text and subsequently explore speech recognition in this context. We will briefly introduce the key frame-based self-attention (KFSA) and the down-sampling process based on the key frame mechanism (KFDS). As shown in the dashed box at the bottom right of Fig. 1, the key frame mechanism selects key frame using the non-blank frame sequence generated by the intermediate CTC loss to remove duplicates and blank frames&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib11\" title=\"\">11</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "kfsa",
                    "kfds",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The KFSA mechanism utilizes previously generated keyframes to reduce the self-attention mechanism module. The KFDS process, as illustrated in Fig. 1, involves down-sampling frames guided by key frames and preserving the frames corresponding to these key frames.</p>\n\n",
                "matched_terms": [
                    "kfds",
                    "kfsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, in prior research, the final output of the encoder was determined using the CTC loss. When we use the KFDS mechanism to downsample speech to a length similar to the text, the speech and text are mapped to the same feature space, and the previous method (CTC) is not suitable for calculating losses.</p>\n\n",
                "matched_terms": [
                    "kfds",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This subsection, we introduce the LSL to deal with it. We introduce two LSL functions to address this issue from two distinct angles. Firstly, we employ a time information removal method (TIL) to compute the loss between the model&#8217;s predictions and the ground truth values. Secondly, we align these elements by adjusting the distance before calculating the CE loss (AXE loss).</p>\n\n",
                "matched_terms": [
                    "til",
                    "loss",
                    "axe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, after frame-downsampling by the KFDS mechanism, the length of the output of the encoder is similar to the target text. However, their corresponding relationship cannot be established directly. The calculation of loss is prevented due to the absence of chronological order and one-to-one correspondence. Consequently, the temporal information was removed, and all information sets were consolidated into a single output denoted as <math alttext=\"Y^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><msup><mi>Y</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">Y^{\\prime}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "kfds",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The AXE loss was initially proposed to solve the problem of excessive penalty for the change of output word order when using CE loss in machine translation<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib16\" title=\"\">16</a>]</cite>. The final prediction sequence of our encoder is very close to the target sequence in length, but there may be insertion and deletion errors, which is unsuitable for directly using CE loss, so the AXE loss function is introduced to calculate the loss.\nOur goal is to find a monotonic alignment between <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math> and <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math> that will minimize the CE loss, and thus focusing the penalty on lexical errors (predicting the wrong token) rather than positional errors (predicting the right token in the wrong place). We define an alignment <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> to be a function that maps target positions to prediction positions, i.e. <math alttext=\"\\{1,\\ldots,L\\}\\to\\{1,\\ldots,T\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m4\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">&#8594;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\{1,\\ldots,L\\}\\to\\{1,\\ldots,T\\}</annotation></semantics></math>. The AXE loss can be depicted with the following Eq.7&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib16\" title=\"\">16</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "loss",
                    "axe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate model training convergence, a three-stage training process is proposed. The training process of our proposed model is depicted in Fig.4. In the first stage, a base Conformer model is trained to utilize intermediate CTC loss to obtain peak information. Subsequently, in the second stage, the KFDS method with key frame and context 2 frames ([-1,+1]+K), initializes the model using the Stage 1 model init for downsampling speech length. In the third stage, to achieve further downsampling, LS loss is adopted, and the model is initialized using the Stage 2 model.</p>\n\n",
                "matched_terms": [
                    "kfds",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this subsection, we verify our proposed KFDS-based Conformer encoder-decoder network on the open-source datasets: AISHELL-1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib17\" title=\"\">17</a>]</cite> and AISHELL-2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib18\" title=\"\">18</a>]</cite> subsets. In all our experiments, 80-dimensional log Mel-filter bank (Fbank) features are extracted from a 25ms window with a 10ms frame shift. SpecAugment is used as acoustic feature augmentation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib19\" title=\"\">19</a>]</cite>. To conduct modeling on the AiSHELL-1, a vocabulary consisting of 5234 labels that incorporate Chinese characters and other special characters is employed.</p>\n\n",
                "matched_terms": [
                    "aishell2",
                    "all",
                    "subsets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As for training details, we follow the training recipes provided by Wenet. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The recipe on AISHELL-1 is publicly available in <span class=\"ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self\">https://github.com/wenet-e2e/wenet/tree/main/examples/aishell/s0/conf</span>;</span></span></span> It will be easy for others to reproduce our experiments. Note that, in order to obtain a better initial intermediate CTC guidance, KFSA and KFDS are introduced after the first <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> normal training epochs. <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is 40 for AISHELL-1 experiments.</p>\n\n",
                "matched_terms": [
                    "kfds",
                    "kfsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">TABLE&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#S3.T1\" title=\"TABLE I &#8227; III-B Overall Results &#8227; III EXPERIMENTS &#8227; End-to-end Speech Recognition with similar length speech and text\"><span class=\"ltx_text ltx_ref_tag\">I</span></a> shows the overall character error rate (CER) on the AISHELL-1 test set. During decoding, CTC prefix beam search is used to generate N-best candidates first and then rescored using a Transformer decoder. We only report the final results here after the Transformer decoder rescore. In Table I, the result of the vanilla Conformer model was from <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib20\" title=\"\">20</a>]</cite>. However, there is no intermediate CTC during model training in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib20\" title=\"\">20</a>]</cite>. For a fair comparison, we add intermediate CTC loss during training of B0 model, which is our baseline model with 4.58% CER. We also compare our methods with Efficient Conformer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib21\" title=\"\">21</a>]</cite>, which downsampled feature sequences uniformly.</p>\n\n",
                "matched_terms": [
                    "efficient",
                    "overall",
                    "test",
                    "result",
                    "baseline",
                    "loss",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Model E1 is trained using the previous KFSA mechanism with &#8220;+ K&#8221;, which means all other key frames are used during attention calculation. E1 is trained using key frames only and obtains 4.58% 9.CER, which also proves that key frames contain more helpful information and are crucial for attention mechanisms. E2 is trained using the KFDS mechanism with local temporal context widths 1 ([-1, +1] + K). Compared with the KFSA-based models E1, the KFDS-based models obtain lower, 4.52% CER and less computational complexity. E1 and E2 are both the previous key frame mechanism models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "all",
                    "kfds",
                    "cer",
                    "kfsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Models E3 and E4 are trained using the specified KFDS mechanism, which uses only keyframes to guide the downsampling of speech frames. Both E3 and E4 adopt extreme downsampling, which reduces the speech frame to the length of the deduplicated non-blank label sequence predicted by the intermediate CTC loss. The E3 and E4 obtain 4.65% and 4.49% CER respectively. E3&#8217;s performance surpasses the vanilla Conformer and is comparable with B0 and E&#64256;icient Conformer. However, it performed wose compared to E1 using the KFSA mechanism and E2 using 3 frames of KFDS. This is likely because this method ignores the order information of the text, which is important in speech recognition. However, this method is also a tradeoff one for calculating the loss when the length of a speech frame sequence is close to that of a text sequence.</p>\n\n",
                "matched_terms": [
                    "models",
                    "kfds",
                    "loss",
                    "cer",
                    "kfsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Especially, E4 obtained 4.49% CER, which surpasses the previous best results with 0.03% absolute CER. This is a very interesting result. When using the KFDS mechanism, E4 with less information actually performs better than E2, indicating that the key frame mechanism has indeed learned enough key information. Meanwhile, compared to E3, the absolute CER decreased by 0.16%, indicating that the time information in the speech frame is very important for predicting the final text sequence. E5 employed an extreme downsampling strategy without computing the loss for the output of the second encoder. Consequently, it achieved a CER of 89.83%, leading to performance degradation. This underscores the crucial necessity of calculating the loss for the encoder 2 output. Additionally, our approach achieves an 87% reduction in frames on the AISHELL-1 dataset, surpassing the previous reduction of 65%.</p>\n\n",
                "matched_terms": [
                    "without",
                    "result",
                    "kfds",
                    "loss",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first explored whether frame fusion is necessary for our experiment and which fusion method is optimal. Note that all models in this experiment were trained through pseudo-one-hot with KL loss. As shown in TABLE&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#S3.T3\" title=\"TABLE III &#8227; III-B3 Frame fusion &#8227; III-B Overall Results &#8227; III EXPERIMENTS &#8227; End-to-end Speech Recognition with similar length speech and text\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>, E6 is a KFDS that only uses keyframes without any frame fusion, but it achieved the worst result 4.81%, which is worse than the E2 of the original KFDS mechanism and even worse than the performance of the three baseline models. By fusing 3 frames into 1 frame, E7 achieved a poor CER effect of 4.71% using an additional network for frame fusion, compared to 4.65% using weighted summation for E8. Finally, we attempted to fuse 5 frames into 1 frame and obtained a CER of 4.74% using a weighted sum method, indicating that the effect of fusing more frames would also degrade, possibly due to the inclusion of information unrelated to the current keyframe.</p>\n\n",
                "matched_terms": [
                    "models",
                    "all",
                    "result",
                    "without",
                    "kfds",
                    "baseline",
                    "loss",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Matching the length of a speech with the length of its target text presents a significant challenge in the field of ASR. In this paper, we investigate ASR within the context of downsampling speech to match the length of the text using KFDS. In response to this challenge, we introduce two novel alignment techniques: a)TIL and b) AXE Loss. To enhance the content of key frames, we implement a frame fusion approach, incorporating weights and summing the key frame with its contextual two frames. Our proposed methodology has exhibited results that superior to the baseline in extensive experiments conducted on the AISHELL-1 dataset. Furthermore, our approach achieves a substantial reduction of at least 87% in the number of frames. Further experiments on a subset of AISHELL-2 reinforce the effectiveness and robustness of our proposed method.</p>\n\n",
                "matched_terms": [
                    "aishell2",
                    "axe",
                    "kfds",
                    "baseline",
                    "loss"
                ]
            }
        ]
    },
    "S3.T3": {
        "source_file": "End-to-end Speech Recognition with similar length speech and text",
        "caption": "TABLE III: \nInvestigate the different frame fusion methods",
        "body": "Models\nFusion method\nTest\n\n\n\n\nE6\nNo fusion\n4.81\n\n\nE7\nConcatenate-based\n4.71\n\n\nE8\nAttention-based(3 frames)\n4.65\n\n\nE9\nAttention-based(5 frames)\n4.74",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Models</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Fusion method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Test</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">E6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">No fusion</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.81</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">E7</td>\n<td class=\"ltx_td ltx_align_center\">Concatenate-based</td>\n<td class=\"ltx_td ltx_align_center\">4.71</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">E8</td>\n<td class=\"ltx_td ltx_align_center\">Attention-based(3 frames)</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.65</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\">E9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">Attention-based(5 frames)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">4.74</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "investigate",
            "concatenatebased",
            "attentionbased3",
            "test",
            "models",
            "frame",
            "different",
            "attentionbased5",
            "method",
            "methods",
            "fusion",
            "iii",
            "frames"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We first explored whether frame fusion is necessary for our experiment and which fusion method is optimal. Note that all models in this experiment were trained through pseudo-one-hot with KL loss. As shown in TABLE&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#S3.T3\" title=\"TABLE III &#8227; III-B3 Frame fusion &#8227; III-B Overall Results &#8227; III EXPERIMENTS &#8227; End-to-end Speech Recognition with similar length speech and text\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>, E6 is a KFDS that only uses keyframes without any frame fusion, but it achieved the worst result 4.81%, which is worse than the E2 of the original KFDS mechanism and even worse than the performance of the three baseline models. By fusing 3 frames into 1 frame, E7 achieved a poor CER effect of 4.71% using an additional network for frame fusion, compared to 4.65% using weighted summation for E8. Finally, we attempted to fuse 5 frames into 1 frame and obtained a CER of 4.74% using a weighted sum method, indicating that the effect of fusing more frames would also degrade, possibly due to the inclusion of information unrelated to the current keyframe.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The mismatch of speech length and text length poses a challenge in automatic speech recognition (ASR). In previous research, various approaches have been employed to align text with speech, including the utilization of Connectionist Temporal Classification (CTC). In earlier work, a key frame mechanism (KFDS) was introduced, utilizing intermediate CTC outputs to guide downsampling and preserve keyframes, but traditional methods (CTC) failed to align speech and text appropriately when downsampling speech to a text-similar length. In this paper, we focus on speech recognition in those cases where the length of speech aligns closely with that of the corresponding text. To address this issue, we introduce two methods for alignment: a) Time Independence Loss (TIL) and b) Aligned Cross Entropy (AXE) Loss, which is based on edit distance. To enhance the information on keyframes, we incorporate frame fusion by applying weights and summing the keyframe with its context 2 frames. Experimental results on AISHELL-1 and AISHELL-2 dataset subsets show that the proposed methods outperform the previous work and achieve a reduction of at least 86% in the number of frames.</p>\n\n",
                "matched_terms": [
                    "frames",
                    "frame",
                    "methods",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the field of speech processing, the challenge of speech length exceeding text length is a noteworthy concern, particularly in tasks related to speech translation and speech recognition. To address this issue, several methods have been developed. For example, text length can be extended to match the length of the speech, as demonstrated in GMM-HMM. Also, alignment of speech and text can be achieved by introducing blank labels through CTC and RNN-T. Another approach is to integrate the speech features of the encoder into the encoder&#8217;s autoregressive decoding process via AED, without the need for explicit alignment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib7\" title=\"\">7</a>]</cite>. These solutions partially solve the problem of speech text mismatch, but the length of speech remains considerable, as does the computational workload. During speech recognition process, speech is usually downsampled by 3-4 times the length, but the speech length is still much longer than the text length. In addition, previous work used CTC outputs to guide downsampling and skip the blank frame for decoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib10\" title=\"\">10</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "frame",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, our work is based on KFDS. We only retain key frames and downsample them to a length similar to the text. However, a new problem arises, that is, CTC loss cannot be applied if the speech length is similar to the text length. Therefore, we introduce the Length Similarity Loss (LSL) to address this issue. LSL comprises two implementation methods: one is the Time Independence Loss (TIL), which removes temporal information from both the input and output. The other is the Aligned Cross-Entropy (AXE) loss, which relies on edit distance alignment to synchronize the input and output before computing the cross-entropy loss. Our work is akin to CIF&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib15\" title=\"\">15</a>]</cite> in terms of achieving alignment between input and output. However, a notable difference is that CIF does not incorporate speech downsampling.</p>\n\n",
                "matched_terms": [
                    "frames",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we focus on ASR with the aim of downsampling speech to a length similar to the corresponding text. Experimental results on the AISHELL-1 dataset and AISHELL-2 dataset subsets demonstrate the effectiveness of our proposed methods, achieving a Character Error Rate (CER) comparable to the baseline. Furthermore, our approach performs comparably to previous methods that utilized CTC loss. Meantime, substantial reduction in computational complexity is achieved, while reducing the number of frames by at least 86%.</p>\n\n",
                "matched_terms": [
                    "frames",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ KFDS to downsample the length of speech to be similar to the text and subsequently explore speech recognition in this context. We will briefly introduce the key frame-based self-attention (KFSA) and the down-sampling process based on the key frame mechanism (KFDS). As shown in the dashed box at the bottom right of Fig. 1, the key frame mechanism selects key frame using the non-blank frame sequence generated by the intermediate CTC loss to remove duplicates and blank frames&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib11\" title=\"\">11</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "frames",
                    "frame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we use the keyframe mechanism for downsampling. Benefiting from the two proposed loss calculation methods, we can achieve extreme downsampling of speech similar in length to the text. If only keyframes are used, a lot of useful information will be discarded. Although CTC has peaks, its adjacent frames also contain a large amount of information. Therefore, we used frame fusion to preserve more information and employed two methods for frame fusion.</p>\n\n",
                "matched_terms": [
                    "frames",
                    "frame",
                    "methods",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Firstly, frame fusion was achieved through attention-based frame fusion without adding new parameters. As shown in Fig.2, the rose red bar represents the key frame, the orange bar represents the left frame of the keyframe, and the light green bar represents the right frame of the key frame. This process is shown in the following Eq. 8 and Eq. 9.</p>\n\n",
                "matched_terms": [
                    "frame",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> is key frame, <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m2\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math> represents the weight of frames from <math alttext=\"t-i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m3\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&#8722;</mo><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">t-i</annotation></semantics></math> to <math alttext=\"t+j\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m4\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>+</mo><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">t+j</annotation></semantics></math>, with <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m5\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> equal to 1 and <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m6\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math> equal to 1, while <math alttext=\"O\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m7\" intent=\":literal\"><semantics><mi>O</mi><annotation encoding=\"application/x-tex\">O</annotation></semantics></math> denotes the fusioned 3-frame.</p>\n\n",
                "matched_terms": [
                    "frames",
                    "frame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Fig.3, we also used concatenate-based frame fusion, concatenated the keyframe sequence with the keyframe context 2 frames on the channel dimension, and then reduced it to the original dimension by using a linear layer.</p>\n\n",
                "matched_terms": [
                    "frame",
                    "concatenatebased",
                    "fusion",
                    "frames"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate model training convergence, a three-stage training process is proposed. The training process of our proposed model is depicted in Fig.4. In the first stage, a base Conformer model is trained to utilize intermediate CTC loss to obtain peak information. Subsequently, in the second stage, the KFDS method with key frame and context 2 frames ([-1,+1]+K), initializes the model using the Stage 1 model init for downsampling speech length. In the third stage, to achieve further downsampling, LS loss is adopted, and the model is initialized using the Stage 2 model.</p>\n\n",
                "matched_terms": [
                    "frames",
                    "frame",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">TABLE&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#S3.T1\" title=\"TABLE I &#8227; III-B Overall Results &#8227; III EXPERIMENTS &#8227; End-to-end Speech Recognition with similar length speech and text\"><span class=\"ltx_text ltx_ref_tag\">I</span></a> shows the overall character error rate (CER) on the AISHELL-1 test set. During decoding, CTC prefix beam search is used to generate N-best candidates first and then rescored using a Transformer decoder. We only report the final results here after the Transformer decoder rescore. In Table I, the result of the vanilla Conformer model was from <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib20\" title=\"\">20</a>]</cite>. However, there is no intermediate CTC during model training in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib20\" title=\"\">20</a>]</cite>. For a fair comparison, we add intermediate CTC loss during training of B0 model, which is our baseline model with 4.58% CER. We also compare our methods with Efficient Conformer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#bib.bib21\" title=\"\">21</a>]</cite>, which downsampled feature sequences uniformly.</p>\n\n",
                "matched_terms": [
                    "methods",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Model E1 is trained using the previous KFSA mechanism with &#8220;+ K&#8221;, which means all other key frames are used during attention calculation. E1 is trained using key frames only and obtains 4.58% 9.CER, which also proves that key frames contain more helpful information and are crucial for attention mechanisms. E2 is trained using the KFDS mechanism with local temporal context widths 1 ([-1, +1] + K). Compared with the KFSA-based models E1, the KFDS-based models obtain lower, 4.52% CER and less computational complexity. E1 and E2 are both the previous key frame mechanism models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "frame",
                    "frames"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Models E3 and E4 are trained using the specified KFDS mechanism, which uses only keyframes to guide the downsampling of speech frames. Both E3 and E4 adopt extreme downsampling, which reduces the speech frame to the length of the deduplicated non-blank label sequence predicted by the intermediate CTC loss. The E3 and E4 obtain 4.65% and 4.49% CER respectively. E3&#8217;s performance surpasses the vanilla Conformer and is comparable with B0 and E&#64256;icient Conformer. However, it performed wose compared to E1 using the KFSA mechanism and E2 using 3 frames of KFDS. This is likely because this method ignores the order information of the text, which is important in speech recognition. However, this method is also a tradeoff one for calculating the loss when the length of a speech frame sequence is close to that of a text sequence.</p>\n\n",
                "matched_terms": [
                    "models",
                    "frame",
                    "method",
                    "frames"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Especially, E4 obtained 4.49% CER, which surpasses the previous best results with 0.03% absolute CER. This is a very interesting result. When using the KFDS mechanism, E4 with less information actually performs better than E2, indicating that the key frame mechanism has indeed learned enough key information. Meanwhile, compared to E3, the absolute CER decreased by 0.16%, indicating that the time information in the speech frame is very important for predicting the final text sequence. E5 employed an extreme downsampling strategy without computing the loss for the output of the second encoder. Consequently, it achieved a CER of 89.83%, leading to performance degradation. This underscores the crucial necessity of calculating the loss for the encoder 2 output. Additionally, our approach achieves an 87% reduction in frames on the AISHELL-1 dataset, surpassing the previous reduction of 65%.</p>\n\n",
                "matched_terms": [
                    "frames",
                    "frame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To verify the effectiveness of our proposed method, we conducted experiments on subsets of the AISHELL-2 dataset. TABLE&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10453v1#S3.T2\" title=\"TABLE II &#8227; III-B1 Results On Aishell-1 &#8227; III-B Overall Results &#8227; III EXPERIMENTS &#8227; End-to-end Speech Recognition with similar length speech and text\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> presents the overall CER on the AISHELL-2 test set. The baseline B0 model with intermediate CTC loss achieves a CER of 8.30%, while the vanilla Conformer has a CER of 8.40%, demonstrating that the intermediate CTC loss method enhances model performance. In addition, the Efficiency Conformer model achieves a CER of 8.42%, performing slightly worse than the first two baseline models but with reduced computational complexity. The KFSA model reaches a CER of 8.21%, and the KFDS model achieves 8.18%, demonstrating the effectiveness of the keyframe mechanism. For the proposed method in this study, TIL yields a CER of 8.76%, the lowest performance among all models, attributed to information loss; temporal information is removed after feature downsampling. The AXE loss model, with a CER of 8.43%, performs worse than both the B0 baseline model and the Efficiency Conformer but achieves substantial downsampling by discarding 86% of the frames. Lastly, when the loss is not computed after the second encoder that performs downsampling, the model&#8217;s CER increases significantly to 87.52%, indicating a substantial degradation in performance. This aligns with the results observed on the AISHELL-1 dataset, underscoring the importance of calculating the loss at this stage.</p>\n\n",
                "matched_terms": [
                    "models",
                    "method",
                    "test",
                    "frames"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Matching the length of a speech with the length of its target text presents a significant challenge in the field of ASR. In this paper, we investigate ASR within the context of downsampling speech to match the length of the text using KFDS. In response to this challenge, we introduce two novel alignment techniques: a)TIL and b) AXE Loss. To enhance the content of key frames, we implement a frame fusion approach, incorporating weights and summing the key frame with its contextual two frames. Our proposed methodology has exhibited results that superior to the baseline in extensive experiments conducted on the AISHELL-1 dataset. Furthermore, our approach achieves a substantial reduction of at least 87% in the number of frames. Further experiments on a subset of AISHELL-2 reinforce the effectiveness and robustness of our proposed method.</p>\n\n",
                "matched_terms": [
                    "investigate",
                    "frame",
                    "method",
                    "fusion",
                    "frames"
                ]
            }
        ]
    }
}