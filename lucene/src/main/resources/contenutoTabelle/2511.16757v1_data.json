{
    "S3.tab1": {
        "caption": "",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_inline-logical-block ltx_minipage ltx_align_top\" style=\"width:174.9pt;\">\n<span class=\"ltx_table ltx_align_center\" id=\"S3.T1\">\n<span class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Comparison of publicly available audio caption datasets. The number of audio-text pairs (#pair) and number of unique words (#vocab) are shown here.</span>\n</span>\n<span class=\"ltx_para\" id=\"S3.p4\">\n<span class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:174.9pt;height:61.9pt;vertical-align:-29.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-101.9pt,36.1pt) scale(0.461912950276766,0.461912950276766) ;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_tt\">Audio Caption Dataset</span>\n<span class=\"ltx_td ltx_align_right ltx_border_tt\">#pair</span>\n<span class=\"ltx_td ltx_align_right ltx_border_tt\">#vocab</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">Human-annotated</span></span>\n<span class=\"ltx_td ltx_border_t\"/>\n<span class=\"ltx_td ltx_border_t\"/></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\">AudioCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>)</cite></span>\n<span class=\"ltx_td ltx_align_right\">46K</span>\n<span class=\"ltx_td ltx_align_right\">4,844</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\">Clotho&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Drossos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>)</cite></span>\n<span class=\"ltx_td ltx_align_right\">5K</span>\n<span class=\"ltx_td ltx_align_right\">4,366</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\">MusicCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Agostinelli et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>)</cite></span>\n<span class=\"ltx_td ltx_align_right\">5K</span>\n<span class=\"ltx_td ltx_align_right\">3,730</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">LLM-augmented</span></span>\n<span class=\"ltx_td ltx_border_t\"/>\n<span class=\"ltx_td ltx_border_t\"/></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\">WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>)</cite></span>\n<span class=\"ltx_td ltx_align_right\">403K</span>\n<span class=\"ltx_td ltx_align_right\">18,372</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\">AudioSetCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>)</cite></span>\n<span class=\"ltx_td ltx_align_right\">1.9M</span>\n<span class=\"ltx_td ltx_align_right\">21,783</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\">FusionAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib18\" title=\"\">2025</a>)</cite></span>\n<span class=\"ltx_td ltx_align_right\">1.2M</span>\n<span class=\"ltx_td ltx_align_right\">18,403</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\">AutoACD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib86\" title=\"\">Sun et&#160;al. </a></cite></span>\n<span class=\"ltx_td ltx_align_right\">1.5M</span>\n<span class=\"ltx_td ltx_align_right\">20,491</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">CaptionStew (Ours)</span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">10.7M</span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">56,586</span></span>\n</span>\n</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_inline-logical-block ltx_minipage ltx_align_top\" style=\"width:198.7pt;\">\n<span class=\"ltx_table ltx_align_center\" id=\"S3.T2\">\n<span class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Datasets used for evaluating linear probing, audio-language task and open-form question answering performance (separated by lines). <sup class=\"ltx_sup\">&#8224;</sup>reported with AIR-Bench&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>)</cite>.</span>\n</span>\n<span class=\"ltx_para\" id=\"S3.p5\">\n<span class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:198.7pt;height:100.6pt;vertical-align:-48.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-68.9pt,34.9pt) scale(0.590664456404354,0.590664456404354) ;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_tt\">Evaluation Dataset</span>\n<span class=\"ltx_td ltx_align_left ltx_border_tt\">Task</span>\n<span class=\"ltx_td ltx_align_left ltx_border_tt\">Metrics</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\">FSD-50k</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\">Multi-label audio event classification</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\">mAP</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\">VggSound</span>\n<span class=\"ltx_td ltx_align_left\">Single-label audio event classification</span>\n<span class=\"ltx_td ltx_align_left\">accuracy</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\">VoxCeleb2</span>\n<span class=\"ltx_td ltx_align_left\">Speaker identification</span>\n<span class=\"ltx_td ltx_align_left\">accuracy</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\">CREMA-D</span>\n<span class=\"ltx_td ltx_align_left\">Speech emotion recognition</span>\n<span class=\"ltx_td ltx_align_left\">accuracy</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\">MagnaTagATune</span>\n<span class=\"ltx_td ltx_align_left\">Music tagging</span>\n<span class=\"ltx_td ltx_align_left\">mAP</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\">NSynth</span>\n<span class=\"ltx_td ltx_align_left\">Musical instrument classification</span>\n<span class=\"ltx_td ltx_align_left\">accuracy</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\">AudioSet-strong</span>\n<span class=\"ltx_td ltx_align_left\">Sound event detection</span>\n<span class=\"ltx_td ltx_align_left\">PSDS1</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\">AudioCaps</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t ltx_rowspan ltx_rowspan_3\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\">Text-to-audio retrieval</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\">Audio captioning</span></span>\n</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t ltx_rowspan ltx_rowspan_3\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\">Recall@1</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\">RougeL</span></span>\n</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\">ParaSpeechCaps</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\">MusicCaps</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\">ClothoAQA</span>\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_3\">Open-formed question answering</span>\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_3\">Score<sup class=\"ltx_sup\">&#8224;</sup></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\">In-house SpeechQA</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb\">MusicQA</span></span>\n</span>\n</span></span>\n</span></span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "score†",
            "audiosetstrong",
            "detection",
            "evaluation",
            "words",
            "instrument",
            "recall1",
            "paraspeechcaps",
            "captioning",
            "accuracy",
            "musicqa",
            "voxceleb2",
            "airbench",
            "available",
            "datasets",
            "musiccaps",
            "ours",
            "wavcaps",
            "probing",
            "openformed",
            "pairs",
            "audiocaps",
            "2024b",
            "107m",
            "event",
            "cremad",
            "texttoaudio",
            "speechqa",
            "vocab",
            "audio",
            "clothoaqa",
            "nsynth",
            "speaker",
            "used",
            "openform",
            "captionstew",
            "lines",
            "magnatagatune",
            "emotion",
            "15m",
            "psds1",
            "12m",
            "yang",
            "answering",
            "sun",
            "kim",
            "fsd50k",
            "llmaugmented",
            "audiosetcaps",
            "retrieval",
            "performance",
            "pair",
            "clotho",
            "musical",
            "classification",
            "speech",
            "audiolanguage",
            "singlelabel",
            "number",
            "here",
            "recognition",
            "mei",
            "task",
            "unique",
            "question",
            "humanannotated",
            "agostinelli",
            "publicly",
            "multilabel",
            "drossos",
            "403k",
            "comparison",
            "evaluating",
            "caption",
            "rougel",
            "vggsound",
            "bai",
            "identification",
            "inhouse",
            "sound",
            "46k",
            "metrics",
            "map",
            "separated",
            "†reported",
            "audiotext",
            "chen",
            "tagging",
            "19m",
            "fusionaudio",
            "music",
            "linear",
            "dataset",
            "autoacd"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Audio-language pretraining holds promise for general-purpose audio understanding, yet remains underexplored compared to its vision counterpart. While vision-language models like CLIP serve as widely adopted foundations, existing audio-language models primarily excel at retrieval tasks with limited adoption as general-purpose encoders.\nWe identify three key barriers: limited large-scale audio-text corpora, insufficient caption diversity, and lack of systematic exploration and evaluation.\nTo this end, we introduce CaptionStew, a 10.7M caption dataset aggregating diverse open-source audio-text corpora across multiple domains and captioning styles.\nUsing this resource, we conduct the first comprehensive evaluation comparing contrastive and captioning objectives for audio representation learning across speech, music, and environmental sound tasks.\nOur results demonstrate that audio-language pretraining yields competitive, transferable representations. Through systematic data-scaling experiments, we reveal complementary objective strengths: contrastive learning achieves superior data efficiency at smaller scales, while captioning demonstrates better scalability on language-involved audio understanding tasks. We also find that common supervised initialization practices provide diminishing returns at scale, challenging current approaches.\nThese findings establish audio-language pretraining as a viable pathway toward general-purpose audio representations, guiding future research. To accelerate progress, we release data preparation recipes, training protocols, and pretrained models, paving the way toward universal audio understanding.\n</p>\n\n",
                "matched_terms": [
                    "audiotext",
                    "caption",
                    "captioning",
                    "audio",
                    "107m",
                    "captionstew",
                    "evaluation",
                    "music",
                    "sound",
                    "speech",
                    "dataset",
                    "audiolanguage",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Representation learning has long been central to audio processing<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>In this work, audio processing refers to audio understanding, speech analysis and music understanding, while excluding automatic speech recognition</span></span></span>, with substantial progress over the past decades.\nEarly advances relied on supervised learning, where models trained on labeled corpora were adapted to related downstream tasks or transferred across domains&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib50\" title=\"\">2020</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib15\" title=\"\">2022a</a>; Snyder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib84\" title=\"\">2018</a>; Desplanques et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib24\" title=\"\">2020</a>)</cite>.\nMore recently, self-supervised learning (SSL) has emerged as the promising paradigm.\nBy pretraining on large-scale unlabeled audio with contrastive objectives or masked modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib40\" title=\"\">2022a</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib46\" title=\"\">2021</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>)</cite>, the resulting models learn rich structural knowledge of audio signals, consistently enhancing performance across many speech and audio benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib96\" title=\"\">2021</a>; Turian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib98\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "audio",
                    "chen",
                    "yang",
                    "music",
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While these techniques have achieved remarkable success, a fundamental limitation persists: existing methods are primarily designed to excel on specific tasks.\nThis domain specificity stems from explicit inductive biases embedded in model architectures and training objectives.\nModels optimized for environmental sounds usually underperform capturing speaker characteristics or paralinguistic information in speech, and vice versa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Turian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>)</cite>.\nAchieving general-purpose audio representations that transfer robustly across diverse audio modalities remains a challenging and actively pursued goal in the field.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An emerging and promising alternative is audio&#8211;language pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>)</cite>, which grounds audio perception with natural language descriptions (e.g. captions).\nIn this framework, text serves as a flexible semantic scaffold, offering supervision potentially spanning multiple levels of granularity, from coarse event categories\nto fine-grained acoustic attributes.\nBy aligning audio with text, audio&#8211;language pretraining provides a unified learning framework for capturing diverse audio information, offering a promising path toward general audio understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib80\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib47\" title=\"\">2025</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>; Su et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib85\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "2024b",
                    "yang",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of vision&#8211;language pretraining underscores this promise.\nModels like CLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib75\" title=\"\">2021</a>)</cite> and AIM-v2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib33\" title=\"\">2025</a>)</cite> not only power vision&#8211;language tasks but also produce representations that benefit a broad range of vision tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib60\" title=\"\">2023</a>; Minderer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib68\" title=\"\">2022</a>; Crowson et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib23\" title=\"\">2022</a>)</cite>.\nIn contrast, audio&#8211;language models have not yet achieved comparable advancements.\nWhile existing models such as CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>)</cite> excel at audio&#8211;text retrieval, their representations have seen limited adoption for broader audio understanding tasks, suggesting fundamental gaps in current approaches.\nWe identify three key challenges that have constrained progress.\nFirst, large-scale, web-mined image&#8211;text corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schuhmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib83\" title=\"\">2022</a>; Gadre et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib35\" title=\"\">2023</a>)</cite> contain billions of pairs, but no comparable resource exists for audio.\nCurrent audio caption datasets barely exceed one million pairs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>)</cite>, often relying on captions synthesized or augmented by large language models, fundamentally limiting the scaling potential of audio&#8211;language models.\nSecond, widely used audio caption corpora focus predominantly on identifying what is presenting in the audio, while lacking coverage of the rich hierarchy of acoustic attributes that define specific audio signals.\nFor instance, captions rarely characterize speaker characteristics (voice timbre, speaking style), musical attributes (harmonic structure, rhythmic patterns), or environmental acoustics (reverberation, background ambiance).\nThis imbalanced focus limits the model&#8217;s ability to learn representations that capture the full spectrum of audio semantics.\nThird, prior work has primarily focused on contrastive learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite> and evaluated on audio&#8211;text retrieval.\nSystematic studies on alternative pretraining objectives (e.g., captioning) and comprehensive evaluations across a wide suite of audio understanding tasks remain scarce, limiting our understanding of what drives effective audio&#8211;language pretraining.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "caption",
                    "mei",
                    "captioning",
                    "audio",
                    "kim",
                    "bai",
                    "musical",
                    "speaker",
                    "drossos",
                    "used",
                    "retrieval",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">CaptionStew</span>, a large-scale aggregation of diverse open-source audio&#8211;text datasets spanning multiple domains and captioning styles, addressing the data scarcity and diversity limitations in current audio-language pretraining.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "captionstew",
                    "audiolanguage",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide the first comprehensive evaluation of audio-language pretraining across diverse tasks and protocols, demonstrating that audio&#8211;language pretraining produces competitive, transferable representations across speech, music, and environmental audio domains.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "evaluation",
                    "music",
                    "speech",
                    "audiolanguage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct the first systematic comparison of contrastive learning and captioning objectives for audio representation learning, revealing that contrastive learning exhibits superior data efficiency while captioning demonstrates better scalability.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "audio",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze key training factors including data scaling effects and supervised pretraining initialization, showing that while AudioSet pretraining provides general benefits, its effects diminish for tasks unrelated to audio event classification and at larger data scales, challenging common practices in the field.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "classification",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Taken together, our results suggests audio&#8211;language pretraining as a practical and competitive approach for learning general-purpose audio representations. To accelerate progress in this direction, we release data preperation recipes, training scripts, evaluation protocols, and pretrained models.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio&#8211;language pretraining learns audio representations by establishing correspondence between audio signals and natural language descriptions. The core objective is to leverage text as structured semantic supervision, enabling models to capture diverse information across speech, music, and environmental sounds within a unified framework.\nAudio&#8211;language models typically employ a two-tower architecture: an audio encoder <math alttext=\"f_{\\text{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mtext>a</mtext></msub><annotation encoding=\"application/x-tex\">f_{\\text{a}}</annotation></semantics></math> that maps raw audio signals into contextual representations, and a text component <math alttext=\"f_{\\text{t}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mtext>t</mtext></msub><annotation encoding=\"application/x-tex\">f_{\\text{t}}</annotation></semantics></math> whose design depends on the training objective.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S2.F1\" title=\"Figure 1 &#8227; 2 Language-audio Pretraining &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we explore two complementary paradigms that differ fundamentally in how they establish audio-text correspondence, contrastive and captioning objective. These approaches represent discriminative and generative perspectives on audio-language alignment, respectively.</p>\n\n",
                "matched_terms": [
                    "audiotext",
                    "audio",
                    "captioning",
                    "music",
                    "speech",
                    "audiolanguage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive Objective</span> is proven to be a robust representation learning method&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib19\" title=\"\">2020b</a>; Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib75\" title=\"\">2021</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>)</cite> and have been a dominant approach for audio-language pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite>.\nThis approach aligns audio and text representations in a shared embedding space by maximizing similarity between paired samples while minimizing similarity between mismatched pairs.\nGiven a batch of paired samples <math alttext=\"\\{(a_{i},t_{i})\\}_{i=1}^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m1\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding=\"application/x-tex\">\\{(a_{i},t_{i})\\}_{i=1}^{N}</annotation></semantics></math>, the audio encoder produces frame- (or patch-) level representations that are pooled and projected to audio embeddings <math alttext=\"\\mathbf{z}^{a}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119859;</mi><mi>i</mi><mi>a</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{z}^{a}_{i}</annotation></semantics></math>, while the text encoder <math alttext=\"f_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">f_{t}</annotation></semantics></math> generates corresponding text embeddings <math alttext=\"\\mathbf{z}^{t}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>&#119859;</mi><mi>i</mi><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{z}^{t}_{i}</annotation></semantics></math>.\nThe symmetric InfoNCE loss&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Oord et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib73\" title=\"\">2018</a>)</cite> is applied to optimize both modalities:</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "audio",
                    "chen",
                    "audiolanguage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Captioning Objective</span> takes a generative approach to audio-language alignment, learning representations by generating textual descriptions from audio.\nWe argue that captioning presents a promising alternative for audio-language pretraining, offering denser token-level supervision compared to contrastive learning and better alignment with recent trends toward general audio understanding systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib38\" title=\"\">2025</a>)</cite>, yet remains underexplored in prior literature.\nGiven an audio signal <math alttext=\"a_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m1\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">a_{i}</annotation></semantics></math>, the encoder <math alttext=\"f_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">f_{a}</annotation></semantics></math> produces contextual representations <math alttext=\"\\mathbf{Z}^{a}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>&#119833;</mi><mi>i</mi><mi>a</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{Z}^{a}_{i}</annotation></semantics></math>, which are fed into a transformer decoder <math alttext=\"g_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m4\" intent=\":literal\"><semantics><msub><mi>g</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">g_{t}</annotation></semantics></math> through cross-attention. Inspired by CapPa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tschannen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib88\" title=\"\">2023</a>)</cite>, we alternate between two decoding modes&#8212;autoregressive and parallel prediction&#8212;to enhance audio encoder representation learning.\nIn the autoregressive decoding, the decoder generates caption tokens <math alttext=\"(y_{1},\\ldots,y_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y_{1},\\ldots,y_{T})</annotation></semantics></math> sequentially, with each token conditioned on the audio representation and previously generated tokens. Training follows the teacher-forcing approach with a cross-entropy loss:</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "audiolanguage",
                    "audio",
                    "caption"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the potential of audio&#8211;language pretraining for general-purpose representation learning, we construct a large-scale and diverse audio caption dataset that addresses key limitations in existing corpora. Audio signals inherently encode information across multiple dimensions&#8212;timbre, pitch, rhythm, semantic events, emotional tone, and acoustic environment&#8212;each amenable to different linguistic descriptions. However, existing large-scale audio caption datasets typically rely on a single generation pipeline, whether human-annotated or LLM-synthesized. While ensuring consistency, this approach inevitably introduces systematic biases in language style and emphasized attributes. Furthermore, single-pipeline captions exhibit limited syntactic diversity and tend to focus on specific audio facets while neglecting complementary aspects.</p>\n\n",
                "matched_terms": [
                    "caption",
                    "audio",
                    "humanannotated",
                    "dataset",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To fully leverage text as a flexible semantic scaffold for diverse audio representation learning, we embrace caption diversity across sources, styles, and descriptive granularities. Rather than creating captions through a single pipeline, we aggregate existing open-source corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib18\" title=\"\">2025</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>; Roy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib79\" title=\"\">2025</a>)</cite>.\nThese datasets span multiple audio domains&#8212;general sound events, expressive speech, and musical performance&#8212;and employ fundamentally different caption creation methodologies. This aggregation yields captions that describe complementary audio aspects with varying granularity, from coarse event categories to fine-grained acoustic attributes.\nPlease refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS2\" title=\"A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a> for detail of each source dataset.\nWhen multiple datasets contain identical audio samples with different captions, we identify these overlaps and consolidate all available captions for each audio file. This multi-caption pairing allows single audio clips to benefit from diverse perspectives and descriptive focuses, enriching the supervision signal.\nTo ensure evaluation integrity, we carefully filter out samples overlapping with development or test sets of downstream benchmarks.</p>\n\n",
                "matched_terms": [
                    "drossos",
                    "caption",
                    "mei",
                    "audio",
                    "bai",
                    "chen",
                    "musical",
                    "dataset",
                    "evaluation",
                    "agostinelli",
                    "event",
                    "sound",
                    "speech",
                    "available",
                    "kim",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The resulting dataset, <span class=\"ltx_text ltx_font_bold\">CaptionStew</span> (denoted by CS10M), contains 9.3 million audio samples paired with 10.7 million captions, spanning 37,290 hours across speech, music, and environmental domains. Compared to existing collections, CaptionStew achieves both greater scale and broader coverage. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S3.T1\" title=\"Table 1 &#8227; 3 CaptionStew Dataset &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a comparison with existing audio caption datasets.</p>\n\n",
                "matched_terms": [
                    "caption",
                    "datasets",
                    "audio",
                    "captionstew",
                    "music",
                    "speech",
                    "dataset",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We pretrain all models on CaptionStew.\nAll audio is resampled to 16 kHz and converted into 80-dimensional log-Mel filterbank features using a 25 ms window length and 10 ms hop size. Text is tokenized with a 50k-vocabulary BPE tokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib54\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "captionstew"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The audio encoder uses a Zipformer-M architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib97\" title=\"\">2024</a>)</cite>, chosen for its efficiency on long sequences and fast convergence.\nZipformer employs six encoder blocks in a U-Net structure that processes sequences at multiple resolutions to capture fine- and coarse-grained temporal information.\nAlthough originally designed for automatic speech recognition, our preliminary experiments confirm Zipformer as a competitive backbone across audio classification tasks (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS3\" title=\"A.3 Zipformer Model &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>).\nFor contrastive pretraining, the text encoder follows BERT-base architecture (12 layers 768 hidden dimensions)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib25\" title=\"\">2019</a>)</cite>.\nFor captioning pretraining, the text decoder adopts the BART-base decoder architecture (6 layers, 768 hidden dimensions)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib54\" title=\"\">2020</a>)</cite>.\nWe use twice as many encoder layers as decoder layers to ensure comparable training speed across objectives.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "audio",
                    "captioning",
                    "classification",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following prior works in audio-language pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>)</cite>, we experiment with two scenarios: training from scratch or initialized from pretrained checkpoints.\nThe audio encoder initializes from a Zipformer-based audio event classifier trained on AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib36\" title=\"\">2017</a>)</cite> with an mAP of 0.46, while text components use corresponding publicly available checkpoints.\nAll models are trained on 8 Tesla V100 GPUs with an effective batch size of 640 seconds of audio per GPU.\nTraining runs for 600k steps from scratch (14 days wall-clock time) or 200k steps if initialized from pretrained checkpoint.</p>\n\n",
                "matched_terms": [
                    "publicly",
                    "mei",
                    "bai",
                    "audio",
                    "event",
                    "map",
                    "available",
                    "audiolanguage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate pretrained audio encoders across three protocols assessing discriminative capabilities, audio-language alignment, and open-formed question answering.\nAll experiments probe frozen representations from the audio encoder&#8217;s final layer to ensure fair model comparison. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S3.T2\" title=\"Table 2 &#8227; 3 CaptionStew Dataset &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS4\" title=\"A.4 Evaluation Datasets &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> details the datasets and metrics for each task.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "task",
                    "audio",
                    "metrics",
                    "answering",
                    "question",
                    "openformed",
                    "audiolanguage",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Linear Probing</span> trains simple linear classifiers on frozen representations.\nFor classification tasks, we experiment with two pooling mechanisms&#8212;mean pooling and multi-head attention pooling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib53\" title=\"\">2019</a>)</cite>&#8212;to obtain clip-level representations.\nWe evaluate across a diverse set of tasks across audio domains, including audio event classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>)</cite>, sound event detection&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib45\" title=\"\">2021</a>)</cite>, speaker-related tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib21\" title=\"\">2018</a>; Cao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib10\" title=\"\">2014</a>)</cite>, and music classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Law et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib52\" title=\"\">2010</a>; Engel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib32\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "chen",
                    "probing",
                    "detection",
                    "classification",
                    "music",
                    "linear",
                    "event",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-language Alignments</span>\nfollow the LiT protocol&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib100\" title=\"\">2022</a>)</cite>, adapting pretrained text components to align with frozen audio representations.\nFor retrieval, we pair audio encoders with pretrained RoBERTa-base text encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib62\" title=\"\">2019</a>)</cite>.\nFor captioning, we use pretrained BART-base decoders&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib54\" title=\"\">2020</a>)</cite>, and only finetune cross-attention layers as we observed more stable training.\nBoth setups finetune on corresponding datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>)</cite> while keeping audio encoders frozen.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "audio",
                    "pair",
                    "captioning",
                    "agostinelli",
                    "kim",
                    "audiolanguage",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Open-formed Question Answering</span>\nAcknowledging the trend of combining audio encoders with large language models (LLMs) for general audio understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib37\" title=\"\">2024</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib42\" title=\"\">2024</a>)</cite>, we connects frozen audio encoders to a LLM (Qwen2.5-7B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib94\" title=\"\">2024a</a>)</cite>) through lightweight adaptors that project audio representations into the LLM&#8217;s embedding space.\nWe train only the adaptor on audio QA datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipping et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib59\" title=\"\">2022</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib61\" title=\"\">2024</a>)</cite> and evaluate on corresponding track in AIR-Bench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>)</cite>.\nDuring training, we carefully monitor instruction-following behavior (<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>99%) to ensure reliable evaluation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "2024b",
                    "yang",
                    "answering",
                    "question",
                    "evaluation",
                    "airbench",
                    "openformed",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recognizing the broad adoption and effectiveness of pretrained audio event classifiers in transfer learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alonso-Jim&#233;nez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib3\" title=\"\">2023</a>; Cappellazzo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib11\" title=\"\">2024</a>)</cite>, audio-language modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>)</cite> and general audio understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib42\" title=\"\">2024</a>; Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib37\" title=\"\">2024</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>)</cite>, we select our pretrained Zipformer-based audio event classifier (described in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.SS1\" title=\"4.1 Implementation Details &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) as the primary baseline.\nIn addition, we compare against representative self-supervised learning (SSL) models, each pretrained under different paradigms and specialized for particular audio domains. BEATs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>)</cite> is an audio SSL model trained with an iterative masked acoustic token prediction framework. Wav2vec 2.0&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>)</cite> learns speech representation by distinguishing target quantized latent representations from disctrators. MERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>)</cite> is a music SSL model trained with masked acoustic modeling, learning to capture acoustic cues and structural information of music.\nTogether, these baselines provide a broad comparative context for studying audio&#8211;language pretraining toward general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "chen",
                    "music",
                    "event",
                    "speech",
                    "audiolanguage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive vs. Captioning Objectives.</span>\nThe two pretraining paradigms exhibit complementary strengths across evaluation protocols.\nOn linear probing tasks, contrastive learning consistently outperforms captioning, particularly excelling at audio event classification and speaker identification.\nHowever, it is worth noting that this gap narrows substantially when the classifier learns to aggregate information across frames through multi-head attention pooling (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS5\" title=\"A.5 Main Results (cont.) &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>).\nThis observation reflects the objectives&#8217; inherent designs: contrastive learning explicitly optimizes for linearly separable clip-level representations, while captioning relies on cross-attention mechanisms over frame-level representations for text sequence generation.\nThis finding aligns with recent work highlighting how downstream module choices significantly impact the assessment of audio representation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zaiem et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib99\" title=\"\">2023</a>)</cite>.\nFor language-involved tasks, both objectives demonstrate competitive performance, with captioning showing slight advantages in open-form question answering across multiple domains.\nThis suggests captioning&#8217;s potential for language-involved audio understanding tasks, aligning with recent trends toward generative audio understanding systems.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "audio",
                    "identification",
                    "probing",
                    "openform",
                    "speaker",
                    "evaluation",
                    "answering",
                    "classification",
                    "question",
                    "event",
                    "linear",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Supervised Initialization.</span> Initializing from supervised pretraining (AS SL) provides substantial benefits across most tasks, with notable improvements on audio event classification, sound event detection and audio-text retreival.\nThe gains are particularly pronounced for contrastive objectives, suggesting that discriminative pretraining provides useful inductive biases for contrastive learning.\nHowever, these benefits diminish (or disappear entirely) when the attributes required for downstream tasks diverge from AudioSet&#8217;s ontology.\nOn speaker identification and music tagging, scratch-trained models often match or exceed initialized variants, indicating that AudioSet&#8217;s focus on distinguishing between sound categories may bias representations toward event-level semantics rather than the acoustic attributes (voice timbre, speaking style) or musical structure (genre, harmony, rhythm) essential for these tasks.\nThese findings challenge common initialization practices for audio-language pretraining and suggest the need for tailored pretraining strategies when targeting general-purpose audio representation learning.</p>\n\n",
                "matched_terms": [
                    "audiotext",
                    "audio",
                    "tagging",
                    "identification",
                    "musical",
                    "detection",
                    "speaker",
                    "classification",
                    "music",
                    "event",
                    "sound",
                    "audiolanguage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Competitive Performance Across Domains.</span> Our audio-language representations achieve strong transferability across diverse audio domains.\nCompared to supervised baselines (Zipformer-AEC), our overall best-performing model (Contrastive-init) demonstrate superior performance on speaker identification, music understanding and audio-text retrieval while maintaining competitiveness on audio-event classification.\nAgainst domain-specialized SSL methods (BEATs, wav2vec2, MERT), our approach consistently shows competitive performance.\nThis consistent cross-domain performance validates our hypothesis that diverse caption aggregation enables broadly transferable representations, establishing audio-language pretraining as a viable path toward learning general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "audiotext",
                    "caption",
                    "audio",
                    "identification",
                    "speaker",
                    "classification",
                    "music",
                    "audiolanguage",
                    "retrieval",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To understand the scalability of audio&#8211;language pretraining, we conduct controlled experiments using CaptionStew subsets at 400K, 1M, 4M, and 10M (whole corpus) audio-text pairs. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.F2\" title=\"Figure 2 &#8227; 4.4 Main Results &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reveals distinct scaling patterns across objectives and evaluation protocols.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "captionstew",
                    "evaluation",
                    "audiotext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scaling Patterns.</span> Most tasks demonstrate consistent performance improvements with increased data scale, validating the potential of large-scale audio-language pretraining.\nHowever, notable exceptions emerge that reveal fundamental limitations of current approaches.\nSound event detection, particularly for models initialized with AudioSet pretraining, exhibits a reverse scaling trend where performance degrades with more caption data.\nThis suggests a potential conflict between natural language supervision&#8211;which typically describes audio characteristics and attributes&#8211;and temporal localization tasks requiring precise event boundaries.\nAdditionally, emotion recognition and instrument classification show weaker scaling gains compared to other tasks, likely reflecting limited caption diversity for these specific attributes in existing corpora, which we will discussed in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.SS6\" title=\"4.6 Dataset Analysis &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "caption",
                    "audio",
                    "detection",
                    "classification",
                    "performance",
                    "event",
                    "instrument",
                    "sound",
                    "audiolanguage",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive vs. Captioning Scaling. </span>\nContrastive learning consistently outperforms captioning at varying data scales, particularly under less data and on discriminative tasks such as audio event classification.\nHowever, captioning demonstrates slightly better scaling properties, with distinct patterns emerging across task categories.\nor language-involved tasks&#8211;especially captioning and question answering&#8211;captioning matches or surpasses contrastive learning at our current 10M-pair scale.\nOn linear probing benchmarks, the gap remains substantial, with scaling trends suggesting captioning would require hundreds of millions of pairs to achieve parity with contrastive methods.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "task",
                    "captioning",
                    "audio",
                    "probing",
                    "question",
                    "classification",
                    "linear",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Initialization at Scale.</span>\nAudioSet initialization provides immediate performance gains but introduces diminishing returns at larger scales. Both contrastive learning and captioning show decreasing benefits from initialization as data scale increases, with scratch and initialized models achieving matched performance at larger scales on some tasks.\nThis suggests that pretrained initialization effectively bootstraps learning at small scales but may constrain the model&#8217;s ability to adapt to the broader semantic space covered by large-scale caption data, potentially due to mismatch between AudioSet&#8217;s ontology and diverse audio descriptions.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "caption",
                    "audio",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, these findings reveal complementary behaviors: contrastive pretraining achieves superior data efficiency at current scales, while captioning shows better scalability, especially for language-involved tasks.\nImportantly, the diminishing returns of initialization at scale indicate that large-scale caption data can provide sufficient semantic supervision independent of domain-specific pretraining, challenging current practices of audio-language pretraining and opening possibilities for learning general-purpose representations from diverse text descriptions alone.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "audiolanguage",
                    "caption"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To understand the linguistic characteristics of CaptionStew, we analyze caption diversity across constituent datasets through visualization and quantitative methods.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.SS6\" title=\"4.6 Dataset Analysis &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4.6</span></a> provides compelling evidence of our aggregation strategy&#8217;s success through t-SNE visualization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Maaten &amp; Hinton, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib65\" title=\"\">2008</a>)</cite> of sentence embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reimers &amp; Gurevych, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib77\" title=\"\">2019</a>)</cite> from sampled captions, revealing distinct clustering patterns by source that demonstrate complementary linguistic perspectives: AudioSetCaps and WavCaps overlap in audio event descriptions and aligns more with human annotated dataset, while JamendoMaxCaps creates a distinct cluster focused on music-specific terminology, and ParaSpeechCaps forms a separate cluster emphasizing speaking styles and paralinguistic attributes.\nThese minimal overlaps confirm that each dataset contributes distinct caption styles and descriptive focuses.</p>\n\n",
                "matched_terms": [
                    "caption",
                    "paraspeechcaps",
                    "wavcaps",
                    "audio",
                    "captionstew",
                    "event",
                    "dataset",
                    "audiosetcaps",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Quantitative analysis reveals both the benefits and limitations (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.T4\" title=\"Table 4 &#8227; 4.6 Dataset Analysis &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).\nCaptionStew achieves substantial vocabulary expansion (56,586 unique words vs. 4,060-27,906 for individual datasets)\nHowever, this growth doesn&#8217;t yield proportional lexical diversity.\nCaptionStew&#8217;s Distinct-n metrics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib55\" title=\"\">2015</a>)</cite> remain low, falling short of image caption dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Changpinyo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib13\" title=\"\">2021</a>)</cite> and text corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Merity et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib67\" title=\"\">2016</a>)</cite>. This constraint stems from datasets with limited linguistic variation, particularly JamendoMaxCaps and ParaSpeechCaps with extremely low Distinct-n scores.</p>\n\n",
                "matched_terms": [
                    "caption",
                    "paraspeechcaps",
                    "metrics",
                    "unique",
                    "captionstew",
                    "words",
                    "dataset",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings highlight that simply combining datasets doesn&#8217;t guarantee improved linguistic diversity, revealing broader limitations in current audio-language pretraining approaches.\nAlso, the constrained diversity in certain aspect may partially explain weaker scaling behavior observed for certain tasks, as models encounter repetitive linguistic patterns despite increased data volume, aligning with vision-language findings on caption diversity&#8217;s importance for representation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Santurkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib82\" title=\"\">2023</a>; Chan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib12\" title=\"\">2022</a>)</cite>.\nThis analysis motivates developing enhanced aggregation pipeline and more diverse caption generation methods to better capture the full spectrum of information in audio signals, thereby fully realizing the potential of large-scale audio-language pretraining.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "audiolanguage",
                    "caption",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Representation Learning.</span>\nThe ultimate goal of audio representation learning is developing a single model suitable for diverse audio understanding tasks.\nSupervised models trained on labeled datasets have been fundamental to the field, including audio event classifiers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib44\" title=\"\">2017</a>; Cramer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib22\" title=\"\">2019</a>; Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib50\" title=\"\">2020</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib39\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib15\" title=\"\">2022a</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib26\" title=\"\">2024</a>)</cite>, speech recognition systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib76\" title=\"\">2023</a>)</cite> and speaker recognition models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Snyder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib84\" title=\"\">2018</a>; Desplanques et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib24\" title=\"\">2020</a>)</cite>.\nThese approaches remain widely adopted due to their strong performance on target tasks. In parallel, self-supervised learning methods have emerged as a complementary approach, offering advances across speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib46\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib16\" title=\"\">2022b</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib6\" title=\"\">2022</a>)</cite>, audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib40\" title=\"\">2022a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib48\" title=\"\">2022</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>; Li &amp; Li, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib56\" title=\"\">2022</a>)</cite>, and music&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib102\" title=\"\">2025</a>)</cite> without requiring labeled data.\nWhile these methods show improved generalization within their target domains, achieving truly general-purpose audio representations remains challenging.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "audio",
                    "chen",
                    "speaker",
                    "music",
                    "event",
                    "speech",
                    "datasets",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio&#8211;Language Pretraining.</span>\nAudio-language models have emerged as a promising approach for learning cross-modal representations. Most existing work focuses on contrastive learning objectives that align audio and text in shared embedding spaces&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite>. Recent extensions have explored combinations with other objectives&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib93\" title=\"\">2023</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib101\" title=\"\">2024</a>; Niizumi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib71\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib72\" title=\"\">2025</a>)</cite>.\nThe field has also witnessed rapid evolution in datasets, transitioning from traditional human-annotated corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>)</cite> to recently constructed LLM-augmented collections&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib18\" title=\"\">2025</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib86\" title=\"\">Sun et&#160;al., </a>)</cite> and domain-specific resources covering speech characteristics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>)</cite>, and musical attributes&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Roy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib79\" title=\"\">2025</a>)</cite>.\nOur work contributes by providing the first systematic comparison between contrastive and captioning objectives, along with comprehensive evaluation toward general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "drossos",
                    "mei",
                    "captioning",
                    "audio",
                    "bai",
                    "chen",
                    "musical",
                    "humanannotated",
                    "evaluation",
                    "llmaugmented",
                    "agostinelli",
                    "speech",
                    "sun",
                    "kim",
                    "audiolanguage",
                    "datasets",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Universal Audio Understanding.</span>\nThe evaluation of audio understanding has evolved from task-specific classification benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib96\" title=\"\">2021</a>; Turian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib98\" title=\"\">2023</a>)</cite> toward more comprehensive assessment frameworks.\nRecent developments have emphasized LLM-based audio understanding systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib37\" title=\"\">2024</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib42\" title=\"\">2024</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib38\" title=\"\">2025</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib20\" title=\"\">2024</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib87\" title=\"\">2024</a>)</cite> that can handle open-form queries and complex reasoning tasks.\nThis shift has driven the development of corresponding evaluation benchmarks that assess models&#8217; abilities across diverse audio understanding scenarios, including question answering, reasoning, and multi-step audio analysis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib80\" title=\"\">2025</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib47\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib64\" title=\"\">2025</a>)</cite>.\nOur work contributes to this trend by providing the first comprehensive evaluation of audio-language pretraining across discriminative tasks, audio-language alignment, and open-form question answering, thereby bridging the gap between traditional representation learning evaluation and modern universal audio understanding.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "2024b",
                    "yang",
                    "answering",
                    "openform",
                    "evaluation",
                    "classification",
                    "question",
                    "audiolanguage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We revisited audio-language pretraining to advance general-purpose audio representation learning. We introduced CaptionStew, a large-scale aggregation of diverse audio caption datasets, and through comprehensive evaluation across tasks, demonstrated that audio-language pretraining produces competitive representations across speech, music, and environmental domains. Our systematic comparison revealed complementary strengths of two audio-language pretraining objectives: contrastive learning excels in data efficiency, while captioning shows better scalability for language-involved tasks.\nData-scaling experiments highlighted diminishing returns of supervised initialization, challenging common practices, while dataset analysis revealed linguistic diversity limitation in current datasets. These findings establish audio-language pretraining as a viable pathway toward universal audio understanding.</p>\n\n",
                "matched_terms": [
                    "caption",
                    "datasets",
                    "audio",
                    "captioning",
                    "evaluation",
                    "captionstew",
                    "music",
                    "speech",
                    "dataset",
                    "audiolanguage",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dataset Construction and Quality.</span> CaptionStew aggregates captions from multiple sources with varying generation methodologies, including LLM-synthesized descriptions that may introduce systematic biases or artifacts.\nWe do not perform extensive quality control or human verification across the aggregated corpus, which could impact model training.\nAdditionally, our dataset analysis reveals that simple aggregation does not guarantee improved linguistic diversity&#8212;CaptionStew&#8217;s lexical diversity metrics remain lower than mature image-text corpora.\nHowever, our design choice prioritizes semantic diversity over linguistic variety, as evidenced by the t-SNE clustering analysis showing distinct descriptive focuses across constituent datasets.\nWhile more sophisticated curation strategies could improve quality, our goal was to establish whether diverse caption aggregation can benefit audio representation learning, which our results support despite these limitations.</p>\n\n",
                "matched_terms": [
                    "caption",
                    "audio",
                    "metrics",
                    "captionstew",
                    "dataset",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited Technical Novelty.</span> Our work primarily combines existing techniques&#8212;contrastive learning, captioning objectives, and dataset aggregation&#8212;rather than introducing fundamentally new methods. The mixed autoregressive/parallel training approach is adapted from vision-language work (CapPa), and our architectural choices follow standard practices.\nWe acknowledge that the technical contributions are largely empirical rather than methodological.\nHowever, this aligns with our primary goal of systematically evaluating audio-language pretraining&#8217;s potential for general-purpose representation learning.\nThe field currently lacks comprehensive comparative studies across objectives, evaluation protocols, and training factors.\nOur systematic analysis reveals important insights about scaling behaviors and initialization effects that have practical implications for practitioners, even if the underlying techniques are not novel.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "evaluation",
                    "dataset",
                    "audiolanguage",
                    "evaluating"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited Model and Data Scalability.</span> Our experiments are constrained to 10M audio-text pairs and relatively modest model sizes compared to state-of-the-art vision-language systems that leverage billions of samples and much larger architectures. This scale limitation may not fully reflect the potential of audio-language pretraining, particularly for the captioning objective which our results suggest benefits from larger-scale training. Additionally, we do not explore recent advances in large language model integration or more sophisticated architectural designs that could improve performance. These constraints stem from computational resource limitations and our focus on controlled comparisons rather than pushing absolute performance boundaries. Future work with larger scales may reveal different scaling dynamics and stronger evidence for general-purpose capabilities.</p>\n\n",
                "matched_terms": [
                    "pairs",
                    "audiotext",
                    "captioning",
                    "audiolanguage",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CaptionStew aggregates eight open-source audio caption datasets to address data scarcity and limited diversity in current audio-language pretraining. The constituent datasets span environmental sounds, music, and expressive speech, with fundamentally different captioning approaches&#8212;from crowdsourced human annotation to expert curation to various LLM-based generation pipelines.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T5\" title=\"Table 5 &#8227; A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T6\" title=\"Table 6 &#8227; A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> detail each dataset&#8217;s characteristics and provide example captions that illustrate the diverse descriptive styles, ranging from concise event descriptions to detailed multi-sentence narratives with fine-grained acoustic and contextual information.\nDuring aggregation, we filter audio samples longer than one minute for computational efficiency and remove samples that overlap with common audio understanding benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>; Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>; Salamon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib81\" title=\"\">2014</a>)</cite> to prevent data leakage. This approach preserves the unique characteristics of each source while creating a unified corpus that captures broader semantic coverage than individual datasets.</p>\n\n",
                "matched_terms": [
                    "caption",
                    "captioning",
                    "audio",
                    "chen",
                    "unique",
                    "captionstew",
                    "music",
                    "agostinelli",
                    "event",
                    "speech",
                    "drossos",
                    "kim",
                    "audiolanguage",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we adopt the Zipformer-M architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib97\" title=\"\">2024</a>)</cite> as the audio encoder, chosen for its memory efficiency on long sequences and strong performance across audio tasks. The architecture employs a U-Net-inspired design with six Transformer stages that process sequences at multiple temporal resolutions. The stages operate at progressively decreasing then increasing frame rates (50, 25, 12.5, 6.25, 12.5, and 25 Hz), with residual and upsampling connections between stages to capture both fine-grained and long-range temporal patterns.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We implement the original 2,2,3,4,3,2 block configuration, where each number indicates the blocks per stage. After processing through all stages, outputs are fused at 25 Hz to produce frame-level embeddings. The model incorporates several architectural improvements from the original work: BiasNorm for gradient stability over long sequences, Swoosh activation functions for better convergence, and compatibility with the ScaledAdam optimizer. The resulting embeddings are 768-dimensional and used consistently across all downstream evaluation tasks.</p>\n\n",
                "matched_terms": [
                    "used",
                    "evaluation",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although Zipformer was originally designed for automatic speech recognition, we conducted preliminary experiments to validate its effectiveness as a general audio encoder across diverse domains.\nAs in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T7\" title=\"Table 7 &#8227; A.3 Zipformer Model &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, our initial studies confirmed that Zipformer achieves competitive performance on environmental sound classification, music understanding, and speaker-related tasks, demonstrating its suitability as a unified backbone for multi-domain audio representation learning. This cross-domain efficacy makes it an appropriate choice for our audio-language pretraining experiments that span speech, music, and environmental audio.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "audio",
                    "classification",
                    "speech",
                    "music",
                    "sound",
                    "audiolanguage",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T8\" title=\"Table 8 &#8227; A.4 Evaluation Datasets &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> details the evaluation datasets and their metrics used for assessing audio representation quality across our three evaluation protocols: linear probing&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Fonseca et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>); Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib21\" title=\"\">2018</a>); Cao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib10\" title=\"\">2014</a>); Law et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib52\" title=\"\">2010</a>); Engel et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib32\" title=\"\">2017</a>); Hershey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib45\" title=\"\">2021</a>); Ebbers et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib30\" title=\"\">2022</a>)</cite>, audio-language alignment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>); Diwan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>); Agostinelli et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib58\" title=\"\">2004</a>)</cite> and open-form question answering&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lipping et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib59\" title=\"\">2022</a>); Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib61\" title=\"\">2024</a>); Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "audio",
                    "2024b",
                    "chen",
                    "yang",
                    "metrics",
                    "probing",
                    "answering",
                    "openform",
                    "evaluation",
                    "question",
                    "linear",
                    "agostinelli",
                    "kim",
                    "audiolanguage",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table 9 presents linear probing results when using multi-head attention pooling instead of mean pooling.\nWith learned attention pooling, the performance gap between contrastive and captioning objectives narrows substantially, particularly evident on speaker identification where captioning-scratch achieves 72.86% compared to 46.67% with mean pooling (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.T3\" title=\"Table 3 &#8227; 4.4 Main Results &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).\nThis demonstrates that captioning models benefit significantly from adaptive pooling mechanisms, while contrastive learning&#8217;s explicit optimization for clip-level representations shows less sensitivity to pooling strategy.\nThese results underscore the critical importance of appropriate downstream module selection when evaluating different pretraining paradigms, as the choice of pooling mechanism can dramatically influence conclusions about objective effectiveness.\nThe improved performance across all methods with attention pooling also suggests that frame-level representations from both objectives contain rich information that can be better exploited through learned aggregation. SOTA results and SSL baseline results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.T3\" title=\"Table 3 &#8227; 4.4 Main Results &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T9\" title=\"Table 9 &#8227; A.5 Main Results (cont.) &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> are quoted collectively from &#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Niizumi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib72\" title=\"\">2025</a>); Turian et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>); Li &amp; Li (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib56\" title=\"\">2022</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib90\" title=\"\">2022</a>); Bharadwaj et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib8\" title=\"\">2025</a>); Gong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib41\" title=\"\">2022b</a>); Lanzend&#246;rfer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib51\" title=\"\">2025</a>); Bai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>); Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "evaluating",
                    "captioning",
                    "2024b",
                    "bai",
                    "probing",
                    "yang",
                    "identification",
                    "speaker",
                    "linear",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Aside from learning representations, we also compare against state-of-the-art audio-text retrieval models to assess our approach&#8217;s performance on the specific task it was designed for. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T10\" title=\"Table 10 &#8227; A.6 Additional Results &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> presents retrieval results for our best-performing model (Contrastive-init) against state-of-the-art\naudio-text retrieval model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>)</cite>.\nOur model achieving comparable or superior results on benchmarks in various audio domains, with particularly strong performance on speech and music retrieval.\nThe results indicate that our general-purpose audio-language pretraining approach can compete with specialized retrieval models while offering broader applicability across diverse usage scenarios.</p>\n\n",
                "matched_terms": [
                    "audiotext",
                    "task",
                    "bai",
                    "audio",
                    "music",
                    "speech",
                    "audiolanguage",
                    "retrieval",
                    "performance"
                ]
            }
        ]
    },
    "S4.T3.st1": {
        "caption": "(a) Linear Probing (with mean pooling)",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\">Method</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"3\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\">Model</span>\n<span class=\"ltx_p\">Initialization</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"3\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\">Audio-language</span>\n<span class=\"ltx_p\">Pretraining</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"7\"><span class=\"ltx_text ltx_font_bold\">linear probing</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AEC</span></span>\n<span class=\"ltx_p\">FSD50k</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AEC</span></span>\n<span class=\"ltx_p\">VggSound</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SID</span></span>\n<span class=\"ltx_p\">VoxCeleb2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SER</span></span>\n<span class=\"ltx_p\">CREMA</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MTAG</span></span>\n<span class=\"ltx_p\">MagnaTagATune</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">INST</span></span>\n<span class=\"ltx_p\">NSynth</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SED</span></span>\n<span class=\"ltx_p\">AS-Strong</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Existing SSL Models</span></th>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">BEATs&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">SSL</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">0.565<sup class=\"ltx_sup\">&#8224;</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">0.400<sup class=\"ltx_sup\">&#8224;</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">75.90</span><sup class=\"ltx_sup\">&#8224;</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.034<sup class=\"ltx_sup\">&#8224;</sup>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">wav2vec 2.0&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Baevski et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">SSL</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">0.342<sup class=\"ltx_sup\">&#8224;</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">51.60</span></td>\n<td class=\"ltx_td ltx_align_center\">56.10</td>\n<td class=\"ltx_td ltx_align_center\">0.317<sup class=\"ltx_sup\">&#8224;</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">40.20<sup class=\"ltx_sup\">&#8224;</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">SSL</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">0.402<sup class=\"ltx_sup\">&#8224;</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">72.60<sup class=\"ltx_sup\">&#8224;</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Our Supervised Baselines</span></th>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Zipformer-AEC&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Yao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib97\" title=\"\">2024</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">AS SL</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">0.656</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">56.46</span></td>\n<td class=\"ltx_td ltx_align_center\">18.84</td>\n<td class=\"ltx_td ltx_align_center\">67.14</td>\n<td class=\"ltx_td ltx_align_center\">0.407</td>\n<td class=\"ltx_td ltx_align_center\">67.19</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.216</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Our Audio-language Pretrained Models</span></th>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Contrastive-<span class=\"ltx_text ltx_font_italic\">scratch</span>\n</th>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">CS10M</td>\n<td class=\"ltx_td ltx_align_center\">0.625</td>\n<td class=\"ltx_td ltx_align_center\">50.87</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">46.67</span></td>\n<td class=\"ltx_td ltx_align_center\">67.71</td>\n<td class=\"ltx_td ltx_align_center\">0.406</td>\n<td class=\"ltx_td ltx_align_center\">67.30</td>\n<td class=\"ltx_td ltx_align_center\">0.132</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Captioning-<span class=\"ltx_text ltx_font_italic\">scratch</span>\n</th>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">CS10M</td>\n<td class=\"ltx_td ltx_align_center\">0.580</td>\n<td class=\"ltx_td ltx_align_center\">47.79</td>\n<td class=\"ltx_td ltx_align_center\">33.43</td>\n<td class=\"ltx_td ltx_align_center\">63.60</td>\n<td class=\"ltx_td ltx_align_center\">0.401</td>\n<td class=\"ltx_td ltx_align_center\">63.10</td>\n<td class=\"ltx_td ltx_align_center\">0.124</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Contrastive-<span class=\"ltx_text ltx_font_italic\">init</span>\n</th>\n<td class=\"ltx_td ltx_align_center\">AS SL</td>\n<td class=\"ltx_td ltx_align_center\">CS10M</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">0.664</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">54.70</span></td>\n<td class=\"ltx_td ltx_align_center\">38.17</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">68.84</span></td>\n<td class=\"ltx_td ltx_align_center\">0.406</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">69.38</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.187</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Captioning-<span class=\"ltx_text ltx_font_italic\">init</span>\n</th>\n<td class=\"ltx_td ltx_align_center\">AS SL</td>\n<td class=\"ltx_td ltx_align_center\">CS10M</td>\n<td class=\"ltx_td ltx_align_center\">0.652</td>\n<td class=\"ltx_td ltx_align_center\">53.13</td>\n<td class=\"ltx_td ltx_align_center\">26.23</td>\n<td class=\"ltx_td ltx_align_center\">65.86</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">0.410</span></td>\n<td class=\"ltx_td ltx_align_center\">67.16</td>\n<td class=\"ltx_td ltx_align_center\">0.145</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_tt\">SOTA<sup class=\"ltx_sup\">&#8225;</sup>\n</th>\n<td class=\"ltx_td ltx_border_bb ltx_border_tt\"/>\n<td class=\"ltx_td ltx_border_bb ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">0.655</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">59.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">96.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">&#8211;<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;&#8224;</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">0.414</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">79.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">0.374</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "crema",
            "beats",
            "aec",
            "0400†",
            "initialization",
            "7590†",
            "0317†",
            "baselines",
            "7260†",
            "baevski",
            "supervised",
            "0034†",
            "0342†",
            "our",
            "0565†",
            "fsd50k",
            "vggsound",
            "4020†",
            "–††",
            "nsynth",
            "zipformeraec",
            "pretraining",
            "sed",
            "pooling",
            "contrastivescratch",
            "captioningscratch",
            "captioninginit",
            "voxceleb2",
            "sota‡",
            "wav2vec",
            "mean",
            "mtag",
            "asstrong",
            "sid",
            "magnatagatune",
            "audiolanguage",
            "yao",
            "pretrained",
            "existing",
            "inst",
            "ser",
            "chen",
            "probing",
            "model",
            "mert",
            "cs10m",
            "linear",
            "contrastiveinit",
            "method",
            "ssl",
            "0402†"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Audio-language pretraining holds promise for general-purpose audio understanding, yet remains underexplored compared to its vision counterpart. While vision-language models like CLIP serve as widely adopted foundations, existing audio-language models primarily excel at retrieval tasks with limited adoption as general-purpose encoders.\nWe identify three key barriers: limited large-scale audio-text corpora, insufficient caption diversity, and lack of systematic exploration and evaluation.\nTo this end, we introduce CaptionStew, a 10.7M caption dataset aggregating diverse open-source audio-text corpora across multiple domains and captioning styles.\nUsing this resource, we conduct the first comprehensive evaluation comparing contrastive and captioning objectives for audio representation learning across speech, music, and environmental sound tasks.\nOur results demonstrate that audio-language pretraining yields competitive, transferable representations. Through systematic data-scaling experiments, we reveal complementary objective strengths: contrastive learning achieves superior data efficiency at smaller scales, while captioning demonstrates better scalability on language-involved audio understanding tasks. We also find that common supervised initialization practices provide diminishing returns at scale, challenging current approaches.\nThese findings establish audio-language pretraining as a viable pathway toward general-purpose audio representations, guiding future research. To accelerate progress, we release data preparation recipes, training protocols, and pretrained models, paving the way toward universal audio understanding.\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "pretrained",
                    "existing",
                    "initialization",
                    "pretraining",
                    "supervised",
                    "audiolanguage",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Representation learning has long been central to audio processing<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>In this work, audio processing refers to audio understanding, speech analysis and music understanding, while excluding automatic speech recognition</span></span></span>, with substantial progress over the past decades.\nEarly advances relied on supervised learning, where models trained on labeled corpora were adapted to related downstream tasks or transferred across domains&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib50\" title=\"\">2020</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib15\" title=\"\">2022a</a>; Snyder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib84\" title=\"\">2018</a>; Desplanques et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib24\" title=\"\">2020</a>)</cite>.\nMore recently, self-supervised learning (SSL) has emerged as the promising paradigm.\nBy pretraining on large-scale unlabeled audio with contrastive objectives or masked modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib40\" title=\"\">2022a</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib46\" title=\"\">2021</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>)</cite>, the resulting models learn rich structural knowledge of audio signals, consistently enhancing performance across many speech and audio benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib96\" title=\"\">2021</a>; Turian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib98\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "chen",
                    "supervised",
                    "pretraining",
                    "ssl",
                    "baevski"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While these techniques have achieved remarkable success, a fundamental limitation persists: existing methods are primarily designed to excel on specific tasks.\nThis domain specificity stems from explicit inductive biases embedded in model architectures and training objectives.\nModels optimized for environmental sounds usually underperform capturing speaker characteristics or paralinguistic information in speech, and vice versa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Turian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>)</cite>.\nAchieving general-purpose audio representations that transfer robustly across diverse audio modalities remains a challenging and actively pursued goal in the field.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "existing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of vision&#8211;language pretraining underscores this promise.\nModels like CLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib75\" title=\"\">2021</a>)</cite> and AIM-v2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib33\" title=\"\">2025</a>)</cite> not only power vision&#8211;language tasks but also produce representations that benefit a broad range of vision tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib60\" title=\"\">2023</a>; Minderer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib68\" title=\"\">2022</a>; Crowson et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib23\" title=\"\">2022</a>)</cite>.\nIn contrast, audio&#8211;language models have not yet achieved comparable advancements.\nWhile existing models such as CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>)</cite> excel at audio&#8211;text retrieval, their representations have seen limited adoption for broader audio understanding tasks, suggesting fundamental gaps in current approaches.\nWe identify three key challenges that have constrained progress.\nFirst, large-scale, web-mined image&#8211;text corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schuhmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib83\" title=\"\">2022</a>; Gadre et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib35\" title=\"\">2023</a>)</cite> contain billions of pairs, but no comparable resource exists for audio.\nCurrent audio caption datasets barely exceed one million pairs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>)</cite>, often relying on captions synthesized or augmented by large language models, fundamentally limiting the scaling potential of audio&#8211;language models.\nSecond, widely used audio caption corpora focus predominantly on identifying what is presenting in the audio, while lacking coverage of the rich hierarchy of acoustic attributes that define specific audio signals.\nFor instance, captions rarely characterize speaker characteristics (voice timbre, speaking style), musical attributes (harmonic structure, rhythmic patterns), or environmental acoustics (reverberation, background ambiance).\nThis imbalanced focus limits the model&#8217;s ability to learn representations that capture the full spectrum of audio semantics.\nThird, prior work has primarily focused on contrastive learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite> and evaluated on audio&#8211;text retrieval.\nSystematic studies on alternative pretraining objectives (e.g., captioning) and comprehensive evaluations across a wide suite of audio understanding tasks remain scarce, limiting our understanding of what drives effective audio&#8211;language pretraining.</p>\n\n",
                "matched_terms": [
                    "models",
                    "pretraining",
                    "existing",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we revisit audio&#8211;language pretraining with the goal of reestablishing its viability as a pathway toward general-purpose audio representation learning. Our contributions are:</p>\n\n",
                "matched_terms": [
                    "pretraining",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">CaptionStew</span>, a large-scale aggregation of diverse open-source audio&#8211;text datasets spanning multiple domains and captioning styles, addressing the data scarcity and diversity limitations in current audio-language pretraining.</p>\n\n",
                "matched_terms": [
                    "audiolanguage",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide the first comprehensive evaluation of audio-language pretraining across diverse tasks and protocols, demonstrating that audio&#8211;language pretraining produces competitive, transferable representations across speech, music, and environmental audio domains.</p>\n\n",
                "matched_terms": [
                    "audiolanguage",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze key training factors including data scaling effects and supervised pretraining initialization, showing that while AudioSet pretraining provides general benefits, its effects diminish for tasks unrelated to audio event classification and at larger data scales, challenging common practices in the field.</p>\n\n",
                "matched_terms": [
                    "supervised",
                    "pretraining",
                    "initialization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Taken together, our results suggests audio&#8211;language pretraining as a practical and competitive approach for learning general-purpose audio representations. To accelerate progress in this direction, we release data preperation recipes, training scripts, evaluation protocols, and pretrained models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "pretrained",
                    "pretraining",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio&#8211;language pretraining learns audio representations by establishing correspondence between audio signals and natural language descriptions. The core objective is to leverage text as structured semantic supervision, enabling models to capture diverse information across speech, music, and environmental sounds within a unified framework.\nAudio&#8211;language models typically employ a two-tower architecture: an audio encoder <math alttext=\"f_{\\text{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mtext>a</mtext></msub><annotation encoding=\"application/x-tex\">f_{\\text{a}}</annotation></semantics></math> that maps raw audio signals into contextual representations, and a text component <math alttext=\"f_{\\text{t}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mtext>t</mtext></msub><annotation encoding=\"application/x-tex\">f_{\\text{t}}</annotation></semantics></math> whose design depends on the training objective.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S2.F1\" title=\"Figure 1 &#8227; 2 Language-audio Pretraining &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we explore two complementary paradigms that differ fundamentally in how they establish audio-text correspondence, contrastive and captioning objective. These approaches represent discriminative and generative perspectives on audio-language alignment, respectively.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audiolanguage",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive Objective</span> is proven to be a robust representation learning method&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib19\" title=\"\">2020b</a>; Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib75\" title=\"\">2021</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>)</cite> and have been a dominant approach for audio-language pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite>.\nThis approach aligns audio and text representations in a shared embedding space by maximizing similarity between paired samples while minimizing similarity between mismatched pairs.\nGiven a batch of paired samples <math alttext=\"\\{(a_{i},t_{i})\\}_{i=1}^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m1\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding=\"application/x-tex\">\\{(a_{i},t_{i})\\}_{i=1}^{N}</annotation></semantics></math>, the audio encoder produces frame- (or patch-) level representations that are pooled and projected to audio embeddings <math alttext=\"\\mathbf{z}^{a}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119859;</mi><mi>i</mi><mi>a</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{z}^{a}_{i}</annotation></semantics></math>, while the text encoder <math alttext=\"f_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">f_{t}</annotation></semantics></math> generates corresponding text embeddings <math alttext=\"\\mathbf{z}^{t}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>&#119859;</mi><mi>i</mi><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{z}^{t}_{i}</annotation></semantics></math>.\nThe symmetric InfoNCE loss&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Oord et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib73\" title=\"\">2018</a>)</cite> is applied to optimize both modalities:</p>\n\n",
                "matched_terms": [
                    "chen",
                    "pretraining",
                    "method",
                    "baevski",
                    "audiolanguage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Captioning Objective</span> takes a generative approach to audio-language alignment, learning representations by generating textual descriptions from audio.\nWe argue that captioning presents a promising alternative for audio-language pretraining, offering denser token-level supervision compared to contrastive learning and better alignment with recent trends toward general audio understanding systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib38\" title=\"\">2025</a>)</cite>, yet remains underexplored in prior literature.\nGiven an audio signal <math alttext=\"a_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m1\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">a_{i}</annotation></semantics></math>, the encoder <math alttext=\"f_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">f_{a}</annotation></semantics></math> produces contextual representations <math alttext=\"\\mathbf{Z}^{a}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>&#119833;</mi><mi>i</mi><mi>a</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{Z}^{a}_{i}</annotation></semantics></math>, which are fed into a transformer decoder <math alttext=\"g_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m4\" intent=\":literal\"><semantics><msub><mi>g</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">g_{t}</annotation></semantics></math> through cross-attention. Inspired by CapPa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tschannen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib88\" title=\"\">2023</a>)</cite>, we alternate between two decoding modes&#8212;autoregressive and parallel prediction&#8212;to enhance audio encoder representation learning.\nIn the autoregressive decoding, the decoder generates caption tokens <math alttext=\"(y_{1},\\ldots,y_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y_{1},\\ldots,y_{T})</annotation></semantics></math> sequentially, with each token conditioned on the audio representation and previously generated tokens. Training follows the teacher-forcing approach with a cross-entropy loss:</p>\n\n",
                "matched_terms": [
                    "audiolanguage",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the potential of audio&#8211;language pretraining for general-purpose representation learning, we construct a large-scale and diverse audio caption dataset that addresses key limitations in existing corpora. Audio signals inherently encode information across multiple dimensions&#8212;timbre, pitch, rhythm, semantic events, emotional tone, and acoustic environment&#8212;each amenable to different linguistic descriptions. However, existing large-scale audio caption datasets typically rely on a single generation pipeline, whether human-annotated or LLM-synthesized. While ensuring consistency, this approach inevitably introduces systematic biases in language style and emphasized attributes. Furthermore, single-pipeline captions exhibit limited syntactic diversity and tend to focus on specific audio facets while neglecting complementary aspects.</p>\n\n",
                "matched_terms": [
                    "pretraining",
                    "existing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To fully leverage text as a flexible semantic scaffold for diverse audio representation learning, we embrace caption diversity across sources, styles, and descriptive granularities. Rather than creating captions through a single pipeline, we aggregate existing open-source corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib18\" title=\"\">2025</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>; Roy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib79\" title=\"\">2025</a>)</cite>.\nThese datasets span multiple audio domains&#8212;general sound events, expressive speech, and musical performance&#8212;and employ fundamentally different caption creation methodologies. This aggregation yields captions that describe complementary audio aspects with varying granularity, from coarse event categories to fine-grained acoustic attributes.\nPlease refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS2\" title=\"A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a> for detail of each source dataset.\nWhen multiple datasets contain identical audio samples with different captions, we identify these overlaps and consolidate all available captions for each audio file. This multi-caption pairing allows single audio clips to benefit from diverse perspectives and descriptive focuses, enriching the supervision signal.\nTo ensure evaluation integrity, we carefully filter out samples overlapping with development or test sets of downstream benchmarks.</p>\n\n",
                "matched_terms": [
                    "chen",
                    "existing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The resulting dataset, <span class=\"ltx_text ltx_font_bold\">CaptionStew</span> (denoted by CS10M), contains 9.3 million audio samples paired with 10.7 million captions, spanning 37,290 hours across speech, music, and environmental domains. Compared to existing collections, CaptionStew achieves both greater scale and broader coverage. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S3.T1\" title=\"Table 1 &#8227; 3 CaptionStew Dataset &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a comparison with existing audio caption datasets.</p>\n\n",
                "matched_terms": [
                    "cs10m",
                    "existing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The audio encoder uses a Zipformer-M architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib97\" title=\"\">2024</a>)</cite>, chosen for its efficiency on long sequences and fast convergence.\nZipformer employs six encoder blocks in a U-Net structure that processes sequences at multiple resolutions to capture fine- and coarse-grained temporal information.\nAlthough originally designed for automatic speech recognition, our preliminary experiments confirm Zipformer as a competitive backbone across audio classification tasks (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS3\" title=\"A.3 Zipformer Model &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>).\nFor contrastive pretraining, the text encoder follows BERT-base architecture (12 layers 768 hidden dimensions)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib25\" title=\"\">2019</a>)</cite>.\nFor captioning pretraining, the text decoder adopts the BART-base decoder architecture (6 layers, 768 hidden dimensions)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib54\" title=\"\">2020</a>)</cite>.\nWe use twice as many encoder layers as decoder layers to ensure comparable training speed across objectives.</p>\n\n",
                "matched_terms": [
                    "pretraining",
                    "yao",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following prior works in audio-language pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>)</cite>, we experiment with two scenarios: training from scratch or initialized from pretrained checkpoints.\nThe audio encoder initializes from a Zipformer-based audio event classifier trained on AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib36\" title=\"\">2017</a>)</cite> with an mAP of 0.46, while text components use corresponding publicly available checkpoints.\nAll models are trained on 8 Tesla V100 GPUs with an effective batch size of 640 seconds of audio per GPU.\nTraining runs for 600k steps from scratch (14 days wall-clock time) or 200k steps if initialized from pretrained checkpoint.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audiolanguage",
                    "pretraining",
                    "pretrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate pretrained audio encoders across three protocols assessing discriminative capabilities, audio-language alignment, and open-formed question answering.\nAll experiments probe frozen representations from the audio encoder&#8217;s final layer to ensure fair model comparison. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S3.T2\" title=\"Table 2 &#8227; 3 CaptionStew Dataset &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS4\" title=\"A.4 Evaluation Datasets &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> details the datasets and metrics for each task.</p>\n\n",
                "matched_terms": [
                    "pretrained",
                    "audiolanguage",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Linear Probing</span> trains simple linear classifiers on frozen representations.\nFor classification tasks, we experiment with two pooling mechanisms&#8212;mean pooling and multi-head attention pooling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib53\" title=\"\">2019</a>)</cite>&#8212;to obtain clip-level representations.\nWe evaluate across a diverse set of tasks across audio domains, including audio event classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>)</cite>, sound event detection&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib45\" title=\"\">2021</a>)</cite>, speaker-related tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib21\" title=\"\">2018</a>; Cao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib10\" title=\"\">2014</a>)</cite>, and music classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Law et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib52\" title=\"\">2010</a>; Engel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib32\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "chen",
                    "linear",
                    "probing",
                    "pooling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-language Alignments</span>\nfollow the LiT protocol&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib100\" title=\"\">2022</a>)</cite>, adapting pretrained text components to align with frozen audio representations.\nFor retrieval, we pair audio encoders with pretrained RoBERTa-base text encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib62\" title=\"\">2019</a>)</cite>.\nFor captioning, we use pretrained BART-base decoders&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib54\" title=\"\">2020</a>)</cite>, and only finetune cross-attention layers as we observed more stable training.\nBoth setups finetune on corresponding datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>)</cite> while keeping audio encoders frozen.</p>\n\n",
                "matched_terms": [
                    "pretrained",
                    "audiolanguage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recognizing the broad adoption and effectiveness of pretrained audio event classifiers in transfer learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alonso-Jim&#233;nez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib3\" title=\"\">2023</a>; Cappellazzo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib11\" title=\"\">2024</a>)</cite>, audio-language modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>)</cite> and general audio understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib42\" title=\"\">2024</a>; Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib37\" title=\"\">2024</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>)</cite>, we select our pretrained Zipformer-based audio event classifier (described in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.SS1\" title=\"4.1 Implementation Details &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) as the primary baseline.\nIn addition, we compare against representative self-supervised learning (SSL) models, each pretrained under different paradigms and specialized for particular audio domains. BEATs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>)</cite> is an audio SSL model trained with an iterative masked acoustic token prediction framework. Wav2vec 2.0&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>)</cite> learns speech representation by distinguishing target quantized latent representations from disctrators. MERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>)</cite> is a music SSL model trained with masked acoustic modeling, learning to capture acoustic cues and structural information of music.\nTogether, these baselines provide a broad comparative context for studying audio&#8211;language pretraining toward general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "pretrained",
                    "beats",
                    "chen",
                    "model",
                    "mert",
                    "baselines",
                    "wav2vec",
                    "pretraining",
                    "ssl",
                    "baevski",
                    "audiolanguage",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present our evaluation results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.T3\" title=\"Table 3 &#8227; 4.4 Main Results &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Our analysis reveals key insights about objective design, representation quality, and the role of initialization.</p>\n\n",
                "matched_terms": [
                    "initialization",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive vs. Captioning Objectives.</span>\nThe two pretraining paradigms exhibit complementary strengths across evaluation protocols.\nOn linear probing tasks, contrastive learning consistently outperforms captioning, particularly excelling at audio event classification and speaker identification.\nHowever, it is worth noting that this gap narrows substantially when the classifier learns to aggregate information across frames through multi-head attention pooling (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS5\" title=\"A.5 Main Results (cont.) &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>).\nThis observation reflects the objectives&#8217; inherent designs: contrastive learning explicitly optimizes for linearly separable clip-level representations, while captioning relies on cross-attention mechanisms over frame-level representations for text sequence generation.\nThis finding aligns with recent work highlighting how downstream module choices significantly impact the assessment of audio representation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zaiem et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib99\" title=\"\">2023</a>)</cite>.\nFor language-involved tasks, both objectives demonstrate competitive performance, with captioning showing slight advantages in open-form question answering across multiple domains.\nThis suggests captioning&#8217;s potential for language-involved audio understanding tasks, aligning with recent trends toward generative audio understanding systems.</p>\n\n",
                "matched_terms": [
                    "pretraining",
                    "probing",
                    "pooling",
                    "linear"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Supervised Initialization.</span> Initializing from supervised pretraining (AS SL) provides substantial benefits across most tasks, with notable improvements on audio event classification, sound event detection and audio-text retreival.\nThe gains are particularly pronounced for contrastive objectives, suggesting that discriminative pretraining provides useful inductive biases for contrastive learning.\nHowever, these benefits diminish (or disappear entirely) when the attributes required for downstream tasks diverge from AudioSet&#8217;s ontology.\nOn speaker identification and music tagging, scratch-trained models often match or exceed initialized variants, indicating that AudioSet&#8217;s focus on distinguishing between sound categories may bias representations toward event-level semantics rather than the acoustic attributes (voice timbre, speaking style) or musical structure (genre, harmony, rhythm) essential for these tasks.\nThese findings challenge common initialization practices for audio-language pretraining and suggest the need for tailored pretraining strategies when targeting general-purpose audio representation learning.</p>\n\n",
                "matched_terms": [
                    "models",
                    "initialization",
                    "pretraining",
                    "supervised",
                    "audiolanguage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Competitive Performance Across Domains.</span> Our audio-language representations achieve strong transferability across diverse audio domains.\nCompared to supervised baselines (Zipformer-AEC), our overall best-performing model (Contrastive-init) demonstrate superior performance on speaker identification, music understanding and audio-text retrieval while maintaining competitiveness on audio-event classification.\nAgainst domain-specialized SSL methods (BEATs, wav2vec2, MERT), our approach consistently shows competitive performance.\nThis consistent cross-domain performance validates our hypothesis that diverse caption aggregation enables broadly transferable representations, establishing audio-language pretraining as a viable path toward learning general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "beats",
                    "model",
                    "mert",
                    "baselines",
                    "zipformeraec",
                    "pretraining",
                    "contrastiveinit",
                    "ssl",
                    "supervised",
                    "audiolanguage",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scaling Patterns.</span> Most tasks demonstrate consistent performance improvements with increased data scale, validating the potential of large-scale audio-language pretraining.\nHowever, notable exceptions emerge that reveal fundamental limitations of current approaches.\nSound event detection, particularly for models initialized with AudioSet pretraining, exhibits a reverse scaling trend where performance degrades with more caption data.\nThis suggests a potential conflict between natural language supervision&#8211;which typically describes audio characteristics and attributes&#8211;and temporal localization tasks requiring precise event boundaries.\nAdditionally, emotion recognition and instrument classification show weaker scaling gains compared to other tasks, likely reflecting limited caption diversity for these specific attributes in existing corpora, which we will discussed in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.SS6\" title=\"4.6 Dataset Analysis &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audiolanguage",
                    "pretraining",
                    "existing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive vs. Captioning Scaling. </span>\nContrastive learning consistently outperforms captioning at varying data scales, particularly under less data and on discriminative tasks such as audio event classification.\nHowever, captioning demonstrates slightly better scaling properties, with distinct patterns emerging across task categories.\nor language-involved tasks&#8211;especially captioning and question answering&#8211;captioning matches or surpasses contrastive learning at our current 10M-pair scale.\nOn linear probing benchmarks, the gap remains substantial, with scaling trends suggesting captioning would require hundreds of millions of pairs to achieve parity with contrastive methods.</p>\n\n",
                "matched_terms": [
                    "linear",
                    "probing",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Initialization at Scale.</span>\nAudioSet initialization provides immediate performance gains but introduces diminishing returns at larger scales. Both contrastive learning and captioning show decreasing benefits from initialization as data scale increases, with scratch and initialized models achieving matched performance at larger scales on some tasks.\nThis suggests that pretrained initialization effectively bootstraps learning at small scales but may constrain the model&#8217;s ability to adapt to the broader semantic space covered by large-scale caption data, potentially due to mismatch between AudioSet&#8217;s ontology and diverse audio descriptions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "pretrained",
                    "initialization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, these findings reveal complementary behaviors: contrastive pretraining achieves superior data efficiency at current scales, while captioning shows better scalability, especially for language-involved tasks.\nImportantly, the diminishing returns of initialization at scale indicate that large-scale caption data can provide sufficient semantic supervision independent of domain-specific pretraining, challenging current practices of audio-language pretraining and opening possibilities for learning general-purpose representations from diverse text descriptions alone.</p>\n\n",
                "matched_terms": [
                    "audiolanguage",
                    "pretraining",
                    "initialization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings highlight that simply combining datasets doesn&#8217;t guarantee improved linguistic diversity, revealing broader limitations in current audio-language pretraining approaches.\nAlso, the constrained diversity in certain aspect may partially explain weaker scaling behavior observed for certain tasks, as models encounter repetitive linguistic patterns despite increased data volume, aligning with vision-language findings on caption diversity&#8217;s importance for representation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Santurkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib82\" title=\"\">2023</a>; Chan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib12\" title=\"\">2022</a>)</cite>.\nThis analysis motivates developing enhanced aggregation pipeline and more diverse caption generation methods to better capture the full spectrum of information in audio signals, thereby fully realizing the potential of large-scale audio-language pretraining.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audiolanguage",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Representation Learning.</span>\nThe ultimate goal of audio representation learning is developing a single model suitable for diverse audio understanding tasks.\nSupervised models trained on labeled datasets have been fundamental to the field, including audio event classifiers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib44\" title=\"\">2017</a>; Cramer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib22\" title=\"\">2019</a>; Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib50\" title=\"\">2020</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib39\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib15\" title=\"\">2022a</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib26\" title=\"\">2024</a>)</cite>, speech recognition systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib76\" title=\"\">2023</a>)</cite> and speaker recognition models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Snyder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib84\" title=\"\">2018</a>; Desplanques et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib24\" title=\"\">2020</a>)</cite>.\nThese approaches remain widely adopted due to their strong performance on target tasks. In parallel, self-supervised learning methods have emerged as a complementary approach, offering advances across speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib46\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib16\" title=\"\">2022b</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib6\" title=\"\">2022</a>)</cite>, audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib40\" title=\"\">2022a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib48\" title=\"\">2022</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>; Li &amp; Li, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib56\" title=\"\">2022</a>)</cite>, and music&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib102\" title=\"\">2025</a>)</cite> without requiring labeled data.\nWhile these methods show improved generalization within their target domains, achieving truly general-purpose audio representations remains challenging.</p>\n\n",
                "matched_terms": [
                    "models",
                    "chen",
                    "model",
                    "supervised",
                    "baevski"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio&#8211;Language Pretraining.</span>\nAudio-language models have emerged as a promising approach for learning cross-modal representations. Most existing work focuses on contrastive learning objectives that align audio and text in shared embedding spaces&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite>. Recent extensions have explored combinations with other objectives&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib93\" title=\"\">2023</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib101\" title=\"\">2024</a>; Niizumi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib71\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib72\" title=\"\">2025</a>)</cite>.\nThe field has also witnessed rapid evolution in datasets, transitioning from traditional human-annotated corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>)</cite> to recently constructed LLM-augmented collections&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib18\" title=\"\">2025</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib86\" title=\"\">Sun et&#160;al., </a>)</cite> and domain-specific resources covering speech characteristics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>)</cite>, and musical attributes&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Roy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib79\" title=\"\">2025</a>)</cite>.\nOur work contributes by providing the first systematic comparison between contrastive and captioning objectives, along with comprehensive evaluation toward general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "existing",
                    "chen",
                    "pretraining",
                    "audiolanguage",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Universal Audio Understanding.</span>\nThe evaluation of audio understanding has evolved from task-specific classification benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib96\" title=\"\">2021</a>; Turian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib98\" title=\"\">2023</a>)</cite> toward more comprehensive assessment frameworks.\nRecent developments have emphasized LLM-based audio understanding systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib37\" title=\"\">2024</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib42\" title=\"\">2024</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib38\" title=\"\">2025</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib20\" title=\"\">2024</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib87\" title=\"\">2024</a>)</cite> that can handle open-form queries and complex reasoning tasks.\nThis shift has driven the development of corresponding evaluation benchmarks that assess models&#8217; abilities across diverse audio understanding scenarios, including question answering, reasoning, and multi-step audio analysis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib80\" title=\"\">2025</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib47\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib64\" title=\"\">2025</a>)</cite>.\nOur work contributes to this trend by providing the first comprehensive evaluation of audio-language pretraining across discriminative tasks, audio-language alignment, and open-form question answering, thereby bridging the gap between traditional representation learning evaluation and modern universal audio understanding.</p>\n\n",
                "matched_terms": [
                    "audiolanguage",
                    "pretraining",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We revisited audio-language pretraining to advance general-purpose audio representation learning. We introduced CaptionStew, a large-scale aggregation of diverse audio caption datasets, and through comprehensive evaluation across tasks, demonstrated that audio-language pretraining produces competitive representations across speech, music, and environmental domains. Our systematic comparison revealed complementary strengths of two audio-language pretraining objectives: contrastive learning excels in data efficiency, while captioning shows better scalability for language-involved tasks.\nData-scaling experiments highlighted diminishing returns of supervised initialization, challenging common practices, while dataset analysis revealed linguistic diversity limitation in current datasets. These findings establish audio-language pretraining as a viable pathway toward universal audio understanding.</p>\n\n",
                "matched_terms": [
                    "initialization",
                    "pretraining",
                    "supervised",
                    "audiolanguage",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While this work provides valuable empirical insights for audio-language pretraining, we acknowledge several important limitations that present opportunities for future research.</p>\n\n",
                "matched_terms": [
                    "audiolanguage",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dataset Construction and Quality.</span> CaptionStew aggregates captions from multiple sources with varying generation methodologies, including LLM-synthesized descriptions that may introduce systematic biases or artifacts.\nWe do not perform extensive quality control or human verification across the aggregated corpus, which could impact model training.\nAdditionally, our dataset analysis reveals that simple aggregation does not guarantee improved linguistic diversity&#8212;CaptionStew&#8217;s lexical diversity metrics remain lower than mature image-text corpora.\nHowever, our design choice prioritizes semantic diversity over linguistic variety, as evidenced by the t-SNE clustering analysis showing distinct descriptive focuses across constituent datasets.\nWhile more sophisticated curation strategies could improve quality, our goal was to establish whether diverse caption aggregation can benefit audio representation learning, which our results support despite these limitations.</p>\n\n",
                "matched_terms": [
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited Technical Novelty.</span> Our work primarily combines existing techniques&#8212;contrastive learning, captioning objectives, and dataset aggregation&#8212;rather than introducing fundamentally new methods. The mixed autoregressive/parallel training approach is adapted from vision-language work (CapPa), and our architectural choices follow standard practices.\nWe acknowledge that the technical contributions are largely empirical rather than methodological.\nHowever, this aligns with our primary goal of systematically evaluating audio-language pretraining&#8217;s potential for general-purpose representation learning.\nThe field currently lacks comprehensive comparative studies across objectives, evaluation protocols, and training factors.\nOur systematic analysis reveals important insights about scaling behaviors and initialization effects that have practical implications for practitioners, even if the underlying techniques are not novel.</p>\n\n",
                "matched_terms": [
                    "audiolanguage",
                    "existing",
                    "initialization",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited Model and Data Scalability.</span> Our experiments are constrained to 10M audio-text pairs and relatively modest model sizes compared to state-of-the-art vision-language systems that leverage billions of samples and much larger architectures. This scale limitation may not fully reflect the potential of audio-language pretraining, particularly for the captioning objective which our results suggest benefits from larger-scale training. Additionally, we do not explore recent advances in large language model integration or more sophisticated architectural designs that could improve performance. These constraints stem from computational resource limitations and our focus on controlled comparisons rather than pushing absolute performance boundaries. Future work with larger scales may reveal different scaling dynamics and stronger evidence for general-purpose capabilities.</p>\n\n",
                "matched_terms": [
                    "audiolanguage",
                    "pretraining",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CaptionStew aggregates eight open-source audio caption datasets to address data scarcity and limited diversity in current audio-language pretraining. The constituent datasets span environmental sounds, music, and expressive speech, with fundamentally different captioning approaches&#8212;from crowdsourced human annotation to expert curation to various LLM-based generation pipelines.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T5\" title=\"Table 5 &#8227; A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T6\" title=\"Table 6 &#8227; A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> detail each dataset&#8217;s characteristics and provide example captions that illustrate the diverse descriptive styles, ranging from concise event descriptions to detailed multi-sentence narratives with fine-grained acoustic and contextual information.\nDuring aggregation, we filter audio samples longer than one minute for computational efficiency and remove samples that overlap with common audio understanding benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>; Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>; Salamon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib81\" title=\"\">2014</a>)</cite> to prevent data leakage. This approach preserves the unique characteristics of each source while creating a unified corpus that captures broader semantic coverage than individual datasets.</p>\n\n",
                "matched_terms": [
                    "chen",
                    "audiolanguage",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although Zipformer was originally designed for automatic speech recognition, we conducted preliminary experiments to validate its effectiveness as a general audio encoder across diverse domains.\nAs in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T7\" title=\"Table 7 &#8227; A.3 Zipformer Model &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, our initial studies confirmed that Zipformer achieves competitive performance on environmental sound classification, music understanding, and speaker-related tasks, demonstrating its suitability as a unified backbone for multi-domain audio representation learning. This cross-domain efficacy makes it an appropriate choice for our audio-language pretraining experiments that span speech, music, and environmental audio.</p>\n\n",
                "matched_terms": [
                    "audiolanguage",
                    "pretraining",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T8\" title=\"Table 8 &#8227; A.4 Evaluation Datasets &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> details the evaluation datasets and their metrics used for assessing audio representation quality across our three evaluation protocols: linear probing&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Fonseca et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>); Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib21\" title=\"\">2018</a>); Cao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib10\" title=\"\">2014</a>); Law et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib52\" title=\"\">2010</a>); Engel et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib32\" title=\"\">2017</a>); Hershey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib45\" title=\"\">2021</a>); Ebbers et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib30\" title=\"\">2022</a>)</cite>, audio-language alignment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>); Diwan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>); Agostinelli et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib58\" title=\"\">2004</a>)</cite> and open-form question answering&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lipping et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib59\" title=\"\">2022</a>); Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib61\" title=\"\">2024</a>); Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "chen",
                    "probing",
                    "linear",
                    "audiolanguage",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table 9 presents linear probing results when using multi-head attention pooling instead of mean pooling.\nWith learned attention pooling, the performance gap between contrastive and captioning objectives narrows substantially, particularly evident on speaker identification where captioning-scratch achieves 72.86% compared to 46.67% with mean pooling (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.T3\" title=\"Table 3 &#8227; 4.4 Main Results &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).\nThis demonstrates that captioning models benefit significantly from adaptive pooling mechanisms, while contrastive learning&#8217;s explicit optimization for clip-level representations shows less sensitivity to pooling strategy.\nThese results underscore the critical importance of appropriate downstream module selection when evaluating different pretraining paradigms, as the choice of pooling mechanism can dramatically influence conclusions about objective effectiveness.\nThe improved performance across all methods with attention pooling also suggests that frame-level representations from both objectives contain rich information that can be better exploited through learned aggregation. SOTA results and SSL baseline results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.T3\" title=\"Table 3 &#8227; 4.4 Main Results &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T9\" title=\"Table 9 &#8227; A.5 Main Results (cont.) &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> are quoted collectively from &#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Niizumi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib72\" title=\"\">2025</a>); Turian et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>); Li &amp; Li (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib56\" title=\"\">2022</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib90\" title=\"\">2022</a>); Bharadwaj et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib8\" title=\"\">2025</a>); Gong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib41\" title=\"\">2022b</a>); Lanzend&#246;rfer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib51\" title=\"\">2025</a>); Bai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>); Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "captioningscratch",
                    "probing",
                    "mean",
                    "pretraining",
                    "linear",
                    "ssl",
                    "pooling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Aside from learning representations, we also compare against state-of-the-art audio-text retrieval models to assess our approach&#8217;s performance on the specific task it was designed for. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T10\" title=\"Table 10 &#8227; A.6 Additional Results &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> presents retrieval results for our best-performing model (Contrastive-init) against state-of-the-art\naudio-text retrieval model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>)</cite>.\nOur model achieving comparable or superior results on benchmarks in various audio domains, with particularly strong performance on speech and music retrieval.\nThe results indicate that our general-purpose audio-language pretraining approach can compete with specialized retrieval models while offering broader applicability across diverse usage scenarios.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "pretraining",
                    "contrastiveinit",
                    "audiolanguage",
                    "our"
                ]
            }
        ]
    },
    "S4.T3.st2": {
        "caption": "(b) Audio-language Alignment / Open-form QA",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\">Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Captioning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Retrieval</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Open-formed QA</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">PSC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">MC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">PSC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">MC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Sound</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Speaker-related<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8225;&#8225;</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Music</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Our Supervised Baselines</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Zipformer-AEC&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Yao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib97\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">46.7</td>\n<td class=\"ltx_td ltx_align_center\">45.5</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">22.9</span></td>\n<td class=\"ltx_td ltx_align_center\">40.5</td>\n<td class=\"ltx_td ltx_align_center\">49.2</td>\n<td class=\"ltx_td ltx_align_center\">24.6</td>\n<td class=\"ltx_td ltx_align_center\">7.01</td>\n<td class=\"ltx_td ltx_align_center\">36.5 / 46.2 / 37.2</td>\n<td class=\"ltx_td ltx_align_center\">5.61</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Our Audio-langauge Pretrained Models</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Contrastive-<span class=\"ltx_text ltx_font_italic\">scratch</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">46.6</td>\n<td class=\"ltx_td ltx_align_center\">46.3</td>\n<td class=\"ltx_td ltx_align_center\">22.1</td>\n<td class=\"ltx_td ltx_align_center\">39.3</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">63.2</span></td>\n<td class=\"ltx_td ltx_align_center\">27.4</td>\n<td class=\"ltx_td ltx_align_center\">6.65</td>\n<td class=\"ltx_td ltx_align_center\">37.9 / <span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">81.3</span> / 63.4</td>\n<td class=\"ltx_td ltx_align_center\">5.86</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Captioning-<span class=\"ltx_text ltx_font_italic\">scratch</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">46.7</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">46.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">22.9</span></td>\n<td class=\"ltx_td ltx_align_center\">36.9</td>\n<td class=\"ltx_td ltx_align_center\">60.2</td>\n<td class=\"ltx_td ltx_align_center\">23.0</td>\n<td class=\"ltx_td ltx_align_center\">6.69</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">44.2</span> / 65.4 / <span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">69.0</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">5.97</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Contrastive-<span class=\"ltx_text ltx_font_italic\">init</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">47.2</span></td>\n<td class=\"ltx_td ltx_align_center\">46.2</td>\n<td class=\"ltx_td ltx_align_center\">22.5</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">42.8</span></td>\n<td class=\"ltx_td ltx_align_center\">60.6</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">29.4</span></td>\n<td class=\"ltx_td ltx_align_center\">6.73</td>\n<td class=\"ltx_td ltx_align_center\">35.1 / 67.3 / 64.5</td>\n<td class=\"ltx_td ltx_align_center\">5.63</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Captioning-<span class=\"ltx_text ltx_font_italic\">init</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">47.2</span></td>\n<td class=\"ltx_td ltx_align_center\">45.9</td>\n<td class=\"ltx_td ltx_align_center\">22.6</td>\n<td class=\"ltx_td ltx_align_center\">42.2</td>\n<td class=\"ltx_td ltx_align_center\">55</td>\n<td class=\"ltx_td ltx_align_center\">28.2</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">7.06</span></td>\n<td class=\"ltx_td ltx_align_center\">32.4 / 49.5 / 45.6</td>\n<td class=\"ltx_td ltx_align_center\">5.50</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_tt\">SOTA<sup class=\"ltx_sup\">&#8225;</sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">52.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">&#8211;<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;&#8224;</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">26.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">44.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">&#8211;<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;&#8224;</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">&#8211;<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;&#8224;</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">6.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">60.0 / 82.5 / 62.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">6.79</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "baselines",
            "supervised",
            "our",
            "captioning",
            "–††",
            "zipformeraec",
            "speakerrelated‡‡",
            "sound",
            "audiolangauge",
            "retrieval",
            "contrastivescratch",
            "captioningscratch",
            "captioninginit",
            "alignment",
            "sota‡",
            "openform",
            "psc",
            "audiolanguage",
            "yao",
            "pretrained",
            "music",
            "contrastiveinit",
            "method",
            "openformed"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Audio-language pretraining holds promise for general-purpose audio understanding, yet remains underexplored compared to its vision counterpart. While vision-language models like CLIP serve as widely adopted foundations, existing audio-language models primarily excel at retrieval tasks with limited adoption as general-purpose encoders.\nWe identify three key barriers: limited large-scale audio-text corpora, insufficient caption diversity, and lack of systematic exploration and evaluation.\nTo this end, we introduce CaptionStew, a 10.7M caption dataset aggregating diverse open-source audio-text corpora across multiple domains and captioning styles.\nUsing this resource, we conduct the first comprehensive evaluation comparing contrastive and captioning objectives for audio representation learning across speech, music, and environmental sound tasks.\nOur results demonstrate that audio-language pretraining yields competitive, transferable representations. Through systematic data-scaling experiments, we reveal complementary objective strengths: contrastive learning achieves superior data efficiency at smaller scales, while captioning demonstrates better scalability on language-involved audio understanding tasks. We also find that common supervised initialization practices provide diminishing returns at scale, challenging current approaches.\nThese findings establish audio-language pretraining as a viable pathway toward general-purpose audio representations, guiding future research. To accelerate progress, we release data preparation recipes, training protocols, and pretrained models, paving the way toward universal audio understanding.\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "pretrained",
                    "captioning",
                    "music",
                    "sound",
                    "supervised",
                    "audiolanguage",
                    "retrieval",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Representation learning has long been central to audio processing<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>In this work, audio processing refers to audio understanding, speech analysis and music understanding, while excluding automatic speech recognition</span></span></span>, with substantial progress over the past decades.\nEarly advances relied on supervised learning, where models trained on labeled corpora were adapted to related downstream tasks or transferred across domains&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib50\" title=\"\">2020</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib15\" title=\"\">2022a</a>; Snyder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib84\" title=\"\">2018</a>; Desplanques et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib24\" title=\"\">2020</a>)</cite>.\nMore recently, self-supervised learning (SSL) has emerged as the promising paradigm.\nBy pretraining on large-scale unlabeled audio with contrastive objectives or masked modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib40\" title=\"\">2022a</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib46\" title=\"\">2021</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>)</cite>, the resulting models learn rich structural knowledge of audio signals, consistently enhancing performance across many speech and audio benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib96\" title=\"\">2021</a>; Turian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib98\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "music",
                    "models",
                    "supervised"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of vision&#8211;language pretraining underscores this promise.\nModels like CLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib75\" title=\"\">2021</a>)</cite> and AIM-v2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib33\" title=\"\">2025</a>)</cite> not only power vision&#8211;language tasks but also produce representations that benefit a broad range of vision tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib60\" title=\"\">2023</a>; Minderer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib68\" title=\"\">2022</a>; Crowson et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib23\" title=\"\">2022</a>)</cite>.\nIn contrast, audio&#8211;language models have not yet achieved comparable advancements.\nWhile existing models such as CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>)</cite> excel at audio&#8211;text retrieval, their representations have seen limited adoption for broader audio understanding tasks, suggesting fundamental gaps in current approaches.\nWe identify three key challenges that have constrained progress.\nFirst, large-scale, web-mined image&#8211;text corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schuhmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib83\" title=\"\">2022</a>; Gadre et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib35\" title=\"\">2023</a>)</cite> contain billions of pairs, but no comparable resource exists for audio.\nCurrent audio caption datasets barely exceed one million pairs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>)</cite>, often relying on captions synthesized or augmented by large language models, fundamentally limiting the scaling potential of audio&#8211;language models.\nSecond, widely used audio caption corpora focus predominantly on identifying what is presenting in the audio, while lacking coverage of the rich hierarchy of acoustic attributes that define specific audio signals.\nFor instance, captions rarely characterize speaker characteristics (voice timbre, speaking style), musical attributes (harmonic structure, rhythmic patterns), or environmental acoustics (reverberation, background ambiance).\nThis imbalanced focus limits the model&#8217;s ability to learn representations that capture the full spectrum of audio semantics.\nThird, prior work has primarily focused on contrastive learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite> and evaluated on audio&#8211;text retrieval.\nSystematic studies on alternative pretraining objectives (e.g., captioning) and comprehensive evaluations across a wide suite of audio understanding tasks remain scarce, limiting our understanding of what drives effective audio&#8211;language pretraining.</p>\n\n",
                "matched_terms": [
                    "models",
                    "captioning",
                    "retrieval",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">CaptionStew</span>, a large-scale aggregation of diverse open-source audio&#8211;text datasets spanning multiple domains and captioning styles, addressing the data scarcity and diversity limitations in current audio-language pretraining.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "audiolanguage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide the first comprehensive evaluation of audio-language pretraining across diverse tasks and protocols, demonstrating that audio&#8211;language pretraining produces competitive, transferable representations across speech, music, and environmental audio domains.</p>\n\n",
                "matched_terms": [
                    "music",
                    "audiolanguage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Taken together, our results suggests audio&#8211;language pretraining as a practical and competitive approach for learning general-purpose audio representations. To accelerate progress in this direction, we release data preperation recipes, training scripts, evaluation protocols, and pretrained models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "pretrained",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio&#8211;language pretraining learns audio representations by establishing correspondence between audio signals and natural language descriptions. The core objective is to leverage text as structured semantic supervision, enabling models to capture diverse information across speech, music, and environmental sounds within a unified framework.\nAudio&#8211;language models typically employ a two-tower architecture: an audio encoder <math alttext=\"f_{\\text{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mtext>a</mtext></msub><annotation encoding=\"application/x-tex\">f_{\\text{a}}</annotation></semantics></math> that maps raw audio signals into contextual representations, and a text component <math alttext=\"f_{\\text{t}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mtext>t</mtext></msub><annotation encoding=\"application/x-tex\">f_{\\text{t}}</annotation></semantics></math> whose design depends on the training objective.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S2.F1\" title=\"Figure 1 &#8227; 2 Language-audio Pretraining &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we explore two complementary paradigms that differ fundamentally in how they establish audio-text correspondence, contrastive and captioning objective. These approaches represent discriminative and generative perspectives on audio-language alignment, respectively.</p>\n\n",
                "matched_terms": [
                    "models",
                    "alignment",
                    "captioning",
                    "music",
                    "audiolanguage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive Objective</span> is proven to be a robust representation learning method&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib19\" title=\"\">2020b</a>; Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib75\" title=\"\">2021</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>)</cite> and have been a dominant approach for audio-language pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite>.\nThis approach aligns audio and text representations in a shared embedding space by maximizing similarity between paired samples while minimizing similarity between mismatched pairs.\nGiven a batch of paired samples <math alttext=\"\\{(a_{i},t_{i})\\}_{i=1}^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m1\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding=\"application/x-tex\">\\{(a_{i},t_{i})\\}_{i=1}^{N}</annotation></semantics></math>, the audio encoder produces frame- (or patch-) level representations that are pooled and projected to audio embeddings <math alttext=\"\\mathbf{z}^{a}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119859;</mi><mi>i</mi><mi>a</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{z}^{a}_{i}</annotation></semantics></math>, while the text encoder <math alttext=\"f_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">f_{t}</annotation></semantics></math> generates corresponding text embeddings <math alttext=\"\\mathbf{z}^{t}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>&#119859;</mi><mi>i</mi><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{z}^{t}_{i}</annotation></semantics></math>.\nThe symmetric InfoNCE loss&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Oord et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib73\" title=\"\">2018</a>)</cite> is applied to optimize both modalities:</p>\n\n",
                "matched_terms": [
                    "audiolanguage",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Captioning Objective</span> takes a generative approach to audio-language alignment, learning representations by generating textual descriptions from audio.\nWe argue that captioning presents a promising alternative for audio-language pretraining, offering denser token-level supervision compared to contrastive learning and better alignment with recent trends toward general audio understanding systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib38\" title=\"\">2025</a>)</cite>, yet remains underexplored in prior literature.\nGiven an audio signal <math alttext=\"a_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m1\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">a_{i}</annotation></semantics></math>, the encoder <math alttext=\"f_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">f_{a}</annotation></semantics></math> produces contextual representations <math alttext=\"\\mathbf{Z}^{a}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>&#119833;</mi><mi>i</mi><mi>a</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{Z}^{a}_{i}</annotation></semantics></math>, which are fed into a transformer decoder <math alttext=\"g_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m4\" intent=\":literal\"><semantics><msub><mi>g</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">g_{t}</annotation></semantics></math> through cross-attention. Inspired by CapPa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tschannen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib88\" title=\"\">2023</a>)</cite>, we alternate between two decoding modes&#8212;autoregressive and parallel prediction&#8212;to enhance audio encoder representation learning.\nIn the autoregressive decoding, the decoder generates caption tokens <math alttext=\"(y_{1},\\ldots,y_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y_{1},\\ldots,y_{T})</annotation></semantics></math> sequentially, with each token conditioned on the audio representation and previously generated tokens. Training follows the teacher-forcing approach with a cross-entropy loss:</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "audiolanguage",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The audio encoder uses a Zipformer-M architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib97\" title=\"\">2024</a>)</cite>, chosen for its efficiency on long sequences and fast convergence.\nZipformer employs six encoder blocks in a U-Net structure that processes sequences at multiple resolutions to capture fine- and coarse-grained temporal information.\nAlthough originally designed for automatic speech recognition, our preliminary experiments confirm Zipformer as a competitive backbone across audio classification tasks (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS3\" title=\"A.3 Zipformer Model &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>).\nFor contrastive pretraining, the text encoder follows BERT-base architecture (12 layers 768 hidden dimensions)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib25\" title=\"\">2019</a>)</cite>.\nFor captioning pretraining, the text decoder adopts the BART-base decoder architecture (6 layers, 768 hidden dimensions)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib54\" title=\"\">2020</a>)</cite>.\nWe use twice as many encoder layers as decoder layers to ensure comparable training speed across objectives.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "yao",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following prior works in audio-language pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>)</cite>, we experiment with two scenarios: training from scratch or initialized from pretrained checkpoints.\nThe audio encoder initializes from a Zipformer-based audio event classifier trained on AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib36\" title=\"\">2017</a>)</cite> with an mAP of 0.46, while text components use corresponding publicly available checkpoints.\nAll models are trained on 8 Tesla V100 GPUs with an effective batch size of 640 seconds of audio per GPU.\nTraining runs for 600k steps from scratch (14 days wall-clock time) or 200k steps if initialized from pretrained checkpoint.</p>\n\n",
                "matched_terms": [
                    "models",
                    "pretrained",
                    "audiolanguage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate pretrained audio encoders across three protocols assessing discriminative capabilities, audio-language alignment, and open-formed question answering.\nAll experiments probe frozen representations from the audio encoder&#8217;s final layer to ensure fair model comparison. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S3.T2\" title=\"Table 2 &#8227; 3 CaptionStew Dataset &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS4\" title=\"A.4 Evaluation Datasets &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> details the datasets and metrics for each task.</p>\n\n",
                "matched_terms": [
                    "openformed",
                    "pretrained",
                    "audiolanguage",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Linear Probing</span> trains simple linear classifiers on frozen representations.\nFor classification tasks, we experiment with two pooling mechanisms&#8212;mean pooling and multi-head attention pooling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib53\" title=\"\">2019</a>)</cite>&#8212;to obtain clip-level representations.\nWe evaluate across a diverse set of tasks across audio domains, including audio event classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>)</cite>, sound event detection&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib45\" title=\"\">2021</a>)</cite>, speaker-related tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib21\" title=\"\">2018</a>; Cao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib10\" title=\"\">2014</a>)</cite>, and music classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Law et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib52\" title=\"\">2010</a>; Engel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib32\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "sound",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-language Alignments</span>\nfollow the LiT protocol&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib100\" title=\"\">2022</a>)</cite>, adapting pretrained text components to align with frozen audio representations.\nFor retrieval, we pair audio encoders with pretrained RoBERTa-base text encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib62\" title=\"\">2019</a>)</cite>.\nFor captioning, we use pretrained BART-base decoders&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib54\" title=\"\">2020</a>)</cite>, and only finetune cross-attention layers as we observed more stable training.\nBoth setups finetune on corresponding datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>)</cite> while keeping audio encoders frozen.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "pretrained",
                    "audiolanguage",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Open-formed Question Answering</span>\nAcknowledging the trend of combining audio encoders with large language models (LLMs) for general audio understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib37\" title=\"\">2024</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib42\" title=\"\">2024</a>)</cite>, we connects frozen audio encoders to a LLM (Qwen2.5-7B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib94\" title=\"\">2024a</a>)</cite>) through lightweight adaptors that project audio representations into the LLM&#8217;s embedding space.\nWe train only the adaptor on audio QA datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipping et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib59\" title=\"\">2022</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib61\" title=\"\">2024</a>)</cite> and evaluate on corresponding track in AIR-Bench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>)</cite>.\nDuring training, we carefully monitor instruction-following behavior (<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>99%) to ensure reliable evaluation.</p>\n\n",
                "matched_terms": [
                    "openformed",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recognizing the broad adoption and effectiveness of pretrained audio event classifiers in transfer learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alonso-Jim&#233;nez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib3\" title=\"\">2023</a>; Cappellazzo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib11\" title=\"\">2024</a>)</cite>, audio-language modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>)</cite> and general audio understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib42\" title=\"\">2024</a>; Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib37\" title=\"\">2024</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>)</cite>, we select our pretrained Zipformer-based audio event classifier (described in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.SS1\" title=\"4.1 Implementation Details &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) as the primary baseline.\nIn addition, we compare against representative self-supervised learning (SSL) models, each pretrained under different paradigms and specialized for particular audio domains. BEATs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>)</cite> is an audio SSL model trained with an iterative masked acoustic token prediction framework. Wav2vec 2.0&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>)</cite> learns speech representation by distinguishing target quantized latent representations from disctrators. MERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>)</cite> is a music SSL model trained with masked acoustic modeling, learning to capture acoustic cues and structural information of music.\nTogether, these baselines provide a broad comparative context for studying audio&#8211;language pretraining toward general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "pretrained",
                    "baselines",
                    "music",
                    "audiolanguage",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive vs. Captioning Objectives.</span>\nThe two pretraining paradigms exhibit complementary strengths across evaluation protocols.\nOn linear probing tasks, contrastive learning consistently outperforms captioning, particularly excelling at audio event classification and speaker identification.\nHowever, it is worth noting that this gap narrows substantially when the classifier learns to aggregate information across frames through multi-head attention pooling (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS5\" title=\"A.5 Main Results (cont.) &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>).\nThis observation reflects the objectives&#8217; inherent designs: contrastive learning explicitly optimizes for linearly separable clip-level representations, while captioning relies on cross-attention mechanisms over frame-level representations for text sequence generation.\nThis finding aligns with recent work highlighting how downstream module choices significantly impact the assessment of audio representation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zaiem et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib99\" title=\"\">2023</a>)</cite>.\nFor language-involved tasks, both objectives demonstrate competitive performance, with captioning showing slight advantages in open-form question answering across multiple domains.\nThis suggests captioning&#8217;s potential for language-involved audio understanding tasks, aligning with recent trends toward generative audio understanding systems.</p>\n\n",
                "matched_terms": [
                    "openform",
                    "captioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Supervised Initialization.</span> Initializing from supervised pretraining (AS SL) provides substantial benefits across most tasks, with notable improvements on audio event classification, sound event detection and audio-text retreival.\nThe gains are particularly pronounced for contrastive objectives, suggesting that discriminative pretraining provides useful inductive biases for contrastive learning.\nHowever, these benefits diminish (or disappear entirely) when the attributes required for downstream tasks diverge from AudioSet&#8217;s ontology.\nOn speaker identification and music tagging, scratch-trained models often match or exceed initialized variants, indicating that AudioSet&#8217;s focus on distinguishing between sound categories may bias representations toward event-level semantics rather than the acoustic attributes (voice timbre, speaking style) or musical structure (genre, harmony, rhythm) essential for these tasks.\nThese findings challenge common initialization practices for audio-language pretraining and suggest the need for tailored pretraining strategies when targeting general-purpose audio representation learning.</p>\n\n",
                "matched_terms": [
                    "models",
                    "music",
                    "sound",
                    "supervised",
                    "audiolanguage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Competitive Performance Across Domains.</span> Our audio-language representations achieve strong transferability across diverse audio domains.\nCompared to supervised baselines (Zipformer-AEC), our overall best-performing model (Contrastive-init) demonstrate superior performance on speaker identification, music understanding and audio-text retrieval while maintaining competitiveness on audio-event classification.\nAgainst domain-specialized SSL methods (BEATs, wav2vec2, MERT), our approach consistently shows competitive performance.\nThis consistent cross-domain performance validates our hypothesis that diverse caption aggregation enables broadly transferable representations, establishing audio-language pretraining as a viable path toward learning general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "zipformeraec",
                    "music",
                    "contrastiveinit",
                    "supervised",
                    "audiolanguage",
                    "retrieval",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scaling Patterns.</span> Most tasks demonstrate consistent performance improvements with increased data scale, validating the potential of large-scale audio-language pretraining.\nHowever, notable exceptions emerge that reveal fundamental limitations of current approaches.\nSound event detection, particularly for models initialized with AudioSet pretraining, exhibits a reverse scaling trend where performance degrades with more caption data.\nThis suggests a potential conflict between natural language supervision&#8211;which typically describes audio characteristics and attributes&#8211;and temporal localization tasks requiring precise event boundaries.\nAdditionally, emotion recognition and instrument classification show weaker scaling gains compared to other tasks, likely reflecting limited caption diversity for these specific attributes in existing corpora, which we will discussed in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.SS6\" title=\"4.6 Dataset Analysis &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "sound",
                    "models",
                    "audiolanguage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive vs. Captioning Scaling. </span>\nContrastive learning consistently outperforms captioning at varying data scales, particularly under less data and on discriminative tasks such as audio event classification.\nHowever, captioning demonstrates slightly better scaling properties, with distinct patterns emerging across task categories.\nor language-involved tasks&#8211;especially captioning and question answering&#8211;captioning matches or surpasses contrastive learning at our current 10M-pair scale.\nOn linear probing benchmarks, the gap remains substantial, with scaling trends suggesting captioning would require hundreds of millions of pairs to achieve parity with contrastive methods.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Initialization at Scale.</span>\nAudioSet initialization provides immediate performance gains but introduces diminishing returns at larger scales. Both contrastive learning and captioning show decreasing benefits from initialization as data scale increases, with scratch and initialized models achieving matched performance at larger scales on some tasks.\nThis suggests that pretrained initialization effectively bootstraps learning at small scales but may constrain the model&#8217;s ability to adapt to the broader semantic space covered by large-scale caption data, potentially due to mismatch between AudioSet&#8217;s ontology and diverse audio descriptions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "pretrained",
                    "captioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, these findings reveal complementary behaviors: contrastive pretraining achieves superior data efficiency at current scales, while captioning shows better scalability, especially for language-involved tasks.\nImportantly, the diminishing returns of initialization at scale indicate that large-scale caption data can provide sufficient semantic supervision independent of domain-specific pretraining, challenging current practices of audio-language pretraining and opening possibilities for learning general-purpose representations from diverse text descriptions alone.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "audiolanguage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings highlight that simply combining datasets doesn&#8217;t guarantee improved linguistic diversity, revealing broader limitations in current audio-language pretraining approaches.\nAlso, the constrained diversity in certain aspect may partially explain weaker scaling behavior observed for certain tasks, as models encounter repetitive linguistic patterns despite increased data volume, aligning with vision-language findings on caption diversity&#8217;s importance for representation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Santurkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib82\" title=\"\">2023</a>; Chan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib12\" title=\"\">2022</a>)</cite>.\nThis analysis motivates developing enhanced aggregation pipeline and more diverse caption generation methods to better capture the full spectrum of information in audio signals, thereby fully realizing the potential of large-scale audio-language pretraining.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audiolanguage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Representation Learning.</span>\nThe ultimate goal of audio representation learning is developing a single model suitable for diverse audio understanding tasks.\nSupervised models trained on labeled datasets have been fundamental to the field, including audio event classifiers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib44\" title=\"\">2017</a>; Cramer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib22\" title=\"\">2019</a>; Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib50\" title=\"\">2020</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib39\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib15\" title=\"\">2022a</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib26\" title=\"\">2024</a>)</cite>, speech recognition systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib76\" title=\"\">2023</a>)</cite> and speaker recognition models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Snyder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib84\" title=\"\">2018</a>; Desplanques et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib24\" title=\"\">2020</a>)</cite>.\nThese approaches remain widely adopted due to their strong performance on target tasks. In parallel, self-supervised learning methods have emerged as a complementary approach, offering advances across speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib46\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib16\" title=\"\">2022b</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib6\" title=\"\">2022</a>)</cite>, audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib40\" title=\"\">2022a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib48\" title=\"\">2022</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>; Li &amp; Li, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib56\" title=\"\">2022</a>)</cite>, and music&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib102\" title=\"\">2025</a>)</cite> without requiring labeled data.\nWhile these methods show improved generalization within their target domains, achieving truly general-purpose audio representations remains challenging.</p>\n\n",
                "matched_terms": [
                    "music",
                    "models",
                    "supervised"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio&#8211;Language Pretraining.</span>\nAudio-language models have emerged as a promising approach for learning cross-modal representations. Most existing work focuses on contrastive learning objectives that align audio and text in shared embedding spaces&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite>. Recent extensions have explored combinations with other objectives&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib93\" title=\"\">2023</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib101\" title=\"\">2024</a>; Niizumi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib71\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib72\" title=\"\">2025</a>)</cite>.\nThe field has also witnessed rapid evolution in datasets, transitioning from traditional human-annotated corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>)</cite> to recently constructed LLM-augmented collections&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib18\" title=\"\">2025</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib86\" title=\"\">Sun et&#160;al., </a>)</cite> and domain-specific resources covering speech characteristics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>)</cite>, and musical attributes&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Roy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib79\" title=\"\">2025</a>)</cite>.\nOur work contributes by providing the first systematic comparison between contrastive and captioning objectives, along with comprehensive evaluation toward general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audiolanguage",
                    "captioning",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Universal Audio Understanding.</span>\nThe evaluation of audio understanding has evolved from task-specific classification benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib96\" title=\"\">2021</a>; Turian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib98\" title=\"\">2023</a>)</cite> toward more comprehensive assessment frameworks.\nRecent developments have emphasized LLM-based audio understanding systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib37\" title=\"\">2024</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib42\" title=\"\">2024</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib38\" title=\"\">2025</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib20\" title=\"\">2024</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib87\" title=\"\">2024</a>)</cite> that can handle open-form queries and complex reasoning tasks.\nThis shift has driven the development of corresponding evaluation benchmarks that assess models&#8217; abilities across diverse audio understanding scenarios, including question answering, reasoning, and multi-step audio analysis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib80\" title=\"\">2025</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib47\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib64\" title=\"\">2025</a>)</cite>.\nOur work contributes to this trend by providing the first comprehensive evaluation of audio-language pretraining across discriminative tasks, audio-language alignment, and open-form question answering, thereby bridging the gap between traditional representation learning evaluation and modern universal audio understanding.</p>\n\n",
                "matched_terms": [
                    "openform",
                    "audiolanguage",
                    "alignment",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We revisited audio-language pretraining to advance general-purpose audio representation learning. We introduced CaptionStew, a large-scale aggregation of diverse audio caption datasets, and through comprehensive evaluation across tasks, demonstrated that audio-language pretraining produces competitive representations across speech, music, and environmental domains. Our systematic comparison revealed complementary strengths of two audio-language pretraining objectives: contrastive learning excels in data efficiency, while captioning shows better scalability for language-involved tasks.\nData-scaling experiments highlighted diminishing returns of supervised initialization, challenging common practices, while dataset analysis revealed linguistic diversity limitation in current datasets. These findings establish audio-language pretraining as a viable pathway toward universal audio understanding.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "music",
                    "supervised",
                    "audiolanguage",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited Technical Novelty.</span> Our work primarily combines existing techniques&#8212;contrastive learning, captioning objectives, and dataset aggregation&#8212;rather than introducing fundamentally new methods. The mixed autoregressive/parallel training approach is adapted from vision-language work (CapPa), and our architectural choices follow standard practices.\nWe acknowledge that the technical contributions are largely empirical rather than methodological.\nHowever, this aligns with our primary goal of systematically evaluating audio-language pretraining&#8217;s potential for general-purpose representation learning.\nThe field currently lacks comprehensive comparative studies across objectives, evaluation protocols, and training factors.\nOur systematic analysis reveals important insights about scaling behaviors and initialization effects that have practical implications for practitioners, even if the underlying techniques are not novel.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "audiolanguage",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited Model and Data Scalability.</span> Our experiments are constrained to 10M audio-text pairs and relatively modest model sizes compared to state-of-the-art vision-language systems that leverage billions of samples and much larger architectures. This scale limitation may not fully reflect the potential of audio-language pretraining, particularly for the captioning objective which our results suggest benefits from larger-scale training. Additionally, we do not explore recent advances in large language model integration or more sophisticated architectural designs that could improve performance. These constraints stem from computational resource limitations and our focus on controlled comparisons rather than pushing absolute performance boundaries. Future work with larger scales may reveal different scaling dynamics and stronger evidence for general-purpose capabilities.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "audiolanguage",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CaptionStew aggregates eight open-source audio caption datasets to address data scarcity and limited diversity in current audio-language pretraining. The constituent datasets span environmental sounds, music, and expressive speech, with fundamentally different captioning approaches&#8212;from crowdsourced human annotation to expert curation to various LLM-based generation pipelines.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T5\" title=\"Table 5 &#8227; A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T6\" title=\"Table 6 &#8227; A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> detail each dataset&#8217;s characteristics and provide example captions that illustrate the diverse descriptive styles, ranging from concise event descriptions to detailed multi-sentence narratives with fine-grained acoustic and contextual information.\nDuring aggregation, we filter audio samples longer than one minute for computational efficiency and remove samples that overlap with common audio understanding benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>; Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>; Salamon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib81\" title=\"\">2014</a>)</cite> to prevent data leakage. This approach preserves the unique characteristics of each source while creating a unified corpus that captures broader semantic coverage than individual datasets.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "music",
                    "audiolanguage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although Zipformer was originally designed for automatic speech recognition, we conducted preliminary experiments to validate its effectiveness as a general audio encoder across diverse domains.\nAs in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T7\" title=\"Table 7 &#8227; A.3 Zipformer Model &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, our initial studies confirmed that Zipformer achieves competitive performance on environmental sound classification, music understanding, and speaker-related tasks, demonstrating its suitability as a unified backbone for multi-domain audio representation learning. This cross-domain efficacy makes it an appropriate choice for our audio-language pretraining experiments that span speech, music, and environmental audio.</p>\n\n",
                "matched_terms": [
                    "sound",
                    "music",
                    "audiolanguage",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T8\" title=\"Table 8 &#8227; A.4 Evaluation Datasets &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> details the evaluation datasets and their metrics used for assessing audio representation quality across our three evaluation protocols: linear probing&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Fonseca et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>); Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib21\" title=\"\">2018</a>); Cao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib10\" title=\"\">2014</a>); Law et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib52\" title=\"\">2010</a>); Engel et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib32\" title=\"\">2017</a>); Hershey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib45\" title=\"\">2021</a>); Ebbers et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib30\" title=\"\">2022</a>)</cite>, audio-language alignment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>); Diwan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>); Agostinelli et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib58\" title=\"\">2004</a>)</cite> and open-form question answering&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lipping et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib59\" title=\"\">2022</a>); Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib61\" title=\"\">2024</a>); Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "openform",
                    "audiolanguage",
                    "alignment",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table 9 presents linear probing results when using multi-head attention pooling instead of mean pooling.\nWith learned attention pooling, the performance gap between contrastive and captioning objectives narrows substantially, particularly evident on speaker identification where captioning-scratch achieves 72.86% compared to 46.67% with mean pooling (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.T3\" title=\"Table 3 &#8227; 4.4 Main Results &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).\nThis demonstrates that captioning models benefit significantly from adaptive pooling mechanisms, while contrastive learning&#8217;s explicit optimization for clip-level representations shows less sensitivity to pooling strategy.\nThese results underscore the critical importance of appropriate downstream module selection when evaluating different pretraining paradigms, as the choice of pooling mechanism can dramatically influence conclusions about objective effectiveness.\nThe improved performance across all methods with attention pooling also suggests that frame-level representations from both objectives contain rich information that can be better exploited through learned aggregation. SOTA results and SSL baseline results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.T3\" title=\"Table 3 &#8227; 4.4 Main Results &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T9\" title=\"Table 9 &#8227; A.5 Main Results (cont.) &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> are quoted collectively from &#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Niizumi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib72\" title=\"\">2025</a>); Turian et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>); Li &amp; Li (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib56\" title=\"\">2022</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib90\" title=\"\">2022</a>); Bharadwaj et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib8\" title=\"\">2025</a>); Gong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib41\" title=\"\">2022b</a>); Lanzend&#246;rfer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib51\" title=\"\">2025</a>); Bai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>); Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "captioningscratch",
                    "captioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Aside from learning representations, we also compare against state-of-the-art audio-text retrieval models to assess our approach&#8217;s performance on the specific task it was designed for. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T10\" title=\"Table 10 &#8227; A.6 Additional Results &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> presents retrieval results for our best-performing model (Contrastive-init) against state-of-the-art\naudio-text retrieval model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>)</cite>.\nOur model achieving comparable or superior results on benchmarks in various audio domains, with particularly strong performance on speech and music retrieval.\nThe results indicate that our general-purpose audio-language pretraining approach can compete with specialized retrieval models while offering broader applicability across diverse usage scenarios.</p>\n\n",
                "matched_terms": [
                    "models",
                    "music",
                    "contrastiveinit",
                    "audiolanguage",
                    "retrieval",
                    "our"
                ]
            }
        ]
    },
    "S4.SS6.tab1": {
        "caption": "",
        "body": "<table class=\"ltx_tabular ltx_align_top\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_inline-logical-block ltx_minipage ltx_align_top\" style=\"width:170.9pt;\">\n<span class=\"ltx_figure ltx_align_center\" id=\"S4.F3\">\n<span class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">Figure 3: </span>t-SNE visualization of sentence embedding of captions grouped by source.</span>\n</span>\n<span class=\"ltx_para\" id=\"S4.SS6.p1\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"443\" id=\"S4.SS6.g1\" src=\"iclr2026/figure/tsne_by_source.png\" width=\"556\"/>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_inline-logical-block ltx_minipage ltx_align_top\" style=\"width:198.7pt;\">\n<span class=\"ltx_table ltx_align_center\" id=\"S4.T4\">\n<span class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Comparison of lexical statistics and diversity across audio caption datasets and text corpora. We report vocabulary size (#vocab), average sentence length (avg. sent), and Distinct-n.</span>\n</span>\n<span class=\"ltx_para\" id=\"S4.SS6.p2\">\n<span class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:198.7pt;height:78.7pt;vertical-align:-37.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-70.2pt,27.8pt) scale(0.586019871368682,0.586019871368682) ;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_tt ltx_rowspan ltx_rowspan_2\">Source</span>\n<span class=\"ltx_td ltx_align_left ltx_border_tt ltx_rowspan ltx_rowspan_2\">#vocab</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2\">avg. sent</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_4\">Distinct-n</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center ltx_border_t\">1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">2</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">3</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">4</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\">AudioCaps</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\">5,572</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">8.46</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.011</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.113</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.309</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.519</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\">WavCaps</span>\n<span class=\"ltx_td ltx_align_left\">18,372</span>\n<span class=\"ltx_td ltx_align_center\">7.77</span>\n<span class=\"ltx_td ltx_align_center\">0.026</span>\n<span class=\"ltx_td ltx_align_center\">0.184</span>\n<span class=\"ltx_td ltx_align_center\">0.420</span>\n<span class=\"ltx_td ltx_align_center\">0.646</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\">AudioSetCaps</span>\n<span class=\"ltx_td ltx_align_left\">21,061</span>\n<span class=\"ltx_td ltx_align_center\">28.22</span>\n<span class=\"ltx_td ltx_align_center\">0.006</span>\n<span class=\"ltx_td ltx_align_center\">0.082</span>\n<span class=\"ltx_td ltx_align_center\">0.249</span>\n<span class=\"ltx_td ltx_align_center\">0.450</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\">FusionAudio</span>\n<span class=\"ltx_td ltx_align_left\">18,403</span>\n<span class=\"ltx_td ltx_align_center\">13.81</span>\n<span class=\"ltx_td ltx_align_center\">0.009</span>\n<span class=\"ltx_td ltx_align_center\">0.111</span>\n<span class=\"ltx_td ltx_align_center\">0.322</span>\n<span class=\"ltx_td ltx_align_center\">0.546</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\">JamendoMaxCaps</span>\n<span class=\"ltx_td ltx_align_left\">27,906</span>\n<span class=\"ltx_td ltx_align_center\">63.29</span>\n<span class=\"ltx_td ltx_align_center\">0.002</span>\n<span class=\"ltx_td ltx_align_center\">0.026</span>\n<span class=\"ltx_td ltx_align_center\">0.079</span>\n<span class=\"ltx_td ltx_align_center\">0.153</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\">ParaSpeechCaps</span>\n<span class=\"ltx_td ltx_align_left\">4,060</span>\n<span class=\"ltx_td ltx_align_center\">28.50</span>\n<span class=\"ltx_td ltx_align_center\">0.001</span>\n<span class=\"ltx_td ltx_align_center\">0.015</span>\n<span class=\"ltx_td ltx_align_center\">0.051</span>\n<span class=\"ltx_td ltx_align_center\">0.112</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\">CaptionStew(Ours)</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\">56,586</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">32.23</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.006</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.080</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.231</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.401</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\">CC12M</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\">366,175</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">17.03</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.046</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.486</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.813</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.927</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb\">WikiText-103</span>\n<span class=\"ltx_td ltx_align_left ltx_border_bb\">531,346</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">74.29</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">0.031</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">0.365</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">0.757</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">0.930</span></span>\n</span>\n</span></span>\n</span></span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "embedding",
            "audiocaps",
            "size",
            "length",
            "lexical",
            "source",
            "wikitext103",
            "comparison",
            "sent",
            "caption",
            "vocab",
            "paraspeechcaps",
            "audio",
            "average",
            "cc12m",
            "statistics",
            "diversity",
            "audiosetcaps",
            "sentence",
            "text",
            "across",
            "avg",
            "captionstewours",
            "grouped",
            "corpora",
            "report",
            "distinctn",
            "vocabulary",
            "datasets",
            "jamendomaxcaps",
            "visualization",
            "wavcaps",
            "tsne",
            "fusionaudio",
            "captions"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Audio-language pretraining holds promise for general-purpose audio understanding, yet remains underexplored compared to its vision counterpart. While vision-language models like CLIP serve as widely adopted foundations, existing audio-language models primarily excel at retrieval tasks with limited adoption as general-purpose encoders.\nWe identify three key barriers: limited large-scale audio-text corpora, insufficient caption diversity, and lack of systematic exploration and evaluation.\nTo this end, we introduce CaptionStew, a 10.7M caption dataset aggregating diverse open-source audio-text corpora across multiple domains and captioning styles.\nUsing this resource, we conduct the first comprehensive evaluation comparing contrastive and captioning objectives for audio representation learning across speech, music, and environmental sound tasks.\nOur results demonstrate that audio-language pretraining yields competitive, transferable representations. Through systematic data-scaling experiments, we reveal complementary objective strengths: contrastive learning achieves superior data efficiency at smaller scales, while captioning demonstrates better scalability on language-involved audio understanding tasks. We also find that common supervised initialization practices provide diminishing returns at scale, challenging current approaches.\nThese findings establish audio-language pretraining as a viable pathway toward general-purpose audio representations, guiding future research. To accelerate progress, we release data preparation recipes, training protocols, and pretrained models, paving the way toward universal audio understanding.\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "caption",
                    "corpora",
                    "audio",
                    "diversity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Representation learning has long been central to audio processing<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>In this work, audio processing refers to audio understanding, speech analysis and music understanding, while excluding automatic speech recognition</span></span></span>, with substantial progress over the past decades.\nEarly advances relied on supervised learning, where models trained on labeled corpora were adapted to related downstream tasks or transferred across domains&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib50\" title=\"\">2020</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib15\" title=\"\">2022a</a>; Snyder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib84\" title=\"\">2018</a>; Desplanques et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib24\" title=\"\">2020</a>)</cite>.\nMore recently, self-supervised learning (SSL) has emerged as the promising paradigm.\nBy pretraining on large-scale unlabeled audio with contrastive objectives or masked modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib40\" title=\"\">2022a</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib46\" title=\"\">2021</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>)</cite>, the resulting models learn rich structural knowledge of audio signals, consistently enhancing performance across many speech and audio benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib96\" title=\"\">2021</a>; Turian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib98\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "corpora",
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While these techniques have achieved remarkable success, a fundamental limitation persists: existing methods are primarily designed to excel on specific tasks.\nThis domain specificity stems from explicit inductive biases embedded in model architectures and training objectives.\nModels optimized for environmental sounds usually underperform capturing speaker characteristics or paralinguistic information in speech, and vice versa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Turian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>)</cite>.\nAchieving general-purpose audio representations that transfer robustly across diverse audio modalities remains a challenging and actively pursued goal in the field.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An emerging and promising alternative is audio&#8211;language pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>)</cite>, which grounds audio perception with natural language descriptions (e.g. captions).\nIn this framework, text serves as a flexible semantic scaffold, offering supervision potentially spanning multiple levels of granularity, from coarse event categories\nto fine-grained acoustic attributes.\nBy aligning audio with text, audio&#8211;language pretraining provides a unified learning framework for capturing diverse audio information, offering a promising path toward general audio understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib80\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib47\" title=\"\">2025</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>; Su et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib85\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text",
                    "captions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of vision&#8211;language pretraining underscores this promise.\nModels like CLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib75\" title=\"\">2021</a>)</cite> and AIM-v2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib33\" title=\"\">2025</a>)</cite> not only power vision&#8211;language tasks but also produce representations that benefit a broad range of vision tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib60\" title=\"\">2023</a>; Minderer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib68\" title=\"\">2022</a>; Crowson et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib23\" title=\"\">2022</a>)</cite>.\nIn contrast, audio&#8211;language models have not yet achieved comparable advancements.\nWhile existing models such as CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>)</cite> excel at audio&#8211;text retrieval, their representations have seen limited adoption for broader audio understanding tasks, suggesting fundamental gaps in current approaches.\nWe identify three key challenges that have constrained progress.\nFirst, large-scale, web-mined image&#8211;text corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schuhmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib83\" title=\"\">2022</a>; Gadre et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib35\" title=\"\">2023</a>)</cite> contain billions of pairs, but no comparable resource exists for audio.\nCurrent audio caption datasets barely exceed one million pairs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>)</cite>, often relying on captions synthesized or augmented by large language models, fundamentally limiting the scaling potential of audio&#8211;language models.\nSecond, widely used audio caption corpora focus predominantly on identifying what is presenting in the audio, while lacking coverage of the rich hierarchy of acoustic attributes that define specific audio signals.\nFor instance, captions rarely characterize speaker characteristics (voice timbre, speaking style), musical attributes (harmonic structure, rhythmic patterns), or environmental acoustics (reverberation, background ambiance).\nThis imbalanced focus limits the model&#8217;s ability to learn representations that capture the full spectrum of audio semantics.\nThird, prior work has primarily focused on contrastive learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite> and evaluated on audio&#8211;text retrieval.\nSystematic studies on alternative pretraining objectives (e.g., captioning) and comprehensive evaluations across a wide suite of audio understanding tasks remain scarce, limiting our understanding of what drives effective audio&#8211;language pretraining.</p>\n\n",
                "matched_terms": [
                    "across",
                    "caption",
                    "corpora",
                    "audio",
                    "captions",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">CaptionStew</span>, a large-scale aggregation of diverse open-source audio&#8211;text datasets spanning multiple domains and captioning styles, addressing the data scarcity and diversity limitations in current audio-language pretraining.</p>\n\n",
                "matched_terms": [
                    "diversity",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide the first comprehensive evaluation of audio-language pretraining across diverse tasks and protocols, demonstrating that audio&#8211;language pretraining produces competitive, transferable representations across speech, music, and environmental audio domains.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct the first systematic comparison of contrastive learning and captioning objectives for audio representation learning, revealing that contrastive learning exhibits superior data efficiency while captioning demonstrates better scalability.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio&#8211;language pretraining learns audio representations by establishing correspondence between audio signals and natural language descriptions. The core objective is to leverage text as structured semantic supervision, enabling models to capture diverse information across speech, music, and environmental sounds within a unified framework.\nAudio&#8211;language models typically employ a two-tower architecture: an audio encoder <math alttext=\"f_{\\text{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mtext>a</mtext></msub><annotation encoding=\"application/x-tex\">f_{\\text{a}}</annotation></semantics></math> that maps raw audio signals into contextual representations, and a text component <math alttext=\"f_{\\text{t}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mtext>t</mtext></msub><annotation encoding=\"application/x-tex\">f_{\\text{t}}</annotation></semantics></math> whose design depends on the training objective.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S2.F1\" title=\"Figure 1 &#8227; 2 Language-audio Pretraining &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we explore two complementary paradigms that differ fundamentally in how they establish audio-text correspondence, contrastive and captioning objective. These approaches represent discriminative and generative perspectives on audio-language alignment, respectively.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive Objective</span> is proven to be a robust representation learning method&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib19\" title=\"\">2020b</a>; Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib75\" title=\"\">2021</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>)</cite> and have been a dominant approach for audio-language pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite>.\nThis approach aligns audio and text representations in a shared embedding space by maximizing similarity between paired samples while minimizing similarity between mismatched pairs.\nGiven a batch of paired samples <math alttext=\"\\{(a_{i},t_{i})\\}_{i=1}^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m1\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding=\"application/x-tex\">\\{(a_{i},t_{i})\\}_{i=1}^{N}</annotation></semantics></math>, the audio encoder produces frame- (or patch-) level representations that are pooled and projected to audio embeddings <math alttext=\"\\mathbf{z}^{a}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119859;</mi><mi>i</mi><mi>a</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{z}^{a}_{i}</annotation></semantics></math>, while the text encoder <math alttext=\"f_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">f_{t}</annotation></semantics></math> generates corresponding text embeddings <math alttext=\"\\mathbf{z}^{t}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>&#119859;</mi><mi>i</mi><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{z}^{t}_{i}</annotation></semantics></math>.\nThe symmetric InfoNCE loss&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Oord et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib73\" title=\"\">2018</a>)</cite> is applied to optimize both modalities:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "embedding",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\text{sim}(\\cdot,\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m5\" intent=\":literal\"><semantics><mrow><mtext>sim</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo rspace=\"0em\">,</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{sim}(\\cdot,\\cdot)</annotation></semantics></math> denotes cosine similarity and <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m6\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> is a learnable temperature parameter.\nThis objective encourages paired audio-text samples to be close in embedding space, encouraging semantic organization where similar content is grouped together.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "grouped"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Captioning Objective</span> takes a generative approach to audio-language alignment, learning representations by generating textual descriptions from audio.\nWe argue that captioning presents a promising alternative for audio-language pretraining, offering denser token-level supervision compared to contrastive learning and better alignment with recent trends toward general audio understanding systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib38\" title=\"\">2025</a>)</cite>, yet remains underexplored in prior literature.\nGiven an audio signal <math alttext=\"a_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m1\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">a_{i}</annotation></semantics></math>, the encoder <math alttext=\"f_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">f_{a}</annotation></semantics></math> produces contextual representations <math alttext=\"\\mathbf{Z}^{a}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>&#119833;</mi><mi>i</mi><mi>a</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{Z}^{a}_{i}</annotation></semantics></math>, which are fed into a transformer decoder <math alttext=\"g_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m4\" intent=\":literal\"><semantics><msub><mi>g</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">g_{t}</annotation></semantics></math> through cross-attention. Inspired by CapPa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tschannen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib88\" title=\"\">2023</a>)</cite>, we alternate between two decoding modes&#8212;autoregressive and parallel prediction&#8212;to enhance audio encoder representation learning.\nIn the autoregressive decoding, the decoder generates caption tokens <math alttext=\"(y_{1},\\ldots,y_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y_{1},\\ldots,y_{T})</annotation></semantics></math> sequentially, with each token conditioned on the audio representation and previously generated tokens. Training follows the teacher-forcing approach with a cross-entropy loss:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "caption"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the potential of audio&#8211;language pretraining for general-purpose representation learning, we construct a large-scale and diverse audio caption dataset that addresses key limitations in existing corpora. Audio signals inherently encode information across multiple dimensions&#8212;timbre, pitch, rhythm, semantic events, emotional tone, and acoustic environment&#8212;each amenable to different linguistic descriptions. However, existing large-scale audio caption datasets typically rely on a single generation pipeline, whether human-annotated or LLM-synthesized. While ensuring consistency, this approach inevitably introduces systematic biases in language style and emphasized attributes. Furthermore, single-pipeline captions exhibit limited syntactic diversity and tend to focus on specific audio facets while neglecting complementary aspects.</p>\n\n",
                "matched_terms": [
                    "across",
                    "caption",
                    "corpora",
                    "audio",
                    "captions",
                    "diversity",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To fully leverage text as a flexible semantic scaffold for diverse audio representation learning, we embrace caption diversity across sources, styles, and descriptive granularities. Rather than creating captions through a single pipeline, we aggregate existing open-source corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib18\" title=\"\">2025</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>; Roy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib79\" title=\"\">2025</a>)</cite>.\nThese datasets span multiple audio domains&#8212;general sound events, expressive speech, and musical performance&#8212;and employ fundamentally different caption creation methodologies. This aggregation yields captions that describe complementary audio aspects with varying granularity, from coarse event categories to fine-grained acoustic attributes.\nPlease refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS2\" title=\"A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a> for detail of each source dataset.\nWhen multiple datasets contain identical audio samples with different captions, we identify these overlaps and consolidate all available captions for each audio file. This multi-caption pairing allows single audio clips to benefit from diverse perspectives and descriptive focuses, enriching the supervision signal.\nTo ensure evaluation integrity, we carefully filter out samples overlapping with development or test sets of downstream benchmarks.</p>\n\n",
                "matched_terms": [
                    "across",
                    "caption",
                    "audio",
                    "corpora",
                    "captions",
                    "diversity",
                    "source",
                    "text",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The resulting dataset, <span class=\"ltx_text ltx_font_bold\">CaptionStew</span> (denoted by CS10M), contains 9.3 million audio samples paired with 10.7 million captions, spanning 37,290 hours across speech, music, and environmental domains. Compared to existing collections, CaptionStew achieves both greater scale and broader coverage. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S3.T1\" title=\"Table 1 &#8227; 3 CaptionStew Dataset &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a comparison with existing audio caption datasets.</p>\n\n",
                "matched_terms": [
                    "across",
                    "caption",
                    "audio",
                    "captions",
                    "datasets",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We pretrain all models on CaptionStew.\nAll audio is resampled to 16 kHz and converted into 80-dimensional log-Mel filterbank features using a 25 ms window length and 10 ms hop size. Text is tokenized with a 50k-vocabulary BPE tokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib54\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "length",
                    "audio",
                    "text",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The audio encoder uses a Zipformer-M architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib97\" title=\"\">2024</a>)</cite>, chosen for its efficiency on long sequences and fast convergence.\nZipformer employs six encoder blocks in a U-Net structure that processes sequences at multiple resolutions to capture fine- and coarse-grained temporal information.\nAlthough originally designed for automatic speech recognition, our preliminary experiments confirm Zipformer as a competitive backbone across audio classification tasks (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS3\" title=\"A.3 Zipformer Model &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>).\nFor contrastive pretraining, the text encoder follows BERT-base architecture (12 layers 768 hidden dimensions)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib25\" title=\"\">2019</a>)</cite>.\nFor captioning pretraining, the text decoder adopts the BART-base decoder architecture (6 layers, 768 hidden dimensions)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib54\" title=\"\">2020</a>)</cite>.\nWe use twice as many encoder layers as decoder layers to ensure comparable training speed across objectives.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following prior works in audio-language pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>)</cite>, we experiment with two scenarios: training from scratch or initialized from pretrained checkpoints.\nThe audio encoder initializes from a Zipformer-based audio event classifier trained on AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib36\" title=\"\">2017</a>)</cite> with an mAP of 0.46, while text components use corresponding publicly available checkpoints.\nAll models are trained on 8 Tesla V100 GPUs with an effective batch size of 640 seconds of audio per GPU.\nTraining runs for 600k steps from scratch (14 days wall-clock time) or 200k steps if initialized from pretrained checkpoint.</p>\n\n",
                "matched_terms": [
                    "size",
                    "audio",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate pretrained audio encoders across three protocols assessing discriminative capabilities, audio-language alignment, and open-formed question answering.\nAll experiments probe frozen representations from the audio encoder&#8217;s final layer to ensure fair model comparison. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S3.T2\" title=\"Table 2 &#8227; 3 CaptionStew Dataset &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS4\" title=\"A.4 Evaluation Datasets &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> details the datasets and metrics for each task.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "datasets",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Linear Probing</span> trains simple linear classifiers on frozen representations.\nFor classification tasks, we experiment with two pooling mechanisms&#8212;mean pooling and multi-head attention pooling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib53\" title=\"\">2019</a>)</cite>&#8212;to obtain clip-level representations.\nWe evaluate across a diverse set of tasks across audio domains, including audio event classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>)</cite>, sound event detection&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib45\" title=\"\">2021</a>)</cite>, speaker-related tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib21\" title=\"\">2018</a>; Cao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib10\" title=\"\">2014</a>)</cite>, and music classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Law et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib52\" title=\"\">2010</a>; Engel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib32\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-language Alignments</span>\nfollow the LiT protocol&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib100\" title=\"\">2022</a>)</cite>, adapting pretrained text components to align with frozen audio representations.\nFor retrieval, we pair audio encoders with pretrained RoBERTa-base text encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib62\" title=\"\">2019</a>)</cite>.\nFor captioning, we use pretrained BART-base decoders&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib54\" title=\"\">2020</a>)</cite>, and only finetune cross-attention layers as we observed more stable training.\nBoth setups finetune on corresponding datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>)</cite> while keeping audio encoders frozen.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Open-formed Question Answering</span>\nAcknowledging the trend of combining audio encoders with large language models (LLMs) for general audio understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib37\" title=\"\">2024</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib42\" title=\"\">2024</a>)</cite>, we connects frozen audio encoders to a LLM (Qwen2.5-7B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib94\" title=\"\">2024a</a>)</cite>) through lightweight adaptors that project audio representations into the LLM&#8217;s embedding space.\nWe train only the adaptor on audio QA datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipping et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib59\" title=\"\">2022</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib61\" title=\"\">2024</a>)</cite> and evaluate on corresponding track in AIR-Bench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>)</cite>.\nDuring training, we carefully monitor instruction-following behavior (<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>99%) to ensure reliable evaluation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "embedding",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive vs. Captioning Objectives.</span>\nThe two pretraining paradigms exhibit complementary strengths across evaluation protocols.\nOn linear probing tasks, contrastive learning consistently outperforms captioning, particularly excelling at audio event classification and speaker identification.\nHowever, it is worth noting that this gap narrows substantially when the classifier learns to aggregate information across frames through multi-head attention pooling (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS5\" title=\"A.5 Main Results (cont.) &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>).\nThis observation reflects the objectives&#8217; inherent designs: contrastive learning explicitly optimizes for linearly separable clip-level representations, while captioning relies on cross-attention mechanisms over frame-level representations for text sequence generation.\nThis finding aligns with recent work highlighting how downstream module choices significantly impact the assessment of audio representation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zaiem et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib99\" title=\"\">2023</a>)</cite>.\nFor language-involved tasks, both objectives demonstrate competitive performance, with captioning showing slight advantages in open-form question answering across multiple domains.\nThis suggests captioning&#8217;s potential for language-involved audio understanding tasks, aligning with recent trends toward generative audio understanding systems.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Supervised Initialization.</span> Initializing from supervised pretraining (AS SL) provides substantial benefits across most tasks, with notable improvements on audio event classification, sound event detection and audio-text retreival.\nThe gains are particularly pronounced for contrastive objectives, suggesting that discriminative pretraining provides useful inductive biases for contrastive learning.\nHowever, these benefits diminish (or disappear entirely) when the attributes required for downstream tasks diverge from AudioSet&#8217;s ontology.\nOn speaker identification and music tagging, scratch-trained models often match or exceed initialized variants, indicating that AudioSet&#8217;s focus on distinguishing between sound categories may bias representations toward event-level semantics rather than the acoustic attributes (voice timbre, speaking style) or musical structure (genre, harmony, rhythm) essential for these tasks.\nThese findings challenge common initialization practices for audio-language pretraining and suggest the need for tailored pretraining strategies when targeting general-purpose audio representation learning.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Competitive Performance Across Domains.</span> Our audio-language representations achieve strong transferability across diverse audio domains.\nCompared to supervised baselines (Zipformer-AEC), our overall best-performing model (Contrastive-init) demonstrate superior performance on speaker identification, music understanding and audio-text retrieval while maintaining competitiveness on audio-event classification.\nAgainst domain-specialized SSL methods (BEATs, wav2vec2, MERT), our approach consistently shows competitive performance.\nThis consistent cross-domain performance validates our hypothesis that diverse caption aggregation enables broadly transferable representations, establishing audio-language pretraining as a viable path toward learning general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "caption"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scaling Patterns.</span> Most tasks demonstrate consistent performance improvements with increased data scale, validating the potential of large-scale audio-language pretraining.\nHowever, notable exceptions emerge that reveal fundamental limitations of current approaches.\nSound event detection, particularly for models initialized with AudioSet pretraining, exhibits a reverse scaling trend where performance degrades with more caption data.\nThis suggests a potential conflict between natural language supervision&#8211;which typically describes audio characteristics and attributes&#8211;and temporal localization tasks requiring precise event boundaries.\nAdditionally, emotion recognition and instrument classification show weaker scaling gains compared to other tasks, likely reflecting limited caption diversity for these specific attributes in existing corpora, which we will discussed in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.SS6\" title=\"4.6 Dataset Analysis &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "corpora",
                    "audio",
                    "caption",
                    "diversity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive vs. Captioning Scaling. </span>\nContrastive learning consistently outperforms captioning at varying data scales, particularly under less data and on discriminative tasks such as audio event classification.\nHowever, captioning demonstrates slightly better scaling properties, with distinct patterns emerging across task categories.\nor language-involved tasks&#8211;especially captioning and question answering&#8211;captioning matches or surpasses contrastive learning at our current 10M-pair scale.\nOn linear probing benchmarks, the gap remains substantial, with scaling trends suggesting captioning would require hundreds of millions of pairs to achieve parity with contrastive methods.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Initialization at Scale.</span>\nAudioSet initialization provides immediate performance gains but introduces diminishing returns at larger scales. Both contrastive learning and captioning show decreasing benefits from initialization as data scale increases, with scratch and initialized models achieving matched performance at larger scales on some tasks.\nThis suggests that pretrained initialization effectively bootstraps learning at small scales but may constrain the model&#8217;s ability to adapt to the broader semantic space covered by large-scale caption data, potentially due to mismatch between AudioSet&#8217;s ontology and diverse audio descriptions.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "caption"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, these findings reveal complementary behaviors: contrastive pretraining achieves superior data efficiency at current scales, while captioning shows better scalability, especially for language-involved tasks.\nImportantly, the diminishing returns of initialization at scale indicate that large-scale caption data can provide sufficient semantic supervision independent of domain-specific pretraining, challenging current practices of audio-language pretraining and opening possibilities for learning general-purpose representations from diverse text descriptions alone.</p>\n\n",
                "matched_terms": [
                    "caption",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To understand the linguistic characteristics of CaptionStew, we analyze caption diversity across constituent datasets through visualization and quantitative methods.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.SS6\" title=\"4.6 Dataset Analysis &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4.6</span></a> provides compelling evidence of our aggregation strategy&#8217;s success through t-SNE visualization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Maaten &amp; Hinton, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib65\" title=\"\">2008</a>)</cite> of sentence embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reimers &amp; Gurevych, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib77\" title=\"\">2019</a>)</cite> from sampled captions, revealing distinct clustering patterns by source that demonstrate complementary linguistic perspectives: AudioSetCaps and WavCaps overlap in audio event descriptions and aligns more with human annotated dataset, while JamendoMaxCaps creates a distinct cluster focused on music-specific terminology, and ParaSpeechCaps forms a separate cluster emphasizing speaking styles and paralinguistic attributes.\nThese minimal overlaps confirm that each dataset contributes distinct caption styles and descriptive focuses.</p>\n\n",
                "matched_terms": [
                    "jamendomaxcaps",
                    "across",
                    "caption",
                    "visualization",
                    "paraspeechcaps",
                    "wavcaps",
                    "audio",
                    "tsne",
                    "captions",
                    "diversity",
                    "source",
                    "audiosetcaps",
                    "sentence",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Quantitative analysis reveals both the benefits and limitations (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.T4\" title=\"Table 4 &#8227; 4.6 Dataset Analysis &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).\nCaptionStew achieves substantial vocabulary expansion (56,586 unique words vs. 4,060-27,906 for individual datasets)\nHowever, this growth doesn&#8217;t yield proportional lexical diversity.\nCaptionStew&#8217;s Distinct-n metrics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib55\" title=\"\">2015</a>)</cite> remain low, falling short of image caption dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Changpinyo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib13\" title=\"\">2021</a>)</cite> and text corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Merity et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib67\" title=\"\">2016</a>)</cite>. This constraint stems from datasets with limited linguistic variation, particularly JamendoMaxCaps and ParaSpeechCaps with extremely low Distinct-n scores.</p>\n\n",
                "matched_terms": [
                    "jamendomaxcaps",
                    "caption",
                    "paraspeechcaps",
                    "corpora",
                    "distinctn",
                    "lexical",
                    "diversity",
                    "vocabulary",
                    "text",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings highlight that simply combining datasets doesn&#8217;t guarantee improved linguistic diversity, revealing broader limitations in current audio-language pretraining approaches.\nAlso, the constrained diversity in certain aspect may partially explain weaker scaling behavior observed for certain tasks, as models encounter repetitive linguistic patterns despite increased data volume, aligning with vision-language findings on caption diversity&#8217;s importance for representation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Santurkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib82\" title=\"\">2023</a>; Chan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib12\" title=\"\">2022</a>)</cite>.\nThis analysis motivates developing enhanced aggregation pipeline and more diverse caption generation methods to better capture the full spectrum of information in audio signals, thereby fully realizing the potential of large-scale audio-language pretraining.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "caption",
                    "datasets",
                    "diversity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Representation Learning.</span>\nThe ultimate goal of audio representation learning is developing a single model suitable for diverse audio understanding tasks.\nSupervised models trained on labeled datasets have been fundamental to the field, including audio event classifiers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib44\" title=\"\">2017</a>; Cramer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib22\" title=\"\">2019</a>; Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib50\" title=\"\">2020</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib39\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib15\" title=\"\">2022a</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib26\" title=\"\">2024</a>)</cite>, speech recognition systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib76\" title=\"\">2023</a>)</cite> and speaker recognition models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Snyder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib84\" title=\"\">2018</a>; Desplanques et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib24\" title=\"\">2020</a>)</cite>.\nThese approaches remain widely adopted due to their strong performance on target tasks. In parallel, self-supervised learning methods have emerged as a complementary approach, offering advances across speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib46\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib16\" title=\"\">2022b</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib6\" title=\"\">2022</a>)</cite>, audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib40\" title=\"\">2022a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib48\" title=\"\">2022</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>; Li &amp; Li, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib56\" title=\"\">2022</a>)</cite>, and music&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib102\" title=\"\">2025</a>)</cite> without requiring labeled data.\nWhile these methods show improved generalization within their target domains, achieving truly general-purpose audio representations remains challenging.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio&#8211;Language Pretraining.</span>\nAudio-language models have emerged as a promising approach for learning cross-modal representations. Most existing work focuses on contrastive learning objectives that align audio and text in shared embedding spaces&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite>. Recent extensions have explored combinations with other objectives&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib93\" title=\"\">2023</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib101\" title=\"\">2024</a>; Niizumi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib71\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib72\" title=\"\">2025</a>)</cite>.\nThe field has also witnessed rapid evolution in datasets, transitioning from traditional human-annotated corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>)</cite> to recently constructed LLM-augmented collections&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib18\" title=\"\">2025</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib86\" title=\"\">Sun et&#160;al., </a>)</cite> and domain-specific resources covering speech characteristics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>)</cite>, and musical attributes&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Roy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib79\" title=\"\">2025</a>)</cite>.\nOur work contributes by providing the first systematic comparison between contrastive and captioning objectives, along with comprehensive evaluation toward general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "datasets",
                    "corpora",
                    "audio",
                    "text",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Universal Audio Understanding.</span>\nThe evaluation of audio understanding has evolved from task-specific classification benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib96\" title=\"\">2021</a>; Turian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib98\" title=\"\">2023</a>)</cite> toward more comprehensive assessment frameworks.\nRecent developments have emphasized LLM-based audio understanding systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib37\" title=\"\">2024</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib42\" title=\"\">2024</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib38\" title=\"\">2025</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib20\" title=\"\">2024</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib87\" title=\"\">2024</a>)</cite> that can handle open-form queries and complex reasoning tasks.\nThis shift has driven the development of corresponding evaluation benchmarks that assess models&#8217; abilities across diverse audio understanding scenarios, including question answering, reasoning, and multi-step audio analysis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib80\" title=\"\">2025</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib47\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib64\" title=\"\">2025</a>)</cite>.\nOur work contributes to this trend by providing the first comprehensive evaluation of audio-language pretraining across discriminative tasks, audio-language alignment, and open-form question answering, thereby bridging the gap between traditional representation learning evaluation and modern universal audio understanding.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We revisited audio-language pretraining to advance general-purpose audio representation learning. We introduced CaptionStew, a large-scale aggregation of diverse audio caption datasets, and through comprehensive evaluation across tasks, demonstrated that audio-language pretraining produces competitive representations across speech, music, and environmental domains. Our systematic comparison revealed complementary strengths of two audio-language pretraining objectives: contrastive learning excels in data efficiency, while captioning shows better scalability for language-involved tasks.\nData-scaling experiments highlighted diminishing returns of supervised initialization, challenging common practices, while dataset analysis revealed linguistic diversity limitation in current datasets. These findings establish audio-language pretraining as a viable pathway toward universal audio understanding.</p>\n\n",
                "matched_terms": [
                    "across",
                    "caption",
                    "datasets",
                    "audio",
                    "diversity",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dataset Construction and Quality.</span> CaptionStew aggregates captions from multiple sources with varying generation methodologies, including LLM-synthesized descriptions that may introduce systematic biases or artifacts.\nWe do not perform extensive quality control or human verification across the aggregated corpus, which could impact model training.\nAdditionally, our dataset analysis reveals that simple aggregation does not guarantee improved linguistic diversity&#8212;CaptionStew&#8217;s lexical diversity metrics remain lower than mature image-text corpora.\nHowever, our design choice prioritizes semantic diversity over linguistic variety, as evidenced by the t-SNE clustering analysis showing distinct descriptive focuses across constituent datasets.\nWhile more sophisticated curation strategies could improve quality, our goal was to establish whether diverse caption aggregation can benefit audio representation learning, which our results support despite these limitations.</p>\n\n",
                "matched_terms": [
                    "across",
                    "caption",
                    "tsne",
                    "audio",
                    "corpora",
                    "captions",
                    "lexical",
                    "diversity",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CaptionStew aggregates eight open-source audio caption datasets to address data scarcity and limited diversity in current audio-language pretraining. The constituent datasets span environmental sounds, music, and expressive speech, with fundamentally different captioning approaches&#8212;from crowdsourced human annotation to expert curation to various LLM-based generation pipelines.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T5\" title=\"Table 5 &#8227; A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T6\" title=\"Table 6 &#8227; A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> detail each dataset&#8217;s characteristics and provide example captions that illustrate the diverse descriptive styles, ranging from concise event descriptions to detailed multi-sentence narratives with fine-grained acoustic and contextual information.\nDuring aggregation, we filter audio samples longer than one minute for computational efficiency and remove samples that overlap with common audio understanding benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>; Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>; Salamon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib81\" title=\"\">2014</a>)</cite> to prevent data leakage. This approach preserves the unique characteristics of each source while creating a unified corpus that captures broader semantic coverage than individual datasets.</p>\n\n",
                "matched_terms": [
                    "caption",
                    "audio",
                    "captions",
                    "diversity",
                    "source",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we adopt the Zipformer-M architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib97\" title=\"\">2024</a>)</cite> as the audio encoder, chosen for its memory efficiency on long sequences and strong performance across audio tasks. The architecture employs a U-Net-inspired design with six Transformer stages that process sequences at multiple temporal resolutions. The stages operate at progressively decreasing then increasing frame rates (50, 25, 12.5, 6.25, 12.5, and 25 Hz), with residual and upsampling connections between stages to capture both fine-grained and long-range temporal patterns.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although Zipformer was originally designed for automatic speech recognition, we conducted preliminary experiments to validate its effectiveness as a general audio encoder across diverse domains.\nAs in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T7\" title=\"Table 7 &#8227; A.3 Zipformer Model &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, our initial studies confirmed that Zipformer achieves competitive performance on environmental sound classification, music understanding, and speaker-related tasks, demonstrating its suitability as a unified backbone for multi-domain audio representation learning. This cross-domain efficacy makes it an appropriate choice for our audio-language pretraining experiments that span speech, music, and environmental audio.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T8\" title=\"Table 8 &#8227; A.4 Evaluation Datasets &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> details the evaluation datasets and their metrics used for assessing audio representation quality across our three evaluation protocols: linear probing&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Fonseca et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>); Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib21\" title=\"\">2018</a>); Cao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib10\" title=\"\">2014</a>); Law et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib52\" title=\"\">2010</a>); Engel et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib32\" title=\"\">2017</a>); Hershey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib45\" title=\"\">2021</a>); Ebbers et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib30\" title=\"\">2022</a>)</cite>, audio-language alignment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>); Diwan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>); Agostinelli et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib58\" title=\"\">2004</a>)</cite> and open-form question answering&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lipping et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib59\" title=\"\">2022</a>); Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib61\" title=\"\">2024</a>); Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Aside from learning representations, we also compare against state-of-the-art audio-text retrieval models to assess our approach&#8217;s performance on the specific task it was designed for. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T10\" title=\"Table 10 &#8227; A.6 Additional Results &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> presents retrieval results for our best-performing model (Contrastive-init) against state-of-the-art\naudio-text retrieval model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>)</cite>.\nOur model achieving comparable or superior results on benchmarks in various audio domains, with particularly strong performance on speech and music retrieval.\nThe results indicate that our general-purpose audio-language pretraining approach can compete with specialized retrieval models while offering broader applicability across diverse usage scenarios.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across"
                ]
            }
        ]
    },
    "A1.T5": {
        "caption": "Table 5: Details of public-available datasets contribute to proposed CaptionStew dataset. We summarize their size, domain coverage, audio sources, captioning style, and generation pipelines.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">crowdsourced /</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">retrieval-based</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">metadata imputation</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">+ LALM captioning</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "their",
            "style",
            "size",
            "details",
            "imputation",
            "sources",
            "audio",
            "captioning",
            "generation",
            "publicavailable",
            "pipelines",
            "lalm",
            "captionstew",
            "metadata",
            "datasets",
            "retrievalbased",
            "contribute",
            "proposed",
            "summarize",
            "domain",
            "dataset",
            "coverage",
            "crowdsourced"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">CaptionStew aggregates eight open-source audio caption datasets to address data scarcity and limited diversity in current audio-language pretraining. The constituent datasets span environmental sounds, music, and expressive speech, with fundamentally different captioning approaches&#8212;from crowdsourced human annotation to expert curation to various LLM-based generation pipelines.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T5\" title=\"Table 5 &#8227; A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T6\" title=\"Table 6 &#8227; A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> detail each dataset&#8217;s characteristics and provide example captions that illustrate the diverse descriptive styles, ranging from concise event descriptions to detailed multi-sentence narratives with fine-grained acoustic and contextual information.\nDuring aggregation, we filter audio samples longer than one minute for computational efficiency and remove samples that overlap with common audio understanding benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>; Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>; Salamon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib81\" title=\"\">2014</a>)</cite> to prevent data leakage. This approach preserves the unique characteristics of each source while creating a unified corpus that captures broader semantic coverage than individual datasets.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Audio-language pretraining holds promise for general-purpose audio understanding, yet remains underexplored compared to its vision counterpart. While vision-language models like CLIP serve as widely adopted foundations, existing audio-language models primarily excel at retrieval tasks with limited adoption as general-purpose encoders.\nWe identify three key barriers: limited large-scale audio-text corpora, insufficient caption diversity, and lack of systematic exploration and evaluation.\nTo this end, we introduce CaptionStew, a 10.7M caption dataset aggregating diverse open-source audio-text corpora across multiple domains and captioning styles.\nUsing this resource, we conduct the first comprehensive evaluation comparing contrastive and captioning objectives for audio representation learning across speech, music, and environmental sound tasks.\nOur results demonstrate that audio-language pretraining yields competitive, transferable representations. Through systematic data-scaling experiments, we reveal complementary objective strengths: contrastive learning achieves superior data efficiency at smaller scales, while captioning demonstrates better scalability on language-involved audio understanding tasks. We also find that common supervised initialization practices provide diminishing returns at scale, challenging current approaches.\nThese findings establish audio-language pretraining as a viable pathway toward general-purpose audio representations, guiding future research. To accelerate progress, we release data preparation recipes, training protocols, and pretrained models, paving the way toward universal audio understanding.\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "captionstew",
                    "dataset",
                    "captioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While these techniques have achieved remarkable success, a fundamental limitation persists: existing methods are primarily designed to excel on specific tasks.\nThis domain specificity stems from explicit inductive biases embedded in model architectures and training objectives.\nModels optimized for environmental sounds usually underperform capturing speaker characteristics or paralinguistic information in speech, and vice versa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Turian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>)</cite>.\nAchieving general-purpose audio representations that transfer robustly across diverse audio modalities remains a challenging and actively pursued goal in the field.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "domain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of vision&#8211;language pretraining underscores this promise.\nModels like CLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib75\" title=\"\">2021</a>)</cite> and AIM-v2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib33\" title=\"\">2025</a>)</cite> not only power vision&#8211;language tasks but also produce representations that benefit a broad range of vision tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib60\" title=\"\">2023</a>; Minderer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib68\" title=\"\">2022</a>; Crowson et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib23\" title=\"\">2022</a>)</cite>.\nIn contrast, audio&#8211;language models have not yet achieved comparable advancements.\nWhile existing models such as CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>)</cite> excel at audio&#8211;text retrieval, their representations have seen limited adoption for broader audio understanding tasks, suggesting fundamental gaps in current approaches.\nWe identify three key challenges that have constrained progress.\nFirst, large-scale, web-mined image&#8211;text corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schuhmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib83\" title=\"\">2022</a>; Gadre et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib35\" title=\"\">2023</a>)</cite> contain billions of pairs, but no comparable resource exists for audio.\nCurrent audio caption datasets barely exceed one million pairs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>)</cite>, often relying on captions synthesized or augmented by large language models, fundamentally limiting the scaling potential of audio&#8211;language models.\nSecond, widely used audio caption corpora focus predominantly on identifying what is presenting in the audio, while lacking coverage of the rich hierarchy of acoustic attributes that define specific audio signals.\nFor instance, captions rarely characterize speaker characteristics (voice timbre, speaking style), musical attributes (harmonic structure, rhythmic patterns), or environmental acoustics (reverberation, background ambiance).\nThis imbalanced focus limits the model&#8217;s ability to learn representations that capture the full spectrum of audio semantics.\nThird, prior work has primarily focused on contrastive learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite> and evaluated on audio&#8211;text retrieval.\nSystematic studies on alternative pretraining objectives (e.g., captioning) and comprehensive evaluations across a wide suite of audio understanding tasks remain scarce, limiting our understanding of what drives effective audio&#8211;language pretraining.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "their",
                    "captioning",
                    "style",
                    "coverage",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">CaptionStew</span>, a large-scale aggregation of diverse open-source audio&#8211;text datasets spanning multiple domains and captioning styles, addressing the data scarcity and diversity limitations in current audio-language pretraining.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "captionstew",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct the first systematic comparison of contrastive learning and captioning objectives for audio representation learning, revealing that contrastive learning exhibits superior data efficiency while captioning demonstrates better scalability.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio&#8211;language pretraining learns audio representations by establishing correspondence between audio signals and natural language descriptions. The core objective is to leverage text as structured semantic supervision, enabling models to capture diverse information across speech, music, and environmental sounds within a unified framework.\nAudio&#8211;language models typically employ a two-tower architecture: an audio encoder <math alttext=\"f_{\\text{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mtext>a</mtext></msub><annotation encoding=\"application/x-tex\">f_{\\text{a}}</annotation></semantics></math> that maps raw audio signals into contextual representations, and a text component <math alttext=\"f_{\\text{t}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mtext>t</mtext></msub><annotation encoding=\"application/x-tex\">f_{\\text{t}}</annotation></semantics></math> whose design depends on the training objective.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S2.F1\" title=\"Figure 1 &#8227; 2 Language-audio Pretraining &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we explore two complementary paradigms that differ fundamentally in how they establish audio-text correspondence, contrastive and captioning objective. These approaches represent discriminative and generative perspectives on audio-language alignment, respectively.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "captioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Captioning Objective</span> takes a generative approach to audio-language alignment, learning representations by generating textual descriptions from audio.\nWe argue that captioning presents a promising alternative for audio-language pretraining, offering denser token-level supervision compared to contrastive learning and better alignment with recent trends toward general audio understanding systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib38\" title=\"\">2025</a>)</cite>, yet remains underexplored in prior literature.\nGiven an audio signal <math alttext=\"a_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m1\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">a_{i}</annotation></semantics></math>, the encoder <math alttext=\"f_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">f_{a}</annotation></semantics></math> produces contextual representations <math alttext=\"\\mathbf{Z}^{a}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>&#119833;</mi><mi>i</mi><mi>a</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{Z}^{a}_{i}</annotation></semantics></math>, which are fed into a transformer decoder <math alttext=\"g_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m4\" intent=\":literal\"><semantics><msub><mi>g</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">g_{t}</annotation></semantics></math> through cross-attention. Inspired by CapPa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tschannen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib88\" title=\"\">2023</a>)</cite>, we alternate between two decoding modes&#8212;autoregressive and parallel prediction&#8212;to enhance audio encoder representation learning.\nIn the autoregressive decoding, the decoder generates caption tokens <math alttext=\"(y_{1},\\ldots,y_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y_{1},\\ldots,y_{T})</annotation></semantics></math> sequentially, with each token conditioned on the audio representation and previously generated tokens. Training follows the teacher-forcing approach with a cross-entropy loss:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "captioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the potential of audio&#8211;language pretraining for general-purpose representation learning, we construct a large-scale and diverse audio caption dataset that addresses key limitations in existing corpora. Audio signals inherently encode information across multiple dimensions&#8212;timbre, pitch, rhythm, semantic events, emotional tone, and acoustic environment&#8212;each amenable to different linguistic descriptions. However, existing large-scale audio caption datasets typically rely on a single generation pipeline, whether human-annotated or LLM-synthesized. While ensuring consistency, this approach inevitably introduces systematic biases in language style and emphasized attributes. Furthermore, single-pipeline captions exhibit limited syntactic diversity and tend to focus on specific audio facets while neglecting complementary aspects.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "style",
                    "generation",
                    "dataset",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To fully leverage text as a flexible semantic scaffold for diverse audio representation learning, we embrace caption diversity across sources, styles, and descriptive granularities. Rather than creating captions through a single pipeline, we aggregate existing open-source corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib18\" title=\"\">2025</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>; Roy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib79\" title=\"\">2025</a>)</cite>.\nThese datasets span multiple audio domains&#8212;general sound events, expressive speech, and musical performance&#8212;and employ fundamentally different caption creation methodologies. This aggregation yields captions that describe complementary audio aspects with varying granularity, from coarse event categories to fine-grained acoustic attributes.\nPlease refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS2\" title=\"A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a> for detail of each source dataset.\nWhen multiple datasets contain identical audio samples with different captions, we identify these overlaps and consolidate all available captions for each audio file. This multi-caption pairing allows single audio clips to benefit from diverse perspectives and descriptive focuses, enriching the supervision signal.\nTo ensure evaluation integrity, we carefully filter out samples overlapping with development or test sets of downstream benchmarks.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "audio",
                    "datasets",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The resulting dataset, <span class=\"ltx_text ltx_font_bold\">CaptionStew</span> (denoted by CS10M), contains 9.3 million audio samples paired with 10.7 million captions, spanning 37,290 hours across speech, music, and environmental domains. Compared to existing collections, CaptionStew achieves both greater scale and broader coverage. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S3.T1\" title=\"Table 1 &#8227; 3 CaptionStew Dataset &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a comparison with existing audio caption datasets.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "captionstew",
                    "dataset",
                    "coverage",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We pretrain all models on CaptionStew.\nAll audio is resampled to 16 kHz and converted into 80-dimensional log-Mel filterbank features using a 25 ms window length and 10 ms hop size. Text is tokenized with a 50k-vocabulary BPE tokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib54\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "size",
                    "audio",
                    "captionstew"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The audio encoder uses a Zipformer-M architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib97\" title=\"\">2024</a>)</cite>, chosen for its efficiency on long sequences and fast convergence.\nZipformer employs six encoder blocks in a U-Net structure that processes sequences at multiple resolutions to capture fine- and coarse-grained temporal information.\nAlthough originally designed for automatic speech recognition, our preliminary experiments confirm Zipformer as a competitive backbone across audio classification tasks (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS3\" title=\"A.3 Zipformer Model &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>).\nFor contrastive pretraining, the text encoder follows BERT-base architecture (12 layers 768 hidden dimensions)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib25\" title=\"\">2019</a>)</cite>.\nFor captioning pretraining, the text decoder adopts the BART-base decoder architecture (6 layers, 768 hidden dimensions)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib54\" title=\"\">2020</a>)</cite>.\nWe use twice as many encoder layers as decoder layers to ensure comparable training speed across objectives.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "captioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following prior works in audio-language pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>)</cite>, we experiment with two scenarios: training from scratch or initialized from pretrained checkpoints.\nThe audio encoder initializes from a Zipformer-based audio event classifier trained on AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib36\" title=\"\">2017</a>)</cite> with an mAP of 0.46, while text components use corresponding publicly available checkpoints.\nAll models are trained on 8 Tesla V100 GPUs with an effective batch size of 640 seconds of audio per GPU.\nTraining runs for 600k steps from scratch (14 days wall-clock time) or 200k steps if initialized from pretrained checkpoint.</p>\n\n",
                "matched_terms": [
                    "size",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate pretrained audio encoders across three protocols assessing discriminative capabilities, audio-language alignment, and open-formed question answering.\nAll experiments probe frozen representations from the audio encoder&#8217;s final layer to ensure fair model comparison. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S3.T2\" title=\"Table 2 &#8227; 3 CaptionStew Dataset &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS4\" title=\"A.4 Evaluation Datasets &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> details the datasets and metrics for each task.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "details",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-language Alignments</span>\nfollow the LiT protocol&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib100\" title=\"\">2022</a>)</cite>, adapting pretrained text components to align with frozen audio representations.\nFor retrieval, we pair audio encoders with pretrained RoBERTa-base text encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib62\" title=\"\">2019</a>)</cite>.\nFor captioning, we use pretrained BART-base decoders&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib54\" title=\"\">2020</a>)</cite>, and only finetune cross-attention layers as we observed more stable training.\nBoth setups finetune on corresponding datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>)</cite> while keeping audio encoders frozen.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "datasets",
                    "captioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Open-formed Question Answering</span>\nAcknowledging the trend of combining audio encoders with large language models (LLMs) for general audio understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib37\" title=\"\">2024</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib42\" title=\"\">2024</a>)</cite>, we connects frozen audio encoders to a LLM (Qwen2.5-7B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib94\" title=\"\">2024a</a>)</cite>) through lightweight adaptors that project audio representations into the LLM&#8217;s embedding space.\nWe train only the adaptor on audio QA datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipping et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib59\" title=\"\">2022</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib61\" title=\"\">2024</a>)</cite> and evaluate on corresponding track in AIR-Bench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>)</cite>.\nDuring training, we carefully monitor instruction-following behavior (<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>99%) to ensure reliable evaluation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive vs. Captioning Objectives.</span>\nThe two pretraining paradigms exhibit complementary strengths across evaluation protocols.\nOn linear probing tasks, contrastive learning consistently outperforms captioning, particularly excelling at audio event classification and speaker identification.\nHowever, it is worth noting that this gap narrows substantially when the classifier learns to aggregate information across frames through multi-head attention pooling (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS5\" title=\"A.5 Main Results (cont.) &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>).\nThis observation reflects the objectives&#8217; inherent designs: contrastive learning explicitly optimizes for linearly separable clip-level representations, while captioning relies on cross-attention mechanisms over frame-level representations for text sequence generation.\nThis finding aligns with recent work highlighting how downstream module choices significantly impact the assessment of audio representation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zaiem et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib99\" title=\"\">2023</a>)</cite>.\nFor language-involved tasks, both objectives demonstrate competitive performance, with captioning showing slight advantages in open-form question answering across multiple domains.\nThis suggests captioning&#8217;s potential for language-involved audio understanding tasks, aligning with recent trends toward generative audio understanding systems.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "generation",
                    "captioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Supervised Initialization.</span> Initializing from supervised pretraining (AS SL) provides substantial benefits across most tasks, with notable improvements on audio event classification, sound event detection and audio-text retreival.\nThe gains are particularly pronounced for contrastive objectives, suggesting that discriminative pretraining provides useful inductive biases for contrastive learning.\nHowever, these benefits diminish (or disappear entirely) when the attributes required for downstream tasks diverge from AudioSet&#8217;s ontology.\nOn speaker identification and music tagging, scratch-trained models often match or exceed initialized variants, indicating that AudioSet&#8217;s focus on distinguishing between sound categories may bias representations toward event-level semantics rather than the acoustic attributes (voice timbre, speaking style) or musical structure (genre, harmony, rhythm) essential for these tasks.\nThese findings challenge common initialization practices for audio-language pretraining and suggest the need for tailored pretraining strategies when targeting general-purpose audio representation learning.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive vs. Captioning Scaling. </span>\nContrastive learning consistently outperforms captioning at varying data scales, particularly under less data and on discriminative tasks such as audio event classification.\nHowever, captioning demonstrates slightly better scaling properties, with distinct patterns emerging across task categories.\nor language-involved tasks&#8211;especially captioning and question answering&#8211;captioning matches or surpasses contrastive learning at our current 10M-pair scale.\nOn linear probing benchmarks, the gap remains substantial, with scaling trends suggesting captioning would require hundreds of millions of pairs to achieve parity with contrastive methods.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "captioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Initialization at Scale.</span>\nAudioSet initialization provides immediate performance gains but introduces diminishing returns at larger scales. Both contrastive learning and captioning show decreasing benefits from initialization as data scale increases, with scratch and initialized models achieving matched performance at larger scales on some tasks.\nThis suggests that pretrained initialization effectively bootstraps learning at small scales but may constrain the model&#8217;s ability to adapt to the broader semantic space covered by large-scale caption data, potentially due to mismatch between AudioSet&#8217;s ontology and diverse audio descriptions.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "captioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To understand the linguistic characteristics of CaptionStew, we analyze caption diversity across constituent datasets through visualization and quantitative methods.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.SS6\" title=\"4.6 Dataset Analysis &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4.6</span></a> provides compelling evidence of our aggregation strategy&#8217;s success through t-SNE visualization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Maaten &amp; Hinton, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib65\" title=\"\">2008</a>)</cite> of sentence embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reimers &amp; Gurevych, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib77\" title=\"\">2019</a>)</cite> from sampled captions, revealing distinct clustering patterns by source that demonstrate complementary linguistic perspectives: AudioSetCaps and WavCaps overlap in audio event descriptions and aligns more with human annotated dataset, while JamendoMaxCaps creates a distinct cluster focused on music-specific terminology, and ParaSpeechCaps forms a separate cluster emphasizing speaking styles and paralinguistic attributes.\nThese minimal overlaps confirm that each dataset contributes distinct caption styles and descriptive focuses.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "captionstew",
                    "datasets",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Quantitative analysis reveals both the benefits and limitations (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.T4\" title=\"Table 4 &#8227; 4.6 Dataset Analysis &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).\nCaptionStew achieves substantial vocabulary expansion (56,586 unique words vs. 4,060-27,906 for individual datasets)\nHowever, this growth doesn&#8217;t yield proportional lexical diversity.\nCaptionStew&#8217;s Distinct-n metrics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib55\" title=\"\">2015</a>)</cite> remain low, falling short of image caption dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Changpinyo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib13\" title=\"\">2021</a>)</cite> and text corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Merity et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib67\" title=\"\">2016</a>)</cite>. This constraint stems from datasets with limited linguistic variation, particularly JamendoMaxCaps and ParaSpeechCaps with extremely low Distinct-n scores.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "captionstew",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings highlight that simply combining datasets doesn&#8217;t guarantee improved linguistic diversity, revealing broader limitations in current audio-language pretraining approaches.\nAlso, the constrained diversity in certain aspect may partially explain weaker scaling behavior observed for certain tasks, as models encounter repetitive linguistic patterns despite increased data volume, aligning with vision-language findings on caption diversity&#8217;s importance for representation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Santurkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib82\" title=\"\">2023</a>; Chan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib12\" title=\"\">2022</a>)</cite>.\nThis analysis motivates developing enhanced aggregation pipeline and more diverse caption generation methods to better capture the full spectrum of information in audio signals, thereby fully realizing the potential of large-scale audio-language pretraining.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "generation",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Representation Learning.</span>\nThe ultimate goal of audio representation learning is developing a single model suitable for diverse audio understanding tasks.\nSupervised models trained on labeled datasets have been fundamental to the field, including audio event classifiers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib44\" title=\"\">2017</a>; Cramer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib22\" title=\"\">2019</a>; Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib50\" title=\"\">2020</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib39\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib15\" title=\"\">2022a</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib26\" title=\"\">2024</a>)</cite>, speech recognition systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib76\" title=\"\">2023</a>)</cite> and speaker recognition models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Snyder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib84\" title=\"\">2018</a>; Desplanques et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib24\" title=\"\">2020</a>)</cite>.\nThese approaches remain widely adopted due to their strong performance on target tasks. In parallel, self-supervised learning methods have emerged as a complementary approach, offering advances across speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib46\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib16\" title=\"\">2022b</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib6\" title=\"\">2022</a>)</cite>, audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib40\" title=\"\">2022a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib48\" title=\"\">2022</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>; Li &amp; Li, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib56\" title=\"\">2022</a>)</cite>, and music&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib102\" title=\"\">2025</a>)</cite> without requiring labeled data.\nWhile these methods show improved generalization within their target domains, achieving truly general-purpose audio representations remains challenging.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "their",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio&#8211;Language Pretraining.</span>\nAudio-language models have emerged as a promising approach for learning cross-modal representations. Most existing work focuses on contrastive learning objectives that align audio and text in shared embedding spaces&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite>. Recent extensions have explored combinations with other objectives&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib93\" title=\"\">2023</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib101\" title=\"\">2024</a>; Niizumi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib71\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib72\" title=\"\">2025</a>)</cite>.\nThe field has also witnessed rapid evolution in datasets, transitioning from traditional human-annotated corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>)</cite> to recently constructed LLM-augmented collections&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib18\" title=\"\">2025</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib86\" title=\"\">Sun et&#160;al., </a>)</cite> and domain-specific resources covering speech characteristics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>)</cite>, and musical attributes&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Roy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib79\" title=\"\">2025</a>)</cite>.\nOur work contributes by providing the first systematic comparison between contrastive and captioning objectives, along with comprehensive evaluation toward general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "datasets",
                    "captioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We revisited audio-language pretraining to advance general-purpose audio representation learning. We introduced CaptionStew, a large-scale aggregation of diverse audio caption datasets, and through comprehensive evaluation across tasks, demonstrated that audio-language pretraining produces competitive representations across speech, music, and environmental domains. Our systematic comparison revealed complementary strengths of two audio-language pretraining objectives: contrastive learning excels in data efficiency, while captioning shows better scalability for language-involved tasks.\nData-scaling experiments highlighted diminishing returns of supervised initialization, challenging common practices, while dataset analysis revealed linguistic diversity limitation in current datasets. These findings establish audio-language pretraining as a viable pathway toward universal audio understanding.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "captioning",
                    "captionstew",
                    "dataset",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dataset Construction and Quality.</span> CaptionStew aggregates captions from multiple sources with varying generation methodologies, including LLM-synthesized descriptions that may introduce systematic biases or artifacts.\nWe do not perform extensive quality control or human verification across the aggregated corpus, which could impact model training.\nAdditionally, our dataset analysis reveals that simple aggregation does not guarantee improved linguistic diversity&#8212;CaptionStew&#8217;s lexical diversity metrics remain lower than mature image-text corpora.\nHowever, our design choice prioritizes semantic diversity over linguistic variety, as evidenced by the t-SNE clustering analysis showing distinct descriptive focuses across constituent datasets.\nWhile more sophisticated curation strategies could improve quality, our goal was to establish whether diverse caption aggregation can benefit audio representation learning, which our results support despite these limitations.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "captionstew",
                    "generation",
                    "dataset",
                    "datasets",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited Technical Novelty.</span> Our work primarily combines existing techniques&#8212;contrastive learning, captioning objectives, and dataset aggregation&#8212;rather than introducing fundamentally new methods. The mixed autoregressive/parallel training approach is adapted from vision-language work (CapPa), and our architectural choices follow standard practices.\nWe acknowledge that the technical contributions are largely empirical rather than methodological.\nHowever, this aligns with our primary goal of systematically evaluating audio-language pretraining&#8217;s potential for general-purpose representation learning.\nThe field currently lacks comprehensive comparative studies across objectives, evaluation protocols, and training factors.\nOur systematic analysis reveals important insights about scaling behaviors and initialization effects that have practical implications for practitioners, even if the underlying techniques are not novel.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T8\" title=\"Table 8 &#8227; A.4 Evaluation Datasets &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> details the evaluation datasets and their metrics used for assessing audio representation quality across our three evaluation protocols: linear probing&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Fonseca et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>); Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib21\" title=\"\">2018</a>); Cao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib10\" title=\"\">2014</a>); Law et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib52\" title=\"\">2010</a>); Engel et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib32\" title=\"\">2017</a>); Hershey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib45\" title=\"\">2021</a>); Ebbers et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib30\" title=\"\">2022</a>)</cite>, audio-language alignment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>); Diwan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>); Agostinelli et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib58\" title=\"\">2004</a>)</cite> and open-form question answering&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lipping et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib59\" title=\"\">2022</a>); Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib61\" title=\"\">2024</a>); Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "their",
                    "details",
                    "datasets"
                ]
            }
        ]
    },
    "A1.T6": {
        "caption": "Table 6: Example caption sampled from each sourced dataset.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">&#8221;A male speaker delivers his words quickly with a medium-pitched voice. His speech exhibits a flowing rhythm and is recorded</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">in an environment that is balanced in clarity. There is a subtle nasal quality to his speech, suggesting an American accent.&#8221;</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "sourced",
            "nasal",
            "accent”",
            "balanced",
            "example",
            "exhibits",
            "each",
            "words",
            "mediumpitched",
            "caption",
            "flowing",
            "sampled",
            "his",
            "voice",
            "speaker",
            "from",
            "male",
            "suggesting",
            "environment",
            "clarity",
            "rhythm",
            "delivers",
            "american",
            "speech",
            "quickly",
            "there",
            "recorded",
            "subtle",
            "dataset",
            "quality"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">CaptionStew aggregates eight open-source audio caption datasets to address data scarcity and limited diversity in current audio-language pretraining. The constituent datasets span environmental sounds, music, and expressive speech, with fundamentally different captioning approaches&#8212;from crowdsourced human annotation to expert curation to various LLM-based generation pipelines.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T5\" title=\"Table 5 &#8227; A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T6\" title=\"Table 6 &#8227; A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> detail each dataset&#8217;s characteristics and provide example captions that illustrate the diverse descriptive styles, ranging from concise event descriptions to detailed multi-sentence narratives with fine-grained acoustic and contextual information.\nDuring aggregation, we filter audio samples longer than one minute for computational efficiency and remove samples that overlap with common audio understanding benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>; Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>; Salamon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib81\" title=\"\">2014</a>)</cite> to prevent data leakage. This approach preserves the unique characteristics of each source while creating a unified corpus that captures broader semantic coverage than individual datasets.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Audio-language pretraining holds promise for general-purpose audio understanding, yet remains underexplored compared to its vision counterpart. While vision-language models like CLIP serve as widely adopted foundations, existing audio-language models primarily excel at retrieval tasks with limited adoption as general-purpose encoders.\nWe identify three key barriers: limited large-scale audio-text corpora, insufficient caption diversity, and lack of systematic exploration and evaluation.\nTo this end, we introduce CaptionStew, a 10.7M caption dataset aggregating diverse open-source audio-text corpora across multiple domains and captioning styles.\nUsing this resource, we conduct the first comprehensive evaluation comparing contrastive and captioning objectives for audio representation learning across speech, music, and environmental sound tasks.\nOur results demonstrate that audio-language pretraining yields competitive, transferable representations. Through systematic data-scaling experiments, we reveal complementary objective strengths: contrastive learning achieves superior data efficiency at smaller scales, while captioning demonstrates better scalability on language-involved audio understanding tasks. We also find that common supervised initialization practices provide diminishing returns at scale, challenging current approaches.\nThese findings establish audio-language pretraining as a viable pathway toward general-purpose audio representations, guiding future research. To accelerate progress, we release data preparation recipes, training protocols, and pretrained models, paving the way toward universal audio understanding.\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset",
                    "caption"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While these techniques have achieved remarkable success, a fundamental limitation persists: existing methods are primarily designed to excel on specific tasks.\nThis domain specificity stems from explicit inductive biases embedded in model architectures and training objectives.\nModels optimized for environmental sounds usually underperform capturing speaker characteristics or paralinguistic information in speech, and vice versa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Turian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>)</cite>.\nAchieving general-purpose audio representations that transfer robustly across diverse audio modalities remains a challenging and actively pursued goal in the field.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of vision&#8211;language pretraining underscores this promise.\nModels like CLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib75\" title=\"\">2021</a>)</cite> and AIM-v2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib33\" title=\"\">2025</a>)</cite> not only power vision&#8211;language tasks but also produce representations that benefit a broad range of vision tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib60\" title=\"\">2023</a>; Minderer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib68\" title=\"\">2022</a>; Crowson et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib23\" title=\"\">2022</a>)</cite>.\nIn contrast, audio&#8211;language models have not yet achieved comparable advancements.\nWhile existing models such as CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>)</cite> excel at audio&#8211;text retrieval, their representations have seen limited adoption for broader audio understanding tasks, suggesting fundamental gaps in current approaches.\nWe identify three key challenges that have constrained progress.\nFirst, large-scale, web-mined image&#8211;text corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schuhmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib83\" title=\"\">2022</a>; Gadre et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib35\" title=\"\">2023</a>)</cite> contain billions of pairs, but no comparable resource exists for audio.\nCurrent audio caption datasets barely exceed one million pairs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>)</cite>, often relying on captions synthesized or augmented by large language models, fundamentally limiting the scaling potential of audio&#8211;language models.\nSecond, widely used audio caption corpora focus predominantly on identifying what is presenting in the audio, while lacking coverage of the rich hierarchy of acoustic attributes that define specific audio signals.\nFor instance, captions rarely characterize speaker characteristics (voice timbre, speaking style), musical attributes (harmonic structure, rhythmic patterns), or environmental acoustics (reverberation, background ambiance).\nThis imbalanced focus limits the model&#8217;s ability to learn representations that capture the full spectrum of audio semantics.\nThird, prior work has primarily focused on contrastive learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite> and evaluated on audio&#8211;text retrieval.\nSystematic studies on alternative pretraining objectives (e.g., captioning) and comprehensive evaluations across a wide suite of audio understanding tasks remain scarce, limiting our understanding of what drives effective audio&#8211;language pretraining.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "caption",
                    "suggesting",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Captioning Objective</span> takes a generative approach to audio-language alignment, learning representations by generating textual descriptions from audio.\nWe argue that captioning presents a promising alternative for audio-language pretraining, offering denser token-level supervision compared to contrastive learning and better alignment with recent trends toward general audio understanding systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib38\" title=\"\">2025</a>)</cite>, yet remains underexplored in prior literature.\nGiven an audio signal <math alttext=\"a_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m1\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">a_{i}</annotation></semantics></math>, the encoder <math alttext=\"f_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">f_{a}</annotation></semantics></math> produces contextual representations <math alttext=\"\\mathbf{Z}^{a}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>&#119833;</mi><mi>i</mi><mi>a</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{Z}^{a}_{i}</annotation></semantics></math>, which are fed into a transformer decoder <math alttext=\"g_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m4\" intent=\":literal\"><semantics><msub><mi>g</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">g_{t}</annotation></semantics></math> through cross-attention. Inspired by CapPa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tschannen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib88\" title=\"\">2023</a>)</cite>, we alternate between two decoding modes&#8212;autoregressive and parallel prediction&#8212;to enhance audio encoder representation learning.\nIn the autoregressive decoding, the decoder generates caption tokens <math alttext=\"(y_{1},\\ldots,y_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y_{1},\\ldots,y_{T})</annotation></semantics></math> sequentially, with each token conditioned on the audio representation and previously generated tokens. Training follows the teacher-forcing approach with a cross-entropy loss:</p>\n\n",
                "matched_terms": [
                    "caption",
                    "each",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the potential of audio&#8211;language pretraining for general-purpose representation learning, we construct a large-scale and diverse audio caption dataset that addresses key limitations in existing corpora. Audio signals inherently encode information across multiple dimensions&#8212;timbre, pitch, rhythm, semantic events, emotional tone, and acoustic environment&#8212;each amenable to different linguistic descriptions. However, existing large-scale audio caption datasets typically rely on a single generation pipeline, whether human-annotated or LLM-synthesized. While ensuring consistency, this approach inevitably introduces systematic biases in language style and emphasized attributes. Furthermore, single-pipeline captions exhibit limited syntactic diversity and tend to focus on specific audio facets while neglecting complementary aspects.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "caption",
                    "rhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To fully leverage text as a flexible semantic scaffold for diverse audio representation learning, we embrace caption diversity across sources, styles, and descriptive granularities. Rather than creating captions through a single pipeline, we aggregate existing open-source corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib18\" title=\"\">2025</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>; Roy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib79\" title=\"\">2025</a>)</cite>.\nThese datasets span multiple audio domains&#8212;general sound events, expressive speech, and musical performance&#8212;and employ fundamentally different caption creation methodologies. This aggregation yields captions that describe complementary audio aspects with varying granularity, from coarse event categories to fine-grained acoustic attributes.\nPlease refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS2\" title=\"A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a> for detail of each source dataset.\nWhen multiple datasets contain identical audio samples with different captions, we identify these overlaps and consolidate all available captions for each audio file. This multi-caption pairing allows single audio clips to benefit from diverse perspectives and descriptive focuses, enriching the supervision signal.\nTo ensure evaluation integrity, we carefully filter out samples overlapping with development or test sets of downstream benchmarks.</p>\n\n",
                "matched_terms": [
                    "caption",
                    "each",
                    "from",
                    "speech",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The resulting dataset, <span class=\"ltx_text ltx_font_bold\">CaptionStew</span> (denoted by CS10M), contains 9.3 million audio samples paired with 10.7 million captions, spanning 37,290 hours across speech, music, and environmental domains. Compared to existing collections, CaptionStew achieves both greater scale and broader coverage. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S3.T1\" title=\"Table 1 &#8227; 3 CaptionStew Dataset &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a comparison with existing audio caption datasets.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset",
                    "caption"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate pretrained audio encoders across three protocols assessing discriminative capabilities, audio-language alignment, and open-formed question answering.\nAll experiments probe frozen representations from the audio encoder&#8217;s final layer to ensure fair model comparison. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S3.T2\" title=\"Table 2 &#8227; 3 CaptionStew Dataset &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS4\" title=\"A.4 Evaluation Datasets &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> details the datasets and metrics for each task.</p>\n\n",
                "matched_terms": [
                    "each",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recognizing the broad adoption and effectiveness of pretrained audio event classifiers in transfer learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alonso-Jim&#233;nez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib3\" title=\"\">2023</a>; Cappellazzo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib11\" title=\"\">2024</a>)</cite>, audio-language modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>)</cite> and general audio understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib42\" title=\"\">2024</a>; Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib37\" title=\"\">2024</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>)</cite>, we select our pretrained Zipformer-based audio event classifier (described in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.SS1\" title=\"4.1 Implementation Details &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) as the primary baseline.\nIn addition, we compare against representative self-supervised learning (SSL) models, each pretrained under different paradigms and specialized for particular audio domains. BEATs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>)</cite> is an audio SSL model trained with an iterative masked acoustic token prediction framework. Wav2vec 2.0&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>)</cite> learns speech representation by distinguishing target quantized latent representations from disctrators. MERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>)</cite> is a music SSL model trained with masked acoustic modeling, learning to capture acoustic cues and structural information of music.\nTogether, these baselines provide a broad comparative context for studying audio&#8211;language pretraining toward general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "each",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive vs. Captioning Objectives.</span>\nThe two pretraining paradigms exhibit complementary strengths across evaluation protocols.\nOn linear probing tasks, contrastive learning consistently outperforms captioning, particularly excelling at audio event classification and speaker identification.\nHowever, it is worth noting that this gap narrows substantially when the classifier learns to aggregate information across frames through multi-head attention pooling (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS5\" title=\"A.5 Main Results (cont.) &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>).\nThis observation reflects the objectives&#8217; inherent designs: contrastive learning explicitly optimizes for linearly separable clip-level representations, while captioning relies on cross-attention mechanisms over frame-level representations for text sequence generation.\nThis finding aligns with recent work highlighting how downstream module choices significantly impact the assessment of audio representation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zaiem et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib99\" title=\"\">2023</a>)</cite>.\nFor language-involved tasks, both objectives demonstrate competitive performance, with captioning showing slight advantages in open-form question answering across multiple domains.\nThis suggests captioning&#8217;s potential for language-involved audio understanding tasks, aligning with recent trends toward generative audio understanding systems.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Supervised Initialization.</span> Initializing from supervised pretraining (AS SL) provides substantial benefits across most tasks, with notable improvements on audio event classification, sound event detection and audio-text retreival.\nThe gains are particularly pronounced for contrastive objectives, suggesting that discriminative pretraining provides useful inductive biases for contrastive learning.\nHowever, these benefits diminish (or disappear entirely) when the attributes required for downstream tasks diverge from AudioSet&#8217;s ontology.\nOn speaker identification and music tagging, scratch-trained models often match or exceed initialized variants, indicating that AudioSet&#8217;s focus on distinguishing between sound categories may bias representations toward event-level semantics rather than the acoustic attributes (voice timbre, speaking style) or musical structure (genre, harmony, rhythm) essential for these tasks.\nThese findings challenge common initialization practices for audio-language pretraining and suggest the need for tailored pretraining strategies when targeting general-purpose audio representation learning.</p>\n\n",
                "matched_terms": [
                    "rhythm",
                    "voice",
                    "speaker",
                    "from",
                    "suggesting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Competitive Performance Across Domains.</span> Our audio-language representations achieve strong transferability across diverse audio domains.\nCompared to supervised baselines (Zipformer-AEC), our overall best-performing model (Contrastive-init) demonstrate superior performance on speaker identification, music understanding and audio-text retrieval while maintaining competitiveness on audio-event classification.\nAgainst domain-specialized SSL methods (BEATs, wav2vec2, MERT), our approach consistently shows competitive performance.\nThis consistent cross-domain performance validates our hypothesis that diverse caption aggregation enables broadly transferable representations, establishing audio-language pretraining as a viable path toward learning general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "caption"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scaling Patterns.</span> Most tasks demonstrate consistent performance improvements with increased data scale, validating the potential of large-scale audio-language pretraining.\nHowever, notable exceptions emerge that reveal fundamental limitations of current approaches.\nSound event detection, particularly for models initialized with AudioSet pretraining, exhibits a reverse scaling trend where performance degrades with more caption data.\nThis suggests a potential conflict between natural language supervision&#8211;which typically describes audio characteristics and attributes&#8211;and temporal localization tasks requiring precise event boundaries.\nAdditionally, emotion recognition and instrument classification show weaker scaling gains compared to other tasks, likely reflecting limited caption diversity for these specific attributes in existing corpora, which we will discussed in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.SS6\" title=\"4.6 Dataset Analysis &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "caption",
                    "exhibits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Initialization at Scale.</span>\nAudioSet initialization provides immediate performance gains but introduces diminishing returns at larger scales. Both contrastive learning and captioning show decreasing benefits from initialization as data scale increases, with scratch and initialized models achieving matched performance at larger scales on some tasks.\nThis suggests that pretrained initialization effectively bootstraps learning at small scales but may constrain the model&#8217;s ability to adapt to the broader semantic space covered by large-scale caption data, potentially due to mismatch between AudioSet&#8217;s ontology and diverse audio descriptions.</p>\n\n",
                "matched_terms": [
                    "caption",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, these findings reveal complementary behaviors: contrastive pretraining achieves superior data efficiency at current scales, while captioning shows better scalability, especially for language-involved tasks.\nImportantly, the diminishing returns of initialization at scale indicate that large-scale caption data can provide sufficient semantic supervision independent of domain-specific pretraining, challenging current practices of audio-language pretraining and opening possibilities for learning general-purpose representations from diverse text descriptions alone.</p>\n\n",
                "matched_terms": [
                    "caption",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To understand the linguistic characteristics of CaptionStew, we analyze caption diversity across constituent datasets through visualization and quantitative methods.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.SS6\" title=\"4.6 Dataset Analysis &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4.6</span></a> provides compelling evidence of our aggregation strategy&#8217;s success through t-SNE visualization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Maaten &amp; Hinton, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib65\" title=\"\">2008</a>)</cite> of sentence embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reimers &amp; Gurevych, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib77\" title=\"\">2019</a>)</cite> from sampled captions, revealing distinct clustering patterns by source that demonstrate complementary linguistic perspectives: AudioSetCaps and WavCaps overlap in audio event descriptions and aligns more with human annotated dataset, while JamendoMaxCaps creates a distinct cluster focused on music-specific terminology, and ParaSpeechCaps forms a separate cluster emphasizing speaking styles and paralinguistic attributes.\nThese minimal overlaps confirm that each dataset contributes distinct caption styles and descriptive focuses.</p>\n\n",
                "matched_terms": [
                    "caption",
                    "sampled",
                    "each",
                    "from",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Quantitative analysis reveals both the benefits and limitations (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.T4\" title=\"Table 4 &#8227; 4.6 Dataset Analysis &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).\nCaptionStew achieves substantial vocabulary expansion (56,586 unique words vs. 4,060-27,906 for individual datasets)\nHowever, this growth doesn&#8217;t yield proportional lexical diversity.\nCaptionStew&#8217;s Distinct-n metrics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib55\" title=\"\">2015</a>)</cite> remain low, falling short of image caption dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Changpinyo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib13\" title=\"\">2021</a>)</cite> and text corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Merity et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib67\" title=\"\">2016</a>)</cite>. This constraint stems from datasets with limited linguistic variation, particularly JamendoMaxCaps and ParaSpeechCaps with extremely low Distinct-n scores.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "caption",
                    "from",
                    "words"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings highlight that simply combining datasets doesn&#8217;t guarantee improved linguistic diversity, revealing broader limitations in current audio-language pretraining approaches.\nAlso, the constrained diversity in certain aspect may partially explain weaker scaling behavior observed for certain tasks, as models encounter repetitive linguistic patterns despite increased data volume, aligning with vision-language findings on caption diversity&#8217;s importance for representation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Santurkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib82\" title=\"\">2023</a>; Chan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib12\" title=\"\">2022</a>)</cite>.\nThis analysis motivates developing enhanced aggregation pipeline and more diverse caption generation methods to better capture the full spectrum of information in audio signals, thereby fully realizing the potential of large-scale audio-language pretraining.</p>\n\n",
                "matched_terms": [
                    "caption",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Representation Learning.</span>\nThe ultimate goal of audio representation learning is developing a single model suitable for diverse audio understanding tasks.\nSupervised models trained on labeled datasets have been fundamental to the field, including audio event classifiers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib44\" title=\"\">2017</a>; Cramer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib22\" title=\"\">2019</a>; Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib50\" title=\"\">2020</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib39\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib15\" title=\"\">2022a</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib26\" title=\"\">2024</a>)</cite>, speech recognition systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib76\" title=\"\">2023</a>)</cite> and speaker recognition models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Snyder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib84\" title=\"\">2018</a>; Desplanques et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib24\" title=\"\">2020</a>)</cite>.\nThese approaches remain widely adopted due to their strong performance on target tasks. In parallel, self-supervised learning methods have emerged as a complementary approach, offering advances across speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib46\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib16\" title=\"\">2022b</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib6\" title=\"\">2022</a>)</cite>, audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib40\" title=\"\">2022a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib48\" title=\"\">2022</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>; Li &amp; Li, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib56\" title=\"\">2022</a>)</cite>, and music&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib102\" title=\"\">2025</a>)</cite> without requiring labeled data.\nWhile these methods show improved generalization within their target domains, achieving truly general-purpose audio representations remains challenging.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio&#8211;Language Pretraining.</span>\nAudio-language models have emerged as a promising approach for learning cross-modal representations. Most existing work focuses on contrastive learning objectives that align audio and text in shared embedding spaces&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite>. Recent extensions have explored combinations with other objectives&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib93\" title=\"\">2023</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib101\" title=\"\">2024</a>; Niizumi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib71\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib72\" title=\"\">2025</a>)</cite>.\nThe field has also witnessed rapid evolution in datasets, transitioning from traditional human-annotated corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>)</cite> to recently constructed LLM-augmented collections&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib18\" title=\"\">2025</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib86\" title=\"\">Sun et&#160;al., </a>)</cite> and domain-specific resources covering speech characteristics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>)</cite>, and musical attributes&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Roy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib79\" title=\"\">2025</a>)</cite>.\nOur work contributes by providing the first systematic comparison between contrastive and captioning objectives, along with comprehensive evaluation toward general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We revisited audio-language pretraining to advance general-purpose audio representation learning. We introduced CaptionStew, a large-scale aggregation of diverse audio caption datasets, and through comprehensive evaluation across tasks, demonstrated that audio-language pretraining produces competitive representations across speech, music, and environmental domains. Our systematic comparison revealed complementary strengths of two audio-language pretraining objectives: contrastive learning excels in data efficiency, while captioning shows better scalability for language-involved tasks.\nData-scaling experiments highlighted diminishing returns of supervised initialization, challenging common practices, while dataset analysis revealed linguistic diversity limitation in current datasets. These findings establish audio-language pretraining as a viable pathway toward universal audio understanding.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset",
                    "caption"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dataset Construction and Quality.</span> CaptionStew aggregates captions from multiple sources with varying generation methodologies, including LLM-synthesized descriptions that may introduce systematic biases or artifacts.\nWe do not perform extensive quality control or human verification across the aggregated corpus, which could impact model training.\nAdditionally, our dataset analysis reveals that simple aggregation does not guarantee improved linguistic diversity&#8212;CaptionStew&#8217;s lexical diversity metrics remain lower than mature image-text corpora.\nHowever, our design choice prioritizes semantic diversity over linguistic variety, as evidenced by the t-SNE clustering analysis showing distinct descriptive focuses across constituent datasets.\nWhile more sophisticated curation strategies could improve quality, our goal was to establish whether diverse caption aggregation can benefit audio representation learning, which our results support despite these limitations.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "caption",
                    "from",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited Technical Novelty.</span> Our work primarily combines existing techniques&#8212;contrastive learning, captioning objectives, and dataset aggregation&#8212;rather than introducing fundamentally new methods. The mixed autoregressive/parallel training approach is adapted from vision-language work (CapPa), and our architectural choices follow standard practices.\nWe acknowledge that the technical contributions are largely empirical rather than methodological.\nHowever, this aligns with our primary goal of systematically evaluating audio-language pretraining&#8217;s potential for general-purpose representation learning.\nThe field currently lacks comprehensive comparative studies across objectives, evaluation protocols, and training factors.\nOur systematic analysis reveals important insights about scaling behaviors and initialization effects that have practical implications for practitioners, even if the underlying techniques are not novel.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We implement the original 2,2,3,4,3,2 block configuration, where each number indicates the blocks per stage. After processing through all stages, outputs are fused at 25 Hz to produce frame-level embeddings. The model incorporates several architectural improvements from the original work: BiasNorm for gradient stability over long sequences, Swoosh activation functions for better convergence, and compatibility with the ScaledAdam optimizer. The resulting embeddings are 768-dimensional and used consistently across all downstream evaluation tasks.</p>\n\n",
                "matched_terms": [
                    "each",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table 9 presents linear probing results when using multi-head attention pooling instead of mean pooling.\nWith learned attention pooling, the performance gap between contrastive and captioning objectives narrows substantially, particularly evident on speaker identification where captioning-scratch achieves 72.86% compared to 46.67% with mean pooling (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.T3\" title=\"Table 3 &#8227; 4.4 Main Results &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).\nThis demonstrates that captioning models benefit significantly from adaptive pooling mechanisms, while contrastive learning&#8217;s explicit optimization for clip-level representations shows less sensitivity to pooling strategy.\nThese results underscore the critical importance of appropriate downstream module selection when evaluating different pretraining paradigms, as the choice of pooling mechanism can dramatically influence conclusions about objective effectiveness.\nThe improved performance across all methods with attention pooling also suggests that frame-level representations from both objectives contain rich information that can be better exploited through learned aggregation. SOTA results and SSL baseline results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.T3\" title=\"Table 3 &#8227; 4.4 Main Results &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T9\" title=\"Table 9 &#8227; A.5 Main Results (cont.) &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> are quoted collectively from &#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Niizumi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib72\" title=\"\">2025</a>); Turian et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>); Li &amp; Li (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib56\" title=\"\">2022</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib90\" title=\"\">2022</a>); Bharadwaj et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib8\" title=\"\">2025</a>); Gong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib41\" title=\"\">2022b</a>); Lanzend&#246;rfer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib51\" title=\"\">2025</a>); Bai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>); Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Aside from learning representations, we also compare against state-of-the-art audio-text retrieval models to assess our approach&#8217;s performance on the specific task it was designed for. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T10\" title=\"Table 10 &#8227; A.6 Additional Results &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> presents retrieval results for our best-performing model (Contrastive-init) against state-of-the-art\naudio-text retrieval model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>)</cite>.\nOur model achieving comparable or superior results on benchmarks in various audio domains, with particularly strong performance on speech and music retrieval.\nThe results indicate that our general-purpose audio-language pretraining approach can compete with specialized retrieval models while offering broader applicability across diverse usage scenarios.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "from"
                ]
            }
        ]
    },
    "A1.T7": {
        "caption": "Table 7: Zipformer performance across audio domains when trained from scratch on individual datasets, demonstrating cross-domain efficacy as a general audio encoder.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">AudioSet (mAP)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">VggSound (acc)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">VoxCeleb2 (acc)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">CREMA (acc)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">MagnaTagATune (mAP)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">NSynth-Instrument (acc)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">54.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">84.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">65.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">78.8</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "crema",
            "domains",
            "when",
            "general",
            "vggsound",
            "audio",
            "from",
            "encoder",
            "demonstrating",
            "zipformer",
            "trained",
            "performance",
            "across",
            "voxceleb2",
            "nsynthinstrument",
            "individual",
            "magnatagatune",
            "map",
            "audioset",
            "datasets",
            "crossdomain",
            "acc",
            "scratch",
            "efficacy"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Although Zipformer was originally designed for automatic speech recognition, we conducted preliminary experiments to validate its effectiveness as a general audio encoder across diverse domains.\nAs in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T7\" title=\"Table 7 &#8227; A.3 Zipformer Model &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, our initial studies confirmed that Zipformer achieves competitive performance on environmental sound classification, music understanding, and speaker-related tasks, demonstrating its suitability as a unified backbone for multi-domain audio representation learning. This cross-domain efficacy makes it an appropriate choice for our audio-language pretraining experiments that span speech, music, and environmental audio.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Audio-language pretraining holds promise for general-purpose audio understanding, yet remains underexplored compared to its vision counterpart. While vision-language models like CLIP serve as widely adopted foundations, existing audio-language models primarily excel at retrieval tasks with limited adoption as general-purpose encoders.\nWe identify three key barriers: limited large-scale audio-text corpora, insufficient caption diversity, and lack of systematic exploration and evaluation.\nTo this end, we introduce CaptionStew, a 10.7M caption dataset aggregating diverse open-source audio-text corpora across multiple domains and captioning styles.\nUsing this resource, we conduct the first comprehensive evaluation comparing contrastive and captioning objectives for audio representation learning across speech, music, and environmental sound tasks.\nOur results demonstrate that audio-language pretraining yields competitive, transferable representations. Through systematic data-scaling experiments, we reveal complementary objective strengths: contrastive learning achieves superior data efficiency at smaller scales, while captioning demonstrates better scalability on language-involved audio understanding tasks. We also find that common supervised initialization practices provide diminishing returns at scale, challenging current approaches.\nThese findings establish audio-language pretraining as a viable pathway toward general-purpose audio representations, guiding future research. To accelerate progress, we release data preparation recipes, training protocols, and pretrained models, paving the way toward universal audio understanding.\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "domains"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Representation learning has long been central to audio processing<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>In this work, audio processing refers to audio understanding, speech analysis and music understanding, while excluding automatic speech recognition</span></span></span>, with substantial progress over the past decades.\nEarly advances relied on supervised learning, where models trained on labeled corpora were adapted to related downstream tasks or transferred across domains&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib50\" title=\"\">2020</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib15\" title=\"\">2022a</a>; Snyder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib84\" title=\"\">2018</a>; Desplanques et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib24\" title=\"\">2020</a>)</cite>.\nMore recently, self-supervised learning (SSL) has emerged as the promising paradigm.\nBy pretraining on large-scale unlabeled audio with contrastive objectives or masked modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib40\" title=\"\">2022a</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib46\" title=\"\">2021</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>)</cite>, the resulting models learn rich structural knowledge of audio signals, consistently enhancing performance across many speech and audio benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib96\" title=\"\">2021</a>; Turian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib98\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "audio",
                    "domains",
                    "trained",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While these techniques have achieved remarkable success, a fundamental limitation persists: existing methods are primarily designed to excel on specific tasks.\nThis domain specificity stems from explicit inductive biases embedded in model architectures and training objectives.\nModels optimized for environmental sounds usually underperform capturing speaker characteristics or paralinguistic information in speech, and vice versa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Turian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>)</cite>.\nAchieving general-purpose audio representations that transfer robustly across diverse audio modalities remains a challenging and actively pursued goal in the field.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An emerging and promising alternative is audio&#8211;language pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>)</cite>, which grounds audio perception with natural language descriptions (e.g. captions).\nIn this framework, text serves as a flexible semantic scaffold, offering supervision potentially spanning multiple levels of granularity, from coarse event categories\nto fine-grained acoustic attributes.\nBy aligning audio with text, audio&#8211;language pretraining provides a unified learning framework for capturing diverse audio information, offering a promising path toward general audio understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib80\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib47\" title=\"\">2025</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>; Su et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib85\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "general",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of vision&#8211;language pretraining underscores this promise.\nModels like CLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib75\" title=\"\">2021</a>)</cite> and AIM-v2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib33\" title=\"\">2025</a>)</cite> not only power vision&#8211;language tasks but also produce representations that benefit a broad range of vision tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib60\" title=\"\">2023</a>; Minderer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib68\" title=\"\">2022</a>; Crowson et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib23\" title=\"\">2022</a>)</cite>.\nIn contrast, audio&#8211;language models have not yet achieved comparable advancements.\nWhile existing models such as CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>)</cite> excel at audio&#8211;text retrieval, their representations have seen limited adoption for broader audio understanding tasks, suggesting fundamental gaps in current approaches.\nWe identify three key challenges that have constrained progress.\nFirst, large-scale, web-mined image&#8211;text corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schuhmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib83\" title=\"\">2022</a>; Gadre et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib35\" title=\"\">2023</a>)</cite> contain billions of pairs, but no comparable resource exists for audio.\nCurrent audio caption datasets barely exceed one million pairs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>)</cite>, often relying on captions synthesized or augmented by large language models, fundamentally limiting the scaling potential of audio&#8211;language models.\nSecond, widely used audio caption corpora focus predominantly on identifying what is presenting in the audio, while lacking coverage of the rich hierarchy of acoustic attributes that define specific audio signals.\nFor instance, captions rarely characterize speaker characteristics (voice timbre, speaking style), musical attributes (harmonic structure, rhythmic patterns), or environmental acoustics (reverberation, background ambiance).\nThis imbalanced focus limits the model&#8217;s ability to learn representations that capture the full spectrum of audio semantics.\nThird, prior work has primarily focused on contrastive learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite> and evaluated on audio&#8211;text retrieval.\nSystematic studies on alternative pretraining objectives (e.g., captioning) and comprehensive evaluations across a wide suite of audio understanding tasks remain scarce, limiting our understanding of what drives effective audio&#8211;language pretraining.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">CaptionStew</span>, a large-scale aggregation of diverse open-source audio&#8211;text datasets spanning multiple domains and captioning styles, addressing the data scarcity and diversity limitations in current audio-language pretraining.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "domains"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide the first comprehensive evaluation of audio-language pretraining across diverse tasks and protocols, demonstrating that audio&#8211;language pretraining produces competitive, transferable representations across speech, music, and environmental audio domains.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "demonstrating",
                    "domains",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze key training factors including data scaling effects and supervised pretraining initialization, showing that while AudioSet pretraining provides general benefits, its effects diminish for tasks unrelated to audio event classification and at larger data scales, challenging common practices in the field.</p>\n\n",
                "matched_terms": [
                    "audioset",
                    "audio",
                    "general"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio&#8211;language pretraining learns audio representations by establishing correspondence between audio signals and natural language descriptions. The core objective is to leverage text as structured semantic supervision, enabling models to capture diverse information across speech, music, and environmental sounds within a unified framework.\nAudio&#8211;language models typically employ a two-tower architecture: an audio encoder <math alttext=\"f_{\\text{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mtext>a</mtext></msub><annotation encoding=\"application/x-tex\">f_{\\text{a}}</annotation></semantics></math> that maps raw audio signals into contextual representations, and a text component <math alttext=\"f_{\\text{t}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mtext>t</mtext></msub><annotation encoding=\"application/x-tex\">f_{\\text{t}}</annotation></semantics></math> whose design depends on the training objective.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S2.F1\" title=\"Figure 1 &#8227; 2 Language-audio Pretraining &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we explore two complementary paradigms that differ fundamentally in how they establish audio-text correspondence, contrastive and captioning objective. These approaches represent discriminative and generative perspectives on audio-language alignment, respectively.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive Objective</span> is proven to be a robust representation learning method&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib19\" title=\"\">2020b</a>; Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib75\" title=\"\">2021</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>)</cite> and have been a dominant approach for audio-language pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite>.\nThis approach aligns audio and text representations in a shared embedding space by maximizing similarity between paired samples while minimizing similarity between mismatched pairs.\nGiven a batch of paired samples <math alttext=\"\\{(a_{i},t_{i})\\}_{i=1}^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m1\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding=\"application/x-tex\">\\{(a_{i},t_{i})\\}_{i=1}^{N}</annotation></semantics></math>, the audio encoder produces frame- (or patch-) level representations that are pooled and projected to audio embeddings <math alttext=\"\\mathbf{z}^{a}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119859;</mi><mi>i</mi><mi>a</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{z}^{a}_{i}</annotation></semantics></math>, while the text encoder <math alttext=\"f_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">f_{t}</annotation></semantics></math> generates corresponding text embeddings <math alttext=\"\\mathbf{z}^{t}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>&#119859;</mi><mi>i</mi><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{z}^{t}_{i}</annotation></semantics></math>.\nThe symmetric InfoNCE loss&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Oord et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib73\" title=\"\">2018</a>)</cite> is applied to optimize both modalities:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Captioning Objective</span> takes a generative approach to audio-language alignment, learning representations by generating textual descriptions from audio.\nWe argue that captioning presents a promising alternative for audio-language pretraining, offering denser token-level supervision compared to contrastive learning and better alignment with recent trends toward general audio understanding systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib38\" title=\"\">2025</a>)</cite>, yet remains underexplored in prior literature.\nGiven an audio signal <math alttext=\"a_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m1\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">a_{i}</annotation></semantics></math>, the encoder <math alttext=\"f_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">f_{a}</annotation></semantics></math> produces contextual representations <math alttext=\"\\mathbf{Z}^{a}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>&#119833;</mi><mi>i</mi><mi>a</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{Z}^{a}_{i}</annotation></semantics></math>, which are fed into a transformer decoder <math alttext=\"g_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m4\" intent=\":literal\"><semantics><msub><mi>g</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">g_{t}</annotation></semantics></math> through cross-attention. Inspired by CapPa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tschannen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib88\" title=\"\">2023</a>)</cite>, we alternate between two decoding modes&#8212;autoregressive and parallel prediction&#8212;to enhance audio encoder representation learning.\nIn the autoregressive decoding, the decoder generates caption tokens <math alttext=\"(y_{1},\\ldots,y_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y_{1},\\ldots,y_{T})</annotation></semantics></math> sequentially, with each token conditioned on the audio representation and previously generated tokens. Training follows the teacher-forcing approach with a cross-entropy loss:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "general",
                    "from",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This parallel mode enforces stronger dependency on the audio encoder by eliminating reliance on autoregressive context. We adopt mixed training where a random fraction of each minibatch uses standard autoregression while the remainder use parallel decoding.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the potential of audio&#8211;language pretraining for general-purpose representation learning, we construct a large-scale and diverse audio caption dataset that addresses key limitations in existing corpora. Audio signals inherently encode information across multiple dimensions&#8212;timbre, pitch, rhythm, semantic events, emotional tone, and acoustic environment&#8212;each amenable to different linguistic descriptions. However, existing large-scale audio caption datasets typically rely on a single generation pipeline, whether human-annotated or LLM-synthesized. While ensuring consistency, this approach inevitably introduces systematic biases in language style and emphasized attributes. Furthermore, single-pipeline captions exhibit limited syntactic diversity and tend to focus on specific audio facets while neglecting complementary aspects.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To fully leverage text as a flexible semantic scaffold for diverse audio representation learning, we embrace caption diversity across sources, styles, and descriptive granularities. Rather than creating captions through a single pipeline, we aggregate existing open-source corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib18\" title=\"\">2025</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>; Roy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib79\" title=\"\">2025</a>)</cite>.\nThese datasets span multiple audio domains&#8212;general sound events, expressive speech, and musical performance&#8212;and employ fundamentally different caption creation methodologies. This aggregation yields captions that describe complementary audio aspects with varying granularity, from coarse event categories to fine-grained acoustic attributes.\nPlease refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS2\" title=\"A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a> for detail of each source dataset.\nWhen multiple datasets contain identical audio samples with different captions, we identify these overlaps and consolidate all available captions for each audio file. This multi-caption pairing allows single audio clips to benefit from diverse perspectives and descriptive focuses, enriching the supervision signal.\nTo ensure evaluation integrity, we carefully filter out samples overlapping with development or test sets of downstream benchmarks.</p>\n\n",
                "matched_terms": [
                    "across",
                    "audio",
                    "from",
                    "when",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The resulting dataset, <span class=\"ltx_text ltx_font_bold\">CaptionStew</span> (denoted by CS10M), contains 9.3 million audio samples paired with 10.7 million captions, spanning 37,290 hours across speech, music, and environmental domains. Compared to existing collections, CaptionStew achieves both greater scale and broader coverage. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S3.T1\" title=\"Table 1 &#8227; 3 CaptionStew Dataset &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a comparison with existing audio caption datasets.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "datasets",
                    "domains"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The audio encoder uses a Zipformer-M architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib97\" title=\"\">2024</a>)</cite>, chosen for its efficiency on long sequences and fast convergence.\nZipformer employs six encoder blocks in a U-Net structure that processes sequences at multiple resolutions to capture fine- and coarse-grained temporal information.\nAlthough originally designed for automatic speech recognition, our preliminary experiments confirm Zipformer as a competitive backbone across audio classification tasks (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS3\" title=\"A.3 Zipformer Model &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>).\nFor contrastive pretraining, the text encoder follows BERT-base architecture (12 layers 768 hidden dimensions)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib25\" title=\"\">2019</a>)</cite>.\nFor captioning pretraining, the text decoder adopts the BART-base decoder architecture (6 layers, 768 hidden dimensions)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib54\" title=\"\">2020</a>)</cite>.\nWe use twice as many encoder layers as decoder layers to ensure comparable training speed across objectives.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "zipformer",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following prior works in audio-language pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>)</cite>, we experiment with two scenarios: training from scratch or initialized from pretrained checkpoints.\nThe audio encoder initializes from a Zipformer-based audio event classifier trained on AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib36\" title=\"\">2017</a>)</cite> with an mAP of 0.46, while text components use corresponding publicly available checkpoints.\nAll models are trained on 8 Tesla V100 GPUs with an effective batch size of 640 seconds of audio per GPU.\nTraining runs for 600k steps from scratch (14 days wall-clock time) or 200k steps if initialized from pretrained checkpoint.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "from",
                    "encoder",
                    "scratch",
                    "map",
                    "audioset",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate pretrained audio encoders across three protocols assessing discriminative capabilities, audio-language alignment, and open-formed question answering.\nAll experiments probe frozen representations from the audio encoder&#8217;s final layer to ensure fair model comparison. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S3.T2\" title=\"Table 2 &#8227; 3 CaptionStew Dataset &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS4\" title=\"A.4 Evaluation Datasets &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> details the datasets and metrics for each task.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "from",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Linear Probing</span> trains simple linear classifiers on frozen representations.\nFor classification tasks, we experiment with two pooling mechanisms&#8212;mean pooling and multi-head attention pooling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib53\" title=\"\">2019</a>)</cite>&#8212;to obtain clip-level representations.\nWe evaluate across a diverse set of tasks across audio domains, including audio event classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>)</cite>, sound event detection&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib45\" title=\"\">2021</a>)</cite>, speaker-related tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib21\" title=\"\">2018</a>; Cao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib10\" title=\"\">2014</a>)</cite>, and music classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Law et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib52\" title=\"\">2010</a>; Engel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib32\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "domains"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-language Alignments</span>\nfollow the LiT protocol&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib100\" title=\"\">2022</a>)</cite>, adapting pretrained text components to align with frozen audio representations.\nFor retrieval, we pair audio encoders with pretrained RoBERTa-base text encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib62\" title=\"\">2019</a>)</cite>.\nFor captioning, we use pretrained BART-base decoders&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib54\" title=\"\">2020</a>)</cite>, and only finetune cross-attention layers as we observed more stable training.\nBoth setups finetune on corresponding datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>)</cite> while keeping audio encoders frozen.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "datasets",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Open-formed Question Answering</span>\nAcknowledging the trend of combining audio encoders with large language models (LLMs) for general audio understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib37\" title=\"\">2024</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib42\" title=\"\">2024</a>)</cite>, we connects frozen audio encoders to a LLM (Qwen2.5-7B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib94\" title=\"\">2024a</a>)</cite>) through lightweight adaptors that project audio representations into the LLM&#8217;s embedding space.\nWe train only the adaptor on audio QA datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipping et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib59\" title=\"\">2022</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib61\" title=\"\">2024</a>)</cite> and evaluate on corresponding track in AIR-Bench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>)</cite>.\nDuring training, we carefully monitor instruction-following behavior (<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>99%) to ensure reliable evaluation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "general",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recognizing the broad adoption and effectiveness of pretrained audio event classifiers in transfer learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alonso-Jim&#233;nez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib3\" title=\"\">2023</a>; Cappellazzo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib11\" title=\"\">2024</a>)</cite>, audio-language modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>)</cite> and general audio understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib42\" title=\"\">2024</a>; Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib37\" title=\"\">2024</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>)</cite>, we select our pretrained Zipformer-based audio event classifier (described in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.SS1\" title=\"4.1 Implementation Details &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) as the primary baseline.\nIn addition, we compare against representative self-supervised learning (SSL) models, each pretrained under different paradigms and specialized for particular audio domains. BEATs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>)</cite> is an audio SSL model trained with an iterative masked acoustic token prediction framework. Wav2vec 2.0&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>)</cite> learns speech representation by distinguishing target quantized latent representations from disctrators. MERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>)</cite> is a music SSL model trained with masked acoustic modeling, learning to capture acoustic cues and structural information of music.\nTogether, these baselines provide a broad comparative context for studying audio&#8211;language pretraining toward general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "from",
                    "domains",
                    "general",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive vs. Captioning Objectives.</span>\nThe two pretraining paradigms exhibit complementary strengths across evaluation protocols.\nOn linear probing tasks, contrastive learning consistently outperforms captioning, particularly excelling at audio event classification and speaker identification.\nHowever, it is worth noting that this gap narrows substantially when the classifier learns to aggregate information across frames through multi-head attention pooling (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS5\" title=\"A.5 Main Results (cont.) &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>).\nThis observation reflects the objectives&#8217; inherent designs: contrastive learning explicitly optimizes for linearly separable clip-level representations, while captioning relies on cross-attention mechanisms over frame-level representations for text sequence generation.\nThis finding aligns with recent work highlighting how downstream module choices significantly impact the assessment of audio representation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zaiem et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib99\" title=\"\">2023</a>)</cite>.\nFor language-involved tasks, both objectives demonstrate competitive performance, with captioning showing slight advantages in open-form question answering across multiple domains.\nThis suggests captioning&#8217;s potential for language-involved audio understanding tasks, aligning with recent trends toward generative audio understanding systems.</p>\n\n",
                "matched_terms": [
                    "across",
                    "audio",
                    "domains",
                    "when",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Supervised Initialization.</span> Initializing from supervised pretraining (AS SL) provides substantial benefits across most tasks, with notable improvements on audio event classification, sound event detection and audio-text retreival.\nThe gains are particularly pronounced for contrastive objectives, suggesting that discriminative pretraining provides useful inductive biases for contrastive learning.\nHowever, these benefits diminish (or disappear entirely) when the attributes required for downstream tasks diverge from AudioSet&#8217;s ontology.\nOn speaker identification and music tagging, scratch-trained models often match or exceed initialized variants, indicating that AudioSet&#8217;s focus on distinguishing between sound categories may bias representations toward event-level semantics rather than the acoustic attributes (voice timbre, speaking style) or musical structure (genre, harmony, rhythm) essential for these tasks.\nThese findings challenge common initialization practices for audio-language pretraining and suggest the need for tailored pretraining strategies when targeting general-purpose audio representation learning.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "from",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Competitive Performance Across Domains.</span> Our audio-language representations achieve strong transferability across diverse audio domains.\nCompared to supervised baselines (Zipformer-AEC), our overall best-performing model (Contrastive-init) demonstrate superior performance on speaker identification, music understanding and audio-text retrieval while maintaining competitiveness on audio-event classification.\nAgainst domain-specialized SSL methods (BEATs, wav2vec2, MERT), our approach consistently shows competitive performance.\nThis consistent cross-domain performance validates our hypothesis that diverse caption aggregation enables broadly transferable representations, establishing audio-language pretraining as a viable path toward learning general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "crossdomain",
                    "across",
                    "audio",
                    "domains",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scaling Patterns.</span> Most tasks demonstrate consistent performance improvements with increased data scale, validating the potential of large-scale audio-language pretraining.\nHowever, notable exceptions emerge that reveal fundamental limitations of current approaches.\nSound event detection, particularly for models initialized with AudioSet pretraining, exhibits a reverse scaling trend where performance degrades with more caption data.\nThis suggests a potential conflict between natural language supervision&#8211;which typically describes audio characteristics and attributes&#8211;and temporal localization tasks requiring precise event boundaries.\nAdditionally, emotion recognition and instrument classification show weaker scaling gains compared to other tasks, likely reflecting limited caption diversity for these specific attributes in existing corpora, which we will discussed in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.SS6\" title=\"4.6 Dataset Analysis &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "audioset",
                    "audio",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive vs. Captioning Scaling. </span>\nContrastive learning consistently outperforms captioning at varying data scales, particularly under less data and on discriminative tasks such as audio event classification.\nHowever, captioning demonstrates slightly better scaling properties, with distinct patterns emerging across task categories.\nor language-involved tasks&#8211;especially captioning and question answering&#8211;captioning matches or surpasses contrastive learning at our current 10M-pair scale.\nOn linear probing benchmarks, the gap remains substantial, with scaling trends suggesting captioning would require hundreds of millions of pairs to achieve parity with contrastive methods.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Initialization at Scale.</span>\nAudioSet initialization provides immediate performance gains but introduces diminishing returns at larger scales. Both contrastive learning and captioning show decreasing benefits from initialization as data scale increases, with scratch and initialized models achieving matched performance at larger scales on some tasks.\nThis suggests that pretrained initialization effectively bootstraps learning at small scales but may constrain the model&#8217;s ability to adapt to the broader semantic space covered by large-scale caption data, potentially due to mismatch between AudioSet&#8217;s ontology and diverse audio descriptions.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "from",
                    "scratch",
                    "audioset",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To understand the linguistic characteristics of CaptionStew, we analyze caption diversity across constituent datasets through visualization and quantitative methods.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.SS6\" title=\"4.6 Dataset Analysis &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4.6</span></a> provides compelling evidence of our aggregation strategy&#8217;s success through t-SNE visualization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Maaten &amp; Hinton, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib65\" title=\"\">2008</a>)</cite> of sentence embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reimers &amp; Gurevych, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib77\" title=\"\">2019</a>)</cite> from sampled captions, revealing distinct clustering patterns by source that demonstrate complementary linguistic perspectives: AudioSetCaps and WavCaps overlap in audio event descriptions and aligns more with human annotated dataset, while JamendoMaxCaps creates a distinct cluster focused on music-specific terminology, and ParaSpeechCaps forms a separate cluster emphasizing speaking styles and paralinguistic attributes.\nThese minimal overlaps confirm that each dataset contributes distinct caption styles and descriptive focuses.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "from",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Quantitative analysis reveals both the benefits and limitations (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.T4\" title=\"Table 4 &#8227; 4.6 Dataset Analysis &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).\nCaptionStew achieves substantial vocabulary expansion (56,586 unique words vs. 4,060-27,906 for individual datasets)\nHowever, this growth doesn&#8217;t yield proportional lexical diversity.\nCaptionStew&#8217;s Distinct-n metrics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib55\" title=\"\">2015</a>)</cite> remain low, falling short of image caption dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Changpinyo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib13\" title=\"\">2021</a>)</cite> and text corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Merity et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib67\" title=\"\">2016</a>)</cite>. This constraint stems from datasets with limited linguistic variation, particularly JamendoMaxCaps and ParaSpeechCaps with extremely low Distinct-n scores.</p>\n\n",
                "matched_terms": [
                    "from",
                    "datasets",
                    "individual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings highlight that simply combining datasets doesn&#8217;t guarantee improved linguistic diversity, revealing broader limitations in current audio-language pretraining approaches.\nAlso, the constrained diversity in certain aspect may partially explain weaker scaling behavior observed for certain tasks, as models encounter repetitive linguistic patterns despite increased data volume, aligning with vision-language findings on caption diversity&#8217;s importance for representation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Santurkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib82\" title=\"\">2023</a>; Chan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib12\" title=\"\">2022</a>)</cite>.\nThis analysis motivates developing enhanced aggregation pipeline and more diverse caption generation methods to better capture the full spectrum of information in audio signals, thereby fully realizing the potential of large-scale audio-language pretraining.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Representation Learning.</span>\nThe ultimate goal of audio representation learning is developing a single model suitable for diverse audio understanding tasks.\nSupervised models trained on labeled datasets have been fundamental to the field, including audio event classifiers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib44\" title=\"\">2017</a>; Cramer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib22\" title=\"\">2019</a>; Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib50\" title=\"\">2020</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib39\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib15\" title=\"\">2022a</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib26\" title=\"\">2024</a>)</cite>, speech recognition systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib76\" title=\"\">2023</a>)</cite> and speaker recognition models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Snyder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib84\" title=\"\">2018</a>; Desplanques et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib24\" title=\"\">2020</a>)</cite>.\nThese approaches remain widely adopted due to their strong performance on target tasks. In parallel, self-supervised learning methods have emerged as a complementary approach, offering advances across speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib46\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib16\" title=\"\">2022b</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib6\" title=\"\">2022</a>)</cite>, audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib40\" title=\"\">2022a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib48\" title=\"\">2022</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>; Li &amp; Li, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib56\" title=\"\">2022</a>)</cite>, and music&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib102\" title=\"\">2025</a>)</cite> without requiring labeled data.\nWhile these methods show improved generalization within their target domains, achieving truly general-purpose audio representations remains challenging.</p>\n\n",
                "matched_terms": [
                    "across",
                    "audio",
                    "domains",
                    "trained",
                    "datasets",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio&#8211;Language Pretraining.</span>\nAudio-language models have emerged as a promising approach for learning cross-modal representations. Most existing work focuses on contrastive learning objectives that align audio and text in shared embedding spaces&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite>. Recent extensions have explored combinations with other objectives&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib93\" title=\"\">2023</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib101\" title=\"\">2024</a>; Niizumi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib71\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib72\" title=\"\">2025</a>)</cite>.\nThe field has also witnessed rapid evolution in datasets, transitioning from traditional human-annotated corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>)</cite> to recently constructed LLM-augmented collections&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib18\" title=\"\">2025</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib86\" title=\"\">Sun et&#160;al., </a>)</cite> and domain-specific resources covering speech characteristics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>)</cite>, and musical attributes&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Roy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib79\" title=\"\">2025</a>)</cite>.\nOur work contributes by providing the first systematic comparison between contrastive and captioning objectives, along with comprehensive evaluation toward general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "from",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Universal Audio Understanding.</span>\nThe evaluation of audio understanding has evolved from task-specific classification benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib96\" title=\"\">2021</a>; Turian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib98\" title=\"\">2023</a>)</cite> toward more comprehensive assessment frameworks.\nRecent developments have emphasized LLM-based audio understanding systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib37\" title=\"\">2024</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib42\" title=\"\">2024</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib38\" title=\"\">2025</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib20\" title=\"\">2024</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib87\" title=\"\">2024</a>)</cite> that can handle open-form queries and complex reasoning tasks.\nThis shift has driven the development of corresponding evaluation benchmarks that assess models&#8217; abilities across diverse audio understanding scenarios, including question answering, reasoning, and multi-step audio analysis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib80\" title=\"\">2025</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib47\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib64\" title=\"\">2025</a>)</cite>.\nOur work contributes to this trend by providing the first comprehensive evaluation of audio-language pretraining across discriminative tasks, audio-language alignment, and open-form question answering, thereby bridging the gap between traditional representation learning evaluation and modern universal audio understanding.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We revisited audio-language pretraining to advance general-purpose audio representation learning. We introduced CaptionStew, a large-scale aggregation of diverse audio caption datasets, and through comprehensive evaluation across tasks, demonstrated that audio-language pretraining produces competitive representations across speech, music, and environmental domains. Our systematic comparison revealed complementary strengths of two audio-language pretraining objectives: contrastive learning excels in data efficiency, while captioning shows better scalability for language-involved tasks.\nData-scaling experiments highlighted diminishing returns of supervised initialization, challenging common practices, while dataset analysis revealed linguistic diversity limitation in current datasets. These findings establish audio-language pretraining as a viable pathway toward universal audio understanding.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "datasets",
                    "domains"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dataset Construction and Quality.</span> CaptionStew aggregates captions from multiple sources with varying generation methodologies, including LLM-synthesized descriptions that may introduce systematic biases or artifacts.\nWe do not perform extensive quality control or human verification across the aggregated corpus, which could impact model training.\nAdditionally, our dataset analysis reveals that simple aggregation does not guarantee improved linguistic diversity&#8212;CaptionStew&#8217;s lexical diversity metrics remain lower than mature image-text corpora.\nHowever, our design choice prioritizes semantic diversity over linguistic variety, as evidenced by the t-SNE clustering analysis showing distinct descriptive focuses across constituent datasets.\nWhile more sophisticated curation strategies could improve quality, our goal was to establish whether diverse caption aggregation can benefit audio representation learning, which our results support despite these limitations.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "from",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited Technical Novelty.</span> Our work primarily combines existing techniques&#8212;contrastive learning, captioning objectives, and dataset aggregation&#8212;rather than introducing fundamentally new methods. The mixed autoregressive/parallel training approach is adapted from vision-language work (CapPa), and our architectural choices follow standard practices.\nWe acknowledge that the technical contributions are largely empirical rather than methodological.\nHowever, this aligns with our primary goal of systematically evaluating audio-language pretraining&#8217;s potential for general-purpose representation learning.\nThe field currently lacks comprehensive comparative studies across objectives, evaluation protocols, and training factors.\nOur systematic analysis reveals important insights about scaling behaviors and initialization effects that have practical implications for practitioners, even if the underlying techniques are not novel.</p>\n\n",
                "matched_terms": [
                    "across",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited Model and Data Scalability.</span> Our experiments are constrained to 10M audio-text pairs and relatively modest model sizes compared to state-of-the-art vision-language systems that leverage billions of samples and much larger architectures. This scale limitation may not fully reflect the potential of audio-language pretraining, particularly for the captioning objective which our results suggest benefits from larger-scale training. Additionally, we do not explore recent advances in large language model integration or more sophisticated architectural designs that could improve performance. These constraints stem from computational resource limitations and our focus on controlled comparisons rather than pushing absolute performance boundaries. Future work with larger scales may reveal different scaling dynamics and stronger evidence for general-purpose capabilities.</p>\n\n",
                "matched_terms": [
                    "from",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CaptionStew aggregates eight open-source audio caption datasets to address data scarcity and limited diversity in current audio-language pretraining. The constituent datasets span environmental sounds, music, and expressive speech, with fundamentally different captioning approaches&#8212;from crowdsourced human annotation to expert curation to various LLM-based generation pipelines.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T5\" title=\"Table 5 &#8227; A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T6\" title=\"Table 6 &#8227; A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> detail each dataset&#8217;s characteristics and provide example captions that illustrate the diverse descriptive styles, ranging from concise event descriptions to detailed multi-sentence narratives with fine-grained acoustic and contextual information.\nDuring aggregation, we filter audio samples longer than one minute for computational efficiency and remove samples that overlap with common audio understanding benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>; Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>; Salamon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib81\" title=\"\">2014</a>)</cite> to prevent data leakage. This approach preserves the unique characteristics of each source while creating a unified corpus that captures broader semantic coverage than individual datasets.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "from",
                    "datasets",
                    "individual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we adopt the Zipformer-M architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib97\" title=\"\">2024</a>)</cite> as the audio encoder, chosen for its memory efficiency on long sequences and strong performance across audio tasks. The architecture employs a U-Net-inspired design with six Transformer stages that process sequences at multiple temporal resolutions. The stages operate at progressively decreasing then increasing frame rates (50, 25, 12.5, 6.25, 12.5, and 25 Hz), with residual and upsampling connections between stages to capture both fine-grained and long-range temporal patterns.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "encoder",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We implement the original 2,2,3,4,3,2 block configuration, where each number indicates the blocks per stage. After processing through all stages, outputs are fused at 25 Hz to produce frame-level embeddings. The model incorporates several architectural improvements from the original work: BiasNorm for gradient stability over long sequences, Swoosh activation functions for better convergence, and compatibility with the ScaledAdam optimizer. The resulting embeddings are 768-dimensional and used consistently across all downstream evaluation tasks.</p>\n\n",
                "matched_terms": [
                    "across",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T8\" title=\"Table 8 &#8227; A.4 Evaluation Datasets &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> details the evaluation datasets and their metrics used for assessing audio representation quality across our three evaluation protocols: linear probing&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Fonseca et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>); Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib21\" title=\"\">2018</a>); Cao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib10\" title=\"\">2014</a>); Law et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib52\" title=\"\">2010</a>); Engel et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib32\" title=\"\">2017</a>); Hershey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib45\" title=\"\">2021</a>); Ebbers et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib30\" title=\"\">2022</a>)</cite>, audio-language alignment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>); Diwan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>); Agostinelli et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib58\" title=\"\">2004</a>)</cite> and open-form question answering&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lipping et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib59\" title=\"\">2022</a>); Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib61\" title=\"\">2024</a>); Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table 9 presents linear probing results when using multi-head attention pooling instead of mean pooling.\nWith learned attention pooling, the performance gap between contrastive and captioning objectives narrows substantially, particularly evident on speaker identification where captioning-scratch achieves 72.86% compared to 46.67% with mean pooling (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.T3\" title=\"Table 3 &#8227; 4.4 Main Results &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).\nThis demonstrates that captioning models benefit significantly from adaptive pooling mechanisms, while contrastive learning&#8217;s explicit optimization for clip-level representations shows less sensitivity to pooling strategy.\nThese results underscore the critical importance of appropriate downstream module selection when evaluating different pretraining paradigms, as the choice of pooling mechanism can dramatically influence conclusions about objective effectiveness.\nThe improved performance across all methods with attention pooling also suggests that frame-level representations from both objectives contain rich information that can be better exploited through learned aggregation. SOTA results and SSL baseline results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.T3\" title=\"Table 3 &#8227; 4.4 Main Results &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T9\" title=\"Table 9 &#8227; A.5 Main Results (cont.) &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> are quoted collectively from &#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Niizumi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib72\" title=\"\">2025</a>); Turian et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>); Li &amp; Li (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib56\" title=\"\">2022</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib90\" title=\"\">2022</a>); Bharadwaj et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib8\" title=\"\">2025</a>); Gong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib41\" title=\"\">2022b</a>); Lanzend&#246;rfer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib51\" title=\"\">2025</a>); Bai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>); Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "from",
                    "when",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Aside from learning representations, we also compare against state-of-the-art audio-text retrieval models to assess our approach&#8217;s performance on the specific task it was designed for. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T10\" title=\"Table 10 &#8227; A.6 Additional Results &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> presents retrieval results for our best-performing model (Contrastive-init) against state-of-the-art\naudio-text retrieval model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>)</cite>.\nOur model achieving comparable or superior results on benchmarks in various audio domains, with particularly strong performance on speech and music retrieval.\nThe results indicate that our general-purpose audio-language pretraining approach can compete with specialized retrieval models while offering broader applicability across diverse usage scenarios.</p>\n\n",
                "matched_terms": [
                    "across",
                    "audio",
                    "from",
                    "domains",
                    "performance"
                ]
            }
        ]
    },
    "A1.T8": {
        "caption": "Table 8: Details of the dataset used for assessing audio representation. †evaluate by GPT-4 in AIR-Bench. ‡synthesized with public available speech datasets (Ardila et al., 2019; Busso et al., 2008; Cao et al., 2014; Livingstone & Russo, 2018; Poria et al., 2018) with fixed question template.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">Recall@1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">RougeL</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "details",
            "recall1",
            "public",
            "russo",
            "rougel",
            "audio",
            "‡synthesized",
            "template",
            "poria",
            "used",
            "†evaluate",
            "airbench",
            "gpt4",
            "speech",
            "available",
            "assessing",
            "ardila",
            "datasets",
            "cao",
            "fixed",
            "busso",
            "representation",
            "question",
            "dataset",
            "livingstone"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T8\" title=\"Table 8 &#8227; A.4 Evaluation Datasets &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> details the evaluation datasets and their metrics used for assessing audio representation quality across our three evaluation protocols: linear probing&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Fonseca et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>); Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib21\" title=\"\">2018</a>); Cao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib10\" title=\"\">2014</a>); Law et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib52\" title=\"\">2010</a>); Engel et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib32\" title=\"\">2017</a>); Hershey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib45\" title=\"\">2021</a>); Ebbers et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib30\" title=\"\">2022</a>)</cite>, audio-language alignment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>); Diwan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>); Agostinelli et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib58\" title=\"\">2004</a>)</cite> and open-form question answering&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lipping et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib59\" title=\"\">2022</a>); Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib61\" title=\"\">2024</a>); Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>)</cite>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Audio-language pretraining holds promise for general-purpose audio understanding, yet remains underexplored compared to its vision counterpart. While vision-language models like CLIP serve as widely adopted foundations, existing audio-language models primarily excel at retrieval tasks with limited adoption as general-purpose encoders.\nWe identify three key barriers: limited large-scale audio-text corpora, insufficient caption diversity, and lack of systematic exploration and evaluation.\nTo this end, we introduce CaptionStew, a 10.7M caption dataset aggregating diverse open-source audio-text corpora across multiple domains and captioning styles.\nUsing this resource, we conduct the first comprehensive evaluation comparing contrastive and captioning objectives for audio representation learning across speech, music, and environmental sound tasks.\nOur results demonstrate that audio-language pretraining yields competitive, transferable representations. Through systematic data-scaling experiments, we reveal complementary objective strengths: contrastive learning achieves superior data efficiency at smaller scales, while captioning demonstrates better scalability on language-involved audio understanding tasks. We also find that common supervised initialization practices provide diminishing returns at scale, challenging current approaches.\nThese findings establish audio-language pretraining as a viable pathway toward general-purpose audio representations, guiding future research. To accelerate progress, we release data preparation recipes, training protocols, and pretrained models, paving the way toward universal audio understanding.\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio",
                    "representation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Representation learning has long been central to audio processing<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>In this work, audio processing refers to audio understanding, speech analysis and music understanding, while excluding automatic speech recognition</span></span></span>, with substantial progress over the past decades.\nEarly advances relied on supervised learning, where models trained on labeled corpora were adapted to related downstream tasks or transferred across domains&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib50\" title=\"\">2020</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib15\" title=\"\">2022a</a>; Snyder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib84\" title=\"\">2018</a>; Desplanques et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib24\" title=\"\">2020</a>)</cite>.\nMore recently, self-supervised learning (SSL) has emerged as the promising paradigm.\nBy pretraining on large-scale unlabeled audio with contrastive objectives or masked modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib40\" title=\"\">2022a</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib46\" title=\"\">2021</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>)</cite>, the resulting models learn rich structural knowledge of audio signals, consistently enhancing performance across many speech and audio benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib96\" title=\"\">2021</a>; Turian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib98\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio",
                    "representation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While these techniques have achieved remarkable success, a fundamental limitation persists: existing methods are primarily designed to excel on specific tasks.\nThis domain specificity stems from explicit inductive biases embedded in model architectures and training objectives.\nModels optimized for environmental sounds usually underperform capturing speaker characteristics or paralinguistic information in speech, and vice versa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Turian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>)</cite>.\nAchieving general-purpose audio representations that transfer robustly across diverse audio modalities remains a challenging and actively pursued goal in the field.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of vision&#8211;language pretraining underscores this promise.\nModels like CLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib75\" title=\"\">2021</a>)</cite> and AIM-v2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib33\" title=\"\">2025</a>)</cite> not only power vision&#8211;language tasks but also produce representations that benefit a broad range of vision tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib60\" title=\"\">2023</a>; Minderer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib68\" title=\"\">2022</a>; Crowson et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib23\" title=\"\">2022</a>)</cite>.\nIn contrast, audio&#8211;language models have not yet achieved comparable advancements.\nWhile existing models such as CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>)</cite> excel at audio&#8211;text retrieval, their representations have seen limited adoption for broader audio understanding tasks, suggesting fundamental gaps in current approaches.\nWe identify three key challenges that have constrained progress.\nFirst, large-scale, web-mined image&#8211;text corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schuhmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib83\" title=\"\">2022</a>; Gadre et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib35\" title=\"\">2023</a>)</cite> contain billions of pairs, but no comparable resource exists for audio.\nCurrent audio caption datasets barely exceed one million pairs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>)</cite>, often relying on captions synthesized or augmented by large language models, fundamentally limiting the scaling potential of audio&#8211;language models.\nSecond, widely used audio caption corpora focus predominantly on identifying what is presenting in the audio, while lacking coverage of the rich hierarchy of acoustic attributes that define specific audio signals.\nFor instance, captions rarely characterize speaker characteristics (voice timbre, speaking style), musical attributes (harmonic structure, rhythmic patterns), or environmental acoustics (reverberation, background ambiance).\nThis imbalanced focus limits the model&#8217;s ability to learn representations that capture the full spectrum of audio semantics.\nThird, prior work has primarily focused on contrastive learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite> and evaluated on audio&#8211;text retrieval.\nSystematic studies on alternative pretraining objectives (e.g., captioning) and comprehensive evaluations across a wide suite of audio understanding tasks remain scarce, limiting our understanding of what drives effective audio&#8211;language pretraining.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "used",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we revisit audio&#8211;language pretraining with the goal of reestablishing its viability as a pathway toward general-purpose audio representation learning. Our contributions are:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "representation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide the first comprehensive evaluation of audio-language pretraining across diverse tasks and protocols, demonstrating that audio&#8211;language pretraining produces competitive, transferable representations across speech, music, and environmental audio domains.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct the first systematic comparison of contrastive learning and captioning objectives for audio representation learning, revealing that contrastive learning exhibits superior data efficiency while captioning demonstrates better scalability.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "representation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio&#8211;language pretraining learns audio representations by establishing correspondence between audio signals and natural language descriptions. The core objective is to leverage text as structured semantic supervision, enabling models to capture diverse information across speech, music, and environmental sounds within a unified framework.\nAudio&#8211;language models typically employ a two-tower architecture: an audio encoder <math alttext=\"f_{\\text{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mtext>a</mtext></msub><annotation encoding=\"application/x-tex\">f_{\\text{a}}</annotation></semantics></math> that maps raw audio signals into contextual representations, and a text component <math alttext=\"f_{\\text{t}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mtext>t</mtext></msub><annotation encoding=\"application/x-tex\">f_{\\text{t}}</annotation></semantics></math> whose design depends on the training objective.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S2.F1\" title=\"Figure 1 &#8227; 2 Language-audio Pretraining &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we explore two complementary paradigms that differ fundamentally in how they establish audio-text correspondence, contrastive and captioning objective. These approaches represent discriminative and generative perspectives on audio-language alignment, respectively.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive Objective</span> is proven to be a robust representation learning method&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib19\" title=\"\">2020b</a>; Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib75\" title=\"\">2021</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>)</cite> and have been a dominant approach for audio-language pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite>.\nThis approach aligns audio and text representations in a shared embedding space by maximizing similarity between paired samples while minimizing similarity between mismatched pairs.\nGiven a batch of paired samples <math alttext=\"\\{(a_{i},t_{i})\\}_{i=1}^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m1\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding=\"application/x-tex\">\\{(a_{i},t_{i})\\}_{i=1}^{N}</annotation></semantics></math>, the audio encoder produces frame- (or patch-) level representations that are pooled and projected to audio embeddings <math alttext=\"\\mathbf{z}^{a}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119859;</mi><mi>i</mi><mi>a</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{z}^{a}_{i}</annotation></semantics></math>, while the text encoder <math alttext=\"f_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">f_{t}</annotation></semantics></math> generates corresponding text embeddings <math alttext=\"\\mathbf{z}^{t}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>&#119859;</mi><mi>i</mi><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{z}^{t}_{i}</annotation></semantics></math>.\nThe symmetric InfoNCE loss&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Oord et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib73\" title=\"\">2018</a>)</cite> is applied to optimize both modalities:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "representation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Captioning Objective</span> takes a generative approach to audio-language alignment, learning representations by generating textual descriptions from audio.\nWe argue that captioning presents a promising alternative for audio-language pretraining, offering denser token-level supervision compared to contrastive learning and better alignment with recent trends toward general audio understanding systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib38\" title=\"\">2025</a>)</cite>, yet remains underexplored in prior literature.\nGiven an audio signal <math alttext=\"a_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m1\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">a_{i}</annotation></semantics></math>, the encoder <math alttext=\"f_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">f_{a}</annotation></semantics></math> produces contextual representations <math alttext=\"\\mathbf{Z}^{a}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>&#119833;</mi><mi>i</mi><mi>a</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{Z}^{a}_{i}</annotation></semantics></math>, which are fed into a transformer decoder <math alttext=\"g_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m4\" intent=\":literal\"><semantics><msub><mi>g</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">g_{t}</annotation></semantics></math> through cross-attention. Inspired by CapPa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tschannen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib88\" title=\"\">2023</a>)</cite>, we alternate between two decoding modes&#8212;autoregressive and parallel prediction&#8212;to enhance audio encoder representation learning.\nIn the autoregressive decoding, the decoder generates caption tokens <math alttext=\"(y_{1},\\ldots,y_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y_{1},\\ldots,y_{T})</annotation></semantics></math> sequentially, with each token conditioned on the audio representation and previously generated tokens. Training follows the teacher-forcing approach with a cross-entropy loss:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "representation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the potential of audio&#8211;language pretraining for general-purpose representation learning, we construct a large-scale and diverse audio caption dataset that addresses key limitations in existing corpora. Audio signals inherently encode information across multiple dimensions&#8212;timbre, pitch, rhythm, semantic events, emotional tone, and acoustic environment&#8212;each amenable to different linguistic descriptions. However, existing large-scale audio caption datasets typically rely on a single generation pipeline, whether human-annotated or LLM-synthesized. While ensuring consistency, this approach inevitably introduces systematic biases in language style and emphasized attributes. Furthermore, single-pipeline captions exhibit limited syntactic diversity and tend to focus on specific audio facets while neglecting complementary aspects.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "representation",
                    "datasets",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To fully leverage text as a flexible semantic scaffold for diverse audio representation learning, we embrace caption diversity across sources, styles, and descriptive granularities. Rather than creating captions through a single pipeline, we aggregate existing open-source corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib18\" title=\"\">2025</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>; Roy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib79\" title=\"\">2025</a>)</cite>.\nThese datasets span multiple audio domains&#8212;general sound events, expressive speech, and musical performance&#8212;and employ fundamentally different caption creation methodologies. This aggregation yields captions that describe complementary audio aspects with varying granularity, from coarse event categories to fine-grained acoustic attributes.\nPlease refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS2\" title=\"A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a> for detail of each source dataset.\nWhen multiple datasets contain identical audio samples with different captions, we identify these overlaps and consolidate all available captions for each audio file. This multi-caption pairing allows single audio clips to benefit from diverse perspectives and descriptive focuses, enriching the supervision signal.\nTo ensure evaluation integrity, we carefully filter out samples overlapping with development or test sets of downstream benchmarks.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "representation",
                    "dataset",
                    "speech",
                    "available",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The resulting dataset, <span class=\"ltx_text ltx_font_bold\">CaptionStew</span> (denoted by CS10M), contains 9.3 million audio samples paired with 10.7 million captions, spanning 37,290 hours across speech, music, and environmental domains. Compared to existing collections, CaptionStew achieves both greater scale and broader coverage. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S3.T1\" title=\"Table 1 &#8227; 3 CaptionStew Dataset &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a comparison with existing audio caption datasets.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio",
                    "datasets",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The audio encoder uses a Zipformer-M architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib97\" title=\"\">2024</a>)</cite>, chosen for its efficiency on long sequences and fast convergence.\nZipformer employs six encoder blocks in a U-Net structure that processes sequences at multiple resolutions to capture fine- and coarse-grained temporal information.\nAlthough originally designed for automatic speech recognition, our preliminary experiments confirm Zipformer as a competitive backbone across audio classification tasks (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS3\" title=\"A.3 Zipformer Model &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>).\nFor contrastive pretraining, the text encoder follows BERT-base architecture (12 layers 768 hidden dimensions)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib25\" title=\"\">2019</a>)</cite>.\nFor captioning pretraining, the text decoder adopts the BART-base decoder architecture (6 layers, 768 hidden dimensions)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib54\" title=\"\">2020</a>)</cite>.\nWe use twice as many encoder layers as decoder layers to ensure comparable training speed across objectives.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following prior works in audio-language pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>)</cite>, we experiment with two scenarios: training from scratch or initialized from pretrained checkpoints.\nThe audio encoder initializes from a Zipformer-based audio event classifier trained on AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib36\" title=\"\">2017</a>)</cite> with an mAP of 0.46, while text components use corresponding publicly available checkpoints.\nAll models are trained on 8 Tesla V100 GPUs with an effective batch size of 640 seconds of audio per GPU.\nTraining runs for 600k steps from scratch (14 days wall-clock time) or 200k steps if initialized from pretrained checkpoint.</p>\n\n",
                "matched_terms": [
                    "available",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate pretrained audio encoders across three protocols assessing discriminative capabilities, audio-language alignment, and open-formed question answering.\nAll experiments probe frozen representations from the audio encoder&#8217;s final layer to ensure fair model comparison. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S3.T2\" title=\"Table 2 &#8227; 3 CaptionStew Dataset &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS4\" title=\"A.4 Evaluation Datasets &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> details the datasets and metrics for each task.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "question",
                    "details",
                    "assessing",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Linear Probing</span> trains simple linear classifiers on frozen representations.\nFor classification tasks, we experiment with two pooling mechanisms&#8212;mean pooling and multi-head attention pooling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib53\" title=\"\">2019</a>)</cite>&#8212;to obtain clip-level representations.\nWe evaluate across a diverse set of tasks across audio domains, including audio event classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>)</cite>, sound event detection&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib45\" title=\"\">2021</a>)</cite>, speaker-related tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib21\" title=\"\">2018</a>; Cao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib10\" title=\"\">2014</a>)</cite>, and music classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Law et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib52\" title=\"\">2010</a>; Engel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib32\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "cao"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-language Alignments</span>\nfollow the LiT protocol&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib100\" title=\"\">2022</a>)</cite>, adapting pretrained text components to align with frozen audio representations.\nFor retrieval, we pair audio encoders with pretrained RoBERTa-base text encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib62\" title=\"\">2019</a>)</cite>.\nFor captioning, we use pretrained BART-base decoders&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib54\" title=\"\">2020</a>)</cite>, and only finetune cross-attention layers as we observed more stable training.\nBoth setups finetune on corresponding datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>)</cite> while keeping audio encoders frozen.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Open-formed Question Answering</span>\nAcknowledging the trend of combining audio encoders with large language models (LLMs) for general audio understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib37\" title=\"\">2024</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib42\" title=\"\">2024</a>)</cite>, we connects frozen audio encoders to a LLM (Qwen2.5-7B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib94\" title=\"\">2024a</a>)</cite>) through lightweight adaptors that project audio representations into the LLM&#8217;s embedding space.\nWe train only the adaptor on audio QA datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipping et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib59\" title=\"\">2022</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib61\" title=\"\">2024</a>)</cite> and evaluate on corresponding track in AIR-Bench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>)</cite>.\nDuring training, we carefully monitor instruction-following behavior (<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>99%) to ensure reliable evaluation.</p>\n\n",
                "matched_terms": [
                    "question",
                    "audio",
                    "datasets",
                    "airbench"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recognizing the broad adoption and effectiveness of pretrained audio event classifiers in transfer learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alonso-Jim&#233;nez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib3\" title=\"\">2023</a>; Cappellazzo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib11\" title=\"\">2024</a>)</cite>, audio-language modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>)</cite> and general audio understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib42\" title=\"\">2024</a>; Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib37\" title=\"\">2024</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>)</cite>, we select our pretrained Zipformer-based audio event classifier (described in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.SS1\" title=\"4.1 Implementation Details &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) as the primary baseline.\nIn addition, we compare against representative self-supervised learning (SSL) models, each pretrained under different paradigms and specialized for particular audio domains. BEATs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>)</cite> is an audio SSL model trained with an iterative masked acoustic token prediction framework. Wav2vec 2.0&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>)</cite> learns speech representation by distinguishing target quantized latent representations from disctrators. MERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>)</cite> is a music SSL model trained with masked acoustic modeling, learning to capture acoustic cues and structural information of music.\nTogether, these baselines provide a broad comparative context for studying audio&#8211;language pretraining toward general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio",
                    "representation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive vs. Captioning Objectives.</span>\nThe two pretraining paradigms exhibit complementary strengths across evaluation protocols.\nOn linear probing tasks, contrastive learning consistently outperforms captioning, particularly excelling at audio event classification and speaker identification.\nHowever, it is worth noting that this gap narrows substantially when the classifier learns to aggregate information across frames through multi-head attention pooling (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS5\" title=\"A.5 Main Results (cont.) &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>).\nThis observation reflects the objectives&#8217; inherent designs: contrastive learning explicitly optimizes for linearly separable clip-level representations, while captioning relies on cross-attention mechanisms over frame-level representations for text sequence generation.\nThis finding aligns with recent work highlighting how downstream module choices significantly impact the assessment of audio representation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zaiem et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib99\" title=\"\">2023</a>)</cite>.\nFor language-involved tasks, both objectives demonstrate competitive performance, with captioning showing slight advantages in open-form question answering across multiple domains.\nThis suggests captioning&#8217;s potential for language-involved audio understanding tasks, aligning with recent trends toward generative audio understanding systems.</p>\n\n",
                "matched_terms": [
                    "question",
                    "audio",
                    "representation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Supervised Initialization.</span> Initializing from supervised pretraining (AS SL) provides substantial benefits across most tasks, with notable improvements on audio event classification, sound event detection and audio-text retreival.\nThe gains are particularly pronounced for contrastive objectives, suggesting that discriminative pretraining provides useful inductive biases for contrastive learning.\nHowever, these benefits diminish (or disappear entirely) when the attributes required for downstream tasks diverge from AudioSet&#8217;s ontology.\nOn speaker identification and music tagging, scratch-trained models often match or exceed initialized variants, indicating that AudioSet&#8217;s focus on distinguishing between sound categories may bias representations toward event-level semantics rather than the acoustic attributes (voice timbre, speaking style) or musical structure (genre, harmony, rhythm) essential for these tasks.\nThese findings challenge common initialization practices for audio-language pretraining and suggest the need for tailored pretraining strategies when targeting general-purpose audio representation learning.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "representation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Competitive Performance Across Domains.</span> Our audio-language representations achieve strong transferability across diverse audio domains.\nCompared to supervised baselines (Zipformer-AEC), our overall best-performing model (Contrastive-init) demonstrate superior performance on speaker identification, music understanding and audio-text retrieval while maintaining competitiveness on audio-event classification.\nAgainst domain-specialized SSL methods (BEATs, wav2vec2, MERT), our approach consistently shows competitive performance.\nThis consistent cross-domain performance validates our hypothesis that diverse caption aggregation enables broadly transferable representations, establishing audio-language pretraining as a viable path toward learning general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "representation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive vs. Captioning Scaling. </span>\nContrastive learning consistently outperforms captioning at varying data scales, particularly under less data and on discriminative tasks such as audio event classification.\nHowever, captioning demonstrates slightly better scaling properties, with distinct patterns emerging across task categories.\nor language-involved tasks&#8211;especially captioning and question answering&#8211;captioning matches or surpasses contrastive learning at our current 10M-pair scale.\nOn linear probing benchmarks, the gap remains substantial, with scaling trends suggesting captioning would require hundreds of millions of pairs to achieve parity with contrastive methods.</p>\n\n",
                "matched_terms": [
                    "question",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To understand the linguistic characteristics of CaptionStew, we analyze caption diversity across constituent datasets through visualization and quantitative methods.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.SS6\" title=\"4.6 Dataset Analysis &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4.6</span></a> provides compelling evidence of our aggregation strategy&#8217;s success through t-SNE visualization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Maaten &amp; Hinton, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib65\" title=\"\">2008</a>)</cite> of sentence embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reimers &amp; Gurevych, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib77\" title=\"\">2019</a>)</cite> from sampled captions, revealing distinct clustering patterns by source that demonstrate complementary linguistic perspectives: AudioSetCaps and WavCaps overlap in audio event descriptions and aligns more with human annotated dataset, while JamendoMaxCaps creates a distinct cluster focused on music-specific terminology, and ParaSpeechCaps forms a separate cluster emphasizing speaking styles and paralinguistic attributes.\nThese minimal overlaps confirm that each dataset contributes distinct caption styles and descriptive focuses.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "datasets",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Quantitative analysis reveals both the benefits and limitations (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.T4\" title=\"Table 4 &#8227; 4.6 Dataset Analysis &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).\nCaptionStew achieves substantial vocabulary expansion (56,586 unique words vs. 4,060-27,906 for individual datasets)\nHowever, this growth doesn&#8217;t yield proportional lexical diversity.\nCaptionStew&#8217;s Distinct-n metrics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib55\" title=\"\">2015</a>)</cite> remain low, falling short of image caption dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Changpinyo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib13\" title=\"\">2021</a>)</cite> and text corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Merity et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib67\" title=\"\">2016</a>)</cite>. This constraint stems from datasets with limited linguistic variation, particularly JamendoMaxCaps and ParaSpeechCaps with extremely low Distinct-n scores.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings highlight that simply combining datasets doesn&#8217;t guarantee improved linguistic diversity, revealing broader limitations in current audio-language pretraining approaches.\nAlso, the constrained diversity in certain aspect may partially explain weaker scaling behavior observed for certain tasks, as models encounter repetitive linguistic patterns despite increased data volume, aligning with vision-language findings on caption diversity&#8217;s importance for representation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Santurkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib82\" title=\"\">2023</a>; Chan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib12\" title=\"\">2022</a>)</cite>.\nThis analysis motivates developing enhanced aggregation pipeline and more diverse caption generation methods to better capture the full spectrum of information in audio signals, thereby fully realizing the potential of large-scale audio-language pretraining.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "representation",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Representation Learning.</span>\nThe ultimate goal of audio representation learning is developing a single model suitable for diverse audio understanding tasks.\nSupervised models trained on labeled datasets have been fundamental to the field, including audio event classifiers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib44\" title=\"\">2017</a>; Cramer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib22\" title=\"\">2019</a>; Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib50\" title=\"\">2020</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib39\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib15\" title=\"\">2022a</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib26\" title=\"\">2024</a>)</cite>, speech recognition systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib76\" title=\"\">2023</a>)</cite> and speaker recognition models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Snyder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib84\" title=\"\">2018</a>; Desplanques et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib24\" title=\"\">2020</a>)</cite>.\nThese approaches remain widely adopted due to their strong performance on target tasks. In parallel, self-supervised learning methods have emerged as a complementary approach, offering advances across speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib46\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib16\" title=\"\">2022b</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib6\" title=\"\">2022</a>)</cite>, audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib40\" title=\"\">2022a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib48\" title=\"\">2022</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>; Li &amp; Li, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib56\" title=\"\">2022</a>)</cite>, and music&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib102\" title=\"\">2025</a>)</cite> without requiring labeled data.\nWhile these methods show improved generalization within their target domains, achieving truly general-purpose audio representations remains challenging.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio",
                    "representation",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio&#8211;Language Pretraining.</span>\nAudio-language models have emerged as a promising approach for learning cross-modal representations. Most existing work focuses on contrastive learning objectives that align audio and text in shared embedding spaces&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite>. Recent extensions have explored combinations with other objectives&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib93\" title=\"\">2023</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib101\" title=\"\">2024</a>; Niizumi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib71\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib72\" title=\"\">2025</a>)</cite>.\nThe field has also witnessed rapid evolution in datasets, transitioning from traditional human-annotated corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>)</cite> to recently constructed LLM-augmented collections&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib18\" title=\"\">2025</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib86\" title=\"\">Sun et&#160;al., </a>)</cite> and domain-specific resources covering speech characteristics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>)</cite>, and musical attributes&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Roy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib79\" title=\"\">2025</a>)</cite>.\nOur work contributes by providing the first systematic comparison between contrastive and captioning objectives, along with comprehensive evaluation toward general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio",
                    "representation",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Universal Audio Understanding.</span>\nThe evaluation of audio understanding has evolved from task-specific classification benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib96\" title=\"\">2021</a>; Turian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib98\" title=\"\">2023</a>)</cite> toward more comprehensive assessment frameworks.\nRecent developments have emphasized LLM-based audio understanding systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib37\" title=\"\">2024</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib42\" title=\"\">2024</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib38\" title=\"\">2025</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib20\" title=\"\">2024</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib87\" title=\"\">2024</a>)</cite> that can handle open-form queries and complex reasoning tasks.\nThis shift has driven the development of corresponding evaluation benchmarks that assess models&#8217; abilities across diverse audio understanding scenarios, including question answering, reasoning, and multi-step audio analysis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib80\" title=\"\">2025</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib47\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib64\" title=\"\">2025</a>)</cite>.\nOur work contributes to this trend by providing the first comprehensive evaluation of audio-language pretraining across discriminative tasks, audio-language alignment, and open-form question answering, thereby bridging the gap between traditional representation learning evaluation and modern universal audio understanding.</p>\n\n",
                "matched_terms": [
                    "question",
                    "audio",
                    "representation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We revisited audio-language pretraining to advance general-purpose audio representation learning. We introduced CaptionStew, a large-scale aggregation of diverse audio caption datasets, and through comprehensive evaluation across tasks, demonstrated that audio-language pretraining produces competitive representations across speech, music, and environmental domains. Our systematic comparison revealed complementary strengths of two audio-language pretraining objectives: contrastive learning excels in data efficiency, while captioning shows better scalability for language-involved tasks.\nData-scaling experiments highlighted diminishing returns of supervised initialization, challenging common practices, while dataset analysis revealed linguistic diversity limitation in current datasets. These findings establish audio-language pretraining as a viable pathway toward universal audio understanding.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "representation",
                    "speech",
                    "dataset",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dataset Construction and Quality.</span> CaptionStew aggregates captions from multiple sources with varying generation methodologies, including LLM-synthesized descriptions that may introduce systematic biases or artifacts.\nWe do not perform extensive quality control or human verification across the aggregated corpus, which could impact model training.\nAdditionally, our dataset analysis reveals that simple aggregation does not guarantee improved linguistic diversity&#8212;CaptionStew&#8217;s lexical diversity metrics remain lower than mature image-text corpora.\nHowever, our design choice prioritizes semantic diversity over linguistic variety, as evidenced by the t-SNE clustering analysis showing distinct descriptive focuses across constituent datasets.\nWhile more sophisticated curation strategies could improve quality, our goal was to establish whether diverse caption aggregation can benefit audio representation learning, which our results support despite these limitations.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "representation",
                    "datasets",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited Technical Novelty.</span> Our work primarily combines existing techniques&#8212;contrastive learning, captioning objectives, and dataset aggregation&#8212;rather than introducing fundamentally new methods. The mixed autoregressive/parallel training approach is adapted from vision-language work (CapPa), and our architectural choices follow standard practices.\nWe acknowledge that the technical contributions are largely empirical rather than methodological.\nHowever, this aligns with our primary goal of systematically evaluating audio-language pretraining&#8217;s potential for general-purpose representation learning.\nThe field currently lacks comprehensive comparative studies across objectives, evaluation protocols, and training factors.\nOur systematic analysis reveals important insights about scaling behaviors and initialization effects that have practical implications for practitioners, even if the underlying techniques are not novel.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "representation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CaptionStew aggregates eight open-source audio caption datasets to address data scarcity and limited diversity in current audio-language pretraining. The constituent datasets span environmental sounds, music, and expressive speech, with fundamentally different captioning approaches&#8212;from crowdsourced human annotation to expert curation to various LLM-based generation pipelines.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T5\" title=\"Table 5 &#8227; A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T6\" title=\"Table 6 &#8227; A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> detail each dataset&#8217;s characteristics and provide example captions that illustrate the diverse descriptive styles, ranging from concise event descriptions to detailed multi-sentence narratives with fine-grained acoustic and contextual information.\nDuring aggregation, we filter audio samples longer than one minute for computational efficiency and remove samples that overlap with common audio understanding benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>; Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>; Salamon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib81\" title=\"\">2014</a>)</cite> to prevent data leakage. This approach preserves the unique characteristics of each source while creating a unified corpus that captures broader semantic coverage than individual datasets.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although Zipformer was originally designed for automatic speech recognition, we conducted preliminary experiments to validate its effectiveness as a general audio encoder across diverse domains.\nAs in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T7\" title=\"Table 7 &#8227; A.3 Zipformer Model &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, our initial studies confirmed that Zipformer achieves competitive performance on environmental sound classification, music understanding, and speaker-related tasks, demonstrating its suitability as a unified backbone for multi-domain audio representation learning. This cross-domain efficacy makes it an appropriate choice for our audio-language pretraining experiments that span speech, music, and environmental audio.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio",
                    "representation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Aside from learning representations, we also compare against state-of-the-art audio-text retrieval models to assess our approach&#8217;s performance on the specific task it was designed for. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T10\" title=\"Table 10 &#8227; A.6 Additional Results &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> presents retrieval results for our best-performing model (Contrastive-init) against state-of-the-art\naudio-text retrieval model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>)</cite>.\nOur model achieving comparable or superior results on benchmarks in various audio domains, with particularly strong performance on speech and music retrieval.\nThe results indicate that our general-purpose audio-language pretraining approach can compete with specialized retrieval models while offering broader applicability across diverse usage scenarios.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio"
                ]
            }
        ]
    },
    "A1.T9": {
        "caption": "Table 9: Linear probing results when using multi-head attention pooling.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\">Method</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"3\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\">Model</span>\n<span class=\"ltx_p\">Initialization</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"3\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\">Audio-language</span>\n<span class=\"ltx_p\">Pretraining</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"6\"><span class=\"ltx_text ltx_font_bold\">linear probing</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AEC</span></span>\n<span class=\"ltx_p\">FSD50k</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AEC</span></span>\n<span class=\"ltx_p\">VggSound</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SID</span></span>\n<span class=\"ltx_p\">VoxCeleb2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SER</span></span>\n<span class=\"ltx_p\">CREMA</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MTAG</span></span>\n<span class=\"ltx_p\">MagnaTagATune</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">INST</span></span>\n<span class=\"ltx_p\">NSynth</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Our Supervised Baselines</span></th>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Zipformer-AEC&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Yao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib97\" title=\"\">2024</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">AS SL</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">0.656</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">56.23</span></td>\n<td class=\"ltx_td ltx_align_center\">58.76</td>\n<td class=\"ltx_td ltx_align_center\">72.52</td>\n<td class=\"ltx_td ltx_align_center\">0.405</td>\n<td class=\"ltx_td ltx_align_center\">67.19</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Our Audio-language Pretrained Models</span></th>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Contrastive-<span class=\"ltx_text ltx_font_italic\">scratch</span>\n</th>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">CS10M</td>\n<td class=\"ltx_td ltx_align_center\">0.640</td>\n<td class=\"ltx_td ltx_align_center\">52.81</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">72.86</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">74.50</span></td>\n<td class=\"ltx_td ltx_align_center\">0.406</td>\n<td class=\"ltx_td ltx_align_center\">75.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Captioning-<span class=\"ltx_text ltx_font_italic\">scratch</span>\n</th>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">CS10M</td>\n<td class=\"ltx_td ltx_align_center\">0.619</td>\n<td class=\"ltx_td ltx_align_center\">50.97</td>\n<td class=\"ltx_td ltx_align_center\">56.64</td>\n<td class=\"ltx_td ltx_align_center\">70.40</td>\n<td class=\"ltx_td ltx_align_center\">0.406</td>\n<td class=\"ltx_td ltx_align_center\">72.10</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Contrastive-<span class=\"ltx_text ltx_font_italic\">init</span>\n</th>\n<td class=\"ltx_td ltx_align_center\">AS SL</td>\n<td class=\"ltx_td ltx_align_center\">CS10M</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">0.670</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">54.89</span></td>\n<td class=\"ltx_td ltx_align_center\">72.24</td>\n<td class=\"ltx_td ltx_align_center\">73.09</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">0.412</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">76.70</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Captioning-<span class=\"ltx_text ltx_font_italic\">init</span>\n</th>\n<td class=\"ltx_td ltx_align_center\">AS SL</td>\n<td class=\"ltx_td ltx_align_center\">CS10M</td>\n<td class=\"ltx_td ltx_align_center\">0.660</td>\n<td class=\"ltx_td ltx_align_center\">53.68</td>\n<td class=\"ltx_td ltx_align_center\">62.24</td>\n<td class=\"ltx_td ltx_align_center\">71.67</td>\n<td class=\"ltx_td ltx_align_center\">0.411</td>\n<td class=\"ltx_td ltx_align_center\">74.49</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_tt\">SOTA<sup class=\"ltx_sup\">&#8225;</sup>\n</th>\n<td class=\"ltx_td ltx_border_bb ltx_border_tt\"/>\n<td class=\"ltx_td ltx_border_bb ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">0.655</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">59.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">96.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">0.414</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">79.20</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "crema",
            "aec",
            "initialization",
            "attention",
            "baselines",
            "when",
            "supervised",
            "our",
            "fsd50k",
            "vggsound",
            "multihead",
            "nsynth",
            "zipformeraec",
            "pretraining",
            "pooling",
            "contrastivescratch",
            "captioningscratch",
            "captioninginit",
            "voxceleb2",
            "sota‡",
            "mtag",
            "results",
            "sid",
            "magnatagatune",
            "audiolanguage",
            "yao",
            "pretrained",
            "inst",
            "ser",
            "probing",
            "model",
            "cs10m",
            "linear",
            "contrastiveinit",
            "method"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table 9 presents linear probing results when using multi-head attention pooling instead of mean pooling.\nWith learned attention pooling, the performance gap between contrastive and captioning objectives narrows substantially, particularly evident on speaker identification where captioning-scratch achieves 72.86% compared to 46.67% with mean pooling (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.T3\" title=\"Table 3 &#8227; 4.4 Main Results &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).\nThis demonstrates that captioning models benefit significantly from adaptive pooling mechanisms, while contrastive learning&#8217;s explicit optimization for clip-level representations shows less sensitivity to pooling strategy.\nThese results underscore the critical importance of appropriate downstream module selection when evaluating different pretraining paradigms, as the choice of pooling mechanism can dramatically influence conclusions about objective effectiveness.\nThe improved performance across all methods with attention pooling also suggests that frame-level representations from both objectives contain rich information that can be better exploited through learned aggregation. SOTA results and SSL baseline results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.T3\" title=\"Table 3 &#8227; 4.4 Main Results &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T9\" title=\"Table 9 &#8227; A.5 Main Results (cont.) &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> are quoted collectively from &#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Niizumi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib72\" title=\"\">2025</a>); Turian et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>); Li &amp; Li (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib56\" title=\"\">2022</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib90\" title=\"\">2022</a>); Bharadwaj et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib8\" title=\"\">2025</a>); Gong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib41\" title=\"\">2022b</a>); Lanzend&#246;rfer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib51\" title=\"\">2025</a>); Bai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>); Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>)</cite>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Audio-language pretraining holds promise for general-purpose audio understanding, yet remains underexplored compared to its vision counterpart. While vision-language models like CLIP serve as widely adopted foundations, existing audio-language models primarily excel at retrieval tasks with limited adoption as general-purpose encoders.\nWe identify three key barriers: limited large-scale audio-text corpora, insufficient caption diversity, and lack of systematic exploration and evaluation.\nTo this end, we introduce CaptionStew, a 10.7M caption dataset aggregating diverse open-source audio-text corpora across multiple domains and captioning styles.\nUsing this resource, we conduct the first comprehensive evaluation comparing contrastive and captioning objectives for audio representation learning across speech, music, and environmental sound tasks.\nOur results demonstrate that audio-language pretraining yields competitive, transferable representations. Through systematic data-scaling experiments, we reveal complementary objective strengths: contrastive learning achieves superior data efficiency at smaller scales, while captioning demonstrates better scalability on language-involved audio understanding tasks. We also find that common supervised initialization practices provide diminishing returns at scale, challenging current approaches.\nThese findings establish audio-language pretraining as a viable pathway toward general-purpose audio representations, guiding future research. To accelerate progress, we release data preparation recipes, training protocols, and pretrained models, paving the way toward universal audio understanding.\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "pretrained",
                    "initialization",
                    "pretraining",
                    "results",
                    "supervised",
                    "audiolanguage",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Representation learning has long been central to audio processing<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>In this work, audio processing refers to audio understanding, speech analysis and music understanding, while excluding automatic speech recognition</span></span></span>, with substantial progress over the past decades.\nEarly advances relied on supervised learning, where models trained on labeled corpora were adapted to related downstream tasks or transferred across domains&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib50\" title=\"\">2020</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib15\" title=\"\">2022a</a>; Snyder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib84\" title=\"\">2018</a>; Desplanques et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib24\" title=\"\">2020</a>)</cite>.\nMore recently, self-supervised learning (SSL) has emerged as the promising paradigm.\nBy pretraining on large-scale unlabeled audio with contrastive objectives or masked modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib40\" title=\"\">2022a</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib46\" title=\"\">2021</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>)</cite>, the resulting models learn rich structural knowledge of audio signals, consistently enhancing performance across many speech and audio benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib96\" title=\"\">2021</a>; Turian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib98\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "supervised",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While these techniques have achieved remarkable success, a fundamental limitation persists: existing methods are primarily designed to excel on specific tasks.\nThis domain specificity stems from explicit inductive biases embedded in model architectures and training objectives.\nModels optimized for environmental sounds usually underperform capturing speaker characteristics or paralinguistic information in speech, and vice versa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Turian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>)</cite>.\nAchieving general-purpose audio representations that transfer robustly across diverse audio modalities remains a challenging and actively pursued goal in the field.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of vision&#8211;language pretraining underscores this promise.\nModels like CLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib75\" title=\"\">2021</a>)</cite> and AIM-v2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib33\" title=\"\">2025</a>)</cite> not only power vision&#8211;language tasks but also produce representations that benefit a broad range of vision tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib60\" title=\"\">2023</a>; Minderer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib68\" title=\"\">2022</a>; Crowson et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib23\" title=\"\">2022</a>)</cite>.\nIn contrast, audio&#8211;language models have not yet achieved comparable advancements.\nWhile existing models such as CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>)</cite> excel at audio&#8211;text retrieval, their representations have seen limited adoption for broader audio understanding tasks, suggesting fundamental gaps in current approaches.\nWe identify three key challenges that have constrained progress.\nFirst, large-scale, web-mined image&#8211;text corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schuhmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib83\" title=\"\">2022</a>; Gadre et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib35\" title=\"\">2023</a>)</cite> contain billions of pairs, but no comparable resource exists for audio.\nCurrent audio caption datasets barely exceed one million pairs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>)</cite>, often relying on captions synthesized or augmented by large language models, fundamentally limiting the scaling potential of audio&#8211;language models.\nSecond, widely used audio caption corpora focus predominantly on identifying what is presenting in the audio, while lacking coverage of the rich hierarchy of acoustic attributes that define specific audio signals.\nFor instance, captions rarely characterize speaker characteristics (voice timbre, speaking style), musical attributes (harmonic structure, rhythmic patterns), or environmental acoustics (reverberation, background ambiance).\nThis imbalanced focus limits the model&#8217;s ability to learn representations that capture the full spectrum of audio semantics.\nThird, prior work has primarily focused on contrastive learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite> and evaluated on audio&#8211;text retrieval.\nSystematic studies on alternative pretraining objectives (e.g., captioning) and comprehensive evaluations across a wide suite of audio understanding tasks remain scarce, limiting our understanding of what drives effective audio&#8211;language pretraining.</p>\n\n",
                "matched_terms": [
                    "models",
                    "pretraining",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we revisit audio&#8211;language pretraining with the goal of reestablishing its viability as a pathway toward general-purpose audio representation learning. Our contributions are:</p>\n\n",
                "matched_terms": [
                    "pretraining",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">CaptionStew</span>, a large-scale aggregation of diverse open-source audio&#8211;text datasets spanning multiple domains and captioning styles, addressing the data scarcity and diversity limitations in current audio-language pretraining.</p>\n\n",
                "matched_terms": [
                    "audiolanguage",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide the first comprehensive evaluation of audio-language pretraining across diverse tasks and protocols, demonstrating that audio&#8211;language pretraining produces competitive, transferable representations across speech, music, and environmental audio domains.</p>\n\n",
                "matched_terms": [
                    "audiolanguage",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze key training factors including data scaling effects and supervised pretraining initialization, showing that while AudioSet pretraining provides general benefits, its effects diminish for tasks unrelated to audio event classification and at larger data scales, challenging common practices in the field.</p>\n\n",
                "matched_terms": [
                    "supervised",
                    "pretraining",
                    "initialization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Taken together, our results suggests audio&#8211;language pretraining as a practical and competitive approach for learning general-purpose audio representations. To accelerate progress in this direction, we release data preperation recipes, training scripts, evaluation protocols, and pretrained models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "pretrained",
                    "pretraining",
                    "results",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio&#8211;language pretraining learns audio representations by establishing correspondence between audio signals and natural language descriptions. The core objective is to leverage text as structured semantic supervision, enabling models to capture diverse information across speech, music, and environmental sounds within a unified framework.\nAudio&#8211;language models typically employ a two-tower architecture: an audio encoder <math alttext=\"f_{\\text{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mtext>a</mtext></msub><annotation encoding=\"application/x-tex\">f_{\\text{a}}</annotation></semantics></math> that maps raw audio signals into contextual representations, and a text component <math alttext=\"f_{\\text{t}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mtext>t</mtext></msub><annotation encoding=\"application/x-tex\">f_{\\text{t}}</annotation></semantics></math> whose design depends on the training objective.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S2.F1\" title=\"Figure 1 &#8227; 2 Language-audio Pretraining &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we explore two complementary paradigms that differ fundamentally in how they establish audio-text correspondence, contrastive and captioning objective. These approaches represent discriminative and generative perspectives on audio-language alignment, respectively.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audiolanguage",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive Objective</span> is proven to be a robust representation learning method&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib19\" title=\"\">2020b</a>; Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib75\" title=\"\">2021</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>)</cite> and have been a dominant approach for audio-language pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite>.\nThis approach aligns audio and text representations in a shared embedding space by maximizing similarity between paired samples while minimizing similarity between mismatched pairs.\nGiven a batch of paired samples <math alttext=\"\\{(a_{i},t_{i})\\}_{i=1}^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m1\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding=\"application/x-tex\">\\{(a_{i},t_{i})\\}_{i=1}^{N}</annotation></semantics></math>, the audio encoder produces frame- (or patch-) level representations that are pooled and projected to audio embeddings <math alttext=\"\\mathbf{z}^{a}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119859;</mi><mi>i</mi><mi>a</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{z}^{a}_{i}</annotation></semantics></math>, while the text encoder <math alttext=\"f_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">f_{t}</annotation></semantics></math> generates corresponding text embeddings <math alttext=\"\\mathbf{z}^{t}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>&#119859;</mi><mi>i</mi><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{z}^{t}_{i}</annotation></semantics></math>.\nThe symmetric InfoNCE loss&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Oord et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib73\" title=\"\">2018</a>)</cite> is applied to optimize both modalities:</p>\n\n",
                "matched_terms": [
                    "audiolanguage",
                    "pretraining",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Captioning Objective</span> takes a generative approach to audio-language alignment, learning representations by generating textual descriptions from audio.\nWe argue that captioning presents a promising alternative for audio-language pretraining, offering denser token-level supervision compared to contrastive learning and better alignment with recent trends toward general audio understanding systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib38\" title=\"\">2025</a>)</cite>, yet remains underexplored in prior literature.\nGiven an audio signal <math alttext=\"a_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m1\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">a_{i}</annotation></semantics></math>, the encoder <math alttext=\"f_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">f_{a}</annotation></semantics></math> produces contextual representations <math alttext=\"\\mathbf{Z}^{a}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>&#119833;</mi><mi>i</mi><mi>a</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{Z}^{a}_{i}</annotation></semantics></math>, which are fed into a transformer decoder <math alttext=\"g_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m4\" intent=\":literal\"><semantics><msub><mi>g</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">g_{t}</annotation></semantics></math> through cross-attention. Inspired by CapPa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tschannen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib88\" title=\"\">2023</a>)</cite>, we alternate between two decoding modes&#8212;autoregressive and parallel prediction&#8212;to enhance audio encoder representation learning.\nIn the autoregressive decoding, the decoder generates caption tokens <math alttext=\"(y_{1},\\ldots,y_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y_{1},\\ldots,y_{T})</annotation></semantics></math> sequentially, with each token conditioned on the audio representation and previously generated tokens. Training follows the teacher-forcing approach with a cross-entropy loss:</p>\n\n",
                "matched_terms": [
                    "audiolanguage",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The audio encoder uses a Zipformer-M architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib97\" title=\"\">2024</a>)</cite>, chosen for its efficiency on long sequences and fast convergence.\nZipformer employs six encoder blocks in a U-Net structure that processes sequences at multiple resolutions to capture fine- and coarse-grained temporal information.\nAlthough originally designed for automatic speech recognition, our preliminary experiments confirm Zipformer as a competitive backbone across audio classification tasks (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS3\" title=\"A.3 Zipformer Model &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>).\nFor contrastive pretraining, the text encoder follows BERT-base architecture (12 layers 768 hidden dimensions)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib25\" title=\"\">2019</a>)</cite>.\nFor captioning pretraining, the text decoder adopts the BART-base decoder architecture (6 layers, 768 hidden dimensions)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib54\" title=\"\">2020</a>)</cite>.\nWe use twice as many encoder layers as decoder layers to ensure comparable training speed across objectives.</p>\n\n",
                "matched_terms": [
                    "pretraining",
                    "yao",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following prior works in audio-language pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>)</cite>, we experiment with two scenarios: training from scratch or initialized from pretrained checkpoints.\nThe audio encoder initializes from a Zipformer-based audio event classifier trained on AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib36\" title=\"\">2017</a>)</cite> with an mAP of 0.46, while text components use corresponding publicly available checkpoints.\nAll models are trained on 8 Tesla V100 GPUs with an effective batch size of 640 seconds of audio per GPU.\nTraining runs for 600k steps from scratch (14 days wall-clock time) or 200k steps if initialized from pretrained checkpoint.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audiolanguage",
                    "pretraining",
                    "pretrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate pretrained audio encoders across three protocols assessing discriminative capabilities, audio-language alignment, and open-formed question answering.\nAll experiments probe frozen representations from the audio encoder&#8217;s final layer to ensure fair model comparison. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S3.T2\" title=\"Table 2 &#8227; 3 CaptionStew Dataset &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS4\" title=\"A.4 Evaluation Datasets &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> details the datasets and metrics for each task.</p>\n\n",
                "matched_terms": [
                    "pretrained",
                    "audiolanguage",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Linear Probing</span> trains simple linear classifiers on frozen representations.\nFor classification tasks, we experiment with two pooling mechanisms&#8212;mean pooling and multi-head attention pooling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib53\" title=\"\">2019</a>)</cite>&#8212;to obtain clip-level representations.\nWe evaluate across a diverse set of tasks across audio domains, including audio event classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>)</cite>, sound event detection&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib45\" title=\"\">2021</a>)</cite>, speaker-related tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib21\" title=\"\">2018</a>; Cao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib10\" title=\"\">2014</a>)</cite>, and music classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Law et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib52\" title=\"\">2010</a>; Engel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib32\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "probing",
                    "multihead",
                    "attention",
                    "linear",
                    "pooling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-language Alignments</span>\nfollow the LiT protocol&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib100\" title=\"\">2022</a>)</cite>, adapting pretrained text components to align with frozen audio representations.\nFor retrieval, we pair audio encoders with pretrained RoBERTa-base text encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib62\" title=\"\">2019</a>)</cite>.\nFor captioning, we use pretrained BART-base decoders&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib54\" title=\"\">2020</a>)</cite>, and only finetune cross-attention layers as we observed more stable training.\nBoth setups finetune on corresponding datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>)</cite> while keeping audio encoders frozen.</p>\n\n",
                "matched_terms": [
                    "pretrained",
                    "audiolanguage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recognizing the broad adoption and effectiveness of pretrained audio event classifiers in transfer learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alonso-Jim&#233;nez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib3\" title=\"\">2023</a>; Cappellazzo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib11\" title=\"\">2024</a>)</cite>, audio-language modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>)</cite> and general audio understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib42\" title=\"\">2024</a>; Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib37\" title=\"\">2024</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>)</cite>, we select our pretrained Zipformer-based audio event classifier (described in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.SS1\" title=\"4.1 Implementation Details &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) as the primary baseline.\nIn addition, we compare against representative self-supervised learning (SSL) models, each pretrained under different paradigms and specialized for particular audio domains. BEATs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>)</cite> is an audio SSL model trained with an iterative masked acoustic token prediction framework. Wav2vec 2.0&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>)</cite> learns speech representation by distinguishing target quantized latent representations from disctrators. MERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>)</cite> is a music SSL model trained with masked acoustic modeling, learning to capture acoustic cues and structural information of music.\nTogether, these baselines provide a broad comparative context for studying audio&#8211;language pretraining toward general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "pretrained",
                    "model",
                    "baselines",
                    "pretraining",
                    "audiolanguage",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present our evaluation results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.T3\" title=\"Table 3 &#8227; 4.4 Main Results &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Our analysis reveals key insights about objective design, representation quality, and the role of initialization.</p>\n\n",
                "matched_terms": [
                    "results",
                    "initialization",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive vs. Captioning Objectives.</span>\nThe two pretraining paradigms exhibit complementary strengths across evaluation protocols.\nOn linear probing tasks, contrastive learning consistently outperforms captioning, particularly excelling at audio event classification and speaker identification.\nHowever, it is worth noting that this gap narrows substantially when the classifier learns to aggregate information across frames through multi-head attention pooling (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.SS5\" title=\"A.5 Main Results (cont.) &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>).\nThis observation reflects the objectives&#8217; inherent designs: contrastive learning explicitly optimizes for linearly separable clip-level representations, while captioning relies on cross-attention mechanisms over frame-level representations for text sequence generation.\nThis finding aligns with recent work highlighting how downstream module choices significantly impact the assessment of audio representation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zaiem et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib99\" title=\"\">2023</a>)</cite>.\nFor language-involved tasks, both objectives demonstrate competitive performance, with captioning showing slight advantages in open-form question answering across multiple domains.\nThis suggests captioning&#8217;s potential for language-involved audio understanding tasks, aligning with recent trends toward generative audio understanding systems.</p>\n\n",
                "matched_terms": [
                    "multihead",
                    "probing",
                    "attention",
                    "pretraining",
                    "linear",
                    "when",
                    "pooling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Supervised Initialization.</span> Initializing from supervised pretraining (AS SL) provides substantial benefits across most tasks, with notable improvements on audio event classification, sound event detection and audio-text retreival.\nThe gains are particularly pronounced for contrastive objectives, suggesting that discriminative pretraining provides useful inductive biases for contrastive learning.\nHowever, these benefits diminish (or disappear entirely) when the attributes required for downstream tasks diverge from AudioSet&#8217;s ontology.\nOn speaker identification and music tagging, scratch-trained models often match or exceed initialized variants, indicating that AudioSet&#8217;s focus on distinguishing between sound categories may bias representations toward event-level semantics rather than the acoustic attributes (voice timbre, speaking style) or musical structure (genre, harmony, rhythm) essential for these tasks.\nThese findings challenge common initialization practices for audio-language pretraining and suggest the need for tailored pretraining strategies when targeting general-purpose audio representation learning.</p>\n\n",
                "matched_terms": [
                    "models",
                    "initialization",
                    "pretraining",
                    "when",
                    "supervised",
                    "audiolanguage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Competitive Performance Across Domains.</span> Our audio-language representations achieve strong transferability across diverse audio domains.\nCompared to supervised baselines (Zipformer-AEC), our overall best-performing model (Contrastive-init) demonstrate superior performance on speaker identification, music understanding and audio-text retrieval while maintaining competitiveness on audio-event classification.\nAgainst domain-specialized SSL methods (BEATs, wav2vec2, MERT), our approach consistently shows competitive performance.\nThis consistent cross-domain performance validates our hypothesis that diverse caption aggregation enables broadly transferable representations, establishing audio-language pretraining as a viable path toward learning general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "baselines",
                    "zipformeraec",
                    "pretraining",
                    "contrastiveinit",
                    "supervised",
                    "audiolanguage",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scaling Patterns.</span> Most tasks demonstrate consistent performance improvements with increased data scale, validating the potential of large-scale audio-language pretraining.\nHowever, notable exceptions emerge that reveal fundamental limitations of current approaches.\nSound event detection, particularly for models initialized with AudioSet pretraining, exhibits a reverse scaling trend where performance degrades with more caption data.\nThis suggests a potential conflict between natural language supervision&#8211;which typically describes audio characteristics and attributes&#8211;and temporal localization tasks requiring precise event boundaries.\nAdditionally, emotion recognition and instrument classification show weaker scaling gains compared to other tasks, likely reflecting limited caption diversity for these specific attributes in existing corpora, which we will discussed in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.SS6\" title=\"4.6 Dataset Analysis &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audiolanguage",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive vs. Captioning Scaling. </span>\nContrastive learning consistently outperforms captioning at varying data scales, particularly under less data and on discriminative tasks such as audio event classification.\nHowever, captioning demonstrates slightly better scaling properties, with distinct patterns emerging across task categories.\nor language-involved tasks&#8211;especially captioning and question answering&#8211;captioning matches or surpasses contrastive learning at our current 10M-pair scale.\nOn linear probing benchmarks, the gap remains substantial, with scaling trends suggesting captioning would require hundreds of millions of pairs to achieve parity with contrastive methods.</p>\n\n",
                "matched_terms": [
                    "linear",
                    "probing",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Initialization at Scale.</span>\nAudioSet initialization provides immediate performance gains but introduces diminishing returns at larger scales. Both contrastive learning and captioning show decreasing benefits from initialization as data scale increases, with scratch and initialized models achieving matched performance at larger scales on some tasks.\nThis suggests that pretrained initialization effectively bootstraps learning at small scales but may constrain the model&#8217;s ability to adapt to the broader semantic space covered by large-scale caption data, potentially due to mismatch between AudioSet&#8217;s ontology and diverse audio descriptions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "pretrained",
                    "initialization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, these findings reveal complementary behaviors: contrastive pretraining achieves superior data efficiency at current scales, while captioning shows better scalability, especially for language-involved tasks.\nImportantly, the diminishing returns of initialization at scale indicate that large-scale caption data can provide sufficient semantic supervision independent of domain-specific pretraining, challenging current practices of audio-language pretraining and opening possibilities for learning general-purpose representations from diverse text descriptions alone.</p>\n\n",
                "matched_terms": [
                    "audiolanguage",
                    "pretraining",
                    "initialization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings highlight that simply combining datasets doesn&#8217;t guarantee improved linguistic diversity, revealing broader limitations in current audio-language pretraining approaches.\nAlso, the constrained diversity in certain aspect may partially explain weaker scaling behavior observed for certain tasks, as models encounter repetitive linguistic patterns despite increased data volume, aligning with vision-language findings on caption diversity&#8217;s importance for representation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Santurkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib82\" title=\"\">2023</a>; Chan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib12\" title=\"\">2022</a>)</cite>.\nThis analysis motivates developing enhanced aggregation pipeline and more diverse caption generation methods to better capture the full spectrum of information in audio signals, thereby fully realizing the potential of large-scale audio-language pretraining.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audiolanguage",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Representation Learning.</span>\nThe ultimate goal of audio representation learning is developing a single model suitable for diverse audio understanding tasks.\nSupervised models trained on labeled datasets have been fundamental to the field, including audio event classifiers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib44\" title=\"\">2017</a>; Cramer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib22\" title=\"\">2019</a>; Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib50\" title=\"\">2020</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib39\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib15\" title=\"\">2022a</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib26\" title=\"\">2024</a>)</cite>, speech recognition systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib76\" title=\"\">2023</a>)</cite> and speaker recognition models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Snyder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib84\" title=\"\">2018</a>; Desplanques et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib24\" title=\"\">2020</a>)</cite>.\nThese approaches remain widely adopted due to their strong performance on target tasks. In parallel, self-supervised learning methods have emerged as a complementary approach, offering advances across speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib46\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib16\" title=\"\">2022b</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib6\" title=\"\">2022</a>)</cite>, audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib40\" title=\"\">2022a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib48\" title=\"\">2022</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>; Li &amp; Li, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib56\" title=\"\">2022</a>)</cite>, and music&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib102\" title=\"\">2025</a>)</cite> without requiring labeled data.\nWhile these methods show improved generalization within their target domains, achieving truly general-purpose audio representations remains challenging.</p>\n\n",
                "matched_terms": [
                    "models",
                    "supervised",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio&#8211;Language Pretraining.</span>\nAudio-language models have emerged as a promising approach for learning cross-modal representations. Most existing work focuses on contrastive learning objectives that align audio and text in shared embedding spaces&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib91\" title=\"\">2022</a>)</cite>. Recent extensions have explored combinations with other objectives&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib93\" title=\"\">2023</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib101\" title=\"\">2024</a>; Niizumi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib71\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib72\" title=\"\">2025</a>)</cite>.\nThe field has also witnessed rapid evolution in datasets, transitioning from traditional human-annotated corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>)</cite> to recently constructed LLM-augmented collections&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib66\" title=\"\">2024</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib18\" title=\"\">2025</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib86\" title=\"\">Sun et&#160;al., </a>)</cite> and domain-specific resources covering speech characteristics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Diwan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>)</cite>, and musical attributes&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Roy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib79\" title=\"\">2025</a>)</cite>.\nOur work contributes by providing the first systematic comparison between contrastive and captioning objectives, along with comprehensive evaluation toward general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audiolanguage",
                    "pretraining",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Universal Audio Understanding.</span>\nThe evaluation of audio understanding has evolved from task-specific classification benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib96\" title=\"\">2021</a>; Turian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib89\" title=\"\">2022</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib98\" title=\"\">2023</a>)</cite> toward more comprehensive assessment frameworks.\nRecent developments have emphasized LLM-based audio understanding systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib37\" title=\"\">2024</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib42\" title=\"\">2024</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>; Goel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib38\" title=\"\">2025</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib20\" title=\"\">2024</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib87\" title=\"\">2024</a>)</cite> that can handle open-form queries and complex reasoning tasks.\nThis shift has driven the development of corresponding evaluation benchmarks that assess models&#8217; abilities across diverse audio understanding scenarios, including question answering, reasoning, and multi-step audio analysis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib80\" title=\"\">2025</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib47\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib64\" title=\"\">2025</a>)</cite>.\nOur work contributes to this trend by providing the first comprehensive evaluation of audio-language pretraining across discriminative tasks, audio-language alignment, and open-form question answering, thereby bridging the gap between traditional representation learning evaluation and modern universal audio understanding.</p>\n\n",
                "matched_terms": [
                    "audiolanguage",
                    "pretraining",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We revisited audio-language pretraining to advance general-purpose audio representation learning. We introduced CaptionStew, a large-scale aggregation of diverse audio caption datasets, and through comprehensive evaluation across tasks, demonstrated that audio-language pretraining produces competitive representations across speech, music, and environmental domains. Our systematic comparison revealed complementary strengths of two audio-language pretraining objectives: contrastive learning excels in data efficiency, while captioning shows better scalability for language-involved tasks.\nData-scaling experiments highlighted diminishing returns of supervised initialization, challenging common practices, while dataset analysis revealed linguistic diversity limitation in current datasets. These findings establish audio-language pretraining as a viable pathway toward universal audio understanding.</p>\n\n",
                "matched_terms": [
                    "initialization",
                    "pretraining",
                    "supervised",
                    "audiolanguage",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While this work provides valuable empirical insights for audio-language pretraining, we acknowledge several important limitations that present opportunities for future research.</p>\n\n",
                "matched_terms": [
                    "audiolanguage",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dataset Construction and Quality.</span> CaptionStew aggregates captions from multiple sources with varying generation methodologies, including LLM-synthesized descriptions that may introduce systematic biases or artifacts.\nWe do not perform extensive quality control or human verification across the aggregated corpus, which could impact model training.\nAdditionally, our dataset analysis reveals that simple aggregation does not guarantee improved linguistic diversity&#8212;CaptionStew&#8217;s lexical diversity metrics remain lower than mature image-text corpora.\nHowever, our design choice prioritizes semantic diversity over linguistic variety, as evidenced by the t-SNE clustering analysis showing distinct descriptive focuses across constituent datasets.\nWhile more sophisticated curation strategies could improve quality, our goal was to establish whether diverse caption aggregation can benefit audio representation learning, which our results support despite these limitations.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited Technical Novelty.</span> Our work primarily combines existing techniques&#8212;contrastive learning, captioning objectives, and dataset aggregation&#8212;rather than introducing fundamentally new methods. The mixed autoregressive/parallel training approach is adapted from vision-language work (CapPa), and our architectural choices follow standard practices.\nWe acknowledge that the technical contributions are largely empirical rather than methodological.\nHowever, this aligns with our primary goal of systematically evaluating audio-language pretraining&#8217;s potential for general-purpose representation learning.\nThe field currently lacks comprehensive comparative studies across objectives, evaluation protocols, and training factors.\nOur systematic analysis reveals important insights about scaling behaviors and initialization effects that have practical implications for practitioners, even if the underlying techniques are not novel.</p>\n\n",
                "matched_terms": [
                    "audiolanguage",
                    "initialization",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited Model and Data Scalability.</span> Our experiments are constrained to 10M audio-text pairs and relatively modest model sizes compared to state-of-the-art vision-language systems that leverage billions of samples and much larger architectures. This scale limitation may not fully reflect the potential of audio-language pretraining, particularly for the captioning objective which our results suggest benefits from larger-scale training. Additionally, we do not explore recent advances in large language model integration or more sophisticated architectural designs that could improve performance. These constraints stem from computational resource limitations and our focus on controlled comparisons rather than pushing absolute performance boundaries. Future work with larger scales may reveal different scaling dynamics and stronger evidence for general-purpose capabilities.</p>\n\n",
                "matched_terms": [
                    "model",
                    "pretraining",
                    "results",
                    "audiolanguage",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CaptionStew aggregates eight open-source audio caption datasets to address data scarcity and limited diversity in current audio-language pretraining. The constituent datasets span environmental sounds, music, and expressive speech, with fundamentally different captioning approaches&#8212;from crowdsourced human annotation to expert curation to various LLM-based generation pipelines.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T5\" title=\"Table 5 &#8227; A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T6\" title=\"Table 6 &#8227; A.2 Sourced Datasets for CaptionStew &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> detail each dataset&#8217;s characteristics and provide example captions that illustrate the diverse descriptive styles, ranging from concise event descriptions to detailed multi-sentence narratives with fine-grained acoustic and contextual information.\nDuring aggregation, we filter audio samples longer than one minute for computational efficiency and remove samples that overlap with common audio understanding benchmarks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Drossos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib29\" title=\"\">2020</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>; Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>; Salamon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib81\" title=\"\">2014</a>)</cite> to prevent data leakage. This approach preserves the unique characteristics of each source while creating a unified corpus that captures broader semantic coverage than individual datasets.</p>\n\n",
                "matched_terms": [
                    "audiolanguage",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although Zipformer was originally designed for automatic speech recognition, we conducted preliminary experiments to validate its effectiveness as a general audio encoder across diverse domains.\nAs in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T7\" title=\"Table 7 &#8227; A.3 Zipformer Model &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, our initial studies confirmed that Zipformer achieves competitive performance on environmental sound classification, music understanding, and speaker-related tasks, demonstrating its suitability as a unified backbone for multi-domain audio representation learning. This cross-domain efficacy makes it an appropriate choice for our audio-language pretraining experiments that span speech, music, and environmental audio.</p>\n\n",
                "matched_terms": [
                    "audiolanguage",
                    "pretraining",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T8\" title=\"Table 8 &#8227; A.4 Evaluation Datasets &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> details the evaluation datasets and their metrics used for assessing audio representation quality across our three evaluation protocols: linear probing&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Fonseca et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib34\" title=\"\">2021</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib14\" title=\"\">2020a</a>); Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib21\" title=\"\">2018</a>); Cao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib10\" title=\"\">2014</a>); Law et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib52\" title=\"\">2010</a>); Engel et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib32\" title=\"\">2017</a>); Hershey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib45\" title=\"\">2021</a>); Ebbers et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib30\" title=\"\">2022</a>)</cite>, audio-language alignment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib49\" title=\"\">2019</a>); Diwan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib28\" title=\"\">2025</a>); Agostinelli et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib2\" title=\"\">2023</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib58\" title=\"\">2004</a>)</cite> and open-form question answering&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lipping et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib59\" title=\"\">2022</a>); Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib61\" title=\"\">2024</a>); Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib95\" title=\"\">2024b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audiolanguage",
                    "probing",
                    "linear",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Aside from learning representations, we also compare against state-of-the-art audio-text retrieval models to assess our approach&#8217;s performance on the specific task it was designed for. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T10\" title=\"Table 10 &#8227; A.6 Additional Results &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> presents retrieval results for our best-performing model (Contrastive-init) against state-of-the-art\naudio-text retrieval model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>)</cite>.\nOur model achieving comparable or superior results on benchmarks in various audio domains, with particularly strong performance on speech and music retrieval.\nThe results indicate that our general-purpose audio-language pretraining approach can compete with specialized retrieval models while offering broader applicability across diverse usage scenarios.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "pretraining",
                    "contrastiveinit",
                    "results",
                    "audiolanguage",
                    "our"
                ]
            }
        ]
    },
    "A1.T10": {
        "caption": "Table 10: audio-text retrieval of the best performing model (Contrastive-init) against state-of-the-art audio-text retrieval model. †reproduce by ourselves.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\">Text-to-audio</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\">Audio-to-text</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">AudioCaps</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">ParaSpeechCaps</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">MusicCaps</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">AudioCaps</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">ParaSpeechCaps</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">MusicCaps</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">AudioSetCaps<sup class=\"ltx_sup\">&#8224;</sup>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">49.7 / 79.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.8 / 2.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">13.4 / 30.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">45.9 / 80.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.2 / 3.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">12.0 / 29.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Contrastive-init (ours)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">44.4 / 79.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">29.6 / 61.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">22.4 / 53.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">47.2 / 78.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">27.0 / 57.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">26.0 / 56.2</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "ours",
            "audiotext",
            "musiccaps",
            "audiocaps",
            "†reproduce",
            "paraspeechcaps",
            "performing",
            "against",
            "model",
            "audiosetcaps†",
            "best",
            "audiototext",
            "contrastiveinit",
            "stateoftheart",
            "retrieval",
            "ourselves",
            "texttoaudio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Aside from learning representations, we also compare against state-of-the-art audio-text retrieval models to assess our approach&#8217;s performance on the specific task it was designed for. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#A1.T10\" title=\"Table 10 &#8227; A.6 Additional Results &#8227; Appendix A Appendix &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> presents retrieval results for our best-performing model (Contrastive-init) against state-of-the-art\naudio-text retrieval model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib7\" title=\"\">2025</a>)</cite>.\nOur model achieving comparable or superior results on benchmarks in various audio domains, with particularly strong performance on speech and music retrieval.\nThe results indicate that our general-purpose audio-language pretraining approach can compete with specialized retrieval models while offering broader applicability across diverse usage scenarios.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Audio-language pretraining holds promise for general-purpose audio understanding, yet remains underexplored compared to its vision counterpart. While vision-language models like CLIP serve as widely adopted foundations, existing audio-language models primarily excel at retrieval tasks with limited adoption as general-purpose encoders.\nWe identify three key barriers: limited large-scale audio-text corpora, insufficient caption diversity, and lack of systematic exploration and evaluation.\nTo this end, we introduce CaptionStew, a 10.7M caption dataset aggregating diverse open-source audio-text corpora across multiple domains and captioning styles.\nUsing this resource, we conduct the first comprehensive evaluation comparing contrastive and captioning objectives for audio representation learning across speech, music, and environmental sound tasks.\nOur results demonstrate that audio-language pretraining yields competitive, transferable representations. Through systematic data-scaling experiments, we reveal complementary objective strengths: contrastive learning achieves superior data efficiency at smaller scales, while captioning demonstrates better scalability on language-involved audio understanding tasks. We also find that common supervised initialization practices provide diminishing returns at scale, challenging current approaches.\nThese findings establish audio-language pretraining as a viable pathway toward general-purpose audio representations, guiding future research. To accelerate progress, we release data preparation recipes, training protocols, and pretrained models, paving the way toward universal audio understanding.\n</p>\n\n",
                "matched_terms": [
                    "audiotext",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recognizing the broad adoption and effectiveness of pretrained audio event classifiers in transfer learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alonso-Jim&#233;nez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib3\" title=\"\">2023</a>; Cappellazzo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib11\" title=\"\">2024</a>)</cite>, audio-language modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib31\" title=\"\">2023</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib92\" title=\"\">2023</a>)</cite> and general audio understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib42\" title=\"\">2024</a>; Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib37\" title=\"\">2024</a>; Dinkel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib27\" title=\"\">2025</a>)</cite>, we select our pretrained Zipformer-based audio event classifier (described in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#S4.SS1\" title=\"4.1 Implementation Details &#8227; 4 Experimental Setup &#8227; Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) as the primary baseline.\nIn addition, we compare against representative self-supervised learning (SSL) models, each pretrained under different paradigms and specialized for particular audio domains. BEATs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib17\" title=\"\">2023</a>)</cite> is an audio SSL model trained with an iterative masked acoustic token prediction framework. Wav2vec 2.0&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib5\" title=\"\">2020</a>)</cite> learns speech representation by distinguishing target quantized latent representations from disctrators. MERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.16757v1#bib.bib57\" title=\"\">2024</a>)</cite> is a music SSL model trained with masked acoustic modeling, learning to capture acoustic cues and structural information of music.\nTogether, these baselines provide a broad comparative context for studying audio&#8211;language pretraining toward general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "against",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Competitive Performance Across Domains.</span> Our audio-language representations achieve strong transferability across diverse audio domains.\nCompared to supervised baselines (Zipformer-AEC), our overall best-performing model (Contrastive-init) demonstrate superior performance on speaker identification, music understanding and audio-text retrieval while maintaining competitiveness on audio-event classification.\nAgainst domain-specialized SSL methods (BEATs, wav2vec2, MERT), our approach consistently shows competitive performance.\nThis consistent cross-domain performance validates our hypothesis that diverse caption aggregation enables broadly transferable representations, establishing audio-language pretraining as a viable path toward learning general-purpose audio representation.</p>\n\n",
                "matched_terms": [
                    "audiotext",
                    "against",
                    "model",
                    "contrastiveinit",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited Model and Data Scalability.</span> Our experiments are constrained to 10M audio-text pairs and relatively modest model sizes compared to state-of-the-art vision-language systems that leverage billions of samples and much larger architectures. This scale limitation may not fully reflect the potential of audio-language pretraining, particularly for the captioning objective which our results suggest benefits from larger-scale training. Additionally, we do not explore recent advances in large language model integration or more sophisticated architectural designs that could improve performance. These constraints stem from computational resource limitations and our focus on controlled comparisons rather than pushing absolute performance boundaries. Future work with larger scales may reveal different scaling dynamics and stronger evidence for general-purpose capabilities.</p>\n\n",
                "matched_terms": [
                    "audiotext",
                    "model",
                    "stateoftheart"
                ]
            }
        ]
    }
}