{
    "S2.T1": {
        "source_file": "AISTAT LAB SYSTEM FOR DCASE 2025 TASK6: LANGUAGE-BASED AUDIO RETREIVAL",
        "caption": "Table 1: System ID (SID) for various training configurations",
        "body": "SID\nDistill\nAugmentation\nCluster weight\n\n\n\n\n1\nX\nX\nX\n\n\n2\nO\nX\nX\n\n\n3\nO\nO\nX\n\n\n4\nO\nO\nFinetuned\n\n\n5\nO\nO\nBERTopic",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">SID</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Distill</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Augmentation</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Cluster weight</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">X</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">X</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">X</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">O</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">X</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">X</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">O</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">O</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">X</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">O</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">O</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">Finetuned</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">O</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">O</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">BERTopic</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "system",
            "sid",
            "configurations",
            "cluster",
            "distill",
            "bertopic",
            "training",
            "weight",
            "various",
            "finetuned",
            "augmentation"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This report presents the AISTAT team&#8217;s submission to the language-based audio retrieval task in DCASE 2025 Task 6. Our proposed system employs dual encoder architecture, where audio and text modalities are encoded separately, and their representations are aligned using contrastive learning. Drawing inspiration from methodologies of the previous year&#8217;s challenge, we implemented a distillation approach and leveraged large language models (LLMs) for effective data augmentation techniques, including back-translation and LLM mix. Additionally, we incorporated clustering to introduce an auxiliary classification task for further finetuning. Our best single system achieved a mAP@16 of 46.62, while an ensem-ble of four systems reached a mAP@16 of 48.83 on the Clotho development test split.</span>\n</p>\n\n",
                "matched_terms": [
                    "system",
                    "augmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">DCASE 2025 Task 6 challenge </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> focuses on language-based audio retrieval, a task that requires retrieving audio recordings from a database that best matches a given textual query, and vice versa. This task is critical for applications such as con-tent-based multimedia search, audio annotation, and cross-modal understanding, where aligning audio and text modalities in a shared semantic space is essential. Unlike traditional audio classification or tagging, language-based audio retrieval demands models that capture nuanced semantic relationships between free-form text descriptions and complex audio sig-nals, which may contain overlapping or ambiguous acoustic concepts.\nOur approach builds on DCASE 2024 Task 8 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, adopt-ing a dual-encoder architecture with advanced techniques, such as distillation loss, LLM-based data augmentation, and auxiliary classification. These methods aim to enhance the model&#8217;s generalization, robustness, and ability to capture fine-grained audio-text relationships.\nThe remainder of this paper is organized as follows. Sec-tion 2 describes the proposed system in detail. Section 3 out-lines the datasets, models, and training protocols. Finally, Section 4 presents the experimental results and describes the submitted systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "system",
                    "training",
                    "augmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our system leverages a dual-encoder architecture, where audio and text inputs are processed by separate encoders and aligned in a joint embedding space. We enhance this frame-work with contrastive learning, distillation loss, an auxiliary classification task, and data augmentation, as detailed below. The overall structure is illustrated in Figure 1.</span>\n</p>\n\n",
                "matched_terms": [
                    "system",
                    "augmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose a novel approach to enhance language-based audio retrieval by introducing an auxiliary classification task to further improve the model&#8217;s representation learning. We perform clustering on all captions in the Clotho dataset to lay the foundation for an auxiliary task. We generate embedding for each caption and apply a clustering method similar to BERTopic </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which typically involves dimensionality reduction, such as UMAP </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, followed by density-based clustering, such as HDBSCAN </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, to group captions into semantically similar clusters. Each caption is thus assigned to a specific cluster, representing latent topics or semantic patterns within the captions.</span>\n</p>\n\n",
                "matched_terms": [
                    "bertopic",
                    "cluster"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Audio inputs were preprocessed to align with the pretraining configurations of the respective models. Specifically, EAT and BEATs used a sampling rate of 16 kHz, while PaSST used 32 kHz. In all cases, audio was converted to log-mel spectrograms as the input representation. All models were trained using the AdamW optimizer. Learning rates were adjusted using a cosine warmup scheduler, with specific values detailed in the respective training stages. The training process was divided into three stages. Initial pretraining was conducted on the CLOTHO, WavCaps, and Audi\noCaps datasets to learn general audio-text alignment, while the subsequent finetuning and re-finetuning stages were performed exclusively on the CLOTHO dataset. Each stage is described below.</span>\n</p>\n\n",
                "matched_terms": [
                    "configurations",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Initial pretraining</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> &#8211; We use a mix of Clotho development training split, AudioCaps, and WavCaps datasets. The training spans 20 epochs. No data augmentation is applied in this phase. Due to computational resource constraints, we set batch size to 64 for PaSST, 24 for EAT, and 16 for BEATs. To accommodate these configurations, we adjusted the learning rates using a cosine warmup scheduler across all training processes. For PaSST, the\nlearning rate decreased from 2e-5 to 1e-7, while for EAT and BEATs, it decreased from 1e-5 to 1e-7. These hyperparameter settings were consistently applied in the subsequent finetuning and re-finetuning stages.</span>\n</p>\n\n",
                "matched_terms": [
                    "configurations",
                    "training",
                    "augmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Re-finetuning with cluster-guided classification</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> &#8211; In the refinetuning phase, we enhanced our model through cluster-guided classification. Clustering was conducted using two weight sources: our finetuned model weights and the pre-trained e5-large-v2 weights, sourced from the e5 model family and utilized within the BERTopic framework [5, 18]. The e5-large-v2 model excels in clustering tasks by generating high-quality sentence embeddings that preserve semantic similarity in the embedding space. For each embedding set, we employed the BERTopic framework with\nHDBSCAN to assign pseudo-labels to text samples, reassigning outliers based on topic probabilities estimated by BERTopic. Refinetuning spanned 20 epochs.</span>\n</p>\n\n",
                "matched_terms": [
                    "weight",
                    "bertopic",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluated four systems combining pretraining, distillation, caption augmentation, and cluster supervision. The configuration of these variants is summarized in Table 1.</span>\n</p>\n\n",
                "matched_terms": [
                    "cluster",
                    "augmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table 2 presents the performance of our four systems on the ClothoV2 development test split. The systems, detailed in Table 1, vary in their use of distillation, data augmentation, and clustering, with three audio models. PaSST consistently outperformed EAT and BEATs across all systems, achieving the highest mAP@16. A weighted ensemble of Systems 2&#8211;5 significantly improved performance over individual systems. We employed two ensem\nble strategies. In methods E1 and E2, we first calculated system level ensembles across Systems 2&#8211;5 and then computed weighted sums for each model. Conversely, in methods E3 and E4, we first computed model-level ensembles for each model by combining outputs from Systems 2&#8211;5, then performed a weighted sum across the systems. The weights for all ensembles were determined through grid search to optimize mAP@16 on the validation set. By leveraging the complementary strengths of the systems and models, the ensembles achieved a highest mAP@16 of 48.83.\nFor the final submission, we retrained all systems on the entire development split of the ClothoV2 dataset and submitted the weighted sum of their similarity matrices using the weights from Table 3.</span>\n</p>\n\n",
                "matched_terms": [
                    "system",
                    "augmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper described the AISTAT Lab&#8217;s system for text-grounded audio retrieval system. Inspired by the methodologies of top-performing teams in the previous year, we applied data augmentation techniques leveraging LLMs and incorporated a distillation loss to enhance our model&#8217;s performance. Furthermore, by utilizing clustering, we introduced an auxiliary classification task to the training process, which contributed to additional performance gains. These combined strategies enabled our system to achieve improved results.</span>\n</p>\n\n",
                "matched_terms": [
                    "system",
                    "training",
                    "augmentation"
                ]
            }
        ]
    },
    "S2.T2": {
        "source_file": "AISTAT LAB SYSTEM FOR DCASE 2025 TASK6: LANGUAGE-BASED AUDIO RETREIVAL",
        "caption": "Table 2: Retrieval performance of the models (first section) and the ensembled systems (second section). Note that SID stands for System ID, which is detailed in Table 1.",
        "body": "SID\n\n\n\n\nAudio model\n\n\nMultiple annotation\nSingle annotation\n\n\n\n\nmAP@10\n\n\n\n\nmAP@16\n\n\n\n\nmAP@10\n\n\n\n\nR@1\n\n\n\n\nR@5\n\n\n\n\nR@10\n\n\n\n\n\n\n1\n\n\n\n\nPaSST\n\n\n\n\n39.45\n\n\n\n\n42.08\n\n\n\n\n35.47\n\n\n\n\n23.35\n\n\n\n\n52.5\n\n\n\n\n65.07\n\n\n\n\n\n\nEAT\n\n\n\n\n38.11\n\n\n\n\n40.41\n\n\n\n\n35.13\n\n\n\n\n23.44\n\n\n\n\n51.12\n\n\n\n\n63.87\n\n\n\n\n\n\nBEATs\n\n\n\n\n35.66\n\n\n\n\n38.12\n\n\n\n\n34.15\n\n\n\n\n22.74\n\n\n\n\n49.51\n\n\n\n\n63.75\n\n\n\n\n\n\n2\n\n\n\n\nPaSST\n\n\n\n\n43.75\n\n\n\n\n46.62\n\n\n\n\n39.32\n\n\n\n\n26.81\n\n\n\n\n56.61\n\n\n\n\n70.07\n\n\n\n\n\n\nEAT\n\n\n\n\n42.83\n\n\n\n\n45.35\n\n\n\n\n39.50\n\n\n\n\n26.79\n\n\n\n\n56.40\n\n\n\n\n69.44\n\n\n\n\n\n\nBEATs\n\n\n\n\n41.36\n\n\n\n\n43.89\n\n\n\n\n37.92\n\n\n\n\n25.26\n\n\n\n\n54.81\n\n\n\n\n69.00\n\n\n\n\n\n\n3\n\n\n\n\nPaSST\n\n\n\n\n43.56\n\n\n\n\n46.41\n\n\n\n\n39.92\n\n\n\n\n27.20\n\n\n\n\n57.84\n\n\n\n\n70.74\n\n\n\n\n\n\nEAT\n\n\n\n\n43.37\n\n\n\n\n46.05\n\n\n\n\n40.28\n\n\n\n\n27.52\n\n\n\n\n57.63\n\n\n\n\n71.35\n\n\n\n\n\n\nBEATs\n\n\n\n\n42.09\n\n\n\n\n44.66\n\n\n\n\n38.42\n\n\n\n\n25.51\n\n\n\n\n56.02\n\n\n\n\n69.44\n\n\n\n\n\n\n4\n\n\n\n\nPaSST\n\n\n\n\n43.61\n\n\n\n\n46.39\n\n\n\n\n39.92\n\n\n\n\n27.2\n\n\n\n\n57.21\n\n\n\n\n70.24\n\n\n\n\n\n\nEAT\n\n\n\n\n42.83\n\n\n\n\n45.34\n\n\n\n\n40.02\n\n\n\n\n27.43\n\n\n\n\n56.59\n\n\n\n\n70.62\n\n\n\n\n\n\nBEATs\n\n\n\n\n42.01\n\n\n\n\n44.58\n\n\n\n\n38.61\n\n\n\n\n25.88\n\n\n\n\n55.94\n\n\n\n\n69.46\n\n\n\n\n\n\n5\n\n\n\n\nPaSST\n\n\n\n\n43.79\n\n\n\n\n46.50\n\n\n\n\n39.58\n\n\n\n\n26.66\n\n\n\n\n57.38\n\n\n\n\n70.14\n\n\n\n\n\n\nEAT\n\n\n\n\n42.65\n\n\n\n\n45.34\n\n\n\n\n39.73\n\n\n\n\n26.67\n\n\n\n\n57.28\n\n\n\n\n70.18\n\n\n\n\n\n\nBEATs\n\n\n\n\n41.32\n\n\n\n\n43.88\n\n\n\n\n38.23\n\n\n\n\n25.26\n\n\n\n\n56.06\n\n\n\n\n69.86\n\n\n\n\nEnsemble\n\n\nE1\n\n\n46.07\n\n\n\n\n48.83\n\n\n\n\n41.60\n\n\n\n\n28.33\n\n\n\n\n59.71\n\n\n\n\n72.06\n\n\n\n\nE2\n\n\n46.05\n\n\n\n\n48.78\n\n\n\n\n41.58\n\n\n\n\n28.34\n\n\n\n\n59.87\n\n\n\n\n72.23\n\n\n\n\nE3\n\n\n46.03\n\n\n\n\n48.80\n\n\n\n\n41.70\n\n\n\n\n28.46\n\n\n\n\n59.85\n\n\n\n\n72.38\n\n\n\n\nE4\n\n\n46.04\n\n\n\n\n48.79\n\n\n\n\n41.72\n\n\n\n\n28.38\n\n\n\n\n60.02\n\n\n\n\n72.46",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">SID</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">Audio model</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Multiple annotation</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\"><span class=\"ltx_text\" style=\"font-size:90%;\">Single annotation</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">mAP@10</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">mAP@16</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">mAP@10</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">R@1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">R@5</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">R@10</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">PaSST</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.45</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">42.08</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">35.47</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.35</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">52.5</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">65.07</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">EAT</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">38.11</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">40.41</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">35.13</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.44</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">51.12</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">63.87</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">BEATs</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">35.66</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">38.12</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">34.15</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">22.74</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">49.51</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">63.75</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">PaSST</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">43.75</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">46.62</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.32</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.81</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.61</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.07</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">EAT</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">42.83</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">45.35</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.50</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.79</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.40</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">69.44</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">BEATs</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.36</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">43.89</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">37.92</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.26</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">54.81</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">69.00</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">PaSST</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">43.56</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">46.41</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.92</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.20</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">57.84</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.74</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">EAT</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">43.37</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">46.05</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">40.28</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">27.52</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.63</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">71.35</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">BEATs</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">42.09</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">44.66</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">38.42</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.51</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.02</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">69.44</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">PaSST</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">43.61</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">46.39</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.92</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.2</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.21</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.24</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">EAT</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">42.83</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">45.34</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">40.02</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.43</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.59</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.62</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">BEATs</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">42.01</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">44.58</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">38.61</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.88</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">55.94</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">69.46</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">PaSST</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">43.79</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">46.50</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.58</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.66</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.38</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.14</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">EAT</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">42.65</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">45.34</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.73</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.67</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.28</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.18</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">BEATs</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.32</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">43.88</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">38.23</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.26</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.06</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">69.86</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"8\"><span class=\"ltx_text\" style=\"font-size:90%;\">Ensemble</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">E1</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">46.07</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">48.83</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.60</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.33</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">59.71</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">72.06</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">E2</span></th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">46.05</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">48.78</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.58</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.34</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">59.87</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">72.23</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">E3</span></th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">46.03</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">48.80</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.70</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">28.46</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">59.85</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">72.38</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">E4</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">46.04</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">48.79</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">41.72</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.38</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">60.02</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">72.46</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "detailed",
            "stands",
            "sid",
            "map16",
            "r10",
            "which",
            "annotation",
            "note",
            "audio",
            "system",
            "systems",
            "performance",
            "passt",
            "multiple",
            "eat",
            "single",
            "second",
            "first",
            "ensemble",
            "beats",
            "models",
            "map10",
            "ensembled",
            "retrieval",
            "model"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This report presents the AISTAT team&#8217;s submission to the language-based audio retrieval task in DCASE 2025 Task 6. Our proposed system employs dual encoder architecture, where audio and text modalities are encoded separately, and their representations are aligned using contrastive learning. Drawing inspiration from methodologies of the previous year&#8217;s challenge, we implemented a distillation approach and leveraged large language models (LLMs) for effective data augmentation techniques, including back-translation and LLM mix. Additionally, we incorporated clustering to introduce an auxiliary classification task for further finetuning. Our best single system achieved a mAP@16 of 46.62, while an ensem-ble of four systems reached a mAP@16 of 48.83 on the Clotho development test split.</span>\n</p>\n\n",
                "matched_terms": [
                    "system",
                    "ensemble",
                    "map16",
                    "systems",
                    "models",
                    "single",
                    "retrieval",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">DCASE 2025 Task 6 challenge </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> focuses on language-based audio retrieval, a task that requires retrieving audio recordings from a database that best matches a given textual query, and vice versa. This task is critical for applications such as con-tent-based multimedia search, audio annotation, and cross-modal understanding, where aligning audio and text modalities in a shared semantic space is essential. Unlike traditional audio classification or tagging, language-based audio retrieval demands models that capture nuanced semantic relationships between free-form text descriptions and complex audio sig-nals, which may contain overlapping or ambiguous acoustic concepts.\nOur approach builds on DCASE 2024 Task 8 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, adopt-ing a dual-encoder architecture with advanced techniques, such as distillation loss, LLM-based data augmentation, and auxiliary classification. These methods aim to enhance the model&#8217;s generalization, robustness, and ability to capture fine-grained audio-text relationships.\nThe remainder of this paper is organized as follows. Sec-tion 2 describes the proposed system in detail. Section 3 out-lines the datasets, models, and training protocols. Finally, Section 4 presents the experimental results and describes the submitted systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "system",
                    "systems",
                    "models",
                    "which",
                    "annotation",
                    "retrieval",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our system leverages a dual-encoder architecture, where audio and text inputs are processed by separate encoders and aligned in a joint embedding space. We enhance this frame-work with contrastive learning, distillation loss, an auxiliary classification task, and data augmentation, as detailed below. The overall structure is illustrated in Figure 1.</span>\n</p>\n\n",
                "matched_terms": [
                    "system",
                    "audio",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We employed a contrastive learning framework as the foundational approach to align audio and text representations. Contrastive learning seeks to create a joint embedding space where corresponding audio-text pairs are closely aligned, while non-corresponding pairs are distanced </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This is accomplished by optimizing the InfoNCE loss, which maximizes the cosine similarity of matched audio-text embeddings and minimizes it for unmatched pairs within a batch.\nLet </span>\n  <math alttext=\"\\phi_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#981;</mi>\n        <mi mathsize=\"0.900em\">a</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\phi_{a}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\phi_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#981;</mi>\n        <mi mathsize=\"0.900em\">c</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\phi_{c}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denote the audio and text encoders, respectively, which map audio inputs </span>\n  <math alttext=\"a_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">a</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">a_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and text captions </span>\n  <math alttext=\"c_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">c</mi>\n        <mi mathsize=\"0.900em\">j</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">c_{j}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to their respective embeddings. The similarity between an audio embedding </span>\n  <math alttext=\"\\phi_{a}(a_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#981;</mi>\n          <mi mathsize=\"0.900em\">a</mi>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">a</mi>\n            <mi mathsize=\"0.900em\">i</mi>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\phi_{a}(a_{i})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and a text embedding </span>\n  <math alttext=\"\\phi_{c}(c_{j})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#981;</mi>\n          <mi mathsize=\"0.900em\">c</mi>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">c</mi>\n            <mi mathsize=\"0.900em\">j</mi>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\phi_{c}(c_{j})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is defined as the normalized cosine similarity:</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address the binary correspondence assumption in audio retrieval datasets like ClothoV2, where captions may describe multiple recordings due to overlapping acoustic concepts or limited diversity, we adopted a distillation loss approach from the top-ranked DCASE 2024 Task 8 system </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This method uses soft correspondence probabilities from an ensemble of pretrained models to capture nuanced audio-text relationships, improving generalization.\nFormally, we first compute the similarity between audio embedding and text embedding as defined in Section 2.1. An ensemble of </span>\n  <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">M</mi>\n      <annotation encoding=\"application/x-tex\">M</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> pretrained models generates soft correspondence probabilities by averaging their similarity scores:</span>\n</p>\n\n",
                "matched_terms": [
                    "system",
                    "ensemble",
                    "multiple",
                    "models",
                    "retrieval",
                    "audio",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose a novel approach to enhance language-based audio retrieval by introducing an auxiliary classification task to further improve the model&#8217;s representation learning. We perform clustering on all captions in the Clotho dataset to lay the foundation for an auxiliary task. We generate embedding for each caption and apply a clustering method similar to BERTopic </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which typically involves dimensionality reduction, such as UMAP </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, followed by density-based clustering, such as HDBSCAN </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, to group captions into semantically similar clusters. Each caption is thus assigned to a specific cluster, representing latent topics or semantic patterns within the captions.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "retrieval",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To leverage the clustering results, we extend the model architecture by adding classification heads to both the text and audio encoders. The classification head for the text encoder is designed to predict the cluster label of the input caption, while the audio encoder&#8217;s classification head predicts the cluster label of the corresponding caption. Specifically, the output of each encoder is processed through two sequential linear layers with a ReLU activation function between them, projecting the output to a vector with dimensions equal to the number of clusters. The intermediate linear layer has a dimension three times that of the input to enhance representation capacity. This setup encourages the audio encoder to learn representations that are aligned with the semantic clusters of the captions, thereby enhancing the fine-grained alignment between audio and text.\nThe total loss combines the supervised contrastive loss </span>\n  <math alttext=\"L_{sup}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">L</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">s</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">u</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">p</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">L_{sup}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from Section 2.1, the distillation loss </span>\n  <math alttext=\"L_{dist}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">L</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">d</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">i</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">s</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">t</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">L_{dist}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from Section 2.2, and the classification losses for the audio and text encoders, denoted </span>\n  <math alttext=\"L_{cls}^{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m3\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">L</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">c</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">l</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">s</mi>\n        </mrow>\n        <mi mathsize=\"0.900em\">a</mi>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">L_{cls}^{a}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"L_{cls}^{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m4\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">L</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">c</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">l</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">s</mi>\n        </mrow>\n        <mi mathsize=\"0.900em\">c</mi>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">L_{cls}^{c}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, respectively:</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To enhance the diversity of captions for our text-grounded audio retrieval, we employed caption augmentation leveraging the capabilities of a large language model (LLM), specifically GPT-4o </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. One of the key techniques utilized was </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">back-translation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This method involves translating the original English captions into a randomly selected language and then translating them back into English. By doing so, back-translation generates captions that retain the same semantic meaning as the originals but feature varied linguistic expressions.\nIn addition to back-translation, we implemented another augmentation technique called </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LLM mix</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to further enrich our dataset. For this method, we randomly selected two audio-text pairs and combined their audio signals to create a new mixed audio sample. To generate a corresponding caption for this mixed audio, we utilized GPT-4o to intelligently merge the captions of the original audio-text pairs. With LLM mix, we created 50,000 new audio-caption pairs, adding substantial variety to our dataset</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AudioCaps</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> consists of 51,308 audio recordings sourced from AudioSet, each 10 seconds long and paired with a single human-generated caption. The captions have an average length of 9.8 words. For our experiments, we combined the training, validation, and test splits of AudioCaps into a single dataset, which was used for pretraining the model.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "single",
                    "model",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">The Patchout faSt Spectrogram Transformer (PaSST)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> leverages pre-trained parameters from a vision transformer and fine-tunes them on the AudioSet dataset for general-purpose audio tagging. By dropping patches from the input sequence, PaSST achieves a low computational and memory footprint. In our experiments, we used a PaSST version without patch overlap, applying structured patchout of 2 and 15 over the frequency and time dimensions, respectively.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "passt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">The Efficient Audio Transformer (EAT)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is an audio self-supervised learning (SSL) model focused on efficient representation learning from unlabeled audio data. It employs a novel Utterance-Frame Objective (UFO) that combines global utterance-level and local frame-level learning to improve audio understanding. We initialized the models with publicly available pretrained weights, namely EAT-base_epoch30_pt.</span>\n</p>\n\n",
                "matched_terms": [
                    "eat",
                    "audio",
                    "model",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Bidirectional Encoder representation from Audio Transformers (BEATs)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a self-supervised learning framework designed for pre-training comprehensive audio representations. It integrates an acoustic tokenizer with an audio SSL model, optimized iteratively to generate discrete labels rich in audio semantics. We also initialized BEATs with publicly available pretrained weights, namely BEATs_iter3_plus_AS2M.</span>\n</p>\n\n",
                "matched_terms": [
                    "beats",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">RoBERTa</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a BERT-based language model developed by Facebook AI that improves upon the original BERT pre-training methodology. By removing the Next Sentence Prediction (NSP) objective, extending training duration, increasing batch size, and leveraging a larger and more diverse corpus, RoBERTa achieves stronger performance in sentence-level representation learning. In our experiments, we used RoBERTa-large as a sentence embedding extractor, utilizing its pretrained parameters to capture rich semantic information from textual inputs.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Audio inputs were preprocessed to align with the pretraining configurations of the respective models. Specifically, EAT and BEATs used a sampling rate of 16 kHz, while PaSST used 32 kHz. In all cases, audio was converted to log-mel spectrograms as the input representation. All models were trained using the AdamW optimizer. Learning rates were adjusted using a cosine warmup scheduler, with specific values detailed in the respective training stages. The training process was divided into three stages. Initial pretraining was conducted on the CLOTHO, WavCaps, and Audi\noCaps datasets to learn general audio-text alignment, while the subsequent finetuning and re-finetuning stages were performed exclusively on the CLOTHO dataset. Each stage is described below.</span>\n</p>\n\n",
                "matched_terms": [
                    "detailed",
                    "passt",
                    "beats",
                    "models",
                    "eat",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Initial pretraining</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> &#8211; We use a mix of Clotho development training split, AudioCaps, and WavCaps datasets. The training spans 20 epochs. No data augmentation is applied in this phase. Due to computational resource constraints, we set batch size to 64 for PaSST, 24 for EAT, and 16 for BEATs. To accommodate these configurations, we adjusted the learning rates using a cosine warmup scheduler across all training processes. For PaSST, the\nlearning rate decreased from 2e-5 to 1e-7, while for EAT and BEATs, it decreased from 1e-5 to 1e-7. These hyperparameter settings were consistently applied in the subsequent finetuning and re-finetuning stages.</span>\n</p>\n\n",
                "matched_terms": [
                    "eat",
                    "passt",
                    "beats"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Finetuning</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> &#8211; In the finetuning phase, models were further trained for 20 epochs using ensemble soft labels. Soft labels were calcu\nlated as the average of the similarity matrices obtained from three audio models, as described in Equation 5, where M equals 3.\nThese soft labels served as targets for a distillation loss, guiding the model toward a consensus representation. To enhance robust\nness, we trained models with and without data augmentation. For the augmented models, we applied data augmentation techniques,\nincluding back-translation and LLM-based caption mixing, as described in section 2.4. Random deletion and synonym replace\nment were also applied to a single word in captions with an 80% probability, further increasing caption diversity.</span>\n</p>\n\n",
                "matched_terms": [
                    "ensemble",
                    "models",
                    "single",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table 2 presents the performance of our four systems on the ClothoV2 development test split. The systems, detailed in Table 1, vary in their use of distillation, data augmentation, and clustering, with three audio models. PaSST consistently outperformed EAT and BEATs across all systems, achieving the highest mAP@16. A weighted ensemble of Systems 2&#8211;5 significantly improved performance over individual systems. We employed two ensem\nble strategies. In methods E1 and E2, we first calculated system level ensembles across Systems 2&#8211;5 and then computed weighted sums for each model. Conversely, in methods E3 and E4, we first computed model-level ensembles for each model by combining outputs from Systems 2&#8211;5, then performed a weighted sum across the systems. The weights for all ensembles were determined through grid search to optimize mAP@16 on the validation set. By leveraging the complementary strengths of the systems and models, the ensembles achieved a highest mAP@16 of 48.83.\nFor the final submission, we retrained all systems on the entire development split of the ClothoV2 dataset and submitted the weighted sum of their similarity matrices using the weights from Table 3.</span>\n</p>\n\n",
                "matched_terms": [
                    "system",
                    "ensemble",
                    "detailed",
                    "map16",
                    "systems",
                    "performance",
                    "passt",
                    "beats",
                    "models",
                    "eat",
                    "audio",
                    "model",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper described the AISTAT Lab&#8217;s system for text-grounded audio retrieval system. Inspired by the methodologies of top-performing teams in the previous year, we applied data augmentation techniques leveraging LLMs and incorporated a distillation loss to enhance our model&#8217;s performance. Furthermore, by utilizing clustering, we introduced an auxiliary classification task to the training process, which contributed to additional performance gains. These combined strategies enabled our system to achieve improved results.</span>\n</p>\n\n",
                "matched_terms": [
                    "system",
                    "performance",
                    "which",
                    "retrieval",
                    "audio"
                ]
            }
        ]
    },
    "S3.T3": {
        "source_file": "AISTAT LAB SYSTEM FOR DCASE 2025 TASK6: LANGUAGE-BASED AUDIO RETREIVAL",
        "caption": "Table 3: Combination coefficients for four submitted system",
        "body": "SID\n2\n3\n4\n5\n\n\nModel\nPaSST\nEAT\nBEATs\nPaSST\nEAT\nBEATs\nPaSST\nEAT\nBEATs\nPaSST\nEAT\nBEATs\n\n\n\n\nE1\n0.2275\n0.07\n0.06\n0\n0.12\n0.045\n0.325\n0\n0.045\n0.0975\n0.01\n0\n\n\nE2\n0.2275\n0.0875\n0.04\n0\n0.15\n0.03\n0.325\n0\n0.03\n0.0975\n0.0125\n0\n\n\nE3\n0.225\n0.175\n0.1\n0.03\n0.01\n0.01\n0.195\n0.045\n0.06\n0.09\n0.03\n0.03\n\n\nE4\n0.18\n0.14\n0.08\n0.09\n0.03\n0.03\n0.13\n0.03\n0.04\n0.15\n0.05\n0.05",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">SID</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">PaSST</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">EAT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">BEATs</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">PaSST</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">EAT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">BEATs</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">PaSST</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">EAT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">BEATs</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">PaSST</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">EAT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">BEATs</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">E1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.2275</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.06</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.045</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.325</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.045</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0975</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">E2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.2275</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0875</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.04</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.15</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.03</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.325</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.03</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0975</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0125</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">E3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.225</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.175</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.03</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.195</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.045</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.06</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.09</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.03</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.03</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">E4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.14</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.09</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.13</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.05</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.05</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "combination",
            "system",
            "sid",
            "submitted",
            "passt",
            "beats",
            "four",
            "eat",
            "model",
            "coefficients"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This report presents the AISTAT team&#8217;s submission to the language-based audio retrieval task in DCASE 2025 Task 6. Our proposed system employs dual encoder architecture, where audio and text modalities are encoded separately, and their representations are aligned using contrastive learning. Drawing inspiration from methodologies of the previous year&#8217;s challenge, we implemented a distillation approach and leveraged large language models (LLMs) for effective data augmentation techniques, including back-translation and LLM mix. Additionally, we incorporated clustering to introduce an auxiliary classification task for further finetuning. Our best single system achieved a mAP@16 of 46.62, while an ensem-ble of four systems reached a mAP@16 of 48.83 on the Clotho development test split.</span>\n</p>\n\n",
                "matched_terms": [
                    "system",
                    "four"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">DCASE 2025 Task 6 challenge </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> focuses on language-based audio retrieval, a task that requires retrieving audio recordings from a database that best matches a given textual query, and vice versa. This task is critical for applications such as con-tent-based multimedia search, audio annotation, and cross-modal understanding, where aligning audio and text modalities in a shared semantic space is essential. Unlike traditional audio classification or tagging, language-based audio retrieval demands models that capture nuanced semantic relationships between free-form text descriptions and complex audio sig-nals, which may contain overlapping or ambiguous acoustic concepts.\nOur approach builds on DCASE 2024 Task 8 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, adopt-ing a dual-encoder architecture with advanced techniques, such as distillation loss, LLM-based data augmentation, and auxiliary classification. These methods aim to enhance the model&#8217;s generalization, robustness, and ability to capture fine-grained audio-text relationships.\nThe remainder of this paper is organized as follows. Sec-tion 2 describes the proposed system in detail. Section 3 out-lines the datasets, models, and training protocols. Finally, Section 4 presents the experimental results and describes the submitted systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "system",
                    "submitted"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">The Efficient Audio Transformer (EAT)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is an audio self-supervised learning (SSL) model focused on efficient representation learning from unlabeled audio data. It employs a novel Utterance-Frame Objective (UFO) that combines global utterance-level and local frame-level learning to improve audio understanding. We initialized the models with publicly available pretrained weights, namely EAT-base_epoch30_pt.</span>\n</p>\n\n",
                "matched_terms": [
                    "eat",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Bidirectional Encoder representation from Audio Transformers (BEATs)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16649v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a self-supervised learning framework designed for pre-training comprehensive audio representations. It integrates an acoustic tokenizer with an audio SSL model, optimized iteratively to generate discrete labels rich in audio semantics. We also initialized BEATs with publicly available pretrained weights, namely BEATs_iter3_plus_AS2M.</span>\n</p>\n\n",
                "matched_terms": [
                    "beats",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Audio inputs were preprocessed to align with the pretraining configurations of the respective models. Specifically, EAT and BEATs used a sampling rate of 16 kHz, while PaSST used 32 kHz. In all cases, audio was converted to log-mel spectrograms as the input representation. All models were trained using the AdamW optimizer. Learning rates were adjusted using a cosine warmup scheduler, with specific values detailed in the respective training stages. The training process was divided into three stages. Initial pretraining was conducted on the CLOTHO, WavCaps, and Audi\noCaps datasets to learn general audio-text alignment, while the subsequent finetuning and re-finetuning stages were performed exclusively on the CLOTHO dataset. Each stage is described below.</span>\n</p>\n\n",
                "matched_terms": [
                    "beats",
                    "eat",
                    "passt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Initial pretraining</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> &#8211; We use a mix of Clotho development training split, AudioCaps, and WavCaps datasets. The training spans 20 epochs. No data augmentation is applied in this phase. Due to computational resource constraints, we set batch size to 64 for PaSST, 24 for EAT, and 16 for BEATs. To accommodate these configurations, we adjusted the learning rates using a cosine warmup scheduler across all training processes. For PaSST, the\nlearning rate decreased from 2e-5 to 1e-7, while for EAT and BEATs, it decreased from 1e-5 to 1e-7. These hyperparameter settings were consistently applied in the subsequent finetuning and re-finetuning stages.</span>\n</p>\n\n",
                "matched_terms": [
                    "beats",
                    "eat",
                    "passt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table 2 presents the performance of our four systems on the ClothoV2 development test split. The systems, detailed in Table 1, vary in their use of distillation, data augmentation, and clustering, with three audio models. PaSST consistently outperformed EAT and BEATs across all systems, achieving the highest mAP@16. A weighted ensemble of Systems 2&#8211;5 significantly improved performance over individual systems. We employed two ensem\nble strategies. In methods E1 and E2, we first calculated system level ensembles across Systems 2&#8211;5 and then computed weighted sums for each model. Conversely, in methods E3 and E4, we first computed model-level ensembles for each model by combining outputs from Systems 2&#8211;5, then performed a weighted sum across the systems. The weights for all ensembles were determined through grid search to optimize mAP@16 on the validation set. By leveraging the complementary strengths of the systems and models, the ensembles achieved a highest mAP@16 of 48.83.\nFor the final submission, we retrained all systems on the entire development split of the ClothoV2 dataset and submitted the weighted sum of their similarity matrices using the weights from Table 3.</span>\n</p>\n\n",
                "matched_terms": [
                    "system",
                    "submitted",
                    "passt",
                    "beats",
                    "four",
                    "eat",
                    "model"
                ]
            }
        ]
    }
}