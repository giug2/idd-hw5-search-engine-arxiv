{
    "S5.T1": {
        "caption": "Table 1: Evaluation results: MOS, MCD, and UAA",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">MOS(&#8593;)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SMOS(&#8593;)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">MCD(&#8595;)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">UAA(&#8593;)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">FS2 + GST</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"3.44\\pm 0.11\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m1\" intent=\":literal\"><semantics><mrow><mn>3.44</mn><mo>&#177;</mo><mn>0.11</mn></mrow><annotation encoding=\"application/x-tex\">3.44\\pm 0.11</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"3.12\\pm 0.07\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m2\" intent=\":literal\"><semantics><mrow><mn>3.12</mn><mo>&#177;</mo><mn>0.07</mn></mrow><annotation encoding=\"application/x-tex\">3.12\\pm 0.07</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"8.75\\pm 0.22\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m3\" intent=\":literal\"><semantics><mrow><mn>8.75</mn><mo>&#177;</mo><mn>0.22</mn></mrow><annotation encoding=\"application/x-tex\">8.75\\pm 0.22</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">74.22%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">StyleSpeech</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"2.95\\pm 0.11\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m4\" intent=\":literal\"><semantics><mrow><mn>2.95</mn><mo>&#177;</mo><mn>0.11</mn></mrow><annotation encoding=\"application/x-tex\">2.95\\pm 0.11</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"3.26\\pm 0.06\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m5\" intent=\":literal\"><semantics><mrow><mn>3.26</mn><mo>&#177;</mo><mn>0.06</mn></mrow><annotation encoding=\"application/x-tex\">3.26\\pm 0.06</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"9.09\\pm 0.23\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m6\" intent=\":literal\"><semantics><mrow><mn>9.09</mn><mo>&#177;</mo><mn>0.23</mn></mrow><annotation encoding=\"application/x-tex\">9.09\\pm 0.23</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">82.22%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">FS2 + MIST</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"3.62\\pm 0.10\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m7\" intent=\":literal\"><semantics><mrow><mn>3.62</mn><mo>&#177;</mo><mn>0.10</mn></mrow><annotation encoding=\"application/x-tex\">3.62\\pm 0.10</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"2.89\\pm 0.08\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m8\" intent=\":literal\"><semantics><mrow><mn>2.89</mn><mo>&#177;</mo><mn>0.08</mn></mrow><annotation encoding=\"application/x-tex\">2.89\\pm 0.08</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"8.65\\pm 0.22\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m9\" intent=\":literal\"><semantics><mrow><mn>8.65</mn><mo>&#177;</mo><mn>0.22</mn></mrow><annotation encoding=\"application/x-tex\">8.65\\pm 0.22</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">76.17%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">DC Comix TTS</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"3.59\\pm 0.11\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m10\" intent=\":literal\"><semantics><mrow><mn>3.59</mn><mo>&#177;</mo><mn>0.11</mn></mrow><annotation encoding=\"application/x-tex\">3.59\\pm 0.11</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"2.97\\pm 0.08\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m11\" intent=\":literal\"><semantics><mrow><mn>2.97</mn><mo>&#177;</mo><mn>0.08</mn></mrow><annotation encoding=\"application/x-tex\">2.97\\pm 0.08</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"9.01\\pm 0.24\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m12\" intent=\":literal\"><semantics><mrow><mn>9.01</mn><mo>&#177;</mo><mn>0.24</mn></mrow><annotation encoding=\"application/x-tex\">9.01\\pm 0.24</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">58.79 %</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Proposed</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><math alttext=\"\\mathbf{3.63\\pm 0.10}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m13\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">3.63</mn><mo>&#177;</mo><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.10</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{3.63\\pm 0.10}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><math alttext=\"\\mathbf{3.41\\pm 0.06}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m14\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">3.41</mn><mo>&#177;</mo><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.06</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{3.41\\pm 0.06}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><math alttext=\"\\mathbf{8.23\\pm 0.22}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m15\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">8.23</mn><mo>&#177;</mo><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.22</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{8.23\\pm 0.22}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><math alttext=\"\\mathbf{82.42\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m16\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">82.42</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{82.42\\%}</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "fs2",
            "comix",
            "mcd",
            "312±007312pm",
            "326±006326pm",
            "evaluation",
            "865±022865pm",
            "mos",
            "289±008289pm",
            "tts",
            "8242mathbf8242",
            "341±006mathbf341pm",
            "875±022875pm",
            "mcd↓",
            "gst",
            "295±011295pm",
            "823±022mathbf823pm",
            "mist",
            "901±024901pm",
            "344±011344pm",
            "359±011359pm",
            "results",
            "909±023909pm",
            "stylespeech",
            "363±010mathbf363pm",
            "proposed",
            "uaa",
            "smos↑",
            "model",
            "297±008297pm",
            "362±010362pm",
            "uaa↑",
            "mos↑"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Across all evaluations we prevented &#8220;content leakage&#8221;&#8212;the artificial boost that arose when the reference speech shared lexical content with the synthesis target.\nSpecifically, each reference mel-spectrogram was drawn (with a fixed random seed) from an utterance that matched the speaker and emotion of the target but <em class=\"ltx_emph ltx_font_italic\">differed in text</em>.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#S5.T1\" title=\"Table 1 &#8227; 5.4 Evaluation Metrics &#8227; 5 Experiments &#8227; Emotional Text-To-Speech Based on Mutual-Information-Guided Emotion-Timbre Disentanglement\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> <span class=\"ltx_text ltx_font_italic\">reported</span> MOS, MCD, and UAA obtained with two pre-trained recognizers.\nOur model attained naturalness that is statistically on par with the best baseline, while delivering markedly superior style consistency, underscoring its effectiveness at reproducing the intended style without sacrificing perceptual quality.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Current emotional Text-To-Speech (TTS) and style transfer methods rely on reference encoders to control global style or emotion vectors, but do not capture nuanced acoustic details of the reference speech. To this end, we propose a novel emotional TTS method that enables fine-grained phoneme-level emotion embedding prediction while disentangling intrinsic attributes of the reference speech. The proposed method employs a style disentanglement method to guide two feature extractors, reducing mutual information between timbre and emotion features, and effectively separating distinct style components from the reference speech. Experimental results demonstrate that our method outperforms baseline TTS systems in generating natural and emotionally rich speech. This work highlights the potential of disentangled and fine-grained representations in advancing the quality and flexibility of emotional TTS systems.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The synthesized audio samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://baleyang.github.io/emotion-timbre-disentangled-tts/\" title=\"\">https://baleyang.github.io/emotion-timbre-disentangled-tts/</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "proposed",
                    "tts",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Deep learning has significantly advanced Text-To-Speech (TTS) technology, surpassing early statistical models that struggled with naturalness and expressiveness. The introduction of deep neural networks (DNNs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx1\" title=\"\">1</a>]</cite> as an acoustic model improved speech fidelity and intelligibility by capturing complex relationships between input text and output speech in TTS. Autoregressive generative models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx4\" title=\"\">4</a>]</cite> enabled end-to-end TTS, which represents the mapping from input text to output speech by stacked DNN modules and enhances synthesis quality and robustness toward prosody drift and mis-alignment in long-form synthesis. More recently, non-autoregressive models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx7\" title=\"\">7</a>]</cite> have attracted increasing attention. In contrast to autoregressive models, which generate speech frames sequentially, non-autoregressive models predict all output frames in parallel. This parallelization significantly speeds up inference, greatly enhances the efficiency of TTS.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite significant advances in naturalness and diversity of synthetic speech by TTS, achieving precise and expressive emotional TTS remains a challenging task. Emotional TTS, particularly in zero-shot settings, aims to generate speech that matches the timbre, emotion, and prosody of a few seconds of reference speech. Traditional methods typically encode the reference speech into a global style vector&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx10\" title=\"\">10</a>]</cite>, which is then fused with the output of a phoneme encoder. Although these approaches effectively capture the overall style, they often struggle to model the phoneme-level variation in emotion and prosody. Moreover, compressing the reference speech into a single global embedding risks losing crucial prosodic details, limiting the expressiveness and control over the synthesized speech.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these limitations, we propose a novel emotional TTS method that (i) predicts fine-grained, phoneme-level emotion embeddings, and (ii) disentangles those embeddings from global timbre information through mutual-information minimization. Central to our method is a dedicated Style Encoder, which comprises two parallel extractors: a global Timbre Extractor, and a phoneme-aware Emotion Extractor that aligns reference acoustics with target phonemes to produce an emotion embedding sequence. An unsupervised Mutual Information Neural Estimation (MINE)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx11\" title=\"\">11</a>]</cite> explicitly pushes the two representations apart, ensuring that the timbre embedding remains speaker-specific information, while the emotion embeddings capture only prosodic nuance, allowing the model to synthesize speech that is simultaneously timbre-consistent and emotionally expressive.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments demonstrate that our method outperforms strong baselines, including Global Style Token(GST)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx8\" title=\"\">8</a>]</cite>, StyleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx9\" title=\"\">9</a>]</cite>, MIST&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx12\" title=\"\">12</a>]</cite>, and DC Comix TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx13\" title=\"\">13</a>]</cite>, on both subjective and objective metrics. t-SNE visualizations further reveal well-separated emotion clusters, confirming effective disentanglement. Our results highlight the value of combining phoneme-level emotion modeling with principled feature disentanglement for expressive, high-fidelity emotional TTS.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "comix",
                    "mist",
                    "results",
                    "stylespeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx8\" title=\"\">8</a>]</cite> first proposed GST&#8212;a bank of learnable tokens attended by a reference encoder&#8212;to condense an utterance into a single &#8220;style vector.&#8221; Subsequent works refined this idea: StyleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx9\" title=\"\">9</a>]</cite> injects the vector into every encoder/decoder block via Style&#8209;Adaptive LayerNorm (SALN); Tacotron&#8209;GST&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx10\" title=\"\">10</a>]</cite> piles GSTs hierarchically to capture speaking styles spanning words to paragraphs. Although these systems improve expressiveness, their utterance&#8209;level embeddings cannot localize phoneme&#8209;wise variations and therefore provide only coarse control over emotion and rhythm.</p>\n\n",
                "matched_terms": [
                    "stylespeech",
                    "proposed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Discrete token representations have recently become a focal point in speech modeling research. Neural audio codecs based on vector&#8209;quantized variational autoencoders&#8212;most notably SoundStream&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx14\" title=\"\">14</a>]</cite> and EnCodec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx15\" title=\"\">15</a>]</cite>&#8212;transform continuous waveforms into sequences of codebook indices, yielding a compact &#8220;speech language&#8221; that lightweight sequence&#8209;to&#8209;sequence decoders can handle efficiently. Building on this foundation, several studies have investigated how such tokens can capture prosody for zero&#8209;shot TTS. A representative example is DC Comix TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx13\" title=\"\">13</a>]</cite>, which tokenizes a reference signal with an EnCodec&#8209;style front end and conditions its decoder on a style embedding derived from the resulting code sequence, achieving high&#8209;fidelity speech from unseen speakers. However, fixed&#8209;rate quantization still blurs micro&#8209;prosodic cues&#8212;such as subtle emotional nuances and pitch inflections&#8212;and the reliance on a global utterance&#8209;level embedding limits phoneme&#8209;level expressive control.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "comix"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the limitations of global style embeddings and leverage MINE for effective speech feature disentanglement, we propose a novel model architecture for emotional TTS that integrates MINE into the style encoding process. Our model introduces two key innovations:</p>\n\n",
                "matched_terms": [
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1. Phoneme-level emotion embedding prediction</span>: The model predicts emotional information at the phoneme level while treating timbre as a global feature, enabling TTS that closely matches the style of the reference speech.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overall architecture of the proposed model is illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Emotional Text-To-Speech Based on Mutual-Information-Guided Emotion-Timbre Disentanglement\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. It is based on FastSpeech 2(FS2)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx7\" title=\"\">7</a>]</cite>, consisting of Encoder-Decoder networks with Variance Adaptor, which following the original FS2 method. To enable emotional TTS, we introduce a Style Encoder after the Phoneme Encoder to predict style-specific representations.</p>\n\n",
                "matched_terms": [
                    "model",
                    "proposed",
                    "fs2",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the second stage, we incorporate the Style Encoder into the model and initialize all encoder parameters using the pre-trained weights from stage one (and freeze these parameters). To ensure the Timbre Extractor and the Emotion Extractor focus on distinct aspects of the reference mel-spectrogram, we adopt a mutual information minimization scheme based on MINE. Specifically, we alternate between updating the TTS model and the MI estimator to encourage the Timbre Extractor and the Emotion Extractor to capture non-overlapping information. We augment our loss with additional pitch and energy terms, as well as classification losses for speaker and emotion. The second-stage objective is defined as:</p>\n\n",
                "matched_terms": [
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathcal{L}_{\\mathrm{pitch}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>pitch</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\mathrm{pitch}}</annotation></semantics></math> and <math alttext=\"\\mathcal{L}_{\\mathrm{energy}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>energy</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\mathrm{energy}}</annotation></semantics></math> measure prediction errors for pitch and energy, respectively, and <math alttext=\"\\mathcal{L}_{\\mathrm{emotion}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>emotion</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\mathrm{emotion}}</annotation></semantics></math> and <math alttext=\"\\mathcal{L}_{\\mathrm{speaker}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>speaker</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\mathrm{speaker}}</annotation></semantics></math> are cross-entropy classification losses for emotion and speaker identification, respectively. The term <math alttext=\"\\hat{\\mathcal{I}}_{T_{\\theta}}(\\mathbf{F}_{\\mathrm{timbre}},\\mathbf{F}_{\\mathrm{emotion\\_global}})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mo>^</mo></mover><msub><mi>T</mi><mi>&#952;</mi></msub></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119813;</mi><mi>timbre</mi></msub><mo>,</mo><msub><mi>&#119813;</mi><mrow><mi>emotion</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>global</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{\\mathcal{I}}_{T_{\\theta}}(\\mathbf{F}_{\\mathrm{timbre}},\\mathbf{F}_{\\mathrm{emotion\\_global}})</annotation></semantics></math> is the estimated mutual information, and <math alttext=\"-\\hat{\\mathcal{I}}_{T_{\\theta}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m6\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><msub><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mo>^</mo></mover><msub><mi>T</mi><mi>&#952;</mi></msub></msub></mrow><annotation encoding=\"application/x-tex\">-\\hat{\\mathcal{I}}_{T_{\\theta}}</annotation></semantics></math> is optimized in the MI estimator to capture any correlation between timbre and emotion features. We empirically set <math alttext=\"\\lambda_{1}=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mn>1</mn></msub><mo>=</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{1}=1.0</annotation></semantics></math>, <math alttext=\"\\lambda_{2}=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m8\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mn>2</mn></msub><mo>=</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{2}=1.0</annotation></semantics></math>, <math alttext=\"\\lambda_{3}=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m9\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mn>3</mn></msub><mo>=</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{3}=1.0</annotation></semantics></math>, <math alttext=\"\\lambda_{4}=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m10\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mn>4</mn></msub><mo>=</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{4}=1.0</annotation></semantics></math>, <math alttext=\"\\lambda_{5}=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m11\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mn>5</mn></msub><mo>=</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{5}=1.0</annotation></semantics></math>, <math alttext=\"\\lambda_{6}=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m12\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mn>6</mn></msub><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{6}=0.1</annotation></semantics></math>, throughout our experiments. By alternating gradient updates between the TTS model and the MI estimator, we encourage the Timbre Extractor and Emotion Extractor to learn disentangled representations.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">FS2 + GST.</span> An expressive TTS model that integrates GSTs&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx8\" title=\"\">8</a>]</cite> into the FS2 method to capture diverse speaking styles.</p>\n\n",
                "matched_terms": [
                    "model",
                    "fs2",
                    "tts",
                    "gst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">StyleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx9\" title=\"\">9</a>]</cite>.</span> An expressive TTS model employing a reference encoder to produce a style embedding, which in turn modulates the output of SALN layers via gain and bias parameters.</p>\n\n",
                "matched_terms": [
                    "stylespeech",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">FS2 + MIST&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx12\" title=\"\">12</a>]</cite>.</span> A FS2-based model introducing MIST, which reduces the mutual information between the phoneme encoder and the style embedding. This encourages disentanglement of style and content for improved expressive synthesis.</p>\n\n",
                "matched_terms": [
                    "mist",
                    "model",
                    "fs2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DC Comix TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx13\" title=\"\">13</a>]</cite>.</span> A variant that replaces GST with a reference encoder based on discrete code.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "comix",
                    "gst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We systematically compared our proposed method with baseline methods, each built on the same FS2 architecture. For a fair comparison, all models followed the same DNN architectures for the encoder, decoder, and variance adaptors. We used the Adam optimizer with <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m1\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> = (0.9, 0.98), <math alttext=\"\\epsilon=1\\times 10^{-8}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#1013;</mi><mo>=</mo><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>8</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\epsilon=1\\times 10^{-8}</annotation></semantics></math>, and the learning rate <math alttext=\"\\ell_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m3\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\ell_{t}</annotation></semantics></math> follows&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx1\" title=\"\">1</a>]</cite>. The mini-batches contained 64 samples, and all models were trained for 60 k optimization steps. For vocoder, We initially employ the official <span class=\"ltx_text ltx_font_typewriter\">UNIVERSAL_V1<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">2</span></span><a class=\"ltx_ref ltx_url\" href=\"https://github.com/jik876/hifi-gan\" title=\"\">https://github.com/jik876/hifi-gan</a></span></span></span></span> version of the pre-trained HiFi-GAN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx24\" title=\"\">24</a>]</cite>, and subsequently perform fine-tuning on the ESD training set to better adapt the vocoder to our data. This adapted vocoder was then used to synthesize the final speech waveforms from the generated mel-spectrograms.</p>\n\n",
                "matched_terms": [
                    "fs2",
                    "proposed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Subjective metrics.</span> Naturalness was evaluated using the classical Mean Opinion Score (MOS), where ten judges rated each utterance on a 1-to-5 scale (1 = bad, 2 = poor, 3 = fair, 4 = good, 5 = excellent). To assess emotional or style similarity between reference and synthesized speech, we adopted the Similarity MOS (SMOS), in which the judges scored each utterance pair on a 4-point scale: 1 = Very dissimilar, 2 = Dissimilar, 3 = Similar, 4 = Very similar. All MOS and SMOS results were reported together with their 95% confidence intervals (CI).</p>\n\n",
                "matched_terms": [
                    "mos",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Objective metrics.</span> Spectral fidelity was measured by mel-cepstral distortion (MCD), where synthetic and reference mel-spectrograms were first aligned via dynamic time warping (DTW). Expressive adequacy was gauged by an emotion-recognition task: we fine-tuned <span class=\"ltx_text ltx_font_typewriter\">openai/whisper-large-v2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">3</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/openai/whisper-large-v2\" title=\"\">https://huggingface.co/openai/whisper-large-v2</a></span></span></span></span> on the ESD training split and report the resulting unweighted average accuracy (UAA) on generated speech.</p>\n\n",
                "matched_terms": [
                    "mcd",
                    "uaa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness of individual components in our proposed model, we conducted ablation experiments, and the results were presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#S5.T2\" title=\"Table 2 &#8227; 5.6 Ablation Study &#8227; 5 Experiments &#8227; Emotional Text-To-Speech Based on Mutual-Information-Guided Emotion-Timbre Disentanglement\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. In this table, &#8220;w/o Predictors&#8221; denoted the removal of the Emotion and Speaker Predictors, and their corresponding loss functions were not optimized, whereas &#8220;w/o MINE&#8221; denoted the exclusion of MINE, meaning the mutual information between the outputs of the timbre and emotion extractors was not minimized. The experimental results demonstrated that incorporating both MINE and the Emotion and Speaker Predictors significantly improved performance. Specifically, the combined use of these components allowed the extractors to better distinguish and capture distinct features from the reference speech. This, in turn, enhanced the overall quality of the synthesized speech.</p>\n\n",
                "matched_terms": [
                    "model",
                    "proposed",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The study introduced a novel emotional TTS method, enhancing the FS2 architecture with a phoneme-level Emotion Extractor and global Timbre Extractor. To achieve effective disentanglement of style representations, we leveraged a MINE to minimize the mutual information between different feature dimensions. Experimental results demonstrated that our approach consistently outperformed baseline models, highlighting its efficacy.</p>\n\n",
                "matched_terms": [
                    "fs2",
                    "tts",
                    "results"
                ]
            }
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Results of ablation study (emotion/speaker predictors & MINE)",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">MOS(&#8593;)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">SMOS(&#8593;)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">MCD(&#8595;)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">UAA(&#8593;)</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Proposed</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"\\mathbf{3.62\\pm 0.10}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m1\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathsize=\"0.800em\" mathvariant=\"bold\">3.62</mn><mo mathsize=\"0.800em\">&#177;</mo><mn class=\"ltx_mathvariant_bold\" mathsize=\"0.800em\" mathvariant=\"bold\">0.10</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{3.62\\pm 0.10}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"\\mathbf{3.54\\pm 0.06}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m2\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathsize=\"0.800em\" mathvariant=\"bold\">3.54</mn><mo mathsize=\"0.800em\">&#177;</mo><mn class=\"ltx_mathvariant_bold\" mathsize=\"0.800em\" mathvariant=\"bold\">0.06</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{3.54\\pm 0.06}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"\\mathbf{8.23\\pm 0.22}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m3\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathsize=\"0.800em\" mathvariant=\"bold\">8.23</mn><mo mathsize=\"0.800em\">&#177;</mo><mn class=\"ltx_mathvariant_bold\" mathsize=\"0.800em\" mathvariant=\"bold\">0.22</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{8.23\\pm 0.22}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"\\mathbf{82.42\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m4\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathsize=\"0.800em\" mathvariant=\"bold\">82.42</mn><mo mathsize=\"0.800em\">%</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{82.42\\%}</annotation></semantics></math></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">w/o Predictors</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"3.53\\pm 0.10\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m5\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">3.53</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.10</mn></mrow><annotation encoding=\"application/x-tex\">3.53\\pm 0.10</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"3.29\\pm 0.07\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m6\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">3.29</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.07</mn></mrow><annotation encoding=\"application/x-tex\">3.29\\pm 0.07</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"9.71\\pm 0.21\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m7\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">9.71</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.21</mn></mrow><annotation encoding=\"application/x-tex\">9.71\\pm 0.21</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">56.84%</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">w/o MINE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"3.50\\pm 0.09\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m8\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">3.50</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.09</mn></mrow><annotation encoding=\"application/x-tex\">3.50\\pm 0.09</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"3.47\\pm 0.06\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m9\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">3.47</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.06</mn></mrow><annotation encoding=\"application/x-tex\">3.47\\pm 0.06</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"8.59\\pm 0.22\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m10\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.800em\">8.59</mn><mo mathsize=\"0.800em\">&#177;</mo><mn mathsize=\"0.800em\">0.22</mn></mrow><annotation encoding=\"application/x-tex\">8.59\\pm 0.22</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">76.37%</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "329±007329pm",
            "971±021971pm",
            "353±010353pm",
            "8242mathbf8242",
            "354±006mathbf354pm",
            "study",
            "mcd↓",
            "ablation",
            "362±010mathbf362pm",
            "mine",
            "predictors",
            "emotionspeaker",
            "823±022mathbf823pm",
            "350±009350pm",
            "859±022859pm",
            "347±006347pm",
            "results",
            "proposed",
            "smos↑",
            "model",
            "uaa↑",
            "mos↑"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To validate the effectiveness of individual components in our proposed model, we conducted ablation experiments, and the results were presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#S5.T2\" title=\"Table 2 &#8227; 5.6 Ablation Study &#8227; 5 Experiments &#8227; Emotional Text-To-Speech Based on Mutual-Information-Guided Emotion-Timbre Disentanglement\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. In this table, &#8220;w/o Predictors&#8221; denoted the removal of the Emotion and Speaker Predictors, and their corresponding loss functions were not optimized, whereas &#8220;w/o MINE&#8221; denoted the exclusion of MINE, meaning the mutual information between the outputs of the timbre and emotion extractors was not minimized. The experimental results demonstrated that incorporating both MINE and the Emotion and Speaker Predictors significantly improved performance. Specifically, the combined use of these components allowed the extractors to better distinguish and capture distinct features from the reference speech. This, in turn, enhanced the overall quality of the synthesized speech.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Current emotional Text-To-Speech (TTS) and style transfer methods rely on reference encoders to control global style or emotion vectors, but do not capture nuanced acoustic details of the reference speech. To this end, we propose a novel emotional TTS method that enables fine-grained phoneme-level emotion embedding prediction while disentangling intrinsic attributes of the reference speech. The proposed method employs a style disentanglement method to guide two feature extractors, reducing mutual information between timbre and emotion features, and effectively separating distinct style components from the reference speech. Experimental results demonstrate that our method outperforms baseline TTS systems in generating natural and emotionally rich speech. This work highlights the potential of disentangled and fine-grained representations in advancing the quality and flexibility of emotional TTS systems.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The synthesized audio samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://baleyang.github.io/emotion-timbre-disentangled-tts/\" title=\"\">https://baleyang.github.io/emotion-timbre-disentangled-tts/</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "proposed",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these limitations, we propose a novel emotional TTS method that (i) predicts fine-grained, phoneme-level emotion embeddings, and (ii) disentangles those embeddings from global timbre information through mutual-information minimization. Central to our method is a dedicated Style Encoder, which comprises two parallel extractors: a global Timbre Extractor, and a phoneme-aware Emotion Extractor that aligns reference acoustics with target phonemes to produce an emotion embedding sequence. An unsupervised Mutual Information Neural Estimation (MINE)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx11\" title=\"\">11</a>]</cite> explicitly pushes the two representations apart, ensuring that the timbre embedding remains speaker-specific information, while the emotion embeddings capture only prosodic nuance, allowing the model to synthesize speech that is simultaneously timbre-consistent and emotionally expressive.</p>\n\n",
                "matched_terms": [
                    "mine",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the limitations of global style embeddings and leverage MINE for effective speech feature disentanglement, we propose a novel model architecture for emotional TTS that integrates MINE into the style encoding process. Our model introduces two key innovations:</p>\n\n",
                "matched_terms": [
                    "mine",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overall architecture of the proposed model is illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Emotional Text-To-Speech Based on Mutual-Information-Guided Emotion-Timbre Disentanglement\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. It is based on FastSpeech 2(FS2)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx7\" title=\"\">7</a>]</cite>, consisting of Encoder-Decoder networks with Variance Adaptor, which following the original FS2 method. To enable emotional TTS, we introduce a Style Encoder after the Phoneme Encoder to predict style-specific representations.</p>\n\n",
                "matched_terms": [
                    "model",
                    "proposed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the dual-feature extractors (Timbre Extractor and Emotion Extractor) in our model, it is essential to disentangle their respective outputs to ensure that they capture distinct speech attributes. To achieve this, we employ MINE to disentangle the outputs of these two extractors.</p>\n\n",
                "matched_terms": [
                    "mine",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous studies have attempted to achieve feature disentanglement by minimizing the mutual information between style embeddings. However, these methods face a common issue: the lack of explicit guidance for the disentanglement process. Simply relying on MINE to minimize the mutual information between style embeddings leaves the model with no clear optimization direction, hindering its ability to effectively separate features. To address this, as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#S3.F2\" title=\"Figure 2 &#8227; 3.3 Style Disentanglement &#8227; 3 Model Architecture &#8227; Emotional Text-To-Speech Based on Mutual-Information-Guided Emotion-Timbre Disentanglement\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we not only minimize the mutual information between the emotion embedding <math alttext=\"\\mathbf{F}_{\\text{emotion}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#119813;</mi><mtext>emotion</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{emotion}}</annotation></semantics></math> and the timbre embedding <math alttext=\"\\mathbf{F}_{\\text{timbre}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#119813;</mi><mtext>timbre</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{timbre}}</annotation></semantics></math> using MINE, but also guide the disentanglement by explicitly predicting emotion and speaker labels from <math alttext=\"\\mathbf{F}_{\\text{emotion}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#119813;</mi><mtext>emotion</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{emotion}}</annotation></semantics></math> and <math alttext=\"\\mathbf{F}_{\\text{timbre}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m4\" intent=\":literal\"><semantics><msub><mi>&#119813;</mi><mtext>timbre</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{timbre}}</annotation></semantics></math>, respectively. This approach provides clear optimization objectives, enabling effective emotional speech synthesis.</p>\n\n",
                "matched_terms": [
                    "mine",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MINE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01722v1#bib.bibx11\" title=\"\">11</a>]</cite> instantiates <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p4.m4\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> as a trainable neural network <math alttext=\"T_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p4.m5\" intent=\":literal\"><semantics><msub><mi>T</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">T_{\\theta}</annotation></semantics></math> and maximizes <math alttext=\"\\hat{\\mathcal{I}}_{T_{\\theta}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p4.m6\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mo>^</mo></mover><msub><mi>T</mi><mi>&#952;</mi></msub></msub><annotation encoding=\"application/x-tex\">\\hat{\\mathcal{I}}_{T_{\\theta}}</annotation></semantics></math> with respect to the model parameter <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p4.m7\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>, thereby tightening the lower bound.</p>\n\n",
                "matched_terms": [
                    "mine",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the second stage, we incorporate the Style Encoder into the model and initialize all encoder parameters using the pre-trained weights from stage one (and freeze these parameters). To ensure the Timbre Extractor and the Emotion Extractor focus on distinct aspects of the reference mel-spectrogram, we adopt a mutual information minimization scheme based on MINE. Specifically, we alternate between updating the TTS model and the MI estimator to encourage the Timbre Extractor and the Emotion Extractor to capture non-overlapping information. We augment our loss with additional pitch and energy terms, as well as classification losses for speaker and emotion. The second-stage objective is defined as:</p>\n\n",
                "matched_terms": [
                    "mine",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The study introduced a novel emotional TTS method, enhancing the FS2 architecture with a phoneme-level Emotion Extractor and global Timbre Extractor. To achieve effective disentanglement of style representations, we leveraged a MINE to minimize the mutual information between different feature dimensions. Experimental results demonstrated that our approach consistently outperformed baseline models, highlighting its efficacy.</p>\n\n",
                "matched_terms": [
                    "study",
                    "mine",
                    "results"
                ]
            }
        ]
    }
}