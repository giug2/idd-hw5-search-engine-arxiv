{
    "S2.T1": {
        "source_file": "TTA: Transcribe, Translate and Alignment for Cross-lingual Speech Representation",
        "caption": "Table 1: Performance of multilingual speech recognition and speech translation.",
        "body": "Datasets\nMetric\nWhisper\nWhisper\nWhisper\nZT\nZT-AED\nZT-AED\nTTA\n\n\n(%)\nMedium\nLarge-v2\nLarge-v3\n(asr)\n(asr)\n\n\n#Params\n762M\n1541M\n1542M\n199M\n246M\n246M\n247M\n\n\n\naishell 1||2\n\nCER↓\\downarrow\n\n6.74||6.23\n\n\n5.90||5.24\n\n\n5.33||4.76\n\n\n1.89||3.14\n\n\n1.82||3.07\n\n\n1.80||3.03\n\n\n1.85||3.09\n\n\n\n\nwenet net||meeting\n\n\n11.00||22.68\n\n\n9.47||22.77\n\n\n9.00||15.68\n\n\n6.91||6.08\n\n\n6.89||6.18\n\n\n6.96||5.94\n\n\n7.06||6.44\n\n\n\n\nlibrispeech clean||other\n\nWER↓\\downarrow\n\n2.88||6.08\n\n\n2.64||5.14\n\n\n2.01||3.89\n\n\n1.58||3.62\n\n\n1.54||3.59\n\n\n1.56||3.76\n\n\n1.58||3.85\n\n\n\n\nAMI || gigaspeech\n\n\n16.77||15.51\n\n\n17.07||15.75\n\n\n15.98||14.53\n\n\n11.11||14.85\n\n\n10.85||14.76\n\n\n10.76||14.99\n\n\n11.06||14.97\n\n\n\ncommonvoice\n\n\n\nWER\n\navg↓\\downarrow\n \n11.86\n9.70\n8.30\n6.92\n6.70\n6.69\n6.76\n\n\nMLS\n7.27\n5.65\n4.48\n5.82\n5.71\n5.72\n5.74\n\n\nvoxpopuli\n12.08\n11.90\n13.78\n11.12\n10.78\n10.88\n10.87\n\n\nfleurs\n6.62\n5.20\n4.51\n6.35\n6.18\n6.17\n6.19\n\n\ncovostv2\n\nBLEU↑\\uparrow\n\n35.12\n38.80\n37.60\n-\n-\n34.72\n35.28",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" rowspan=\"2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Datasets</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Metric</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Whisper</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Whisper</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Whisper</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ZT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ZT-AED</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ZT-AED</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">TTA</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">(%)</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Medium</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Large-v2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Large-v3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">(asr)</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">(asr)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">#Params</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">762M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1541M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1542M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">199M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">246M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">246M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">247M</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">aishell</span><span class=\"ltx_text\" style=\"font-size:90%;\"> 1</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m1\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">2</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">6.74</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m3\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">6.23</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">5.90</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m4\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">5.24</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">5.33</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m5\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">4.76</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">1.89</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m6\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">3.14</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">1.82</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m7\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">3.07</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">1.80</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m8\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">3.03</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">1.85</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m9\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">3.09</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">wenet</span><span class=\"ltx_text\" style=\"font-size:90%;\"> net</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m10\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">meeting</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">11.00</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m11\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">22.68</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">9.47</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m12\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">22.77</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">9.00</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m13\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">15.68</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">6.91</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m14\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">6.08</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">6.89</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m15\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">6.18</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">6.96</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m16\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">5.94</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">7.06</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m17\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">6.44</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">librispeech</span><span class=\"ltx_text\" style=\"font-size:90%;\"> clean</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m18\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">other</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m19\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">2.88</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m20\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">6.08</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">2.64</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m21\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">5.14</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">2.01</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m22\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">3.89</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">1.58</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m23\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">3.62</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">1.54</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m24\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">3.59</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">1.56</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m25\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">3.76</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">1.58</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m26\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">3.85</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AMI</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m27\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">gigaspeech</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">16.77</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m28\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">15.51</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">17.07</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m29\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">15.75</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">15.98</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m30\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">14.53</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">11.11</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m31\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">14.85</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">10.85</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m32\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">14.76</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">10.76</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m33\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">14.99</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">11.06</span><math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m34\" intent=\":literal\"><semantics><mo fence=\"false\" maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">14.97</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">commonvoice</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"4\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">WER</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">avg<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m35\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></span>\n</span> </span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.30</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.69</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.76</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MLS</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.27</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.48</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.82</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.71</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.72</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.74</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">voxpopuli</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.08</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.78</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.12</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.78</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.88</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.87</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">fleurs</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.62</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.51</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.35</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.18</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.17</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.19</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">covostv2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">BLEU</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m36\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">35.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">38.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">37.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">34.72</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">35.28</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "recognition",
            "ztaed",
            "199m",
            "wer",
            "speech",
            "datasets",
            "wenet",
            "commonvoice",
            "multilingual",
            "metric",
            "1542m",
            "netmeeting",
            "asr",
            "762m",
            "gigaspeech",
            "bleu↑uparrow",
            "wer↓downarrow",
            "cer↓downarrow",
            "performance",
            "fleurs",
            "ami",
            "avg↓downarrow",
            "medium",
            "1541m",
            "tta",
            "mls",
            "params",
            "voxpopuli",
            "246m",
            "translation",
            "whisper",
            "cleanother",
            "librispeech",
            "covostv2",
            "247m",
            "largev2",
            "aishell",
            "largev3"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">A comprehensive evaluation of ASR and ST performance across multiple benchmarks is presented in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14410v1#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2 TTA &#8227; TTA: Transcribe, Translate and Alignment for Cross-lingual Speech Representation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe ZT-based models are directly compared against the Whisper series in terms of model scale, recognition WER, and translation BLEU score.\nOn widely used Chinese and English test sets, including Aishell, WenetSpeech, and LibriSpeech, the TTA model significantly outperforms Whisper models. This performance advantage can be attributed in part to the substantial proportion of Chinese and English data within our training data.\nIn Multilingual ASR benchmarks, the TTA model demonstrates a considerable WER on the CommonVoice dataset, even compared with Whisper Large-v3 (6.76% vs. 8.30%).\nClear advantages on MLS and VoxPopuli datasets are also observed.\nFor zero-shot evaluation on Fleurs, the TTA model fails to surpass Whisper Large models, while still behaves better than Whisper Medium.\nThis partially examines the recognition generalization of the TTA model, given its lightweight design and significantly fewer parameters even compared to Whisper Medium.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We further inspect Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14410v1#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2 TTA &#8227; TTA: Transcribe, Translate and Alignment for Cross-lingual Speech Representation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> by comparing four TTA series models under identical conditions, i.e., with the same training steps and learning schedule.\nThese model includes: ZT(asr), ZT-AED(asr), ZT-AED, and TTA.\nModels marked with &#8220;(asr)&#8221; are trained exclusively on ASR data, and ZT-AED differs from TTA by a single </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Alignment</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> component.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Speech-LLM models have demonstrated great performance in multi-modal and multi-task speech understanding.\nA typical speech-LLM paradigm is integrating speech modality with a large language model (LLM).\nWhile the Whisper encoder was frequently adopted in previous studies for speech input, it shows limitations regarding input format, model scale and semantic performance.\nTo this end, we propose a lightweight TTA model specialized in speech semantics for more effective LLM integration.\nWith large-scale training of 358k hours of speech data on multilingual speech recognition (ASR), speech translation (ST) and speech-text alignment tasks, TTA is capable of producing robust cross-lingual speech representations.\nExtensive evaluations across diverse benchmarks, including ASR/ST, speech retrieval, and ASR-LLM performance assessments, demonstrate TTA&#8217;s superiority over Whisper.\nFurthermore, we rigorously validate the interplay between cross-lingual capabilities and ASR/ST performance.\nThe model weights and training recipes of TTA will be released as part of an audio understanding toolkit <span class=\"ltx_text ltx_font_italic\">Auden</span>.</span>\n</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "translation",
                    "whisper",
                    "performance",
                    "asr",
                    "speech",
                    "tta",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nSpeech semantics, Multilingual speech recognition, Speech translation, Cross-lingual speech representation</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "speech",
                    "translation",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The remarkable success of large-scale pre-trained models in natural language processing has spurred significant interest in developing analogous speech foundation models. These models utilize a single architecture to perform a wide array of speech-related tasks to achieve generalized speech understanding. Multilingual speech recognition (MASR) and speech translation (ST) are two primary tasks to learn speech semantic information, such as Whisper</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, USM</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023google</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, OWSM </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/asru/PengTYBCLSACSZSSJMW23</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Canary</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">canary</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. More recently, the powerful Large Language Models (LLMs) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">brown2020language</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">achiam2023gpt</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2024qwen2</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in text comprehension and reasoning have led to efforts to integrate speech modality to develop speech-LLM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chu2023qwen</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chu2024qwen2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/iclr/TangYSC000M024</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/emnlp/HuZ0CMHPL0SLW24</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lu2025desta2</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, thereby enhancing multi-modal speech understanding. A common practice for speech-LLM is to attach a speech foundation model, often its encoder component, with an LLM to facilitate seamless cross-modal comprehension.</span>\n</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "speech",
                    "translation",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent works on speech-LLM like Qwen-Audio&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chu2023qwen</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chu2024qwen2</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, WavLLM </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/emnlp/HuZ0CMHPL0SLW24</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, SALMONN&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/iclr/TangYSC000M024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, DESTA2.5&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lu2025desta2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> mostly connect Whisper encoder to LLM. Qwen-Audio, for instance, pairs Whisper Large-v2 encoder with Qwen-7B&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2024qwen2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and achieves promising results in audio question-answering and chat tasks. However, the Whisper encoder has several limitations standing out:\n(1) default constraint of 30-second speech input (unless fine-tuned); (2) focusing on speech semantics (with weaker performance for Chinese) while lacking paralinguistic information; and (3) large model sizes. To address these, we posit that a lightweight design can balance superior efficiency and performance. This paper focuses on developing a specialized speech semantic model. Unlike Whisper and its open-source reproduction OWSM, which primarily utilize a Transformer-based encoder-decoder architecture, ours employs a combined Zipformer-based</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yao2023zipformer</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Transducer (ZT) and attention-based encoder-decoder (AED) architecture. This hybrid design (ZT-AED) is specifically engineered to enhance the semantic representational capacity of the encoder component.</span>\n</p>\n\n",
                "matched_terms": [
                    "ztaed",
                    "whisper",
                    "performance",
                    "largev2",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The training tasks used to learn speech semantics are MASR and ST. Prior work has empirically demonstrated the benefits of joint ASR-ST training </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/asru/PengTYBCLSACSZSSJMW23</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">canary</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The basic assumption is that language-invariant information (e.g., semantics) can be shared. Learning cross-lingual speech representations aligned in a shared multilingual space is necessary.\nHowever, two key questions remain unaddressed: the under-explored relationship between cross-lingual representation and ASR/ST learning, and the extent to which ST impacts ASR performance under fair settings.</span>\n</p>\n\n",
                "matched_terms": [
                    "asr",
                    "speech",
                    "multilingual",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this study, we introduce </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">TTA</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">T</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">ranscribe, </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">T</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">ranslate and </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">A</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">lignment), a highly efficient speech semantic foundation model, less than 250M parameters. TTA employs an innovative ZT-AED joint architecture optimized to capture rich speech semantics via MASR and ST learning.\nIt further incorporates a contrastive loss to align speech features and text embeddings in a multilingual space, explicitly enhancing cross-lingual representation learning. Trained on 358k hours of speech data, TTA consistently outperforms Whisper Medium on multiple MASR and ST benchmarks, while surpassing Whisper Large in some in-domain benchmarks. Our key contributions are: (1) A lightweight model exceeding Whisper-medium performance, validating the ZT-AED architecture&#8217;s effectiveness; (2) Enhanced cross-lingual speech representations via alignment loss, significantly boosting speech retrieval performance. Furthermore, we provide insights into the interplay between cross-lingual capability, ASR, and ST; (3) Demonstrated speech semantic superiority of TTA&#8217;s encoder over Whisper encoders for LLM integration.\n\n</span>\n</p>\n\n",
                "matched_terms": [
                    "ztaed",
                    "whisper",
                    "performance",
                    "speech",
                    "asr",
                    "medium",
                    "tta",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">TTA adopts a hybrid ZT-AED architecture as shown in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14410v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2 TTA &#8227; TTA: Transcribe, Translate and Alignment for Cross-lingual Speech Representation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe model integrates three components: a Zipformer-based Transducer, an attention-based Transformer decoder, and a BERT-based</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2019bert</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> speech-text alignment module.\nThe Zipformer encoder is a fast and memory-efficient variant of the Conformer</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gulati2020conformer</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> architecture, which serves as the backbone to encode the input speech feature </span>\n  <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119831;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into high-level speech representations </span>\n  <math alttext=\"\\mathbf{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119815;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThese representations are subsequently processed by three distinct branches.</span>\n</p>\n\n",
                "matched_terms": [
                    "ztaed",
                    "speech",
                    "tta"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The last branch is designed for speech-text semantic alignment in the multilingual embedding space.\nA frozen multilingual BERT encoder</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://huggingface.co/google-bert/bert-base-multilingual-uncased</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is employed to provide the semantic anchor. Specifically, the BERT encoder extracts text embedding </span>\n  <math alttext=\"\\mathbf{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119827;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{T}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from </span>\n  <math alttext=\"\\mathbf{\\tilde{Y}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mover accent=\"true\">\n        <mi mathsize=\"0.900em\">&#119832;</mi>\n        <mo mathsize=\"0.900em\">~</mo>\n      </mover>\n      <annotation encoding=\"application/x-tex\">\\mathbf{\\tilde{Y}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe speech representation </span>\n  <math alttext=\"\\mathbf{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119815;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is projected by a linear network to match the dimension of </span>\n  <math alttext=\"\\mathbf{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119827;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{T}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and a SigLIP</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhai2023sigmoid</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> contrastive loss is applied on the a batch of </span>\n  <math alttext=\"\\{\\bar{\\mathbf{H}},\\bar{\\mathbf{T}}\\}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mover accent=\"true\">\n            <mi mathsize=\"0.900em\">&#119815;</mi>\n            <mo mathsize=\"0.900em\">&#175;</mo>\n          </mover>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mover accent=\"true\">\n            <mi mathsize=\"0.900em\">&#119827;</mi>\n            <mo mathsize=\"0.900em\">&#175;</mo>\n          </mover>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\{\\bar{\\mathbf{H}},\\bar{\\mathbf{T}}\\}_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"\\bar{\\mathbf{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m6\" intent=\":literal\">\n    <semantics>\n      <mover accent=\"true\">\n        <mi mathsize=\"0.900em\">&#119815;</mi>\n        <mo mathsize=\"0.900em\">&#175;</mo>\n      </mover>\n      <annotation encoding=\"application/x-tex\">\\bar{\\mathbf{H}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\bar{\\mathbf{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m7\" intent=\":literal\">\n    <semantics>\n      <mover accent=\"true\">\n        <mi mathsize=\"0.900em\">&#119827;</mi>\n        <mo mathsize=\"0.900em\">&#175;</mo>\n      </mover>\n      <annotation encoding=\"application/x-tex\">\\bar{\\mathbf{T}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are the utterance-level average pooling of the sequence </span>\n  <math alttext=\"\\mathbf{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m8\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119815;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\mathbf{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m9\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119827;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{T}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> respectively.\nThis contrastive objective minimizes the distances between positive pairs and pushes other negative, semantically dissimilar pairs further apart.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Overall, TTA is designed to integrate multilingual speech recognition, speech translation and speech-text alignment within a lightweight model.\nThese tasks can equip the model with the capabilities of general cross-lingual speech understanding.</span>\n</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "translation",
                    "speech",
                    "tta",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ASR training data.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nThe training data comprise a combination of in-house and open-source datasets covering 10 languages: Chinese (zh), English (en), Japanese (ja), Korean (ko), Russian (ru), Vietnamese (vi), Indonesian (id), French (fr), Spanish (es) and Portuguese (pt).\nFigure.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14410v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2 TTA &#8227; TTA: Transcribe, Translate and Alignment for Cross-lingual Speech Representation\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> illustrates the language distribution of the total 357,982 hours of ASR training data.\nApproximately half of this corpus was collected from publicly available datasets, such as Aishell</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bu2017aishell</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2018aishell</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, WenetSpeech</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022wenetspeech</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, LibriSpeech</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">librispeech</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, MLS</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pratap2020mls</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, VoxPopuli</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2021voxpopuli</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, LibriHeavy</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kang2024libriheavy</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, GigaSpeech</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2021gigaspeech</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CommonVoice</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ardila2020common</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nTo ensure high data quality, all the data sources have undergone rigorous filtering procedures.\nSpecifically, Whisper Large-v3 was employed to identify and exclude samples with incorrect language labels, and verify transcription using a WER threshold of 10-20%.</span>\n</p>\n\n",
                "matched_terms": [
                    "whisper",
                    "wer",
                    "asr",
                    "datasets",
                    "largev3"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AST training data.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nFor speech translation, our supervised data are restricted solely to the X</span>\n  <math alttext=\"\\xrightarrow{}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mover accent=\"true\">\n        <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n        <mi/>\n      </mover>\n      <annotation encoding=\"application/x-tex\">\\xrightarrow{}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">EN splits from CoVoSTv2</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2020covost</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Europarl-ST</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">iranzo2020europarl</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nSynthesized X</span>\n  <math alttext=\"\\xrightarrow{}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mover accent=\"true\">\n        <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n        <mi/>\n      </mover>\n      <annotation encoding=\"application/x-tex\">\\xrightarrow{}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">EN translation pairs were generated from the aforementioned ASR training data using LLM</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://huggingface.co/Qwen/Qwen2.5-7B-Instruct</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with heuristic rules to eliminate potential hallucinations.\nThe resulting ST data comprises approximately 217,000 hours in total, with each sample explicitly linked to its ASR data source.\nThis enables precise control over data sampling ratio between ASR and ST training.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "translation",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Testing data and decoding.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nTest sets from public datasets were used to measure ASR/AST performance across all the 10 languages.\nFor ASR, Aishell, WenetSpeech, LibriSpeech, GigaSpeech, and AMI were adopted for zh/en assessment;\nCommonVoice-15, MLS, and VoxPopuli were adopted for multilingual assessment;\nFleurs</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau2023fleurs</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> was preserved for zero-shot testing with its training set excluded from our training data.\nThe translation capability was examined by test data of CoVoSTv2.\nIn ASR decoding, we use the transducer branch to perform language-agnostic greedy search.\nThe recognition accuracy of attention decoding from the AED branch is slightly better than transducer in beam search, but similar in greedy search. We use it for language identification and ST decoding.\nThe Whisper baselines adopt attention greedy search, providing the ground truth language.</span>\n</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "translation",
                    "whisper",
                    "performance",
                    "librispeech",
                    "ami",
                    "covostv2",
                    "asr",
                    "aishell",
                    "datasets",
                    "gigaspeech",
                    "mls",
                    "multilingual",
                    "voxpopuli"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">TTA model is composed of a Zipformer-large encoder with output dimension of 256, a standard Transducer network</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/interspeech/KuangGKLLYP22</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a 6-layer attention decoder, and a contrastive module for speech-text alignment.\nThe encoder consumes filter-bank features from 80-dim log-mel spectrum extracted with a window of 25 ms and a stride of 10 ms.\nData-loading details are implemented with the Lhotse</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zelasko2021lhotse</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> framework, which features an efficient batching strategy with data buffers to shuffle and group samples according to their duration.\nThe Transducer loss is averaged with the attention decoder loss, and the alignment loss is added with the weight of 0.1.\nModels are trained on 32 V100 GPUs with </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">DynamicBucketingSampler</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of a 250s </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">max_duration</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Scaled_Adam</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> optimizer with a peak learning rate of 0.035, and </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Eden</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> scheduler with warm-up of 2000 steps.\nFor data balance over different languages and data sources, the dataset muxing weights </span>\n  <math alttext=\"w_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">w</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">w_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are modified to </span>\n  <math alttext=\"w_{i}=h_{i}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">w</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <msubsup>\n          <mi mathsize=\"0.900em\">h</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n        </msubsup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">w_{i}=h_{i}^{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which means scale the total duration </span>\n  <math alttext=\"h_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">h</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">h_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of dataset </span>\n  <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">i</mi>\n      <annotation encoding=\"application/x-tex\">i</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with temperature </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nTo make the training process more stable, TTA was trained with a multi-stage startegy.\nIn Stage 1, a ZT model was trained on ASR data for 250,000 steps.\nNext in Stage 2, different models were initialized from the same ZT checkpoint and continued the ASR training from lr of 0.005 for 200,000 steps.\nIn Stage 3, models with ST were trained on the joint ASR/ST data with a </span>\n  <math alttext=\"3:2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">3</mn>\n        <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n        <mn mathsize=\"0.900em\">2</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">3:2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> mixing ratio from lr of 0.002 for 500,000 steps, while the ASR-only models were kept the same.\n</span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> was originally set as </span>\n  <math alttext=\"1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m8\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">1.0</mn>\n      <annotation encoding=\"application/x-tex\">1.0</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and gradually adjust to </span>\n  <math alttext=\"0.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m9\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.2</mn>\n      <annotation encoding=\"application/x-tex\">0.2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> during the last training stage.</span>\n</p>\n\n",
                "matched_terms": [
                    "tta",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For speech translation performance measured with BLEU score on CoVoSTv2, the TTA model exhibits better performance than Whisper Medium, while still lagging behind Whisper Large models. This ST performance ceiling is primarily constrained by model capacity.\nWe validate this assumption on another model with doubled model dimension which shows a significant boost.</span>\n</p>\n\n",
                "matched_terms": [
                    "translation",
                    "covostv2",
                    "performance",
                    "whisper",
                    "speech",
                    "medium",
                    "tta"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further validate the effectiveness of the TTA architecture, an ablation study is conducted on the probing task of ST.\nSeveral models from training stage 2 without ST training are selected for comparison, namely ZT, ZT-AED, ZT-Align and TTA.\nTheir encoders are frozen and connected to a randomly initialized attention decoder to train the ST task.\nValidation loss curves averaged over 10 language pairs of each model during ST training are plotted in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14410v1#S4.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 4.3 Relationship between Cross-lingual and ASR/ST &#8227; 4 Results &#8227; TTA: Transcribe, Translate and Alignment for Cross-lingual Speech Representation\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (a).\nWe found that models with </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Alignment</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> components perform consistently better.\nThis suggests that explicit semantic alignment through the speech-text alignment module provides effective multilingual semantic anchors, thereby enhancing the encoder&#8217;s cross-lingual representations and facilitating speech translation.</span>\n</p>\n\n",
                "matched_terms": [
                    "ztaed",
                    "translation",
                    "speech",
                    "tta",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The TTA model also demostrates strong capability in language identification.\nAs evaluated on Fleurs, it achieves 100% accuracy for the ten training languages.\nIn contrast, Whisper Large-v3 performs generally the same but worse in Indonesian with 81% accuracy, although it supports more languages.</span>\n</p>\n\n",
                "matched_terms": [
                    "largev3",
                    "tta",
                    "whisper",
                    "fleurs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Speech-to-speech retrieval provides a direct approach to assessing the cross-lingual capabilities of semantic representations.\nSpeech samples with the same semantic meaning in different languages are supposed to encoded closer in the embedding space.\nFollowing </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/naacl/MaQFTGK25</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">dev</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">test</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> sets from Fleurs are merged as an evaluation corpus with approximately 500 semantically aligned samples across all languages.\nWe extract speech representations using various encoders and performed cross-lingual retrieval based on cosine similarity, where each query utterance is matched against all candidate utterances from another language.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "fleurs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The detailed results of speech retrieval are presented in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14410v1#S4.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 4.2 Cross-lingual Speech Retrieval &#8227; 4 Results &#8227; TTA: Transcribe, Translate and Alignment for Cross-lingual Speech Representation\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nRetrieval accuracy is generally higher among Indo-European languages, which can be attributed to their linguistic proximity.\nIn terms of model performance, the ZT-AED model, which incorporates speech translation, slightly outperforms ASR-only baselines and performs comparably to Whisper Medium.\nNotably, the TTA model achieves a substantial improvement in retrieval accuracy, even surpassing Whisper Large-v2.\nThis enhancement underscores the efficacy of its explicit speech-text alignment mechanism in learning language-agnostic semantic representations.</span>\n</p>\n\n",
                "matched_terms": [
                    "ztaed",
                    "translation",
                    "whisper",
                    "performance",
                    "largev2",
                    "speech",
                    "medium",
                    "tta"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The performance gap between ZT(asr) and ZT-AED(asr) highlights the advantage of the hybrid architecture.\nComparing TTA with ZT-AED, the additional cross-lingual alignment yields clear benefits for ST performance, with increase of approximately 0.6 BLEU score.\nHowever, the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Alignment</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is observed to bring minor degradation in ASR performance.\nWith careful tuning of the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Alignment</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> weight, this degradation of less than 0.1% WER can be neglected.</span>\n</p>\n\n",
                "matched_terms": [
                    "ztaed",
                    "performance",
                    "wer",
                    "asr",
                    "tta"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">On the other hand, it implies that ASR performance may not really benefit from the cross-lingual ability enhancement.\nAs indicated in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14410v1#S4.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 4.2 Cross-lingual Speech Retrieval &#8227; 4 Results &#8227; TTA: Transcribe, Translate and Alignment for Cross-lingual Speech Representation\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, ZT-AED(asr) has shown basic cross-lingual abilities when learning and sharing the language-invariant information across multiple languages. For example, it tends to align the language </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">en</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> together with {</span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">fr,es,pt</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">} languages.\nEnforcing a more aggressive cross-lingual representation in the encoder output space may deviate from the objective of ASR, although it is proven to benefit the ST and speech retrieval task.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Lastly, we compare ZT-AED(asr) with ZT-AED and examine the effect of joint ASR-ST training.\nIt is important to note that this comparison is conducted under a controlled setting where both ASR and ST data originate from the same source, which guarantees the joint ASR-ST training does not introduce new training data.\nIn this setting, we surprisingly found that there is no ASR performance gain from additional ST training.\nWe speculate that the empirical advantages of joint ASR-ST training may come from the new data sources introduced by ST.</span>\n</p>\n\n",
                "matched_terms": [
                    "ztaed",
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further evaluate the effectiveness of semantic representations learned by the TTA encoder, we employ ASR-LLM as a linear probing task for comparative analysis. A single MLP layer is trained on top of each encoder to align its output representations with the token embedding space of LLM.\nThe whole system is then trained on ASR using speech inputs and text-based prompts like </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">&#8217;Please repeat the following content:&#8217;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAll models are trained under consistent settings using the Aishell and LibriSpeech datasets with a batch size of 4 for 100,000 steps.\nThe recognition performance is evaluated on Aishell-1 test-set and LibriSpeech test-clean.</span>\n</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "performance",
                    "librispeech",
                    "speech",
                    "asr",
                    "aishell",
                    "datasets",
                    "tta"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As illustrated in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14410v1#S4.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 4.3 Relationship between Cross-lingual and ASR/ST &#8227; 4 Results &#8227; TTA: Transcribe, Translate and Alignment for Cross-lingual Speech Representation\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (b), the training dynamics of ASR-LLM models with different encoders reveal that Whisper-based models demonstrate markedly lower optimization efficiency compared to those with Zipformer encoders.\nThe WER results in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14410v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.4 Semantic Encoder Evaluation in ASR-LLM &#8227; 4 Results &#8227; TTA: Transcribe, Translate and Alignment for Cross-lingual Speech Representation\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> indicate that, recognition performance on Chinese data generally improves under the ASR-LLM framework, whereas a slight degradation is observed for English data. This discrepancy may be attributed to the inherent preference over Chinese of the Qwen LLM.\nNotably, the ASR-LLM model with TTA encoder exhibits superior recognition performance comparable to the Transducer decoding from TTA model. In contrast to the ZT-AED model, the TTA approach underscores the importance of explicit semantic alignment in effectively integrating speech representations with LLMs.</span>\n</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "ztaed",
                    "performance",
                    "wer",
                    "speech",
                    "tta"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Additionally, we observe that the TTA model demonstrates significantly improved computational efficiency, operating at approximately twice the speed of Whisper Large-v2 and 1.5 times of Whisper Medium during training.</span>\n</p>\n\n",
                "matched_terms": [
                    "tta",
                    "whisper",
                    "medium",
                    "largev2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we present TTA, an efficient Zipformer-based model capable of performing MASR and ST with high-quality semantic representations across ten languages.\nWith less than 300M parameters, TTA achieves superior performance in both speech recognition and translation tasks, while demonstrating higher efficiency and effectiveness within the downstream LLM integration task.\nThrough extensive experimental analysis, we systematically examine the mutual influence between MASR, ST, and cross-lingual representation learning, highlighting the importance of contrastive semantic alignment.\nTo promote reproducibility and further research, the model weights and codes will be released as part of the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Auden</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> project.</span>\n</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "translation",
                    "performance",
                    "speech",
                    "tta"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "TTA: Transcribe, Translate and Alignment for Cross-lingual Speech Representation",
        "caption": "Table 2: Recognition performance comparison of different encoders in ASR-LLM. CER (%) for Aishell and WER (%) for Librispeech.",
        "body": "Whisper-M\nWhisper-L\nZT-AED\nTTA\n\n\nAishell\n5.47\n4.87\n2.92\n1.92\n\n\nLibrispeech\n4.66\n3.64\n2.30\n1.95",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r ltx_border_tt\" style=\"padding:-0.45pt 4.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.45pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Whisper-M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.45pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Whisper-L</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.45pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ZT-AED</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.45pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">TTA</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:-0.45pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Aishell</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.45pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.45pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.45pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.45pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.92</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding:-0.45pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Librispeech</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.45pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.66</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.45pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.64</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.45pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.30</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.45pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.95</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "recognition",
            "ztaed",
            "performance",
            "librispeech",
            "wer",
            "asrllm",
            "encoders",
            "whisperl",
            "comparison",
            "aishell",
            "different",
            "tta",
            "whisperm",
            "cer"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As illustrated in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14410v1#S4.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 4.3 Relationship between Cross-lingual and ASR/ST &#8227; 4 Results &#8227; TTA: Transcribe, Translate and Alignment for Cross-lingual Speech Representation\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (b), the training dynamics of ASR-LLM models with different encoders reveal that Whisper-based models demonstrate markedly lower optimization efficiency compared to those with Zipformer encoders.\nThe WER results in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14410v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.4 Semantic Encoder Evaluation in ASR-LLM &#8227; 4 Results &#8227; TTA: Transcribe, Translate and Alignment for Cross-lingual Speech Representation\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> indicate that, recognition performance on Chinese data generally improves under the ASR-LLM framework, whereas a slight degradation is observed for English data. This discrepancy may be attributed to the inherent preference over Chinese of the Qwen LLM.\nNotably, the ASR-LLM model with TTA encoder exhibits superior recognition performance comparable to the Transducer decoding from TTA model. In contrast to the ZT-AED model, the TTA approach underscores the importance of explicit semantic alignment in effectively integrating speech representations with LLMs.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Speech-LLM models have demonstrated great performance in multi-modal and multi-task speech understanding.\nA typical speech-LLM paradigm is integrating speech modality with a large language model (LLM).\nWhile the Whisper encoder was frequently adopted in previous studies for speech input, it shows limitations regarding input format, model scale and semantic performance.\nTo this end, we propose a lightweight TTA model specialized in speech semantics for more effective LLM integration.\nWith large-scale training of 358k hours of speech data on multilingual speech recognition (ASR), speech translation (ST) and speech-text alignment tasks, TTA is capable of producing robust cross-lingual speech representations.\nExtensive evaluations across diverse benchmarks, including ASR/ST, speech retrieval, and ASR-LLM performance assessments, demonstrate TTA&#8217;s superiority over Whisper.\nFurthermore, we rigorously validate the interplay between cross-lingual capabilities and ASR/ST performance.\nThe model weights and training recipes of TTA will be released as part of an audio understanding toolkit <span class=\"ltx_text ltx_font_italic\">Auden</span>.</span>\n</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "asrllm",
                    "tta",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent works on speech-LLM like Qwen-Audio&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chu2023qwen</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chu2024qwen2</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, WavLLM </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/emnlp/HuZ0CMHPL0SLW24</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, SALMONN&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/iclr/TangYSC000M024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, DESTA2.5&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lu2025desta2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> mostly connect Whisper encoder to LLM. Qwen-Audio, for instance, pairs Whisper Large-v2 encoder with Qwen-7B&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2024qwen2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and achieves promising results in audio question-answering and chat tasks. However, the Whisper encoder has several limitations standing out:\n(1) default constraint of 30-second speech input (unless fine-tuned); (2) focusing on speech semantics (with weaker performance for Chinese) while lacking paralinguistic information; and (3) large model sizes. To address these, we posit that a lightweight design can balance superior efficiency and performance. This paper focuses on developing a specialized speech semantic model. Unlike Whisper and its open-source reproduction OWSM, which primarily utilize a Transformer-based encoder-decoder architecture, ours employs a combined Zipformer-based</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yao2023zipformer</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Transducer (ZT) and attention-based encoder-decoder (AED) architecture. This hybrid design (ZT-AED) is specifically engineered to enhance the semantic representational capacity of the encoder component.</span>\n</p>\n\n",
                "matched_terms": [
                    "ztaed",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this study, we introduce </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">TTA</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">T</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">ranscribe, </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">T</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">ranslate and </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">A</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">lignment), a highly efficient speech semantic foundation model, less than 250M parameters. TTA employs an innovative ZT-AED joint architecture optimized to capture rich speech semantics via MASR and ST learning.\nIt further incorporates a contrastive loss to align speech features and text embeddings in a multilingual space, explicitly enhancing cross-lingual representation learning. Trained on 358k hours of speech data, TTA consistently outperforms Whisper Medium on multiple MASR and ST benchmarks, while surpassing Whisper Large in some in-domain benchmarks. Our key contributions are: (1) A lightweight model exceeding Whisper-medium performance, validating the ZT-AED architecture&#8217;s effectiveness; (2) Enhanced cross-lingual speech representations via alignment loss, significantly boosting speech retrieval performance. Furthermore, we provide insights into the interplay between cross-lingual capability, ASR, and ST; (3) Demonstrated speech semantic superiority of TTA&#8217;s encoder over Whisper encoders for LLM integration.\n\n</span>\n</p>\n\n",
                "matched_terms": [
                    "ztaed",
                    "tta",
                    "performance",
                    "encoders"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">TTA adopts a hybrid ZT-AED architecture as shown in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14410v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2 TTA &#8227; TTA: Transcribe, Translate and Alignment for Cross-lingual Speech Representation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe model integrates three components: a Zipformer-based Transducer, an attention-based Transformer decoder, and a BERT-based</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2019bert</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> speech-text alignment module.\nThe Zipformer encoder is a fast and memory-efficient variant of the Conformer</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gulati2020conformer</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> architecture, which serves as the backbone to encode the input speech feature </span>\n  <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119831;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into high-level speech representations </span>\n  <math alttext=\"\\mathbf{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119815;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThese representations are subsequently processed by three distinct branches.</span>\n</p>\n\n",
                "matched_terms": [
                    "ztaed",
                    "tta"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Overall, TTA is designed to integrate multilingual speech recognition, speech translation and speech-text alignment within a lightweight model.\nThese tasks can equip the model with the capabilities of general cross-lingual speech understanding.</span>\n</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "tta"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Testing data and decoding.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nTest sets from public datasets were used to measure ASR/AST performance across all the 10 languages.\nFor ASR, Aishell, WenetSpeech, LibriSpeech, GigaSpeech, and AMI were adopted for zh/en assessment;\nCommonVoice-15, MLS, and VoxPopuli were adopted for multilingual assessment;\nFleurs</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">conneau2023fleurs</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> was preserved for zero-shot testing with its training set excluded from our training data.\nThe translation capability was examined by test data of CoVoSTv2.\nIn ASR decoding, we use the transducer branch to perform language-agnostic greedy search.\nThe recognition accuracy of attention decoding from the AED branch is slightly better than transducer in beam search, but similar in greedy search. We use it for language identification and ST decoding.\nThe Whisper baselines adopt attention greedy search, providing the ground truth language.</span>\n</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "aishell",
                    "performance",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">TTA model is composed of a Zipformer-large encoder with output dimension of 256, a standard Transducer network</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/interspeech/KuangGKLLYP22</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a 6-layer attention decoder, and a contrastive module for speech-text alignment.\nThe encoder consumes filter-bank features from 80-dim log-mel spectrum extracted with a window of 25 ms and a stride of 10 ms.\nData-loading details are implemented with the Lhotse</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zelasko2021lhotse</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> framework, which features an efficient batching strategy with data buffers to shuffle and group samples according to their duration.\nThe Transducer loss is averaged with the attention decoder loss, and the alignment loss is added with the weight of 0.1.\nModels are trained on 32 V100 GPUs with </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">DynamicBucketingSampler</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of a 250s </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">max_duration</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Scaled_Adam</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> optimizer with a peak learning rate of 0.035, and </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Eden</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> scheduler with warm-up of 2000 steps.\nFor data balance over different languages and data sources, the dataset muxing weights </span>\n  <math alttext=\"w_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">w</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">w_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are modified to </span>\n  <math alttext=\"w_{i}=h_{i}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">w</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <msubsup>\n          <mi mathsize=\"0.900em\">h</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n        </msubsup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">w_{i}=h_{i}^{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which means scale the total duration </span>\n  <math alttext=\"h_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">h</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">h_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of dataset </span>\n  <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">i</mi>\n      <annotation encoding=\"application/x-tex\">i</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with temperature </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nTo make the training process more stable, TTA was trained with a multi-stage startegy.\nIn Stage 1, a ZT model was trained on ASR data for 250,000 steps.\nNext in Stage 2, different models were initialized from the same ZT checkpoint and continued the ASR training from lr of 0.005 for 200,000 steps.\nIn Stage 3, models with ST were trained on the joint ASR/ST data with a </span>\n  <math alttext=\"3:2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">3</mn>\n        <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n        <mn mathsize=\"0.900em\">2</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">3:2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> mixing ratio from lr of 0.002 for 500,000 steps, while the ASR-only models were kept the same.\n</span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> was originally set as </span>\n  <math alttext=\"1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m8\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">1.0</mn>\n      <annotation encoding=\"application/x-tex\">1.0</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and gradually adjust to </span>\n  <math alttext=\"0.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m9\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.2</mn>\n      <annotation encoding=\"application/x-tex\">0.2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> during the last training stage.</span>\n</p>\n\n",
                "matched_terms": [
                    "tta",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">A comprehensive evaluation of ASR and ST performance across multiple benchmarks is presented in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14410v1#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2 TTA &#8227; TTA: Transcribe, Translate and Alignment for Cross-lingual Speech Representation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe ZT-based models are directly compared against the Whisper series in terms of model scale, recognition WER, and translation BLEU score.\nOn widely used Chinese and English test sets, including Aishell, WenetSpeech, and LibriSpeech, the TTA model significantly outperforms Whisper models. This performance advantage can be attributed in part to the substantial proportion of Chinese and English data within our training data.\nIn Multilingual ASR benchmarks, the TTA model demonstrates a considerable WER on the CommonVoice dataset, even compared with Whisper Large-v3 (6.76% vs. 8.30%).\nClear advantages on MLS and VoxPopuli datasets are also observed.\nFor zero-shot evaluation on Fleurs, the TTA model fails to surpass Whisper Large models, while still behaves better than Whisper Medium.\nThis partially examines the recognition generalization of the TTA model, given its lightweight design and significantly fewer parameters even compared to Whisper Medium.</span>\n</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "performance",
                    "librispeech",
                    "wer",
                    "aishell",
                    "tta"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For speech translation performance measured with BLEU score on CoVoSTv2, the TTA model exhibits better performance than Whisper Medium, while still lagging behind Whisper Large models. This ST performance ceiling is primarily constrained by model capacity.\nWe validate this assumption on another model with doubled model dimension which shows a significant boost.</span>\n</p>\n\n",
                "matched_terms": [
                    "tta",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further validate the effectiveness of the TTA architecture, an ablation study is conducted on the probing task of ST.\nSeveral models from training stage 2 without ST training are selected for comparison, namely ZT, ZT-AED, ZT-Align and TTA.\nTheir encoders are frozen and connected to a randomly initialized attention decoder to train the ST task.\nValidation loss curves averaged over 10 language pairs of each model during ST training are plotted in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14410v1#S4.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 4.3 Relationship between Cross-lingual and ASR/ST &#8227; 4 Results &#8227; TTA: Transcribe, Translate and Alignment for Cross-lingual Speech Representation\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (a).\nWe found that models with </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Alignment</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> components perform consistently better.\nThis suggests that explicit semantic alignment through the speech-text alignment module provides effective multilingual semantic anchors, thereby enhancing the encoder&#8217;s cross-lingual representations and facilitating speech translation.</span>\n</p>\n\n",
                "matched_terms": [
                    "ztaed",
                    "tta",
                    "comparison",
                    "encoders"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Speech-to-speech retrieval provides a direct approach to assessing the cross-lingual capabilities of semantic representations.\nSpeech samples with the same semantic meaning in different languages are supposed to encoded closer in the embedding space.\nFollowing </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/naacl/MaQFTGK25</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">dev</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">test</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> sets from Fleurs are merged as an evaluation corpus with approximately 500 semantically aligned samples across all languages.\nWe extract speech representations using various encoders and performed cross-lingual retrieval based on cosine similarity, where each query utterance is matched against all candidate utterances from another language.</span>\n</p>\n\n",
                "matched_terms": [
                    "encoders",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The detailed results of speech retrieval are presented in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14410v1#S4.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 4.2 Cross-lingual Speech Retrieval &#8227; 4 Results &#8227; TTA: Transcribe, Translate and Alignment for Cross-lingual Speech Representation\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nRetrieval accuracy is generally higher among Indo-European languages, which can be attributed to their linguistic proximity.\nIn terms of model performance, the ZT-AED model, which incorporates speech translation, slightly outperforms ASR-only baselines and performs comparably to Whisper Medium.\nNotably, the TTA model achieves a substantial improvement in retrieval accuracy, even surpassing Whisper Large-v2.\nThis enhancement underscores the efficacy of its explicit speech-text alignment mechanism in learning language-agnostic semantic representations.</span>\n</p>\n\n",
                "matched_terms": [
                    "ztaed",
                    "tta",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We further inspect Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14410v1#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2 TTA &#8227; TTA: Transcribe, Translate and Alignment for Cross-lingual Speech Representation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> by comparing four TTA series models under identical conditions, i.e., with the same training steps and learning schedule.\nThese model includes: ZT(asr), ZT-AED(asr), ZT-AED, and TTA.\nModels marked with &#8220;(asr)&#8221; are trained exclusively on ASR data, and ZT-AED differs from TTA by a single </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Alignment</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> component.</span>\n</p>\n\n",
                "matched_terms": [
                    "ztaed",
                    "tta"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The performance gap between ZT(asr) and ZT-AED(asr) highlights the advantage of the hybrid architecture.\nComparing TTA with ZT-AED, the additional cross-lingual alignment yields clear benefits for ST performance, with increase of approximately 0.6 BLEU score.\nHowever, the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Alignment</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is observed to bring minor degradation in ASR performance.\nWith careful tuning of the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Alignment</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> weight, this degradation of less than 0.1% WER can be neglected.</span>\n</p>\n\n",
                "matched_terms": [
                    "wer",
                    "ztaed",
                    "tta",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Lastly, we compare ZT-AED(asr) with ZT-AED and examine the effect of joint ASR-ST training.\nIt is important to note that this comparison is conducted under a controlled setting where both ASR and ST data originate from the same source, which guarantees the joint ASR-ST training does not introduce new training data.\nIn this setting, we surprisingly found that there is no ASR performance gain from additional ST training.\nWe speculate that the empirical advantages of joint ASR-ST training may come from the new data sources introduced by ST.</span>\n</p>\n\n",
                "matched_terms": [
                    "ztaed",
                    "comparison",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further evaluate the effectiveness of semantic representations learned by the TTA encoder, we employ ASR-LLM as a linear probing task for comparative analysis. A single MLP layer is trained on top of each encoder to align its output representations with the token embedding space of LLM.\nThe whole system is then trained on ASR using speech inputs and text-based prompts like </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">&#8217;Please repeat the following content:&#8217;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAll models are trained under consistent settings using the Aishell and LibriSpeech datasets with a batch size of 4 for 100,000 steps.\nThe recognition performance is evaluated on Aishell-1 test-set and LibriSpeech test-clean.</span>\n</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "performance",
                    "librispeech",
                    "asrllm",
                    "aishell",
                    "tta"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we present TTA, an efficient Zipformer-based model capable of performing MASR and ST with high-quality semantic representations across ten languages.\nWith less than 300M parameters, TTA achieves superior performance in both speech recognition and translation tasks, while demonstrating higher efficiency and effectiveness within the downstream LLM integration task.\nThrough extensive experimental analysis, we systematically examine the mutual influence between MASR, ST, and cross-lingual representation learning, highlighting the importance of contrastive semantic alignment.\nTo promote reproducibility and further research, the model weights and codes will be released as part of the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Auden</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> project.</span>\n</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "tta",
                    "performance"
                ]
            }
        ]
    }
}