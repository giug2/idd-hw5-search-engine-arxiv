{
    "S3.T1": {
        "source_file": "CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation",
        "caption": "Table 1: Ablation study: RAAP with text similarity, accent confidence, and text adaptations. ‘Max’ means adapted text selection described in Eqn. 4.",
        "body": "Text Sim.\nAccent Score\nText Apt.\nACC (%)\nNISQA\n\n\n\n\n✗\n✗\n✗\n45.60\n4.21\n\n\n✓\n✗\n✗\n46.26\n4.28\n\n\n✓\n✓\n✗\n61.74\n4.25\n\n\n✓\n✓\nLLaMA\n62.47\n4.33\n\n\n✓\n✓\nGPT\n63.36\n4.31\n\n\n✓\n✓\nMax\n62.38\n4.29",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Text Sim.</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Accent Score</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Text Apt.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">ACC (%)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">NISQA</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">&#10007;</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">&#10007;</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">45.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.21</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">&#10003;</td>\n<td class=\"ltx_td ltx_align_left\">&#10007;</td>\n<td class=\"ltx_td ltx_align_left\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">46.26</td>\n<td class=\"ltx_td ltx_align_center\">4.28</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">&#10003;</td>\n<td class=\"ltx_td ltx_align_left\">&#10003;</td>\n<td class=\"ltx_td ltx_align_left\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">61.74</td>\n<td class=\"ltx_td ltx_align_center\">4.25</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">&#10003;</td>\n<td class=\"ltx_td ltx_align_left\">&#10003;</td>\n<td class=\"ltx_td ltx_align_left\">LLaMA</td>\n<td class=\"ltx_td ltx_align_center\">62.47</td>\n<td class=\"ltx_td ltx_align_center\">4.33</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">&#10003;</td>\n<td class=\"ltx_td ltx_align_left\">&#10003;</td>\n<td class=\"ltx_td ltx_align_left\">GPT</td>\n<td class=\"ltx_td ltx_align_center\">63.36</td>\n<td class=\"ltx_td ltx_align_center\">4.31</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">&#10003;</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">&#10003;</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Max</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">62.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4.29</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "confidence",
            "ablation",
            "text",
            "accent",
            "‘max’",
            "llama",
            "sim",
            "max",
            "score",
            "means",
            "gpt",
            "described",
            "acc",
            "apt",
            "adaptations",
            "adapted",
            "similarity",
            "raap",
            "study",
            "selection",
            "eqn",
            "nisqa"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">3. Ablation Study of CLARITY:</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S3.T1\" title=\"Table 1 &#8227; 3.1 Experimental Settings &#8227; 3 Experiments &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the ablation study, showing that accent-score&#8211;guided prompt selection is highly effective, while adding text adaptation further improves accent accuracy. All variants achieve NISQA above 4.21, rising to 4.31 with RAAP and text adaptation, confirming the robustness of CLARITY. Among CLARITY variants, using GPT-adapted text performs best, while with LLaMA-adapted text is competitive, though its adapted text scores are lower (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S3.F3\" title=\"Figure 3 &#8227; 3.1 Experimental Settings &#8227; 3 Experiments &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), indicating potential mismatches from using the GPT-5 as judge. Comparison of LLaMA and GPT for text selection does not improve results, suggesting that GPT-5 scores may not be the optimum<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>Using multiple LLM-as-Judge models, e.g., GPT-5 and LLaMA-4, may reduce such mismatches, we leave this for future work.</span></span></span>. Since GPT-adapted CLARITY performs best in the objective test, we adopt this setting for CLARITY in the following experiments.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Instruction-guided text-to-speech (TTS) research has reached a maturity level where excellent speech generation quality is possible on demand, yet two coupled biases persist: <span class=\"ltx_text ltx_font_italic\">accent bias</span>, where models default to dominant phonetic patterns, and <span class=\"ltx_text ltx_font_italic\">linguistic bias</span>, where dialect-specific lexical and cultural cues are ignored.\nThese biases are interdependent, as authentic accent generation requires both accent fidelity and localized text.\nWe present <span class=\"ltx_text ltx_font_bold\">CLARITY</span> (<span class=\"ltx_text ltx_font_bold\">C</span>ontextual <span class=\"ltx_text ltx_font_bold\">L</span>inguistic <span class=\"ltx_text ltx_font_bold\">A</span>daptation and <span class=\"ltx_text ltx_font_bold\">R</span>etrieval for <span class=\"ltx_text ltx_font_bold\">I</span>nclusive <span class=\"ltx_text ltx_font_bold\">T</span>TS s<span class=\"ltx_text ltx_font_bold\">Y</span>nthesis), a backbone-agnostic framework that addresses these biases through dual-signal optimization: (i) contextual linguistic adaptation that localizes input text to the target dialect, and (ii) retrieval-augmented accent prompting (RAAP) that supplies accent-consistent speech prompts. Across twelve English accents, CLARITY improves accent accuracy and fairness while maintaining strong perceptual quality<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code and audio samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/ICT-SIT/CLARITY\" title=\"\">https://github.com/ICT-SIT/CLARITY</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "raap",
                    "accent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Instruction-guided TTS systems <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lyth2024natural</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhou2024voxinstruct</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025spark</span>]</cite> taking two inputs: 1) transcript text and 2) speaker descriptions of desired speech characteristics, have achieved promising instruction-following abilities.\nHowever, when prompted with natural language instructions, such TTS systems are susceptible to various biases.\nWhile gender bias has been examined by prior work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuan-lee-2025-gender</span>]</cite>, to the best of our knowledge, no study has jointly addressed accent and linguistic bias. The two are interrelated but accent bias stems from the TTS system, while linguistic bias is often implicit in user input.\nThe <span class=\"ltx_text ltx_font_italic\">dual bias</span>, as illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, reduces generated speech authenticity by misaligning <span class=\"ltx_text ltx_font_italic\">what</span> is said and <span class=\"ltx_text ltx_font_italic\">how</span> it is spoken&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">burns2019speaking</span>]</cite>.\nFor example, instructing TTS to speak in a British Accent but with American English oriented text<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Note that traditional TTS assumes fixed text. This work addresses scenarios where users want to generate speech in a specific accent but cannot provide text in that variant of English. Hence, the input text must be slightly adapted to the accent specified in the speaker instruction.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "accent",
                    "adapted",
                    "study"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CLARITY</span>:\nTo jointly mitigate this <span class=\"ltx_text ltx_font_italic\">dual bias</span>, we present <span class=\"ltx_text ltx_font_bold\">C</span>ontextual <span class=\"ltx_text ltx_font_bold\">L</span>inguistic <span class=\"ltx_text ltx_font_bold\">A</span>daptation and <span class=\"ltx_text ltx_font_bold\">R</span>etrieval for <span class=\"ltx_text ltx_font_bold\">I</span>nclusive <span class=\"ltx_text ltx_font_bold\">T</span>TS s<span class=\"ltx_text ltx_font_bold\">Y</span>nthesis (CLARITY).\nOur proposed CLARITY is a backbone-agnostic framework to enhance zero-shot TTS by providing richer guidance for both linguistic content and accent style. Its novelty lies in a two-signal optimization to jointly mitigate the two biases through:\n(i) an LLM-guided text adaptation module to localize input to the target dialect, and\n(ii) a metadata-driven retrieval mechanism to supply accent-consistent prompts.</p>\n\n",
                "matched_terms": [
                    "text",
                    "accent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our Contributions. </span>\nWe introduce a sociolinguistic innovation in TTS by using large language models (LLMs) for on-the-fly text style transfer, leveraging their knowledge of dialects and slang to enhance authenticity.\nBy pairing this with retrieval-augmented prompting, CLARITY achieves accent control and dialect-aware synthesis.\nTo our knowledge, this is the first framework to jointly address user-side linguistic bias and system-side accent bias. Objective and subjective evaluations show that CLARITY improves fairness, accent fidelity, and overall authenticity.</p>\n\n",
                "matched_terms": [
                    "text",
                    "accent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The CLARITY framework promotes inclusive TTS by mitigating linguistic and accent biases through two signals: an adapted text <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> and an accent-consistent prompt <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math>, as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S2.F2\" title=\"Figure 2 &#8227; 2 The Proposed CLARITY &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nWe cast this as a two-signal optimization problem, aiming to maximize both linguistic and accent fidelity under the synthesis constraint of a backbone TTS model.\nFormally, the objective is defined as\n   </p>\n\n",
                "matched_terms": [
                    "text",
                    "accent",
                    "adapted"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"J_{\\text{LLM}}(x^{*},m)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>J</mi><mtext>LLM</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mo>&#8727;</mo></msup><mo>,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">J_{\\text{LLM}}(x^{*},m)</annotation></semantics></math> denotes the judgment score assigned by\nan LLM-as-a-Judge evaluating whether the adapted text <math alttext=\"x^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m4\" intent=\":literal\"><semantics><msup><mi>x</mi><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">x^{*}</annotation></semantics></math> aligns with the requested metadata <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m5\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>,\nand <math alttext=\"C(s^{*},m)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m6\" intent=\":literal\"><semantics><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>s</mi><mo>&#8727;</mo></msup><mo>,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">C(s^{*},m)</annotation></semantics></math> is the accent recognition confidence score that the retrieved prompt <math alttext=\"s^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m7\" intent=\":literal\"><semantics><msup><mi>s</mi><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">s^{*}</annotation></semantics></math> matches the target accent attributes. The synthesis function <math alttext=\"g_{\\text{TTS}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m8\" intent=\":literal\"><semantics><msub><mi>g</mi><mtext>TTS</mtext></msub><annotation encoding=\"application/x-tex\">g_{\\text{TTS}}</annotation></semantics></math> represents any zero-shot backbone TTS model.</p>\n\n",
                "matched_terms": [
                    "confidence",
                    "text",
                    "accent",
                    "adapted",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure that the adapted text <math alttext=\"x^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m1\" intent=\":literal\"><semantics><msup><mi>x</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">x^{\\prime}</annotation></semantics></math> is linguistically faithful to the target dialect <math alttext=\"\\mathcal{D}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{D}_{t}</annotation></semantics></math>,\nwe generate candidate adaptations <math alttext=\"\\{x^{\\prime}_{1},x^{\\prime}_{2},\\dots,x^{\\prime}_{K}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>x</mi><mn>1</mn><mo>&#8242;</mo></msubsup><mo>,</mo><msubsup><mi>x</mi><mn>2</mn><mo>&#8242;</mo></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>x</mi><mi>K</mi><mo>&#8242;</mo></msubsup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{x^{\\prime}_{1},x^{\\prime}_{2},\\dots,x^{\\prime}_{K}\\}</annotation></semantics></math> using different LLMs (e.g., GPT and LLaMA).\nWe compare three candidates: the original standard text <math alttext=\"x_{\\text{std}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m4\" intent=\":literal\"><semantics><msub><mi>x</mi><mtext>std</mtext></msub><annotation encoding=\"application/x-tex\">x_{\\text{std}}</annotation></semantics></math> and adapted texts\n<math alttext=\"k\\in\\mathcal{K}=\\{\\text{GPT},\\text{LLaMA}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m5\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119974;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mtext>GPT</mtext><mo>,</mo><mtext>LLaMA</mtext><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">k\\in\\mathcal{K}=\\{\\text{GPT},\\text{LLaMA}\\}</annotation></semantics></math> produced by different LLMs.\nAn LLM-as-a-Judge assigns each candidate a score\n<math alttext=\"J_{\\text{LLM}}(x,m)\\in[0,10],\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m6\" intent=\":literal\"><semantics><mrow><mrow><mrow><msub><mi>J</mi><mtext>LLM</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>10</mn><mo stretchy=\"false\">]</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">J_{\\text{LLM}}(x,m)\\in[0,10],</annotation></semantics></math>\nwhich reflects how well <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m7\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> aligns with the metadata <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m8\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>, particularly the accent attribute.\nThe final text is then chosen as the candidate with the highest judgment score:</p>\n\n",
                "matched_terms": [
                    "gpt",
                    "text",
                    "adaptations",
                    "accent",
                    "llama",
                    "adapted",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Retrieval-Augmented Accent Prompting (RAAP):</span>\n\nSystem-side accent bias arises when a TTS backbone trained on imbalanced data defaults to majority accents <math alttext=\"\\mathcal{A}_{\\text{dom}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mtext>dom</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{\\text{dom}}</annotation></semantics></math>. To counter this, we introduce Retrieval-Augmented Accent Prompting (RAAP), which selects accent-consistent\nprompts from a curated pool <math alttext=\"\\mathcal{P}=\\{(a_{i},z_{i},s_{i})\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo>,</mo><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}=\\{(a_{i},z_{i},s_{i})\\}</annotation></semantics></math> containing metadata,\ntranscript, and speech. Given target metadata <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m3\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>, we filter <math alttext=\"\\mathcal{P}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><annotation encoding=\"application/x-tex\">\\mathcal{P}</annotation></semantics></math> by\n<math alttext=\"a_{i}\\approx m\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m5\" intent=\":literal\"><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>&#8776;</mo><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">a_{i}\\approx m</annotation></semantics></math> to form <math alttext=\"\\mathcal{P}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{P}_{m}</annotation></semantics></math>, then score each candidate <math alttext=\"s_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m7\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">s_{i}</annotation></semantics></math> using an\nECAPA-TDNN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">desplanques2020ecapa</span>]</cite> for accent confidence\n<math alttext=\"C(s_{i},m)\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m8\" intent=\":literal\"><semantics><mrow><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">C(s_{i},m)\\in[0,1]</annotation></semantics></math>, and select those with the highest scores to guide accented\nspeech generation. Mathematically,</p>\n\n",
                "matched_terms": [
                    "confidence",
                    "accent",
                    "raap",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further align the text content of the prompt speech with the user&#8217;s text input,\nRAAP calculates the text similarity between the prompt speech candidates and the\nuser-provided (standard) text input using\n<math alttext=\"\\cos(\\phi(z_{i}),\\phi(x))\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m1\" intent=\":literal\"><semantics><mrow><mi>cos</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\cos(\\phi(z_{i}),\\phi(x))</annotation></semantics></math>,\nwhere <math alttext=\"\\phi(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m2\" intent=\":literal\"><semantics><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi(\\cdot)</annotation></semantics></math> is a TF&#8211;IDF&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">salton1988term</span>]</cite> text embedding representation\nand <math alttext=\"\\cos(\\cdot,\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m3\" intent=\":literal\"><semantics><mrow><mi>cos</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo rspace=\"0em\">,</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\cos(\\cdot,\\cdot)</annotation></semantics></math> denotes cosine similarity.\nSpecifically, we define the effective ranking score as</p>\n\n",
                "matched_terms": [
                    "text",
                    "raap",
                    "similarity",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Backbone-agnostic Guided Synthesis:</span>\nThe final stage of CLARITY aligns the two signals: the adapted text <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p8.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> and the accent-consistent prompt <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p8.m2\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math>, for guided synthesis. Let <math alttext=\"g_{\\text{TTS}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p8.m3\" intent=\":literal\"><semantics><msub><mi>g</mi><mtext>TTS</mtext></msub><annotation encoding=\"application/x-tex\">g_{\\text{TTS}}</annotation></semantics></math> be the backbone TTS with parameters <math alttext=\"\\theta_{\\text{TTS}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p8.m4\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mtext>TTS</mtext></msub><annotation encoding=\"application/x-tex\">\\theta_{\\text{TTS}}</annotation></semantics></math>. As a backbone-agnostic framework, CLARITY can use any instruction-guided, zero-shot TTS model; here we adopt the SOTA CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>]</cite> with its zero-shot functionality. The synthesis is defined as</p>\n\n",
                "matched_terms": [
                    "text",
                    "adapted"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text and Free-form Instructions Curation</span>: For transcript, GPT-4<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openai.com/index/gpt-4-research/\" title=\"\">https://openai.com/index/gpt-4-research/</a></span></span></span> generated standard sentences across four scenarios (restaurant, university, workplace, supermarket). Speaker instructions derived from accent-pool metadata (accent, age, gender) served both as prompts and ground-truth labels for retrieval. Each instruction&#8211;text pair was then adapted by GPT-4o-mini<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/\" title=\"\">https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/</a></span></span></span>\nand LLaMA-3.1-8B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/meta-llama/Llama-3.1-8B\" title=\"\">https://huggingface.co/meta-llama/Llama-3.1-8B</a></span></span></span> to produce accent-specific variants. Fidelity was evaluated by GPT-5<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openai.com/index/introducing-gpt-5/\" title=\"\">https://openai.com/index/introducing-gpt-5/</a></span></span></span>, which rated text&#8211;accent alignment on a 1&#8211;10 scale.</p>\n\n",
                "matched_terms": [
                    "text",
                    "accent",
                    "adapted"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Object Evaluation Metrics:</span>\nTo quantify the accent level of the generated speech, we calculate accent accuracy using the ECAPA-TDNN accent recognition model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Jzuluaga/accent-id-commonaccent_ecapa\" title=\"\">https://huggingface.co/Jzuluaga/accent-id-commonaccent_ecapa</a></span></span></span>, finetuned on the accent pool (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S3.SS1\" title=\"3.1 Experimental Settings &#8227; 3 Experiments &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>), as in RAAP. Speech quality is measured with NISQA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mittag2021nisqa</span>]</cite> (1&#8211;5 scale) using its pre-trained model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/gabrielmittag/NISQA\" title=\"\">https://github.com/gabrielmittag/NISQA</a></span></span></span>.\nBias/Fairness of the generated speech is evaluated with the Fairness Discrepancy Rate (FDR)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">de2021fairness</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">estevez2023study</span>]</cite>, which quantifies disparities in false alarm/reject rates across accents, with 1 indicating perfect fairness.</p>\n\n",
                "matched_terms": [
                    "raap",
                    "accent",
                    "nisqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Subject Evaluation Metrics:</span>\nWe conducted human listening tests on four accented English samples (CN, SG, IN, GB). For each accent, 10 utterances (5F/5M) were generated from three systems: CosyVoice2 (standard text), ParlerTTS (standard text), and CLARITY. Native speakers of each accent (fluent in English), rated corresponding samples (e.g., Chinese listeners for Chinese-accented English<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>The setting assumes that Chinese listeners are more sensitive to Chinese-accented English speech.</span></span></span>). There were 4F/4M listeners for CN/SG and 2F/2M for IN/GB. Samples were evaluated on a 1&#8211;5 scale for naturalness, accent accuracy, age, gender consistency, and overall instruction match.</p>\n\n",
                "matched_terms": [
                    "text",
                    "accent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">1. RAAP and Contextual Linguistic Text Adaptation:</span>\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S3.F3\" title=\"Figure 3 &#8227; 3.1 Experimental Settings &#8227; 3 Experiments &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the RAAP accuracy (left) and the adapted scores generated by GPT-5 and LLaMA models (right). RAAP achieves nearly 100% accuracy for most accents in terms of accent and gender retrieval, while age prediction is less accurate but acceptable, given its known difficulty.\nFor the LLM-as-judge scores, we observe that the standard text already achieves reasonable accent scores, with all scores above 6. After adaptation using GPT-4o-mini, four accents (GB, MY, SG, and US) show improved adaptation scores. Interestingly, these accents correspond to locations where the language of daily communciation tends to be English. A possible explanation is that for these speakers, the model had higher confidence in modifying the text, while the training corpus distribution likely reinforces this behaviour, which GPT-4 effectively captures. In contrast, LLaMA shows decreased accent scores after adaptation, although we doubt these results since the judge is GPT-5, not LLaMA, and continue to evaluate both adapted texts.</p>\n\n",
                "matched_terms": [
                    "confidence",
                    "raap",
                    "text",
                    "accent",
                    "adapted",
                    "llama"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">2. Comparision of Baselines and CLARITY:</span>\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S3.F4\" title=\"Figure 4 &#8227; 3.1 Experimental Settings &#8227; 3 Experiments &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> plots accent accuracy and NISQA scores for generated speech across various systems, along with the averaged scores. CosyVoice2 and ParlerTTS achieve very low accent accuracy (8.75% and 6.14%), likely due to training sets lacking accented speech.\nPer-accent results reveal higher scores for accents better represented in training data. CLARITY with GPT-adapted text achieves the best overall accuracy, though ES, JP, and PT remain below 50%, showing the difficulty of these accents.\nIn terms of NISQA, ParlerTTS averages 4.67, while CLARITY_GPT achieves 4.36, outperforming CosyVoice2 (3.16) and even ground truth on some accents.</p>\n\n",
                "matched_terms": [
                    "text",
                    "accent",
                    "nisqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Subjective Evaluation:</span>\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S3.F5\" title=\"Figure 5 &#8227; 3.1 Experimental Settings &#8227; 3 Experiments &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows violin plots of the listening test results. For naturalness, ParlerTTS performs best on IN, while CLARITY outperforms both alternatives on CN and GB. SG results are similar across systems, with CosyVoice2 slightly worse. This aligns with the NISQA scores in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S3.F4\" title=\"Figure 4 &#8227; 3.1 Experimental Settings &#8227; 3 Experiments &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. CLARITY demonstrates superior accent consistency with reduced bias across all accents. Both CLARITY and ParlerTTS maintain gender consistency, whereas CosyVoice2 mimics prompt speech rather than following user instructions. Overall, CLARITY best aligns with user instructions.</p>\n\n",
                "matched_terms": [
                    "accent",
                    "nisqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remaining two accents, Malaysian (MY) and Singaporean (SG), are represented by 22,108 utterances selected from the SEAME dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lyu2010seame</span>]</cite>, a large-scale, code-switching English&#8211;Mandarin corpus collected from spontaneous conversations in Malaysia and Singapore. It contains speech from both Malaysian and Singaporean speakers, with detailed accent and speaker annotations. To construct our subset, we extracted English-dominant utterances from both conversational and interview recordings, retaining only those labeled as English or code-switched (EN and CS) in the transcript metadata. Purely Mandarin (ZH) segments were excluded. Each utterance was required to contain at least 5 words, with a maximum of 100 utterances per speaker, prioritizing longer segments. After extraction, only speakers with English accent confidence scores greater than 0.9 were retained. These filtering criteria yielded a balanced selection of Malaysian and Singaporean speakers across both genders and an age range of 18 to 33 years.</p>\n\n",
                "matched_terms": [
                    "confidence",
                    "accent",
                    "selection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To parse user instructions, generate localised text, and select the best final spoken text, a total of four LLMs were used. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#A1.F9\" title=\"Figure 9 &#8227; A.1.2 Instruction Curation &#8227; A.1 Experimental Protocol &#8227; Appendix A Appendix &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> illustrates the entire process from the initial user instruction to the final selection of the localised text.</p>\n\n",
                "matched_terms": [
                    "text",
                    "selection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, GPT-5-mini was employed as an LLM-as-a-Judge, evaluating the standard text and both adapted versions. It provided quantitative scores and qualitative reasoning for each output.\nAn example of the judging prompt is shown below:</p>\n\n",
                "matched_terms": [
                    "text",
                    "adapted"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#A1.F11\" title=\"Figure 11 &#8227; A.1.3 Adapted Text Generation &#8227; A.1 Experimental Protocol &#8227; Appendix A Appendix &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> shows the average NISQA score for each accent on the accent pool data.\nThe generated speech maintains high perceptual quality across all accents,\nwith scores ranging from 4.0 to 4.6 on a five-point scale.\nSingaporean (SG, 4.57), Japanese (JP, 4.41), and British (GBR, 4.40) accents\nachieve the best quality, indicating clear and natural synthesis.\nSpanish (ESP, 4.00) and Chinese (CHN, 4.13) exhibit slightly lower scores,\nlikely due to accent-specific variability or limited training coverage.\nDespite these differences, the narrow score range demonstrates strong cross-accent generalization and consistent perceptual quality.</p>\n\n",
                "matched_terms": [
                    "accent",
                    "nisqa",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To align the text content of the prompt speech with the user&#8217;s input, RAAP calculates the text similarity between the prompt speech candidates and the user-provided (standard) text input, as explained in Eqn.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S2.E6\" title=\"In 2 The Proposed CLARITY &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Alternatively, the adapted text can also be used as the query to help identify prompt speech with similar content.</p>\n\n",
                "matched_terms": [
                    "raap",
                    "text",
                    "adapted",
                    "similarity",
                    "eqn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The objective results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#A1.T5\" title=\"Table 5 &#8227; A.2.2 Speech Quality Evaluation &#8227; A.2 Additional Results on Accent Pool data &#8227; Appendix A Appendix &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, where adapted text yields slightly lower accent accuracy for both LLaMA and GPT, while achieving comparable NISQA scores. In the subjective listening test, GPT-based methods using either adapted or standard text were evaluated. The violin plots in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#A1.F12\" title=\"Figure 12 &#8227; A.2.2 Speech Quality Evaluation &#8227; A.2 Additional Results on Accent Pool data &#8227; Appendix A Appendix &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> show that for SG, the results are almost identical between standard and adapted text; for CN, a similar trend is observed except that MOS with standard text achieves slightly better performance; while for GB and IN, GPT with standard text as the basis for text similarity calculation tends to obtain higher scores across all perceptual evaluation metrics.</p>\n\n",
                "matched_terms": [
                    "gpt",
                    "text",
                    "accent",
                    "adapted",
                    "llama",
                    "similarity",
                    "nisqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The P-value <span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\">12</span>https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binomtest.html</span></span></span> of US and CA accents are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#A1.T6\" title=\"Table 6 &#8227; A.3.3 Bias Analysis &#8227; A.3 Additional Results on Generated Speech &#8227; Appendix A Appendix &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. The P-value indicates the result of the Binomial test, where a very small P-value (typically <math alttext=\"p\\leq 0.05\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.SSS3.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&#8804;</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">p\\leq 0.05</annotation></semantics></math>) means the observed proportion is significantly higher than the random chance, thus confirming the existence of a statistically significant bias.\nWe can see that CosyVoice2 has a extreme and consistent bias towards US/CA accents across all tested non-US/CA accents. CLARITY_GPT demonstrates a much lower and more variable bias compared to CosyVoice2 which reduced overall bias. For British (GB), India (IN), Japan (JP), Malaysian (MY) and Singapore (SG) accents, there is no statistically significant evidence of bias toward US/CA accents. Although the magnitude of the bias is lower, CLARITY_GPT still exhibits a statistically significant bias for several accents, particularly Chinese (CN), Spanish (ES), Korean (KR) and Portuguese (PT). Bias towards US accent is more significant than CA accent.</p>\n\n",
                "matched_terms": [
                    "accent",
                    "means"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation Study:</span> as shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#A1.F15\" title=\"Figure 15 &#8227; A.2.2 Speech Quality Evaluation &#8227; A.2 Additional Results on Accent Pool data &#8227; Appendix A Appendix &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, this ablation study details the impact of various model configurations on accent accuracy and systemic bias, demonstrating a phased improvement across experimental setups. The initial phase, which involved replacing the near-silent prompt audio with accent pool sourced prompt audio dynamically selected via instruction extracted speaker features, alone yielded a significant boost in both accent accuracy and fairness metrics. Building upon this foundation, performance saw further incremental gains through the subsequent integration of target text (standard) to prompt transcription similarity and the strategic use of prompt speech exhibiting a more pronounced accent (higher accent score). As mentioned in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#A1.SS3.SSS1\" title=\"A.3.1 Results on Retrieved Prompt Speech &#8227; A.3 Additional Results on Generated Speech &#8227; Appendix A Appendix &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">A.3.1</span></a>, we also compared the results of standard and adapted text similarity. Ultimately, the most effective configurations (proposed) were achieved by leveraging Llama or GPT to perform adaptation on the target text, resulting in the highest overall accuracy and the lowest observed degree of bias among all tested methodologies.</p>\n\n",
                "matched_terms": [
                    "gpt",
                    "ablation",
                    "text",
                    "study",
                    "accent",
                    "llama",
                    "adapted",
                    "similarity",
                    "score"
                ]
            }
        ]
    },
    "A1.T3": {
        "source_file": "CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation",
        "caption": "Table 3: The statistics of Accent Pool (SEAME)",
        "body": "Accent\n#Spk\n#Utt\n#F\n#M\nAge\n\n\n\n\n\n\n\n\n\n(min/max)\n\n\nMY\n40\n4,358\n21\n19\n20/33\n\n\nSG\n114\n17,750\n63\n51\n18/24",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Accent</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">#Spk</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">#Utt</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">#F</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">#M</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Age</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">(min/max)</th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">MY</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4,358</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">20/33</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">SG</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">114</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">17,750</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">18/24</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "spk",
            "minmax",
            "age",
            "accent",
            "pool",
            "utt",
            "seame",
            "statistics"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluate our proposed method on twelve English accents. The accent pool is composed of accents from two datasets. Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#A1.T2\" title=\"Table 2 &#8227; A.1.1 Accent Pool Data Generation &#8227; A.1 Experimental Protocol &#8227; Appendix A Appendix &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#A1.T3\" title=\"Table 3 &#8227; A.1.1 Accent Pool Data Generation &#8227; A.1 Experimental Protocol &#8227; Appendix A Appendix &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> summarize the statistics of the accent pool, including the number of speakers, utterances, gender distribution, and age range. Specifically, ten accents are drawn from the AESRC dataset, which was introduced in the Interspeech 2020 Accented English Speech Recognition Challenge <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2021accented</span>]</cite>. This dataset comprises approximately 180 hours of speech spanning the ten English accents. Each speaker reads sentences covering common conversational topics as well as human&#8211;computer interaction commands. For our experiments, we selected a subset of 52,614 utterances, balanced by gender and covering a broad age range. For each speaker, up to 100 longest utterances were retained to ensure consistent recording quality and sufficient duration per sample. The selected subset includes the following accents: Canadian (CA), Chinese (CN), Spanish (ES), British (GB), Indian (IN), Japanese (JP), Korean (KR), Portuguese (PT), Russian (RU), and American (US).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Accent Bias</span> &#8211;\narises from training data imbalance, where mainstream accents (e.g., American English) dominate and less common ones are under-represented. As a result, models may default to generic accents even when instructed otherwise. Recent research confirms performance disparities across accents in synthetic speech. For example, <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">10.1145/3715275.3732018</span>]</cite> found significant quality gaps when cloning voices with different English accents, potentially reinforcing accent-based discrimination in AI speech. Broader evaluations like AudioTrust <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2025audiotrust</span>]</cite> revealed that accent is a stronger source of unfair bias in audio models than age or gender, Current models generally default to American-accented speech profiles.</p>\n\n",
                "matched_terms": [
                    "age",
                    "accent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"f_{\\text{parse}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mtext>parse</mtext></msub><annotation encoding=\"application/x-tex\">f_{\\text{parse}}</annotation></semantics></math> is realized by prompting the LLM with schema-specific instructions,\nand <math alttext=\"\\theta_{\\text{LLM}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m4\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mtext>LLM</mtext></msub><annotation encoding=\"application/x-tex\">\\theta_{\\text{LLM}}</annotation></semantics></math> are its parameters.\nFor each attribute <math alttext=\"m_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m5\" intent=\":literal\"><semantics><msub><mi>m</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">m_{j}</annotation></semantics></math> (e.g., accent, gender, age), the LLM estimates a posterior distribution, <math alttext=\"P(m_{j}\\mid u;\\theta_{\\text{LLM}}),\\quad m_{j}\\in\\mathcal{V}_{j},\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m6\" intent=\":literal\"><semantics><mrow><mrow><mrow><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>m</mi><mi>j</mi></msub><mo>&#8739;</mo><mrow><mi>u</mi><mo>;</mo><msub><mi>&#952;</mi><mtext>LLM</mtext></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"1.167em\">,</mo><msub><mi>m</mi><mi>j</mi></msub></mrow><mo>&#8712;</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>j</mi></msub></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">P(m_{j}\\mid u;\\theta_{\\text{LLM}}),\\quad m_{j}\\in\\mathcal{V}_{j},</annotation></semantics></math>\nwhere <math alttext=\"\\mathcal{V}_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m7\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{V}_{j}</annotation></semantics></math> denotes the vocabulary/domain of attribute <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m8\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math>.\nThe predicted value is chosen as <math alttext=\"m_{j}=\\arg\\max_{v\\in\\mathcal{V}_{j}}P(m_{j}=v\\mid u;\\theta_{\\text{LLM}}).\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m9\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>m</mi><mi>j</mi></msub><mo>=</mo><mrow><mrow><mi>arg</mi><mo lspace=\"0.167em\">&#8289;</mo><mrow><msub><mi>max</mi><mrow><mi>v</mi><mo>&#8712;</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>j</mi></msub></mrow></msub><mo lspace=\"0.167em\">&#8289;</mo><mi>P</mi></mrow></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>m</mi><mi>j</mi></msub><mo>=</mo><mrow><mi>v</mi><mo>&#8739;</mo><mrow><mi>u</mi><mo>;</mo><msub><mi>&#952;</mi><mtext>LLM</mtext></msub></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">m_{j}=\\arg\\max_{v\\in\\mathcal{V}_{j}}P(m_{j}=v\\mid u;\\theta_{\\text{LLM}}).</annotation></semantics></math>\nIn cases where <math alttext=\"u\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m10\" intent=\":literal\"><semantics><mi>u</mi><annotation encoding=\"application/x-tex\">u</annotation></semantics></math> provides insufficient information to infer slot <math alttext=\"m_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m11\" intent=\":literal\"><semantics><msub><mi>m</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">m_{j}</annotation></semantics></math>, the system falls back to default <math alttext=\"\\pi_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m12\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">\\pi_{j}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "age",
                    "accent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Retrieval-Augmented Accent Prompting (RAAP):</span>\n\nSystem-side accent bias arises when a TTS backbone trained on imbalanced data defaults to majority accents <math alttext=\"\\mathcal{A}_{\\text{dom}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mtext>dom</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{\\text{dom}}</annotation></semantics></math>. To counter this, we introduce Retrieval-Augmented Accent Prompting (RAAP), which selects accent-consistent\nprompts from a curated pool <math alttext=\"\\mathcal{P}=\\{(a_{i},z_{i},s_{i})\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo>,</mo><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}=\\{(a_{i},z_{i},s_{i})\\}</annotation></semantics></math> containing metadata,\ntranscript, and speech. Given target metadata <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m3\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>, we filter <math alttext=\"\\mathcal{P}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><annotation encoding=\"application/x-tex\">\\mathcal{P}</annotation></semantics></math> by\n<math alttext=\"a_{i}\\approx m\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m5\" intent=\":literal\"><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>&#8776;</mo><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">a_{i}\\approx m</annotation></semantics></math> to form <math alttext=\"\\mathcal{P}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{P}_{m}</annotation></semantics></math>, then score each candidate <math alttext=\"s_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m7\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">s_{i}</annotation></semantics></math> using an\nECAPA-TDNN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">desplanques2020ecapa</span>]</cite> for accent confidence\n<math alttext=\"C(s_{i},m)\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m8\" intent=\":literal\"><semantics><mrow><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">C(s_{i},m)\\in[0,1]</annotation></semantics></math>, and select those with the highest scores to guide accented\nspeech generation. Mathematically,</p>\n\n",
                "matched_terms": [
                    "accent",
                    "pool"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Accent Pool Construction:</span>\nWe evaluated our method on twelve English accents: ten accents (Canadian (CA), Chinese (CN), Spanish (ES), British (GB), Indian (IN), Japanese (JP), Korean (KR), Portuguese (PT), Russian (RU), and American (US)) are from the AESRC dataset (52,614 utterances from 528 speakers) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2021accented</span>]</cite>, each contributing 44&#8211;92 speakers and 4.2k&#8211;7k utterances, balanced by gender and ages 15&#8211;69.\nThe remaining two accents are drawn from the SEAME dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lyu2010seame</span>]</cite>: 40 Malaysian speakers (4,358 utterances, aged 20 to 33) and 114 Singaporean speakers (17,750 utterances, aged 18 to 24), with code-switching between English and Mandarin.</p>\n\n",
                "matched_terms": [
                    "accent",
                    "pool",
                    "seame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text and Free-form Instructions Curation</span>: For transcript, GPT-4<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openai.com/index/gpt-4-research/\" title=\"\">https://openai.com/index/gpt-4-research/</a></span></span></span> generated standard sentences across four scenarios (restaurant, university, workplace, supermarket). Speaker instructions derived from accent-pool metadata (accent, age, gender) served both as prompts and ground-truth labels for retrieval. Each instruction&#8211;text pair was then adapted by GPT-4o-mini<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/\" title=\"\">https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/</a></span></span></span>\nand LLaMA-3.1-8B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/meta-llama/Llama-3.1-8B\" title=\"\">https://huggingface.co/meta-llama/Llama-3.1-8B</a></span></span></span> to produce accent-specific variants. Fidelity was evaluated by GPT-5<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openai.com/index/introducing-gpt-5/\" title=\"\">https://openai.com/index/introducing-gpt-5/</a></span></span></span>, which rated text&#8211;accent alignment on a 1&#8211;10 scale.</p>\n\n",
                "matched_terms": [
                    "age",
                    "accent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Object Evaluation Metrics:</span>\nTo quantify the accent level of the generated speech, we calculate accent accuracy using the ECAPA-TDNN accent recognition model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Jzuluaga/accent-id-commonaccent_ecapa\" title=\"\">https://huggingface.co/Jzuluaga/accent-id-commonaccent_ecapa</a></span></span></span>, finetuned on the accent pool (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S3.SS1\" title=\"3.1 Experimental Settings &#8227; 3 Experiments &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>), as in RAAP. Speech quality is measured with NISQA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mittag2021nisqa</span>]</cite> (1&#8211;5 scale) using its pre-trained model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/gabrielmittag/NISQA\" title=\"\">https://github.com/gabrielmittag/NISQA</a></span></span></span>.\nBias/Fairness of the generated speech is evaluated with the Fairness Discrepancy Rate (FDR)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">de2021fairness</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">estevez2023study</span>]</cite>, which quantifies disparities in false alarm/reject rates across accents, with 1 indicating perfect fairness.</p>\n\n",
                "matched_terms": [
                    "accent",
                    "pool"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Subject Evaluation Metrics:</span>\nWe conducted human listening tests on four accented English samples (CN, SG, IN, GB). For each accent, 10 utterances (5F/5M) were generated from three systems: CosyVoice2 (standard text), ParlerTTS (standard text), and CLARITY. Native speakers of each accent (fluent in English), rated corresponding samples (e.g., Chinese listeners for Chinese-accented English<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>The setting assumes that Chinese listeners are more sensitive to Chinese-accented English speech.</span></span></span>). There were 4F/4M listeners for CN/SG and 2F/2M for IN/GB. Samples were evaluated on a 1&#8211;5 scale for naturalness, accent accuracy, age, gender consistency, and overall instruction match.</p>\n\n",
                "matched_terms": [
                    "age",
                    "accent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">1. RAAP and Contextual Linguistic Text Adaptation:</span>\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S3.F3\" title=\"Figure 3 &#8227; 3.1 Experimental Settings &#8227; 3 Experiments &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the RAAP accuracy (left) and the adapted scores generated by GPT-5 and LLaMA models (right). RAAP achieves nearly 100% accuracy for most accents in terms of accent and gender retrieval, while age prediction is less accurate but acceptable, given its known difficulty.\nFor the LLM-as-judge scores, we observe that the standard text already achieves reasonable accent scores, with all scores above 6. After adaptation using GPT-4o-mini, four accents (GB, MY, SG, and US) show improved adaptation scores. Interestingly, these accents correspond to locations where the language of daily communciation tends to be English. A possible explanation is that for these speakers, the model had higher confidence in modifying the text, while the training corpus distribution likely reinforces this behaviour, which GPT-4 effectively captures. In contrast, LLaMA shows decreased accent scores after adaptation, although we doubt these results since the judge is GPT-5, not LLaMA, and continue to evaluate both adapted texts.</p>\n\n",
                "matched_terms": [
                    "age",
                    "accent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide additional information about the proposed system, including details of the experimental protocol, as well as supplementary results on the accent pool data and generated speech.</p>\n\n",
                "matched_terms": [
                    "accent",
                    "pool"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:234.1pt;height:145.2pt;vertical-align:-70.1pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Accent</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">#Spk</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">#Utt</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">#F</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">#M</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Age</span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td\"/>\n<span class=\"ltx_td ltx_th ltx_th_column\"/>\n<span class=\"ltx_td ltx_th ltx_th_column\"/>\n<span class=\"ltx_td ltx_th ltx_th_column\"/>\n<span class=\"ltx_td ltx_th ltx_th_column\"/>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">(min/max)</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center ltx_border_t\">CA</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">44</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">4,400</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">22</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">22</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">18/49</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center\">CN</span>\n<span class=\"ltx_td ltx_align_center\">50</span>\n<span class=\"ltx_td ltx_align_center\">5,000</span>\n<span class=\"ltx_td ltx_align_center\">26</span>\n<span class=\"ltx_td ltx_align_center\">24</span>\n<span class=\"ltx_td ltx_align_center\">17/38</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center\">ES</span>\n<span class=\"ltx_td ltx_align_center\">44</span>\n<span class=\"ltx_td ltx_align_center\">4,400</span>\n<span class=\"ltx_td ltx_align_center\">22</span>\n<span class=\"ltx_td ltx_align_center\">22</span>\n<span class=\"ltx_td ltx_align_center\">19/45</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center\">GB</span>\n<span class=\"ltx_td ltx_align_center\">92</span>\n<span class=\"ltx_td ltx_align_center\">9,031</span>\n<span class=\"ltx_td ltx_align_center\">42</span>\n<span class=\"ltx_td ltx_align_center\">50</span>\n<span class=\"ltx_td ltx_align_center\">16/65</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center\">IN</span>\n<span class=\"ltx_td ltx_align_center\">42</span>\n<span class=\"ltx_td ltx_align_center\">4,200</span>\n<span class=\"ltx_td ltx_align_center\">22</span>\n<span class=\"ltx_td ltx_align_center\">20</span>\n<span class=\"ltx_td ltx_align_center\">15/37</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center\">JP</span>\n<span class=\"ltx_td ltx_align_center\">46</span>\n<span class=\"ltx_td ltx_align_center\">4,600</span>\n<span class=\"ltx_td ltx_align_center\">23</span>\n<span class=\"ltx_td ltx_align_center\">23</span>\n<span class=\"ltx_td ltx_align_center\">18/69</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center\">KR</span>\n<span class=\"ltx_td ltx_align_center\">46</span>\n<span class=\"ltx_td ltx_align_center\">4,600</span>\n<span class=\"ltx_td ltx_align_center\">23</span>\n<span class=\"ltx_td ltx_align_center\">23</span>\n<span class=\"ltx_td ltx_align_center\">19/39</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center\">PT</span>\n<span class=\"ltx_td ltx_align_center\">53</span>\n<span class=\"ltx_td ltx_align_center\">5,283</span>\n<span class=\"ltx_td ltx_align_center\">27</span>\n<span class=\"ltx_td ltx_align_center\">26</span>\n<span class=\"ltx_td ltx_align_center\">18/62</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center\">RU</span>\n<span class=\"ltx_td ltx_align_center\">41</span>\n<span class=\"ltx_td ltx_align_center\">4,100</span>\n<span class=\"ltx_td ltx_align_center\">21</span>\n<span class=\"ltx_td ltx_align_center\">20</span>\n<span class=\"ltx_td ltx_align_center\">18/41</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">US</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">70</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">7,000</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">37</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">33</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">15/63</span></span>\n</span>\n</span></span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "spk",
                    "minmax",
                    "age",
                    "accent",
                    "utt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remaining two accents, Malaysian (MY) and Singaporean (SG), are represented by 22,108 utterances selected from the SEAME dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lyu2010seame</span>]</cite>, a large-scale, code-switching English&#8211;Mandarin corpus collected from spontaneous conversations in Malaysia and Singapore. It contains speech from both Malaysian and Singaporean speakers, with detailed accent and speaker annotations. To construct our subset, we extracted English-dominant utterances from both conversational and interview recordings, retaining only those labeled as English or code-switched (EN and CS) in the transcript metadata. Purely Mandarin (ZH) segments were excluded. Each utterance was required to contain at least 5 words, with a maximum of 100 utterances per speaker, prioritizing longer segments. After extraction, only speakers with English accent confidence scores greater than 0.9 were retained. These filtering criteria yielded a balanced selection of Malaysian and Singaporean speakers across both genders and an age range of 18 to 33 years.</p>\n\n",
                "matched_terms": [
                    "age",
                    "accent",
                    "seame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dataset spans <span class=\"ltx_text ltx_font_italic\">three conversational scenarios</span>, each chosen to represent a distinct communicative domain where accent and prosody vary naturally.\nFor each scenario, we defined ten standard sentences and paired them with twelve target accents\n(<span class=\"ltx_text ltx_font_typewriter\">CA, CN, ES, GB, IN, JP, KR, MY, PT, RU, SG, US</span>).\nEach accent was then combined with ten randomly sampled metadata combinations (age, gender, and language background),\nresulting in approximately 1,200 implicit instructions per scenario and 3,600 in total.</p>\n\n",
                "matched_terms": [
                    "age",
                    "accent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the performance of the accent preservation on the generated speech, we fine-tuned ECAPA-TDNN model, pretrained on the CommonAccent corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zuluaga2023commonaccent</span>]</cite>, for 12-class accent classification (CA, CN, GB, IN, JP, KR, MY, PT, RU, SG, ES, US) using the AESRC2020 and SEAME datasets. The model extracts 80-dimensional log-Mel filterbank features (25 ms window, 10 ms hop) followed by mean-variance normalization.\nThe ECAPA-TDNN backbone (embedding dimension = 192, <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>6.2 M parameters) was kept frozen, and a single linear layer with 12 output units was fine-tuned.\nWe used a batch size of 8, AdamW optimizer with a learning rate of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math>, and AAM loss as the objective. The model was trained for 15 epochs with a random seed of 42 on an NVIDIA RTX 4070 GPU. The dataset was randomly divided into 80% for training and 20% for validation.</p>\n\n",
                "matched_terms": [
                    "accent",
                    "seame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#A1.F10\" title=\"Figure 10 &#8227; A.1.3 Adapted Text Generation &#8227; A.1 Experimental Protocol &#8227; Appendix A Appendix &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> presents both the per-accent classification accuracy and the confusion matrix on the validation set of the accent pool data.\nThe model achieves high recognition performance for several distinct accent categories, such as Indian (IN, 99%), Singaporean (SG, 91%), and British (GB, 89%) English.\nHowever, recognition accuracy decreases noticeably for Japanese (JP, 50%), Chinese (CN, 59%), and Portuguese (PT, 57%) accents, reflecting the challenges posed by their acoustic and phonological proximity to other varieties.</p>\n\n",
                "matched_terms": [
                    "accent",
                    "pool"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#A1.F11\" title=\"Figure 11 &#8227; A.1.3 Adapted Text Generation &#8227; A.1 Experimental Protocol &#8227; Appendix A Appendix &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> shows the average NISQA score for each accent on the accent pool data.\nThe generated speech maintains high perceptual quality across all accents,\nwith scores ranging from 4.0 to 4.6 on a five-point scale.\nSingaporean (SG, 4.57), Japanese (JP, 4.41), and British (GBR, 4.40) accents\nachieve the best quality, indicating clear and natural synthesis.\nSpanish (ESP, 4.00) and Chinese (CHN, 4.13) exhibit slightly lower scores,\nlikely due to accent-specific variability or limited training coverage.\nDespite these differences, the narrow score range demonstrates strong cross-accent generalization and consistent perceptual quality.</p>\n\n",
                "matched_terms": [
                    "accent",
                    "pool"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings suggest that the accent pool data is of sufficient quality to be used as prompt speech in the speech generation stage.</p>\n\n",
                "matched_terms": [
                    "accent",
                    "pool"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation Study:</span> as shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#A1.F15\" title=\"Figure 15 &#8227; A.2.2 Speech Quality Evaluation &#8227; A.2 Additional Results on Accent Pool data &#8227; Appendix A Appendix &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, this ablation study details the impact of various model configurations on accent accuracy and systemic bias, demonstrating a phased improvement across experimental setups. The initial phase, which involved replacing the near-silent prompt audio with accent pool sourced prompt audio dynamically selected via instruction extracted speaker features, alone yielded a significant boost in both accent accuracy and fairness metrics. Building upon this foundation, performance saw further incremental gains through the subsequent integration of target text (standard) to prompt transcription similarity and the strategic use of prompt speech exhibiting a more pronounced accent (higher accent score). As mentioned in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#A1.SS3.SSS1\" title=\"A.3.1 Results on Retrieved Prompt Speech &#8227; A.3 Additional Results on Generated Speech &#8227; Appendix A Appendix &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">A.3.1</span></a>, we also compared the results of standard and adapted text similarity. Ultimately, the most effective configurations (proposed) were achieved by leveraging Llama or GPT to perform adaptation on the target text, resulting in the highest overall accuracy and the lowest observed degree of bias among all tested methodologies.</p>\n\n",
                "matched_terms": [
                    "accent",
                    "pool"
                ]
            }
        ]
    },
    "A1.T4": {
        "source_file": "CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation",
        "caption": "Table 4: Overview of scenarios and metadata coverage in the curated instruction dataset.",
        "body": "Scenario\nDescription\nUnique Metadata Combinations\nApprox. Instructions\n\n\n\n\nRestaurant / Coffee Shop\nInformal, service-oriented interactions such as ordering or small talk.\n575\n\n∼\\sim1,200\n\n\nUniversity / School\nFormal and instructional contexts such as lectures or classroom exchanges.\n585\n\n∼\\sim1,200\n\n\nWorkplace / Office\nProfessional settings emphasizing collaboration and presentation.\n578\n\n∼\\sim1,200\n\n\nTotal\n—\n—\n3,600",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Scenario</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Description</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Unique Metadata Combinations</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Approx. Instructions</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Restaurant / Coffee Shop</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Informal, service-oriented interactions such as ordering or small talk.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">575</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>1,200</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">University / School</td>\n<td class=\"ltx_td ltx_align_left\">Formal and instructional contexts such as lectures or classroom exchanges.</td>\n<td class=\"ltx_td ltx_align_center\">585</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>1,200</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Workplace / Office</td>\n<td class=\"ltx_td ltx_align_left\">Professional settings emphasizing collaboration and presentation.</td>\n<td class=\"ltx_td ltx_align_center\">578</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>1,200</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Total</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">&#8212;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3,600</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "lectures",
            "contexts",
            "∼sim1200",
            "formal",
            "overview",
            "restaurant",
            "scenario",
            "presentation",
            "exchanges",
            "unique",
            "office",
            "interactions",
            "informal",
            "workplace",
            "school",
            "university",
            "collaboration",
            "instruction",
            "shop",
            "serviceoriented",
            "description",
            "settings",
            "classroom",
            "such",
            "emphasizing",
            "dataset",
            "scenarios",
            "combinations",
            "professional",
            "approx",
            "coverage",
            "ordering",
            "instructional",
            "coffee",
            "small",
            "total",
            "talk",
            "metadata",
            "instructions",
            "curated"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#A1.T4\" title=\"Table 4 &#8227; A.1.1 Accent Pool Data Generation &#8227; A.1 Experimental Protocol &#8227; Appendix A Appendix &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarizes the three scenarios and their corresponding metadata diversity.\nAll implicit instructions were generated automatically using the GPT-4 language model <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">openai2023gpt4</span>]</cite>.\nFor each pair of a base sentence and sampled metadata, the model was prompted to provide paralinguistic guidance that specifies\n<em class=\"ltx_emph ltx_font_italic\">how</em> the sentence should be spoken.\nThe generation prompt was structured as follows:</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Instruction-guided TTS systems <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lyth2024natural</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhou2024voxinstruct</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025spark</span>]</cite> taking two inputs: 1) transcript text and 2) speaker descriptions of desired speech characteristics, have achieved promising instruction-following abilities.\nHowever, when prompted with natural language instructions, such TTS systems are susceptible to various biases.\nWhile gender bias has been examined by prior work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuan-lee-2025-gender</span>]</cite>, to the best of our knowledge, no study has jointly addressed accent and linguistic bias. The two are interrelated but accent bias stems from the TTS system, while linguistic bias is often implicit in user input.\nThe <span class=\"ltx_text ltx_font_italic\">dual bias</span>, as illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, reduces generated speech authenticity by misaligning <span class=\"ltx_text ltx_font_italic\">what</span> is said and <span class=\"ltx_text ltx_font_italic\">how</span> it is spoken&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">burns2019speaking</span>]</cite>.\nFor example, instructing TTS to speak in a British Accent but with American English oriented text<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Note that traditional TTS assumes fixed text. This work addresses scenarios where users want to generate speech in a specific accent but cannot provide text in that variant of English. Hence, the input text must be slightly adapted to the accent specified in the speaker instruction.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "scenarios",
                    "such",
                    "instructions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM-Guided Instruction Parsing:</span>\nLet <math alttext=\"u\\in\\mathcal{U}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>u</mi><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi></mrow><annotation encoding=\"application/x-tex\">u\\in\\mathcal{U}</annotation></semantics></math> be a free-form user instruction in natural language, and <math alttext=\"m\\in\\mathcal{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi></mrow><annotation encoding=\"application/x-tex\">m\\in\\mathcal{M}</annotation></semantics></math> the structured metadata schema,\n<math alttext=\"m=(\\text{accent}\\in\\mathcal{A},;\\text{gender}\\in{M,F},;\\text{age}\\in\\mathbb{N},\\ldots)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.p2.m3\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mtext>accent</mtext><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo>,</mo><mo>;</mo><mtext>gender</mtext><mo>&#8712;</mo><mi>M</mi><mo>,</mo><mi>F</mi><mo>,</mo><mo>;</mo><mtext>age</mtext><mo>&#8712;</mo><mi>&#8469;</mi><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m=(\\text{accent}\\in\\mathcal{A},;\\text{gender}\\in{M,F},;\\text{age}\\in\\mathbb{N},\\ldots)</annotation></semantics></math>,\nwhere <math alttext=\"\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><annotation encoding=\"application/x-tex\">\\mathcal{A}</annotation></semantics></math> is the set of supported accents. The challenge is that <math alttext=\"u\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m5\" intent=\":literal\"><semantics><mi>u</mi><annotation encoding=\"application/x-tex\">u</annotation></semantics></math> may encode attributes explicitly, vaguely, or implicitly; for example, instead of the prescriptive instructions shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S2.F2\" title=\"Figure 2 &#8227; 2 The Proposed CLARITY &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, there may be one open to interpretation such as <math alttext=\"u=\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m6\" intent=\":literal\"><semantics><mrow><mi>u</mi><mo>=</mo><mi/></mrow><annotation encoding=\"application/x-tex\">u=</annotation></semantics></math> &#8220;<span class=\"ltx_text ltx_font_italic\">Read this like a local in Singapore</span>&#8221; (a locale with multiple races and accents).</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "metadata",
                    "such",
                    "instructions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Retrieval-Augmented Accent Prompting (RAAP):</span>\n\nSystem-side accent bias arises when a TTS backbone trained on imbalanced data defaults to majority accents <math alttext=\"\\mathcal{A}_{\\text{dom}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mtext>dom</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{\\text{dom}}</annotation></semantics></math>. To counter this, we introduce Retrieval-Augmented Accent Prompting (RAAP), which selects accent-consistent\nprompts from a curated pool <math alttext=\"\\mathcal{P}=\\{(a_{i},z_{i},s_{i})\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo>,</mo><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}=\\{(a_{i},z_{i},s_{i})\\}</annotation></semantics></math> containing metadata,\ntranscript, and speech. Given target metadata <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m3\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>, we filter <math alttext=\"\\mathcal{P}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><annotation encoding=\"application/x-tex\">\\mathcal{P}</annotation></semantics></math> by\n<math alttext=\"a_{i}\\approx m\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m5\" intent=\":literal\"><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>&#8776;</mo><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">a_{i}\\approx m</annotation></semantics></math> to form <math alttext=\"\\mathcal{P}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{P}_{m}</annotation></semantics></math>, then score each candidate <math alttext=\"s_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m7\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">s_{i}</annotation></semantics></math> using an\nECAPA-TDNN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">desplanques2020ecapa</span>]</cite> for accent confidence\n<math alttext=\"C(s_{i},m)\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m8\" intent=\":literal\"><semantics><mrow><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">C(s_{i},m)\\in[0,1]</annotation></semantics></math>, and select those with the highest scores to guide accented\nspeech generation. Mathematically,</p>\n\n",
                "matched_terms": [
                    "metadata",
                    "curated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text and Free-form Instructions Curation</span>: For transcript, GPT-4<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openai.com/index/gpt-4-research/\" title=\"\">https://openai.com/index/gpt-4-research/</a></span></span></span> generated standard sentences across four scenarios (restaurant, university, workplace, supermarket). Speaker instructions derived from accent-pool metadata (accent, age, gender) served both as prompts and ground-truth labels for retrieval. Each instruction&#8211;text pair was then adapted by GPT-4o-mini<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/\" title=\"\">https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/</a></span></span></span>\nand LLaMA-3.1-8B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/meta-llama/Llama-3.1-8B\" title=\"\">https://huggingface.co/meta-llama/Llama-3.1-8B</a></span></span></span> to produce accent-specific variants. Fidelity was evaluated by GPT-5<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openai.com/index/introducing-gpt-5/\" title=\"\">https://openai.com/index/introducing-gpt-5/</a></span></span></span>, which rated text&#8211;accent alignment on a 1&#8211;10 scale.</p>\n\n",
                "matched_terms": [
                    "workplace",
                    "scenarios",
                    "university",
                    "restaurant",
                    "metadata",
                    "instructions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remaining two accents, Malaysian (MY) and Singaporean (SG), are represented by 22,108 utterances selected from the SEAME dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lyu2010seame</span>]</cite>, a large-scale, code-switching English&#8211;Mandarin corpus collected from spontaneous conversations in Malaysia and Singapore. It contains speech from both Malaysian and Singaporean speakers, with detailed accent and speaker annotations. To construct our subset, we extracted English-dominant utterances from both conversational and interview recordings, retaining only those labeled as English or code-switched (EN and CS) in the transcript metadata. Purely Mandarin (ZH) segments were excluded. Each utterance was required to contain at least 5 words, with a maximum of 100 utterances per speaker, prioritizing longer segments. After extraction, only speakers with English accent confidence scores greater than 0.9 were retained. These filtering criteria yielded a balanced selection of Malaysian and Singaporean speakers across both genders and an age range of 18 to 33 years.</p>\n\n",
                "matched_terms": [
                    "metadata",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To develop realistic and context-sensitive guidance for instruction-driven speech synthesis,\nwe curated a dataset of <span class=\"ltx_text ltx_font_italic\">3,600 implicit instructions</span> that describe how a sentence should be spoken under different social and accent conditions.\nEach instruction captures paralinguistic elements such as tone, pacing, rhythm, and articulation, which are crucial for generating speech that aligns with human accent perception.</p>\n\n",
                "matched_terms": [
                    "such",
                    "dataset",
                    "instruction",
                    "instructions",
                    "curated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dataset spans <span class=\"ltx_text ltx_font_italic\">three conversational scenarios</span>, each chosen to represent a distinct communicative domain where accent and prosody vary naturally.\nFor each scenario, we defined ten standard sentences and paired them with twelve target accents\n(<span class=\"ltx_text ltx_font_typewriter\">CA, CN, ES, GB, IN, JP, KR, MY, PT, RU, SG, US</span>).\nEach accent was then combined with ten randomly sampled metadata combinations (age, gender, and language background),\nresulting in approximately 1,200 implicit instructions per scenario and 3,600 in total.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "total",
                    "scenarios",
                    "scenario",
                    "metadata",
                    "instructions",
                    "combinations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each implicit instruction acts as a <em class=\"ltx_emph ltx_font_italic\">behavioral abstraction</em> derived from the metadata.\nIt does not restate all demographic attributes explicitly;\ninstead, GPT-4 selectively emphasizes the subset that influences speech prosody and delivery.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "metadata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across all 3,600 instructions, the gender and age distributions are balanced,\nensuring broad representation of speaker profiles for instruction-guided text-to-speech synthesis.\nIn total, the dataset includes 1,804 male speakers (50.1%) and 1,796 female speakers (49.9%). The age range spans from 15 to 69 years, with a mean of 32.3 years, a median of 30.0 years, and a standard deviation of 11.9.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#A1.F8\" title=\"Figure 8 &#8227; A.1.1 Accent Pool Data Generation &#8227; A.1 Experimental Protocol &#8227; Appendix A Appendix &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> illustrates the gender and age distributions within the curated dataset.</p>\n\n",
                "matched_terms": [
                    "total",
                    "instructions",
                    "dataset",
                    "curated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To parse user instructions, generate localised text, and select the best final spoken text, a total of four LLMs were used. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#A1.F9\" title=\"Figure 9 &#8227; A.1.2 Instruction Curation &#8227; A.1 Experimental Protocol &#8227; Appendix A Appendix &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> illustrates the entire process from the initial user instruction to the final selection of the localised text.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "total",
                    "instructions"
                ]
            }
        ]
    },
    "A1.T5": {
        "source_file": "CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation",
        "caption": "Table 5: Ablation study of RAAP, comparing text similarity using standard user-provided text or LLM-adapted text.",
        "body": "Text Sim.\nAccent Score\nText Apt.\nACC (%)\nNISQA\n\n\n✗\n✗\n✗\n45.60\n4.21\n\n\n\n\nAdapted\n✓\nLLaMA\n62.23\n4.30\n\n\nStandard\n✓\nLLaMA\n62.47\n4.33\n\n\nAdapted\n✓\nGPT\n62.26\n4.35\n\n\nStandard\n✓\nGPT\n63.36\n4.31",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Text Sim.</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Accent Score</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Text Apt.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">ACC (%)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">NISQA</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">&#10007;</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">&#10007;</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">&#10007;</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">45.60</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">4.21</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Adapted</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">&#10003;</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">LLaMA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">62.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.30</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Standard</td>\n<td class=\"ltx_td ltx_align_left\">&#10003;</td>\n<td class=\"ltx_td ltx_align_left\">LLaMA</td>\n<td class=\"ltx_td ltx_align_center\">62.47</td>\n<td class=\"ltx_td ltx_align_center\">4.33</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Adapted</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">&#10003;</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">GPT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">62.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.35</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Standard</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">&#10003;</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">GPT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">63.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4.31</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "raap",
            "standard",
            "apt",
            "acc",
            "gpt",
            "study",
            "text",
            "ablation",
            "userprovided",
            "accent",
            "adapted",
            "llama",
            "sim",
            "similarity",
            "score",
            "comparing",
            "nisqa",
            "llmadapted"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The objective results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#A1.T5\" title=\"Table 5 &#8227; A.2.2 Speech Quality Evaluation &#8227; A.2 Additional Results on Accent Pool data &#8227; Appendix A Appendix &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, where adapted text yields slightly lower accent accuracy for both LLaMA and GPT, while achieving comparable NISQA scores. In the subjective listening test, GPT-based methods using either adapted or standard text were evaluated. The violin plots in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#A1.F12\" title=\"Figure 12 &#8227; A.2.2 Speech Quality Evaluation &#8227; A.2 Additional Results on Accent Pool data &#8227; Appendix A Appendix &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> show that for SG, the results are almost identical between standard and adapted text; for CN, a similar trend is observed except that MOS with standard text achieves slightly better performance; while for GB and IN, GPT with standard text as the basis for text similarity calculation tends to obtain higher scores across all perceptual evaluation metrics.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Instruction-guided text-to-speech (TTS) research has reached a maturity level where excellent speech generation quality is possible on demand, yet two coupled biases persist: <span class=\"ltx_text ltx_font_italic\">accent bias</span>, where models default to dominant phonetic patterns, and <span class=\"ltx_text ltx_font_italic\">linguistic bias</span>, where dialect-specific lexical and cultural cues are ignored.\nThese biases are interdependent, as authentic accent generation requires both accent fidelity and localized text.\nWe present <span class=\"ltx_text ltx_font_bold\">CLARITY</span> (<span class=\"ltx_text ltx_font_bold\">C</span>ontextual <span class=\"ltx_text ltx_font_bold\">L</span>inguistic <span class=\"ltx_text ltx_font_bold\">A</span>daptation and <span class=\"ltx_text ltx_font_bold\">R</span>etrieval for <span class=\"ltx_text ltx_font_bold\">I</span>nclusive <span class=\"ltx_text ltx_font_bold\">T</span>TS s<span class=\"ltx_text ltx_font_bold\">Y</span>nthesis), a backbone-agnostic framework that addresses these biases through dual-signal optimization: (i) contextual linguistic adaptation that localizes input text to the target dialect, and (ii) retrieval-augmented accent prompting (RAAP) that supplies accent-consistent speech prompts. Across twelve English accents, CLARITY improves accent accuracy and fairness while maintaining strong perceptual quality<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code and audio samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/ICT-SIT/CLARITY\" title=\"\">https://github.com/ICT-SIT/CLARITY</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "raap",
                    "accent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Instruction-guided TTS systems <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lyth2024natural</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhou2024voxinstruct</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025spark</span>]</cite> taking two inputs: 1) transcript text and 2) speaker descriptions of desired speech characteristics, have achieved promising instruction-following abilities.\nHowever, when prompted with natural language instructions, such TTS systems are susceptible to various biases.\nWhile gender bias has been examined by prior work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuan-lee-2025-gender</span>]</cite>, to the best of our knowledge, no study has jointly addressed accent and linguistic bias. The two are interrelated but accent bias stems from the TTS system, while linguistic bias is often implicit in user input.\nThe <span class=\"ltx_text ltx_font_italic\">dual bias</span>, as illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, reduces generated speech authenticity by misaligning <span class=\"ltx_text ltx_font_italic\">what</span> is said and <span class=\"ltx_text ltx_font_italic\">how</span> it is spoken&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">burns2019speaking</span>]</cite>.\nFor example, instructing TTS to speak in a British Accent but with American English oriented text<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Note that traditional TTS assumes fixed text. This work addresses scenarios where users want to generate speech in a specific accent but cannot provide text in that variant of English. Hence, the input text must be slightly adapted to the accent specified in the speaker instruction.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "accent",
                    "adapted",
                    "study"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CLARITY</span>:\nTo jointly mitigate this <span class=\"ltx_text ltx_font_italic\">dual bias</span>, we present <span class=\"ltx_text ltx_font_bold\">C</span>ontextual <span class=\"ltx_text ltx_font_bold\">L</span>inguistic <span class=\"ltx_text ltx_font_bold\">A</span>daptation and <span class=\"ltx_text ltx_font_bold\">R</span>etrieval for <span class=\"ltx_text ltx_font_bold\">I</span>nclusive <span class=\"ltx_text ltx_font_bold\">T</span>TS s<span class=\"ltx_text ltx_font_bold\">Y</span>nthesis (CLARITY).\nOur proposed CLARITY is a backbone-agnostic framework to enhance zero-shot TTS by providing richer guidance for both linguistic content and accent style. Its novelty lies in a two-signal optimization to jointly mitigate the two biases through:\n(i) an LLM-guided text adaptation module to localize input to the target dialect, and\n(ii) a metadata-driven retrieval mechanism to supply accent-consistent prompts.</p>\n\n",
                "matched_terms": [
                    "text",
                    "accent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our Contributions. </span>\nWe introduce a sociolinguistic innovation in TTS by using large language models (LLMs) for on-the-fly text style transfer, leveraging their knowledge of dialects and slang to enhance authenticity.\nBy pairing this with retrieval-augmented prompting, CLARITY achieves accent control and dialect-aware synthesis.\nTo our knowledge, this is the first framework to jointly address user-side linguistic bias and system-side accent bias. Objective and subjective evaluations show that CLARITY improves fairness, accent fidelity, and overall authenticity.</p>\n\n",
                "matched_terms": [
                    "text",
                    "accent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The CLARITY framework promotes inclusive TTS by mitigating linguistic and accent biases through two signals: an adapted text <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> and an accent-consistent prompt <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math>, as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S2.F2\" title=\"Figure 2 &#8227; 2 The Proposed CLARITY &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nWe cast this as a two-signal optimization problem, aiming to maximize both linguistic and accent fidelity under the synthesis constraint of a backbone TTS model.\nFormally, the objective is defined as\n   </p>\n\n",
                "matched_terms": [
                    "text",
                    "accent",
                    "adapted"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"J_{\\text{LLM}}(x^{*},m)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>J</mi><mtext>LLM</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mo>&#8727;</mo></msup><mo>,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">J_{\\text{LLM}}(x^{*},m)</annotation></semantics></math> denotes the judgment score assigned by\nan LLM-as-a-Judge evaluating whether the adapted text <math alttext=\"x^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m4\" intent=\":literal\"><semantics><msup><mi>x</mi><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">x^{*}</annotation></semantics></math> aligns with the requested metadata <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m5\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>,\nand <math alttext=\"C(s^{*},m)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m6\" intent=\":literal\"><semantics><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>s</mi><mo>&#8727;</mo></msup><mo>,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">C(s^{*},m)</annotation></semantics></math> is the accent recognition confidence score that the retrieved prompt <math alttext=\"s^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m7\" intent=\":literal\"><semantics><msup><mi>s</mi><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">s^{*}</annotation></semantics></math> matches the target accent attributes. The synthesis function <math alttext=\"g_{\\text{TTS}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m8\" intent=\":literal\"><semantics><msub><mi>g</mi><mtext>TTS</mtext></msub><annotation encoding=\"application/x-tex\">g_{\\text{TTS}}</annotation></semantics></math> represents any zero-shot backbone TTS model.</p>\n\n",
                "matched_terms": [
                    "text",
                    "accent",
                    "adapted",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure that the adapted text <math alttext=\"x^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m1\" intent=\":literal\"><semantics><msup><mi>x</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">x^{\\prime}</annotation></semantics></math> is linguistically faithful to the target dialect <math alttext=\"\\mathcal{D}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{D}_{t}</annotation></semantics></math>,\nwe generate candidate adaptations <math alttext=\"\\{x^{\\prime}_{1},x^{\\prime}_{2},\\dots,x^{\\prime}_{K}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>x</mi><mn>1</mn><mo>&#8242;</mo></msubsup><mo>,</mo><msubsup><mi>x</mi><mn>2</mn><mo>&#8242;</mo></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>x</mi><mi>K</mi><mo>&#8242;</mo></msubsup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{x^{\\prime}_{1},x^{\\prime}_{2},\\dots,x^{\\prime}_{K}\\}</annotation></semantics></math> using different LLMs (e.g., GPT and LLaMA).\nWe compare three candidates: the original standard text <math alttext=\"x_{\\text{std}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m4\" intent=\":literal\"><semantics><msub><mi>x</mi><mtext>std</mtext></msub><annotation encoding=\"application/x-tex\">x_{\\text{std}}</annotation></semantics></math> and adapted texts\n<math alttext=\"k\\in\\mathcal{K}=\\{\\text{GPT},\\text{LLaMA}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m5\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119974;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mtext>GPT</mtext><mo>,</mo><mtext>LLaMA</mtext><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">k\\in\\mathcal{K}=\\{\\text{GPT},\\text{LLaMA}\\}</annotation></semantics></math> produced by different LLMs.\nAn LLM-as-a-Judge assigns each candidate a score\n<math alttext=\"J_{\\text{LLM}}(x,m)\\in[0,10],\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m6\" intent=\":literal\"><semantics><mrow><mrow><mrow><msub><mi>J</mi><mtext>LLM</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>10</mn><mo stretchy=\"false\">]</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">J_{\\text{LLM}}(x,m)\\in[0,10],</annotation></semantics></math>\nwhich reflects how well <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m7\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> aligns with the metadata <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m8\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>, particularly the accent attribute.\nThe final text is then chosen as the candidate with the highest judgment score:</p>\n\n",
                "matched_terms": [
                    "standard",
                    "gpt",
                    "text",
                    "accent",
                    "adapted",
                    "llama",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Retrieval-Augmented Accent Prompting (RAAP):</span>\n\nSystem-side accent bias arises when a TTS backbone trained on imbalanced data defaults to majority accents <math alttext=\"\\mathcal{A}_{\\text{dom}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mtext>dom</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{\\text{dom}}</annotation></semantics></math>. To counter this, we introduce Retrieval-Augmented Accent Prompting (RAAP), which selects accent-consistent\nprompts from a curated pool <math alttext=\"\\mathcal{P}=\\{(a_{i},z_{i},s_{i})\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo>,</mo><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}=\\{(a_{i},z_{i},s_{i})\\}</annotation></semantics></math> containing metadata,\ntranscript, and speech. Given target metadata <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m3\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>, we filter <math alttext=\"\\mathcal{P}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><annotation encoding=\"application/x-tex\">\\mathcal{P}</annotation></semantics></math> by\n<math alttext=\"a_{i}\\approx m\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m5\" intent=\":literal\"><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>&#8776;</mo><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">a_{i}\\approx m</annotation></semantics></math> to form <math alttext=\"\\mathcal{P}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{P}_{m}</annotation></semantics></math>, then score each candidate <math alttext=\"s_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m7\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">s_{i}</annotation></semantics></math> using an\nECAPA-TDNN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">desplanques2020ecapa</span>]</cite> for accent confidence\n<math alttext=\"C(s_{i},m)\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m8\" intent=\":literal\"><semantics><mrow><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">C(s_{i},m)\\in[0,1]</annotation></semantics></math>, and select those with the highest scores to guide accented\nspeech generation. Mathematically,</p>\n\n",
                "matched_terms": [
                    "raap",
                    "accent",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further align the text content of the prompt speech with the user&#8217;s text input,\nRAAP calculates the text similarity between the prompt speech candidates and the\nuser-provided (standard) text input using\n<math alttext=\"\\cos(\\phi(z_{i}),\\phi(x))\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m1\" intent=\":literal\"><semantics><mrow><mi>cos</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\cos(\\phi(z_{i}),\\phi(x))</annotation></semantics></math>,\nwhere <math alttext=\"\\phi(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m2\" intent=\":literal\"><semantics><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi(\\cdot)</annotation></semantics></math> is a TF&#8211;IDF&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">salton1988term</span>]</cite> text embedding representation\nand <math alttext=\"\\cos(\\cdot,\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m3\" intent=\":literal\"><semantics><mrow><mi>cos</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo rspace=\"0em\">,</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\cos(\\cdot,\\cdot)</annotation></semantics></math> denotes cosine similarity.\nSpecifically, we define the effective ranking score as</p>\n\n",
                "matched_terms": [
                    "raap",
                    "standard",
                    "userprovided",
                    "text",
                    "similarity",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Backbone-agnostic Guided Synthesis:</span>\nThe final stage of CLARITY aligns the two signals: the adapted text <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p8.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> and the accent-consistent prompt <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p8.m2\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math>, for guided synthesis. Let <math alttext=\"g_{\\text{TTS}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p8.m3\" intent=\":literal\"><semantics><msub><mi>g</mi><mtext>TTS</mtext></msub><annotation encoding=\"application/x-tex\">g_{\\text{TTS}}</annotation></semantics></math> be the backbone TTS with parameters <math alttext=\"\\theta_{\\text{TTS}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p8.m4\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mtext>TTS</mtext></msub><annotation encoding=\"application/x-tex\">\\theta_{\\text{TTS}}</annotation></semantics></math>. As a backbone-agnostic framework, CLARITY can use any instruction-guided, zero-shot TTS model; here we adopt the SOTA CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>]</cite> with its zero-shot functionality. The synthesis is defined as</p>\n\n",
                "matched_terms": [
                    "text",
                    "adapted"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text and Free-form Instructions Curation</span>: For transcript, GPT-4<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openai.com/index/gpt-4-research/\" title=\"\">https://openai.com/index/gpt-4-research/</a></span></span></span> generated standard sentences across four scenarios (restaurant, university, workplace, supermarket). Speaker instructions derived from accent-pool metadata (accent, age, gender) served both as prompts and ground-truth labels for retrieval. Each instruction&#8211;text pair was then adapted by GPT-4o-mini<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/\" title=\"\">https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/</a></span></span></span>\nand LLaMA-3.1-8B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/meta-llama/Llama-3.1-8B\" title=\"\">https://huggingface.co/meta-llama/Llama-3.1-8B</a></span></span></span> to produce accent-specific variants. Fidelity was evaluated by GPT-5<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openai.com/index/introducing-gpt-5/\" title=\"\">https://openai.com/index/introducing-gpt-5/</a></span></span></span>, which rated text&#8211;accent alignment on a 1&#8211;10 scale.</p>\n\n",
                "matched_terms": [
                    "text",
                    "standard",
                    "accent",
                    "adapted"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline Systems</span>: Two open-source SOTA prompt-based TTS systems are selected as baselines: ParlerTTS <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lacombe-etal-2024-parler-tts</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lyth2024natural</span>]</cite> and CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice</span>]</cite>. Both take user instructions and standard text as input, but CosyVoice2 requires additional prompt speech even for instruction-guided functionality<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/Render-AI/CosyVoice2\" title=\"\">https://github.com/Render-AI/CosyVoice2</a></span></span></span> where a silent speech sample is provided.</p>\n\n",
                "matched_terms": [
                    "text",
                    "standard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Object Evaluation Metrics:</span>\nTo quantify the accent level of the generated speech, we calculate accent accuracy using the ECAPA-TDNN accent recognition model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Jzuluaga/accent-id-commonaccent_ecapa\" title=\"\">https://huggingface.co/Jzuluaga/accent-id-commonaccent_ecapa</a></span></span></span>, finetuned on the accent pool (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S3.SS1\" title=\"3.1 Experimental Settings &#8227; 3 Experiments &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>), as in RAAP. Speech quality is measured with NISQA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mittag2021nisqa</span>]</cite> (1&#8211;5 scale) using its pre-trained model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/gabrielmittag/NISQA\" title=\"\">https://github.com/gabrielmittag/NISQA</a></span></span></span>.\nBias/Fairness of the generated speech is evaluated with the Fairness Discrepancy Rate (FDR)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">de2021fairness</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">estevez2023study</span>]</cite>, which quantifies disparities in false alarm/reject rates across accents, with 1 indicating perfect fairness.</p>\n\n",
                "matched_terms": [
                    "raap",
                    "accent",
                    "nisqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Subject Evaluation Metrics:</span>\nWe conducted human listening tests on four accented English samples (CN, SG, IN, GB). For each accent, 10 utterances (5F/5M) were generated from three systems: CosyVoice2 (standard text), ParlerTTS (standard text), and CLARITY. Native speakers of each accent (fluent in English), rated corresponding samples (e.g., Chinese listeners for Chinese-accented English<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>The setting assumes that Chinese listeners are more sensitive to Chinese-accented English speech.</span></span></span>). There were 4F/4M listeners for CN/SG and 2F/2M for IN/GB. Samples were evaluated on a 1&#8211;5 scale for naturalness, accent accuracy, age, gender consistency, and overall instruction match.</p>\n\n",
                "matched_terms": [
                    "text",
                    "standard",
                    "accent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">1. RAAP and Contextual Linguistic Text Adaptation:</span>\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S3.F3\" title=\"Figure 3 &#8227; 3.1 Experimental Settings &#8227; 3 Experiments &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the RAAP accuracy (left) and the adapted scores generated by GPT-5 and LLaMA models (right). RAAP achieves nearly 100% accuracy for most accents in terms of accent and gender retrieval, while age prediction is less accurate but acceptable, given its known difficulty.\nFor the LLM-as-judge scores, we observe that the standard text already achieves reasonable accent scores, with all scores above 6. After adaptation using GPT-4o-mini, four accents (GB, MY, SG, and US) show improved adaptation scores. Interestingly, these accents correspond to locations where the language of daily communciation tends to be English. A possible explanation is that for these speakers, the model had higher confidence in modifying the text, while the training corpus distribution likely reinforces this behaviour, which GPT-4 effectively captures. In contrast, LLaMA shows decreased accent scores after adaptation, although we doubt these results since the judge is GPT-5, not LLaMA, and continue to evaluate both adapted texts.</p>\n\n",
                "matched_terms": [
                    "raap",
                    "standard",
                    "text",
                    "accent",
                    "adapted",
                    "llama"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">2. Comparision of Baselines and CLARITY:</span>\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S3.F4\" title=\"Figure 4 &#8227; 3.1 Experimental Settings &#8227; 3 Experiments &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> plots accent accuracy and NISQA scores for generated speech across various systems, along with the averaged scores. CosyVoice2 and ParlerTTS achieve very low accent accuracy (8.75% and 6.14%), likely due to training sets lacking accented speech.\nPer-accent results reveal higher scores for accents better represented in training data. CLARITY with GPT-adapted text achieves the best overall accuracy, though ES, JP, and PT remain below 50%, showing the difficulty of these accents.\nIn terms of NISQA, ParlerTTS averages 4.67, while CLARITY_GPT achieves 4.36, outperforming CosyVoice2 (3.16) and even ground truth on some accents.</p>\n\n",
                "matched_terms": [
                    "text",
                    "accent",
                    "nisqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">3. Ablation Study of CLARITY:</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S3.T1\" title=\"Table 1 &#8227; 3.1 Experimental Settings &#8227; 3 Experiments &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the ablation study, showing that accent-score&#8211;guided prompt selection is highly effective, while adding text adaptation further improves accent accuracy. All variants achieve NISQA above 4.21, rising to 4.31 with RAAP and text adaptation, confirming the robustness of CLARITY. Among CLARITY variants, using GPT-adapted text performs best, while with LLaMA-adapted text is competitive, though its adapted text scores are lower (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S3.F3\" title=\"Figure 3 &#8227; 3.1 Experimental Settings &#8227; 3 Experiments &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), indicating potential mismatches from using the GPT-5 as judge. Comparison of LLaMA and GPT for text selection does not improve results, suggesting that GPT-5 scores may not be the optimum<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>Using multiple LLM-as-Judge models, e.g., GPT-5 and LLaMA-4, may reduce such mismatches, we leave this for future work.</span></span></span>. Since GPT-adapted CLARITY performs best in the objective test, we adopt this setting for CLARITY in the following experiments.</p>\n\n",
                "matched_terms": [
                    "raap",
                    "gpt",
                    "ablation",
                    "text",
                    "study",
                    "accent",
                    "adapted",
                    "llama",
                    "nisqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Subjective Evaluation:</span>\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S3.F5\" title=\"Figure 5 &#8227; 3.1 Experimental Settings &#8227; 3 Experiments &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows violin plots of the listening test results. For naturalness, ParlerTTS performs best on IN, while CLARITY outperforms both alternatives on CN and GB. SG results are similar across systems, with CosyVoice2 slightly worse. This aligns with the NISQA scores in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S3.F4\" title=\"Figure 4 &#8227; 3.1 Experimental Settings &#8227; 3 Experiments &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. CLARITY demonstrates superior accent consistency with reduced bias across all accents. Both CLARITY and ParlerTTS maintain gender consistency, whereas CosyVoice2 mimics prompt speech rather than following user instructions. Overall, CLARITY best aligns with user instructions.</p>\n\n",
                "matched_terms": [
                    "accent",
                    "nisqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dataset spans <span class=\"ltx_text ltx_font_italic\">three conversational scenarios</span>, each chosen to represent a distinct communicative domain where accent and prosody vary naturally.\nFor each scenario, we defined ten standard sentences and paired them with twelve target accents\n(<span class=\"ltx_text ltx_font_typewriter\">CA, CN, ES, GB, IN, JP, KR, MY, PT, RU, SG, US</span>).\nEach accent was then combined with ten randomly sampled metadata combinations (age, gender, and language background),\nresulting in approximately 1,200 implicit instructions per scenario and 3,600 in total.</p>\n\n",
                "matched_terms": [
                    "standard",
                    "accent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, GPT-5-mini was employed as an LLM-as-a-Judge, evaluating the standard text and both adapted versions. It provided quantitative scores and qualitative reasoning for each output.\nAn example of the judging prompt is shown below:</p>\n\n",
                "matched_terms": [
                    "text",
                    "standard",
                    "adapted"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#A1.F11\" title=\"Figure 11 &#8227; A.1.3 Adapted Text Generation &#8227; A.1 Experimental Protocol &#8227; Appendix A Appendix &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> shows the average NISQA score for each accent on the accent pool data.\nThe generated speech maintains high perceptual quality across all accents,\nwith scores ranging from 4.0 to 4.6 on a five-point scale.\nSingaporean (SG, 4.57), Japanese (JP, 4.41), and British (GBR, 4.40) accents\nachieve the best quality, indicating clear and natural synthesis.\nSpanish (ESP, 4.00) and Chinese (CHN, 4.13) exhibit slightly lower scores,\nlikely due to accent-specific variability or limited training coverage.\nDespite these differences, the narrow score range demonstrates strong cross-accent generalization and consistent perceptual quality.</p>\n\n",
                "matched_terms": [
                    "accent",
                    "nisqa",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To align the text content of the prompt speech with the user&#8217;s input, RAAP calculates the text similarity between the prompt speech candidates and the user-provided (standard) text input, as explained in Eqn.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#S2.E6\" title=\"In 2 The Proposed CLARITY &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Alternatively, the adapted text can also be used as the query to help identify prompt speech with similar content.</p>\n\n",
                "matched_terms": [
                    "raap",
                    "standard",
                    "userprovided",
                    "text",
                    "adapted",
                    "similarity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation Study:</span> as shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#A1.F15\" title=\"Figure 15 &#8227; A.2.2 Speech Quality Evaluation &#8227; A.2 Additional Results on Accent Pool data &#8227; Appendix A Appendix &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, this ablation study details the impact of various model configurations on accent accuracy and systemic bias, demonstrating a phased improvement across experimental setups. The initial phase, which involved replacing the near-silent prompt audio with accent pool sourced prompt audio dynamically selected via instruction extracted speaker features, alone yielded a significant boost in both accent accuracy and fairness metrics. Building upon this foundation, performance saw further incremental gains through the subsequent integration of target text (standard) to prompt transcription similarity and the strategic use of prompt speech exhibiting a more pronounced accent (higher accent score). As mentioned in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.11104v1#A1.SS3.SSS1\" title=\"A.3.1 Results on Retrieved Prompt Speech &#8227; A.3 Additional Results on Generated Speech &#8227; Appendix A Appendix &#8227; CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">A.3.1</span></a>, we also compared the results of standard and adapted text similarity. Ultimately, the most effective configurations (proposed) were achieved by leveraging Llama or GPT to perform adaptation on the target text, resulting in the highest overall accuracy and the lowest observed degree of bias among all tested methodologies.</p>\n\n",
                "matched_terms": [
                    "standard",
                    "gpt",
                    "ablation",
                    "text",
                    "study",
                    "accent",
                    "adapted",
                    "llama",
                    "similarity",
                    "score"
                ]
            }
        ]
    }
}