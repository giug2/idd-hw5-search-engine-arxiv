{
    "S3.T1": {
        "source_file": "InconVAD: A Two-Stage Dual-Tower Framework for Multimodal Emotion Inconsistency Detection",
        "caption": "Table 1: Comparison of unimodal and fusion towers using Concordance Correlation Coefficient (CCC).",
        "body": "Method\n\n\nV\n\n\n\n\nA\n\n\n\n\nD\n\n\n\n\nAvg\n\n\n\n\nOurs (Speech Tower)\n\n\n0.639\n\n\n\n\n0.669\n\n\n\n\n0.541\n\n\n\n\n0.616\n\n\n\n\nOurs (Text Tower)\n\n\n0.784\n\n\n\n\n0.419\n\n\n\n\n0.443\n\n\n\n\n0.549\n\n\n\n\n\nDimensional MTL [23]\n\n\n\n0.446\n\n\n\n\n0.594\n\n\n\n\n0.485\n\n\n\n\n0.508\n\n\n\n\n\nTwo-stage SVM [24]\n\n\n\n0.595\n\n\n\n\n0.601\n\n\n\n\n0.499\n\n\n\n\n0.565\n\n\n\n\n\nRL-MT [25]\n\n\n\n0.648\n\n\n\n\n0.668\n\n\n\n\n0.537\n\n\n\n\n0.618\n\n\n\n\n\nMFCNN14 [26]\n\n\n\n0.714\n\n\n\n\n0.639\n\n\n\n\n0.575\n\n\n\n\n0.642\n\n\n\n\n\nW2v2-b + BERT-b + L [27]\n\n\n\n0.625\n\n\n\n\n0.661\n\n\n\n\n0.570\n\n\n\n\n0.618\n\n\n\n\nOurs (Fusion Tower)\n\n\n0.741\n\n\n\n\n0.644\n\n\n\n\n0.586\n\n\n\n\n0.657",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Method</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">V</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">A</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">D</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Avg</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Ours (Speech Tower)</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.639</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.669</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.541</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.616</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Ours (Text Tower)</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.784</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.419</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.443</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.549</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">Dimensional MTL&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib23\" title=\"\">23</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.446</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.594</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.485</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.508</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">Two-stage SVM&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib24\" title=\"\">24</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.595</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.601</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.499</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.565</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">RL-MT&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib25\" title=\"\">25</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.648</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.668</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.537</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.618</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">MFCNN14&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib26\" title=\"\">26</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.714</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.639</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.575</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.642</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">W2v2-b + BERT-b + L&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib27\" title=\"\">27</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.625</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.661</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.570</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.618</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Ours (Fusion Tower)</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.741</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.644</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.586</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.657</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "dimensional",
            "concordance",
            "twostage",
            "rlmt",
            "text",
            "ours",
            "avg",
            "mtl",
            "bertb",
            "speech",
            "unimodal",
            "svm",
            "tower",
            "mfcnn14",
            "w2v2b",
            "fusion",
            "correlation",
            "ccc",
            "method",
            "towers",
            "coefficient",
            "comparison"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In Phase&#160;A, our unimodal speech and text towers obtain average CCCs of 0.616 on IMEOCAP dataset and 0.549 on Emobank dataset, respectively. The fusion tower achieves 0.657, surpassing the existing state-of-the-art fusion models, including Dimensional MTL&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Two-stage SVM &#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, RL-MT &#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, MFCNN14 &#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and W2v2-b + BERT-b + L &#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, as shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Datasets &#8227; 3 EXPERIMENTAL SETUPS &#8227; InconVAD: A Two-Stage Dual-Tower Framework for Multimodal Emotion Inconsistency Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The consistent gains across Valence, Arousal, and Dominance highlight the complementary strengths of speech and text tower, and the effectiveness of our transformer blocks and gated fusion mechanism.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Detecting emotional inconsistency across modalities is a key challenge in affective computing, as speech and text often convey conflicting cues. Existing approaches generally rely on incomplete emotion representations and employ unconditional fusion, which weakens performance when modalities are inconsistent. Moreover, little prior work explicitly addresses inconsistency detection itself. We propose InconVAD, a two-stage framework grounded in the Valence&#8211;Arousal&#8211;Dominance (VAD) space. In the first stage, independent uncertainty-aware models yield robust unimodal predictions. In the second stage, a classifier identifies cross-modal inconsistency and selectively integrates consistent signals. Extensive experiments show that InconVAD surpasses existing methods in both multimodal emotion inconsistency detection and modeling, offering a more reliable and interpretable solution for emotion analysis.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "unimodal",
                    "twostage",
                    "text",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">With the rapid development of human&#8211;computer interaction systems, accurately modeling emotions across multiple modalities has become a central challenge in affective computing </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. A key difficulty lies in the fact that speech and text do not always convey consistent affective states. Such inconsistencies may reflect complex psychological mechanisms, social strategies, or even clinical conditions </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Detecting and quantifying multimodal emotion inconsistency therefore requires a reliable framework that directly compares emotional representations across modalities, rather than relying solely on unimodal cues or generic fusion strategies.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "unimodal",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Second, most multimodal emotion recognition methods are built upon the implicit assumption that modalities such as speech and text are emotionally congruent </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. When this assumption is violated, their fusion strategies&#8212;typically averaging or concatenation&#8212;produce vague intermediate representations that dilute the literal sentiment expressed in text and obscure the authentic emotional cues in speech </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib6\" title=\"\">6</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Moreover, these models generally lack mechanisms for uncertainty awareness, treating all inputs as equally reliable and failing to assign greater weight to the clearer or more trustworthy modality when discrepancies arise. The absence of explicit modeling for emotion inconsistency thus remains a critical deficiency in current multimodal emotion recognition research, underscoring the need for frameworks that directly address inconsistency rather than treating it as a byproduct of fusion </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these issues, we propose InconVAD, a two-stage framework grounded in the Valence&#8211;Arousal&#8211;Dominance (VAD) space. In the first stage, modality-specific towers independently predict VAD values with uncertainty-aware estimation, providing robust and comparable unimodal representations. In the second stage, a classifier explicitly detects cross-modal inconsistency, while a gated fusion module selectively integrates predictions only for consistent pairs. This design prevents representation collapse in cases such as sarcasm and preserves modality-specific cues that would otherwise be lost. Extensive experiments demonstrate that InconVAD surpasses existing methods in both multimodal emotion inconsistency detection and modeling, while offering greater interpretability through modality-specific VAD predictions.</span>\n</p>\n\n",
                "matched_terms": [
                    "towers",
                    "unimodal",
                    "twostage",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As illustrated in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; InconVAD: A Two-Stage Dual-Tower Framework for Multimodal Emotion Inconsistency Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the proposed </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">InconVAD</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> framework operates in two stages. Stage 1 employs two unimodal towers for VAD pretraining: the speech tower and the text tower process raw speech and text inputs, respectively, and output modality-specific representations\n</span>\n  <math alttext=\"\\mathbf{h}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119841;</mi>\n        <mi mathsize=\"0.900em\">s</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{h}_{s}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\mathbf{h}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119841;</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{h}_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, together with VAD means </span>\n  <math alttext=\"\\boldsymbol{\\mu}_{s},\\boldsymbol{\\mu}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#120641;</mi>\n          <mi mathsize=\"0.900em\">s</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">,</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">&#120641;</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n        </msub>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{\\mu}_{s},\\boldsymbol{\\mu}_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and log-variances </span>\n  <math alttext=\"\\log\\boldsymbol{\\sigma}^{2}_{s},\\log\\boldsymbol{\\sigma}^{2}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">log</mi>\n          <mo lspace=\"0.167em\">&#8289;</mo>\n          <msubsup>\n            <mi mathsize=\"0.900em\">&#120648;</mi>\n            <mi mathsize=\"0.900em\">s</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msubsup>\n        </mrow>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mrow>\n          <mi mathsize=\"0.900em\">log</mi>\n          <mo lspace=\"0.167em\">&#8289;</mo>\n          <msubsup>\n            <mi mathsize=\"0.900em\">&#120648;</mi>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msubsup>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\log\\boldsymbol{\\sigma}^{2}_{s},\\log\\boldsymbol{\\sigma}^{2}_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nStage 2 comprises an inconsistency detection head and a gated fusion tower. It takes the outputs of Stage&#160;1, where the detection head predicts an inconsistency score </span>\n  <math alttext=\"p_{\\text{inc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mtext mathsize=\"0.900em\">inc</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">p_{\\text{inc}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the fusion tower, activated only for consistent pairs, generates a fused representation </span>\n  <math alttext=\"\\mathbf{h}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119841;</mi>\n        <mi mathsize=\"0.900em\">f</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{h}_{f}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and the final VAD prediction </span>\n  <math alttext=\"\\mathbf{y}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119858;</mi>\n        <mi mathsize=\"0.900em\">f</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{y}_{f}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe following subsections describe each component in detail.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "unimodal",
                    "text",
                    "tower",
                    "fusion",
                    "towers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The speech tower extracts both acoustic and prosodic cues for reliable VAD estimation.\nWe employ a pre-trained Wav2Vec2-base model </span>\n  <math alttext=\"f_{SE}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">f</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">S</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">E</mi>\n          </mrow>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">f_{SE}(\\cdot)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to generate frame-level acoustic embeddings </span>\n  <math alttext=\"\\mathbf{H}_{s}\\in\\mathbb{R}^{T_{s}\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119815;</mi>\n          <mi mathsize=\"0.900em\">s</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">T</mi>\n              <mi mathsize=\"0.900em\">s</mi>\n            </msub>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{s}\\in\\mathbb{R}^{T_{s}\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and a prosody extractor </span>\n  <math alttext=\"f_{PE}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">f</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">P</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">E</mi>\n          </mrow>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">f_{PE}(\\cdot)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to compute pitch and energy features </span>\n  <math alttext=\"\\mathbf{p}\\in\\mathbb{R}^{T_{s}\\times 2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119849;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">T</mi>\n              <mi mathsize=\"0.900em\">s</mi>\n            </msub>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mn mathsize=\"0.900em\">2</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{p}\\in\\mathbb{R}^{T_{s}\\times 2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThese complementary features are concatenated and projected through a linear layer to form the input </span>\n  <math alttext=\"\\mathbf{H}_{\\mathrm{in}}\\in\\mathbb{R}^{T_{s}\\times D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119815;</mi>\n          <mi mathsize=\"0.900em\">in</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">T</mi>\n              <mi mathsize=\"0.900em\">s</mi>\n            </msub>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msup>\n              <mi mathsize=\"0.900em\">D</mi>\n              <mo mathsize=\"0.900em\">&#8242;</mo>\n            </msup>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{\\mathrm{in}}\\in\\mathbb{R}^{T_{s}\\times D^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, ensuring that prosodic variation is explicitly incorporated into the acoustic space.\nThe sequence is then processed by two Conformer blocks </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which integrate multi-head self-attention, convolutional modules, and Macaron-style feed-forward layers to capture both local dynamics and long-range dependencies.\nThe resulting contextualized features </span>\n  <math alttext=\"\\mathbf{H}^{\\prime}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">&#119815;</mi>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}^{\\prime}_{s}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are aggregated by an ASPool module </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, producing a fixed-dimensional utterance-level embedding </span>\n  <math alttext=\"\\mathbf{h}_{s}\\in\\mathbb{R}^{D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119841;</mi>\n          <mi mathsize=\"0.900em\">s</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <msup>\n            <mi mathsize=\"0.900em\">D</mi>\n            <mo mathsize=\"0.900em\">&#8242;</mo>\n          </msup>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{h}_{s}\\in\\mathbb{R}^{D^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFinally, prediction heads output per-dimension means </span>\n  <math alttext=\"\\boldsymbol{\\mu}_{s}\\in\\mathbb{R}^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p1.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#120641;</mi>\n          <mi mathsize=\"0.900em\">s</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mn mathsize=\"0.900em\">3</mn>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{\\mu}_{s}\\in\\mathbb{R}^{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and log-variances </span>\n  <math alttext=\"\\log\\boldsymbol{\\sigma}^{2}_{s}\\in\\mathbb{R}^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p1.m9\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">log</mi>\n          <mo lspace=\"0.167em\">&#8289;</mo>\n          <msubsup>\n            <mi mathsize=\"0.900em\">&#120648;</mi>\n            <mi mathsize=\"0.900em\">s</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msubsup>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mn mathsize=\"0.900em\">3</mn>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\log\\boldsymbol{\\sigma}^{2}_{s}\\in\\mathbb{R}^{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, yielding uncertainty-aware unimodal estimates:</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tower",
                    "unimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The text tower captures semantic and lexical-level affective cues, complementing the speech tower.\nA RoBERTa-base encoder </span>\n  <math alttext=\"f_{TE}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">f</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">E</mi>\n          </mrow>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">f_{TE}(\\cdot)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> maps tokenized inputs </span>\n  <math alttext=\"\\mathbf{x}\\in\\mathcal{V}_{\\rm text}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119857;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msubsup>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119985;</mi>\n          <mi mathsize=\"0.900em\">text</mi>\n          <mi mathsize=\"0.900em\">L</mi>\n        </msubsup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{x}\\in\\mathcal{V}_{\\rm text}^{L}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to contextual embeddings </span>\n  <math alttext=\"\\mathbf{H}_{t}\\in\\mathbb{R}^{T_{t}\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119815;</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">T</mi>\n              <mi mathsize=\"0.900em\">t</mi>\n            </msub>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{t}\\in\\mathbb{R}^{T_{t}\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which encode rich semantic and syntactic information.\nTo explicitly inject affective knowledge, we employ the NRC VAD Lexicon v2 </span>\n  <math alttext=\"f_{\\mathrm{Prior}}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">f</mi>\n          <mi mathsize=\"0.900em\">Prior</mi>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">f_{\\mathrm{Prior}}(\\cdot)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to derive token-level prior vectors </span>\n  <math alttext=\"\\mathbf{p}_{t}\\in\\mathbb{R}^{T_{t}\\times 3}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119849;</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">T</mi>\n              <mi mathsize=\"0.900em\">t</mi>\n            </msub>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mn mathsize=\"0.900em\">3</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{p}_{t}\\in\\mathbb{R}^{T_{t}\\times 3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, representing valence, arousal, and dominance values.\nThese priors are integrated with the encoder outputs using a FiLM layer </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, producing gated representations </span>\n  <math alttext=\"\\mathbf{H}_{t}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS2.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">&#119815;</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{t}^{\\prime}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> that remain dimensionally consistent while incorporating explicit affective supervision.\nAn ASPool module then aggregates </span>\n  <math alttext=\"\\mathbf{H}_{t}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS2.p1.m7\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">&#119815;</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{t}^{\\prime}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into an utterance-level embedding </span>\n  <math alttext=\"\\mathbf{h}_{t}\\in\\mathbb{R}^{D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS2.p1.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119841;</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <msup>\n            <mi mathsize=\"0.900em\">D</mi>\n            <mo mathsize=\"0.900em\">&#8242;</mo>\n          </msup>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{h}_{t}\\in\\mathbb{R}^{D^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is passed to prediction heads to produce </span>\n  <math alttext=\"\\boldsymbol{\\mu}_{t}\\in\\mathbb{R}^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS2.p1.m9\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#120641;</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mn mathsize=\"0.900em\">3</mn>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{\\mu}_{t}\\in\\mathbb{R}^{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\log\\boldsymbol{\\sigma}^{2}_{t}\\in\\mathbb{R}^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS2.p1.m10\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">log</mi>\n          <mo lspace=\"0.167em\">&#8289;</mo>\n          <msubsup>\n            <mi mathsize=\"0.900em\">&#120648;</mi>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msubsup>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mn mathsize=\"0.900em\">3</mn>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\log\\boldsymbol{\\sigma}^{2}_{t}\\in\\mathbb{R}^{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThis process yields modality-specific textual predictions that align with the speech tower outputs in the shared VAD space:</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "tower",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To optimize the unimodal towers, we adopt a heteroscedastic regression framework, where the prediction uncertainty is explicitly modeled through variance estimation. The training objective is the Gaussian Negative Log-Likelihood (NLL) of the ground-truth labels </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For a given modality </span>\n  <math alttext=\"m\\in\\{s,t\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">m</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mi mathsize=\"0.900em\">s</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">t</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">m\\in\\{s,t\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (speech or text) and dimension </span>\n  <math alttext=\"k\\in\\{V,A,D\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">k</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mi mathsize=\"0.900em\">V</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">A</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">D</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">k\\in\\{V,A,D\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the loss is defined as:</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "towers",
                    "speech",
                    "unimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The inconsistency classifier is designed to assess cross-modal inconsistency without modifying the unimodal feature extractors trained in Phase&#160;A. It operates on speech and text representations, </span>\n  <math alttext=\"\\mathbf{H}_{s}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">&#119815;</mi>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{s}^{\\prime}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\mathbf{H}_{t}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">&#119815;</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{t}^{\\prime}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and determines whether they convey consistent affective states.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "unimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our fusion module is an end-to-end architecture that integrates speech and text features to predict VAD emotional dimensions.\nBuilding on the outputs of the speech and text towers, the projected sequences are passed into a cross-modal fusion module.\nInspired by </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we design a Transformer block that jointly models intra- and inter-modal dependencies.\nSpecifically, multi-head self-attention (MHSA) is applied to capture intra-modal relationships (</span>\n  <math alttext=\"\\text{speech}\\!\\to\\!\\text{speech}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mtext mathsize=\"0.900em\">speech</mtext>\n        <mo lspace=\"0.108em\" mathsize=\"0.900em\" rspace=\"0.108em\" stretchy=\"false\">&#8594;</mo>\n        <mtext mathsize=\"0.900em\">speech</mtext>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\text{speech}\\!\\to\\!\\text{speech}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\text{text}\\!\\to\\!\\text{text}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mtext mathsize=\"0.900em\">text</mtext>\n        <mo lspace=\"0.108em\" mathsize=\"0.900em\" rspace=\"0.108em\" stretchy=\"false\">&#8594;</mo>\n        <mtext mathsize=\"0.900em\">text</mtext>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\text{text}\\!\\to\\!\\text{text}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), while multi-head cross-attention (MHCA) enables cross-modal interactions (</span>\n  <math alttext=\"\\text{speech}\\!\\to\\!\\text{text}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mtext mathsize=\"0.900em\">speech</mtext>\n        <mo lspace=\"0.108em\" mathsize=\"0.900em\" rspace=\"0.108em\" stretchy=\"false\">&#8594;</mo>\n        <mtext mathsize=\"0.900em\">text</mtext>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\text{speech}\\!\\to\\!\\text{text}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\text{text}\\!\\to\\!\\text{speech}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mtext mathsize=\"0.900em\">text</mtext>\n        <mo lspace=\"0.108em\" mathsize=\"0.900em\" rspace=\"0.108em\" stretchy=\"false\">&#8594;</mo>\n        <mtext mathsize=\"0.900em\">speech</mtext>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\text{text}\\!\\to\\!\\text{speech}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nThe outputs are subsequently processed with LayerNorm (LN) and feed-forward networks (FFN) to obtain modality-specific contextual representations </span>\n  <math alttext=\"\\mathbf{f}_{s},\\mathbf{f}_{t}\\in\\mathbb{R}^{L\\times D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119839;</mi>\n            <mi mathsize=\"0.900em\">s</mi>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119839;</mi>\n            <mi mathsize=\"0.900em\">t</mi>\n          </msub>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">L</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msup>\n              <mi mathsize=\"0.900em\">D</mi>\n              <mo mathsize=\"0.900em\">&#8242;</mo>\n            </msup>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{f}_{s},\\mathbf{f}_{t}\\in\\mathbb{R}^{L\\times D^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "towers",
                    "speech",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"\\mathbf{W}_{s},\\mathbf{W}_{t}\\in\\mathbb{R}^{D^{\\prime}\\times D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119830;</mi>\n            <mi mathsize=\"0.900em\">s</mi>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119830;</mi>\n            <mi mathsize=\"0.900em\">t</mi>\n          </msub>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <msup>\n              <mi mathsize=\"0.900em\">D</mi>\n              <mo mathsize=\"0.900em\">&#8242;</mo>\n            </msup>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msup>\n              <mi mathsize=\"0.900em\">D</mi>\n              <mo mathsize=\"0.900em\">&#8242;</mo>\n            </msup>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{W}_{s},\\mathbf{W}_{t}\\in\\mathbb{R}^{D^{\\prime}\\times D^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are learnable projections,\n</span>\n  <math alttext=\"[\\mathbf{g}_{s},\\mathbf{g}_{t}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119840;</mi>\n          <mi mathsize=\"0.900em\">s</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">,</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119840;</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n        </msub>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">[\\mathbf{g}_{s},\\mathbf{g}_{t}]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are learned gates for speech and text, and </span>\n  <math alttext=\"\\odot\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#8857;</mo>\n      <annotation encoding=\"application/x-tex\">\\odot</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the element-wise product with broadcasting along the feature dimension.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For the fusion module, training is guided by two complementary objectives.\nOn labeled data with available VAD annotations, we apply the same Gaussian NLL loss as in Stage&#160;1.\nFor unlabeled pairs without VAD ground truth, we introduce a selective agreement loss, which encourages the fused prediction </span>\n  <math alttext=\"(\\boldsymbol{\\mu}_{f},\\log\\boldsymbol{\\sigma}_{f}^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">&#120641;</mi>\n          <mi mathsize=\"0.900em\">f</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mrow>\n          <mi mathsize=\"0.900em\">log</mi>\n          <mo lspace=\"0.167em\">&#8289;</mo>\n          <msubsup>\n            <mi mathsize=\"0.900em\">&#120648;</mi>\n            <mi mathsize=\"0.900em\">f</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msubsup>\n        </mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">(\\boldsymbol{\\mu}_{f},\\log\\boldsymbol{\\sigma}_{f}^{2})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to agree with a Gaussian target derived from the unimodal outputs, defined as</span>\n</p>\n\n",
                "matched_terms": [
                    "unimodal",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"\\mu_{\\text{agree}}^{k}=\\tfrac{\\mu_{s}^{k}/{\\sigma_{s}^{k}}^{2}+\\mu_{t}^{k}/{\\sigma_{t}^{k}}^{2}}{1/{\\sigma_{s}^{k}}^{2}+1/{\\sigma_{t}^{k}}^{2}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msubsup>\n          <mi mathsize=\"0.900em\">&#956;</mi>\n          <mtext mathsize=\"0.900em\">agree</mtext>\n          <mi mathsize=\"0.900em\">k</mi>\n        </msubsup>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mfrac>\n          <mrow>\n            <mrow>\n              <msubsup>\n                <mi mathsize=\"0.900em\">&#956;</mi>\n                <mi mathsize=\"0.900em\">s</mi>\n                <mi mathsize=\"0.900em\">k</mi>\n              </msubsup>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n              <mmultiscripts>\n                <mi mathsize=\"0.900em\">&#963;</mi>\n                <mi mathsize=\"0.900em\">s</mi>\n                <mi mathsize=\"0.900em\">k</mi>\n                <mrow/>\n                <mn mathsize=\"0.900em\">2</mn>\n              </mmultiscripts>\n            </mrow>\n            <mo mathsize=\"0.900em\">+</mo>\n            <mrow>\n              <msubsup>\n                <mi mathsize=\"0.900em\">&#956;</mi>\n                <mi mathsize=\"0.900em\">t</mi>\n                <mi mathsize=\"0.900em\">k</mi>\n              </msubsup>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n              <mmultiscripts>\n                <mi mathsize=\"0.900em\">&#963;</mi>\n                <mi mathsize=\"0.900em\">t</mi>\n                <mi mathsize=\"0.900em\">k</mi>\n                <mrow/>\n                <mn mathsize=\"0.900em\">2</mn>\n              </mmultiscripts>\n            </mrow>\n          </mrow>\n          <mrow>\n            <mrow>\n              <mn mathsize=\"0.900em\">1</mn>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n              <mmultiscripts>\n                <mi mathsize=\"0.900em\">&#963;</mi>\n                <mi mathsize=\"0.900em\">s</mi>\n                <mi mathsize=\"0.900em\">k</mi>\n                <mrow/>\n                <mn mathsize=\"0.900em\">2</mn>\n              </mmultiscripts>\n            </mrow>\n            <mo mathsize=\"0.900em\">+</mo>\n            <mrow>\n              <mn mathsize=\"0.900em\">1</mn>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n              <mmultiscripts>\n                <mi mathsize=\"0.900em\">&#963;</mi>\n                <mi mathsize=\"0.900em\">t</mi>\n                <mi mathsize=\"0.900em\">k</mi>\n                <mrow/>\n                <mn mathsize=\"0.900em\">2</mn>\n              </mmultiscripts>\n            </mrow>\n          </mrow>\n        </mfrac>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mu_{\\text{agree}}^{k}=\\tfrac{\\mu_{s}^{k}/{\\sigma_{s}^{k}}^{2}+\\mu_{t}^{k}/{\\sigma_{t}^{k}}^{2}}{1/{\\sigma_{s}^{k}}^{2}+1/{\\sigma_{t}^{k}}^{2}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"{\\sigma_{\\text{agree}}^{k}}^{2}=\\tfrac{1}{1/{\\sigma_{s}^{k}}^{2}+1/{\\sigma_{t}^{k}}^{2}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mmultiscripts>\n          <mi mathsize=\"0.900em\">&#963;</mi>\n          <mtext mathsize=\"0.900em\">agree</mtext>\n          <mi mathsize=\"0.900em\">k</mi>\n          <mrow/>\n          <mn mathsize=\"0.900em\">2</mn>\n        </mmultiscripts>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mfrac>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mrow>\n            <mrow>\n              <mn mathsize=\"0.900em\">1</mn>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n              <mmultiscripts>\n                <mi mathsize=\"0.900em\">&#963;</mi>\n                <mi mathsize=\"0.900em\">s</mi>\n                <mi mathsize=\"0.900em\">k</mi>\n                <mrow/>\n                <mn mathsize=\"0.900em\">2</mn>\n              </mmultiscripts>\n            </mrow>\n            <mo mathsize=\"0.900em\">+</mo>\n            <mrow>\n              <mn mathsize=\"0.900em\">1</mn>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n              <mmultiscripts>\n                <mi mathsize=\"0.900em\">&#963;</mi>\n                <mi mathsize=\"0.900em\">t</mi>\n                <mi mathsize=\"0.900em\">k</mi>\n                <mrow/>\n                <mn mathsize=\"0.900em\">2</mn>\n              </mmultiscripts>\n            </mrow>\n          </mrow>\n        </mfrac>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">{\\sigma_{\\text{agree}}^{k}}^{2}=\\tfrac{1}{1/{\\sigma_{s}^{k}}^{2}+1/{\\sigma_{t}^{k}}^{2}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe overall training objective of the fusion tower combines the supervised and selective terms:</span>\n</p>\n\n",
                "matched_terms": [
                    "tower",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For Phase&#160;A, we use the IEMOCAP </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset for the speech tower and the EmoBank </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset for the text tower, both with dimensional Valence&#8211;Arousal&#8211;Dominance (VAD) annotations. To ensure label comparability across datasets, we apply a parametric Beta Cumulative Distribution Function (CDF) transform that maps each original label </span>\n  <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">v</mi>\n      <annotation encoding=\"application/x-tex\">v</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into an aligned value </span>\n  <math alttext=\"v^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">v</mi>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msup>\n      <annotation encoding=\"application/x-tex\">v^{\\prime}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in a shared target distribution. A source value </span>\n  <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">v</mi>\n      <annotation encoding=\"application/x-tex\">v</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is first normalized to </span>\n  <math alttext=\"[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n        <mn mathsize=\"0.900em\">0</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">[0,1]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, then mapped to its quantile </span>\n  <math alttext=\"u=F_{src}(v)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">u</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <msub>\n            <mi mathsize=\"0.900em\">F</mi>\n            <mrow>\n              <mi mathsize=\"0.900em\">s</mi>\n              <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n              <mi mathsize=\"0.900em\">r</mi>\n              <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n              <mi mathsize=\"0.900em\">c</mi>\n            </mrow>\n          </msub>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mi mathsize=\"0.900em\">v</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">u=F_{src}(v)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> using the source CDF, and finally aligned by applying the target inverse CDF, </span>\n  <math alttext=\"v^{\\prime}=F_{tgt}^{-1}(u)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msup>\n          <mi mathsize=\"0.900em\">v</mi>\n          <mo mathsize=\"0.900em\">&#8242;</mo>\n        </msup>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <msubsup>\n            <mi mathsize=\"0.900em\">F</mi>\n            <mrow>\n              <mi mathsize=\"0.900em\">t</mi>\n              <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n              <mi mathsize=\"0.900em\">g</mi>\n              <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n              <mi mathsize=\"0.900em\">t</mi>\n            </mrow>\n            <mrow>\n              <mo mathsize=\"0.900em\">&#8722;</mo>\n              <mn mathsize=\"0.900em\">1</mn>\n            </mrow>\n          </msubsup>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mi mathsize=\"0.900em\">u</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">v^{\\prime}=F_{tgt}^{-1}(u)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The aligned labels </span>\n  <math alttext=\"v^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">v</mi>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msup>\n      <annotation encoding=\"application/x-tex\">v^{\\prime}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are used as training targets, while during evaluation the model predictions </span>\n  <math alttext=\"\\hat{v}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">v</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\hat{v}^{\\prime}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are mapped back through the inverse transform to obtain </span>\n  <math alttext=\"\\hat{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\">\n    <semantics>\n      <mover accent=\"true\">\n        <mi mathsize=\"0.900em\">v</mi>\n        <mo mathsize=\"0.900em\">^</mo>\n      </mover>\n      <annotation encoding=\"application/x-tex\">\\hat{v}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for comparison against the original labels </span>\n  <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m10\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">v</mi>\n      <annotation encoding=\"application/x-tex\">v</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The Beta-CDF process can be formulated as:</span>\n</p>\n\n",
                "matched_terms": [
                    "dimensional",
                    "speech",
                    "text",
                    "tower",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For Phase B, to train the inconsistency classifier, we use IEMOCAP </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset and the EmoV-DB </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset to construct binary-labeled data pairs. Consistency pairs (label </span>\n  <math alttext=\"y=1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">y=1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) include speech-text pairs from IEMOCAP and neutral emotion speech-text pairs from EmoV-DB. Inconsistent pairs (</span>\n  <math alttext=\"y=0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">y=0</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) are generated from EmoV-DB by pairing neutral transcripts with non-neutral speech recordings of the same utterance ID. In addition, to train the fusion tower, we use only consistency pairs (</span>\n  <math alttext=\"y=1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">y=1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), as cross-modal fusion is meaningful only when the two modalities are emotionally aligned. The tower is trained to integrate the unimodal predictions into a single fused VAD output </span>\n  <math alttext=\"\\mathbf{y}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119858;</mi>\n        <mi mathsize=\"0.900em\">f</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{y}_{f}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, providing a unified estimate rather than separate outputs for each modality.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tower",
                    "unimodal",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In Phase&#160;A, both towers use pretrained backbones, Wav2Vec2-base for speech and RoBERTa-base for text (hidden size 768), followed by projection layers of dimension 256. Training is performed with a batch size of 16 for up to 50 epochs with early stopping (patience = 5). Optimization uses AdamW with learning rates of </span>\n  <math alttext=\"2\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">2</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">5</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">2\\times 10^{-5}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for the backbone and </span>\n  <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for the heads, combined with a cosine schedule and 10% warm-up. Weight decay is set to 0.01. The minimum variance of </span>\n  <math alttext=\"2\\times 10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">2</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">3</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">2\\times 10^{-3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is used by the heteroscedastic Gaussian NLL loss function. We use the concordance correlation coefficient (CCC) as the evaluation metric. To avoid data leakage, we use a speaker-independent split for IEMOCAP. All utterances from each of the ten speakers go to a single partition with an 8/1/1 train/validation/test ratio via group-based splitting. For EmoBank, we follow the official train/validation/test split annotated in the corpus.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "concordance",
                    "correlation",
                    "text",
                    "ccc",
                    "towers",
                    "coefficient"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For Phase&#160;B inconsistency detection, both the speech and text towers are kept frozen, and optimization is performed solely on the classifier head. Each pair of data forms the input after modality-specific linear projections to a 256-dimensional space. We use a batch size of 32 and train for up to 50 epochs with early stopping (patience = 5). Optimization uses AdamW on classifier parameters, with a learning rate of </span>\n  <math alttext=\"1\\times 10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">3</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\times 10^{-3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and weight decay </span>\n  <math alttext=\"0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.01</mn>\n      <annotation encoding=\"application/x-tex\">0.01</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The loss combines binary cross-entropy and a margin term (margin </span>\n  <math alttext=\"m=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">m</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.9</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">m=0.9</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"\\lambda=0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#955;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.15</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\lambda=0.15</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). For fusion tower, we keep the batch size at 16 and train for up to 50 epochs. Optimization uses AdamW with learning rate </span>\n  <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, weight decay 0.01, and a cosine schedule with 10% warm-up. We use CCC as the evaluation metric. For data split, we use the same speaker-independent split as in Phase&#160;A for IEMOCAP dataset, while EmoV-DB utterances are partitioned into 8/1/1 train/validation/test sets.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "ccc",
                    "tower",
                    "fusion",
                    "towers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We conduct a series of ablation studies to validate the necessity our architectural design as shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4 EXPERIMENTAL RESULTS &#8227; InconVAD: A Two-Stage Dual-Tower Framework for Multimodal Emotion Inconsistency Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For the speech tower, removing the Conformer blocks or prosody injection substantially reduces average CCC, highlighting the importance of temporal modeling and prosodic cues. Eliminating ASPool module also leads to a consistent drop, confirming its role in emphasizing salient acoustic features. For text tower, removing the affect prior gating decreases average CCC from 0.549 to 0.543, validating the benefit of injecting lexical affective knowledge. Similarly, discarding ASPool module lowers overall performance. For the Fusion Tower, removing Transformer block or the gated multimodal fusion mechanism degrades the average CCC from 0.657 to 0.641 and 0.625, respectively. These results confirm that modeling mutual interactions and dimension-specific modality weighting are both critical for robust cross-modal integration.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "ccc",
                    "tower",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this study, we propose InconVAD, a cross-modal emotion inconsistency detection framework grounded in a shared three-dimensional VAD emotion space. The framework produces interpretable and comparable VAD predictions from both speech and text, enabling effective inconsistency detection across modalities. This work establishes a foundation for building more emotionally intelligent and trustworthy human&#8211;computer interaction systems in real-world applications. Furthermore, our study highlights the importance of explicitly modeling cross-modal inconsistencies rather than assuming unimodal agreement, paving the way for more reliable multimodal affective computing systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "unimodal"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "InconVAD: A Two-Stage Dual-Tower Framework for Multimodal Emotion Inconsistency Detection",
        "caption": "Table 2: Comparison with SOTA methods on emotional inconsistency detection.",
        "body": "Method\n\n\nAccuracy\n\n\n\n\nF1-Score\n\n\n\n\nPrecision\n\n\n\n\nRecall\n\n\n\n\n\n\n\nSVM [28]\n\n\n\n85.7\n\n\n\n\n86.4\n\n\n\n\n80.3\n\n\n\n\n93.6\n\n\n\n\n\nATEI [2]\n\n\n\n83.4\n\n\n\n\n83.6\n\n\n\n\n82.2\n\n\n\n\n85.0\n\n\n\n\nOurs (Classifier)\n\n\n92.3\n\n\n\n\n92.2\n\n\n\n\n93.6\n\n\n\n\n90.9",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt\" style=\"width:34.1pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Accuracy</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt\" style=\"width:34.1pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">F1-Score</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt\" style=\"width:34.1pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Precision</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt\" style=\"width:34.1pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Recall</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">SVM&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib28\" title=\"\">28</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:34.1pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">85.7</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:34.1pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">86.4</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:34.1pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">80.3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:34.1pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">93.6</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">ATEI&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib2\" title=\"\">2</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:34.1pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">83.4</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:34.1pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">83.6</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:34.1pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">82.2</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:34.1pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">85.0</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Ours (Classifier)</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb\" style=\"width:34.1pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">92.3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb\" style=\"width:34.1pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">92.2</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb\" style=\"width:34.1pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">93.6</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb\" style=\"width:34.1pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">90.9</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "detection",
            "recall",
            "emotional",
            "atei",
            "inconsistency",
            "accuracy",
            "f1score",
            "svm",
            "ours",
            "method",
            "methods",
            "classifier",
            "sota",
            "comparison",
            "precision"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For the inconsistency detection task, our classifier achieves the best performance across reported metrics. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4 EXPERIMENTAL RESULTS &#8227; InconVAD: A Two-Stage Dual-Tower Framework for Multimodal Emotion Inconsistency Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, On the test set, it attains an accuracy of 92.3% and an F1-score of 92.2%, surpassing prior methods (SVM </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib28\" title=\"\">28</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, ATEI </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). Precision and recall are likewise competitive, confirming that the leakage-free training protocol and composite loss design enable clear separation between consistent and inconsistent pairs. The decision threshold </span>\n  <math alttext=\"\\tau^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">&#964;</mi>\n        <mo mathsize=\"0.900em\">&#8727;</mo>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\tau^{*}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> was fixed based on validation by maximizing Youden&#8217;s </span>\n  <math alttext=\"J\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">J</mi>\n      <annotation encoding=\"application/x-tex\">J</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, ensuring fair evaluation.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Detecting emotional inconsistency across modalities is a key challenge in affective computing, as speech and text often convey conflicting cues. Existing approaches generally rely on incomplete emotion representations and employ unconditional fusion, which weakens performance when modalities are inconsistent. Moreover, little prior work explicitly addresses inconsistency detection itself. We propose InconVAD, a two-stage framework grounded in the Valence&#8211;Arousal&#8211;Dominance (VAD) space. In the first stage, independent uncertainty-aware models yield robust unimodal predictions. In the second stage, a classifier identifies cross-modal inconsistency and selectively integrates consistent signals. Extensive experiments show that InconVAD surpasses existing methods in both multimodal emotion inconsistency detection and modeling, offering a more reliable and interpretable solution for emotion analysis.</span>\n</p>\n\n",
                "matched_terms": [
                    "detection",
                    "emotional",
                    "inconsistency",
                    "methods",
                    "classifier"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nMultimodal emotion inconsistency detection, Affective computing, Cross-modal representation learning, Multimodal emotion analysis</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "detection",
                    "inconsistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">With the rapid development of human&#8211;computer interaction systems, accurately modeling emotions across multiple modalities has become a central challenge in affective computing </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. A key difficulty lies in the fact that speech and text do not always convey consistent affective states. Such inconsistencies may reflect complex psychological mechanisms, social strategies, or even clinical conditions </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Detecting and quantifying multimodal emotion inconsistency therefore requires a reliable framework that directly compares emotional representations across modalities, rather than relying solely on unimodal cues or generic fusion strategies.</span>\n</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "inconsistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Current approaches to cross-modal emotion analysis face two fundamental challenges. First, they often rely on inadequate emotional representation models, such as discrete emotion categories, which fail to capture the nuance and continuity of real-world affective expressions. Existing studies on emotion inconsistency at the categorical level typically reduce the task to comparing emotion labels across modalities. For example, </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduced the Multimodal Cross-Attention Bayesian Network (MCABN), which employed attention mechanisms to identify inconsistencies between images and text on social media, using category-based labels with pseudo-label guidance to resolve conflicts. Similarly, </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> proposed a framework for detecting Acoustic&#8211;Text Emotion Inconsistency (ATEI) in depression diagnosis by categorizing emotions into three classes (positive, negative, neutral). While such label-based methods are computationally efficient and offer clear interpretability, they overlook variations in emotional intensity. As a result, they provide only a coarse quantification of inconsistency, leading to the loss of fine-grained affective information in practical applications that require detailed emotional analysis.</span>\n</p>\n\n",
                "matched_terms": [
                    "atei",
                    "emotional",
                    "methods",
                    "inconsistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Second, most multimodal emotion recognition methods are built upon the implicit assumption that modalities such as speech and text are emotionally congruent </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. When this assumption is violated, their fusion strategies&#8212;typically averaging or concatenation&#8212;produce vague intermediate representations that dilute the literal sentiment expressed in text and obscure the authentic emotional cues in speech </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib6\" title=\"\">6</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Moreover, these models generally lack mechanisms for uncertainty awareness, treating all inputs as equally reliable and failing to assign greater weight to the clearer or more trustworthy modality when discrepancies arise. The absence of explicit modeling for emotion inconsistency thus remains a critical deficiency in current multimodal emotion recognition research, underscoring the need for frameworks that directly address inconsistency rather than treating it as a byproduct of fusion </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "methods",
                    "inconsistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these issues, we propose InconVAD, a two-stage framework grounded in the Valence&#8211;Arousal&#8211;Dominance (VAD) space. In the first stage, modality-specific towers independently predict VAD values with uncertainty-aware estimation, providing robust and comparable unimodal representations. In the second stage, a classifier explicitly detects cross-modal inconsistency, while a gated fusion module selectively integrates predictions only for consistent pairs. This design prevents representation collapse in cases such as sarcasm and preserves modality-specific cues that would otherwise be lost. Extensive experiments demonstrate that InconVAD surpasses existing methods in both multimodal emotion inconsistency detection and modeling, while offering greater interpretability through modality-specific VAD predictions.</span>\n</p>\n\n",
                "matched_terms": [
                    "detection",
                    "methods",
                    "inconsistency",
                    "classifier"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As illustrated in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; InconVAD: A Two-Stage Dual-Tower Framework for Multimodal Emotion Inconsistency Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the proposed </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">InconVAD</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> framework operates in two stages. Stage 1 employs two unimodal towers for VAD pretraining: the speech tower and the text tower process raw speech and text inputs, respectively, and output modality-specific representations\n</span>\n  <math alttext=\"\\mathbf{h}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119841;</mi>\n        <mi mathsize=\"0.900em\">s</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{h}_{s}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\mathbf{h}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119841;</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{h}_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, together with VAD means </span>\n  <math alttext=\"\\boldsymbol{\\mu}_{s},\\boldsymbol{\\mu}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#120641;</mi>\n          <mi mathsize=\"0.900em\">s</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">,</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">&#120641;</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n        </msub>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{\\mu}_{s},\\boldsymbol{\\mu}_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and log-variances </span>\n  <math alttext=\"\\log\\boldsymbol{\\sigma}^{2}_{s},\\log\\boldsymbol{\\sigma}^{2}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">log</mi>\n          <mo lspace=\"0.167em\">&#8289;</mo>\n          <msubsup>\n            <mi mathsize=\"0.900em\">&#120648;</mi>\n            <mi mathsize=\"0.900em\">s</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msubsup>\n        </mrow>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mrow>\n          <mi mathsize=\"0.900em\">log</mi>\n          <mo lspace=\"0.167em\">&#8289;</mo>\n          <msubsup>\n            <mi mathsize=\"0.900em\">&#120648;</mi>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msubsup>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\log\\boldsymbol{\\sigma}^{2}_{s},\\log\\boldsymbol{\\sigma}^{2}_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nStage 2 comprises an inconsistency detection head and a gated fusion tower. It takes the outputs of Stage&#160;1, where the detection head predicts an inconsistency score </span>\n  <math alttext=\"p_{\\text{inc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mtext mathsize=\"0.900em\">inc</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">p_{\\text{inc}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the fusion tower, activated only for consistent pairs, generates a fused representation </span>\n  <math alttext=\"\\mathbf{h}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119841;</mi>\n        <mi mathsize=\"0.900em\">f</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{h}_{f}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and the final VAD prediction </span>\n  <math alttext=\"\\mathbf{y}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119858;</mi>\n        <mi mathsize=\"0.900em\">f</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{y}_{f}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe following subsections describe each component in detail.</span>\n</p>\n\n",
                "matched_terms": [
                    "detection",
                    "inconsistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The inconsistency classifier is designed to assess cross-modal inconsistency without modifying the unimodal feature extractors trained in Phase&#160;A. It operates on speech and text representations, </span>\n  <math alttext=\"\\mathbf{H}_{s}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">&#119815;</mi>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{s}^{\\prime}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\mathbf{H}_{t}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">&#119815;</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{t}^{\\prime}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and determines whether they convey consistent affective states.</span>\n</p>\n\n",
                "matched_terms": [
                    "classifier",
                    "inconsistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To align the modalities, the representations are first projected into a shared latent space through lightweight projectors </span>\n  <math alttext=\"f_{SP}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">f</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">S</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">P</mi>\n          </mrow>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">f_{SP}(\\cdot)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"f_{TP}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">f</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">P</mi>\n          </mrow>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">f_{TP}(\\cdot)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, producing </span>\n  <math alttext=\"\\tilde{\\mathbf{H}}_{s},\\tilde{\\mathbf{H}}_{t}\\in\\mathbb{R}^{T\\times D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mover accent=\"true\">\n              <mi mathsize=\"0.900em\">&#119815;</mi>\n              <mo mathsize=\"0.900em\">~</mo>\n            </mover>\n            <mi mathsize=\"0.900em\">s</mi>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mover accent=\"true\">\n              <mi mathsize=\"0.900em\">&#119815;</mi>\n              <mo mathsize=\"0.900em\">~</mo>\n            </mover>\n            <mi mathsize=\"0.900em\">t</mi>\n          </msub>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msup>\n              <mi mathsize=\"0.900em\">D</mi>\n              <mo mathsize=\"0.900em\">&#8242;</mo>\n            </msup>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{H}}_{s},\\tilde{\\mathbf{H}}_{t}\\in\\mathbb{R}^{T\\times D^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These projections normalize the features and reduce domain gaps between modalities. The two projected sequences are then concatenated to form a joint representation </span>\n  <math alttext=\"\\tilde{\\mathbf{H}}\\in\\mathbb{R}^{T\\times 2D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">&#119815;</mi>\n          <mo mathsize=\"0.900em\">~</mo>\n        </mover>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mrow>\n              <mi mathsize=\"0.900em\">T</mi>\n              <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n              <mn mathsize=\"0.900em\">2</mn>\n            </mrow>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <msup>\n              <mi mathsize=\"0.900em\">D</mi>\n              <mo mathsize=\"0.900em\">&#8242;</mo>\n            </msup>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{H}}\\in\\mathbb{R}^{T\\times 2D^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is passed to a binary classifier </span>\n  <math alttext=\"f_{C}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">f</mi>\n          <mi mathsize=\"0.900em\">C</mi>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">f_{C}(\\cdot)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The classifier consists of two linear layers with GELU activation, followed by LayerNorm and a Sigmoid output, yielding the predicted inconsistency score </span>\n  <math alttext=\"p_{\\text{inc}}\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">p</mi>\n          <mtext mathsize=\"0.900em\">inc</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <mn mathsize=\"0.900em\">0</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p_{\\text{inc}}\\in[0,1]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe overall classification process can be expressed as:</span>\n</p>\n\n",
                "matched_terms": [
                    "classifier",
                    "inconsistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The inconsistency classifier is trained with a joint objective that combines classification accuracy and geometric regularization of the latent space.\nThe primary term is the Binary Cross-Entropy loss, </span>\n  <math alttext=\"\\mathcal{L}_{\\text{BCE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n        <mtext mathsize=\"0.900em\">BCE</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{BCE}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which compares the predicted inconsistency score </span>\n  <math alttext=\"p_{\\text{inc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mtext mathsize=\"0.900em\">inc</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">p_{\\text{inc}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> against the ground-truth label </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nTo further structure the latent space, we introduce a margin-based auxiliary loss, </span>\n  <math alttext=\"\\mathcal{L}_{\\text{margin}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n        <mtext mathsize=\"0.900em\">margin</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{margin}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, applied to the projected representations </span>\n  <math alttext=\"\\tilde{\\mathbf{H}}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">&#119815;</mi>\n          <mo mathsize=\"0.900em\">~</mo>\n        </mover>\n        <mi mathsize=\"0.900em\">s</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{H}}_{s}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\tilde{\\mathbf{H}}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">&#119815;</mi>\n          <mo mathsize=\"0.900em\">~</mo>\n        </mover>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{H}}_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">:</span>\n</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "classifier",
                    "inconsistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For Phase B, to train the inconsistency classifier, we use IEMOCAP </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset and the EmoV-DB </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset to construct binary-labeled data pairs. Consistency pairs (label </span>\n  <math alttext=\"y=1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">y=1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) include speech-text pairs from IEMOCAP and neutral emotion speech-text pairs from EmoV-DB. Inconsistent pairs (</span>\n  <math alttext=\"y=0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">y=0</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) are generated from EmoV-DB by pairing neutral transcripts with non-neutral speech recordings of the same utterance ID. In addition, to train the fusion tower, we use only consistency pairs (</span>\n  <math alttext=\"y=1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">y=1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), as cross-modal fusion is meaningful only when the two modalities are emotionally aligned. The tower is trained to integrate the unimodal predictions into a single fused VAD output </span>\n  <math alttext=\"\\mathbf{y}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119858;</mi>\n        <mi mathsize=\"0.900em\">f</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{y}_{f}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, providing a unified estimate rather than separate outputs for each modality.</span>\n</p>\n\n",
                "matched_terms": [
                    "classifier",
                    "inconsistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For Phase&#160;B inconsistency detection, both the speech and text towers are kept frozen, and optimization is performed solely on the classifier head. Each pair of data forms the input after modality-specific linear projections to a 256-dimensional space. We use a batch size of 32 and train for up to 50 epochs with early stopping (patience = 5). Optimization uses AdamW on classifier parameters, with a learning rate of </span>\n  <math alttext=\"1\\times 10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">3</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\times 10^{-3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and weight decay </span>\n  <math alttext=\"0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.01</mn>\n      <annotation encoding=\"application/x-tex\">0.01</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The loss combines binary cross-entropy and a margin term (margin </span>\n  <math alttext=\"m=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">m</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.9</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">m=0.9</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"\\lambda=0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#955;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.15</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\lambda=0.15</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). For fusion tower, we keep the batch size at 16 and train for up to 50 epochs. Optimization uses AdamW with learning rate </span>\n  <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, weight decay 0.01, and a cosine schedule with 10% warm-up. We use CCC as the evaluation metric. For data split, we use the same speaker-independent split as in Phase&#160;A for IEMOCAP dataset, while EmoV-DB utterances are partitioned into 8/1/1 train/validation/test sets.</span>\n</p>\n\n",
                "matched_terms": [
                    "detection",
                    "classifier",
                    "inconsistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this study, we propose InconVAD, a cross-modal emotion inconsistency detection framework grounded in a shared three-dimensional VAD emotion space. The framework produces interpretable and comparable VAD predictions from both speech and text, enabling effective inconsistency detection across modalities. This work establishes a foundation for building more emotionally intelligent and trustworthy human&#8211;computer interaction systems in real-world applications. Furthermore, our study highlights the importance of explicitly modeling cross-modal inconsistencies rather than assuming unimodal agreement, paving the way for more reliable multimodal affective computing systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "detection",
                    "inconsistency"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "InconVAD: A Two-Stage Dual-Tower Framework for Multimodal Emotion Inconsistency Detection",
        "caption": "Table 3: Ablation results for the speech, text, and fusion towers, with all metrics reported in CCC.",
        "body": "Method\n\n\nV\n\n\n\n\nA\n\n\n\n\nD\n\n\n\n\nAvg\n\n\n\n\nw/o Prosody Injection\n\n\n0.608\n\n\n\n\n0.634\n\n\n\n\n0.514\n\n\n\n\n0.585\n\n\n\n\nw/o Conformer Blocks\n\n\n0.592\n\n\n\n\n0.661\n\n\n\n\n0.499\n\n\n\n\n0.584\n\n\n\n\nw/o Attentive Statistics Pooling\n\n\n0.627\n\n\n\n\n0.654\n\n\n\n\n0.556\n\n\n\n\n0.612\n\n\n\n\nOurs (Speech Tower)\n\n\n0.639\n\n\n\n\n0.669\n\n\n\n\n0.541\n\n\n\n\n0.616\n\n\n\n\nw/o Affect Prior Gating\n\n\n0.776\n\n\n\n\n0.447\n\n\n\n\n0.406\n\n\n\n\n0.543\n\n\n\n\nw/o Attentive Statistics Pooling\n\n\n0.778\n\n\n\n\n0.426\n\n\n\n\n0.435\n\n\n\n\n0.546\n\n\n\n\nOurs (Text Tower)\n\n\n0.784\n\n\n\n\n0.419\n\n\n\n\n0.443\n\n\n\n\n0.549\n\n\n\n\nw/o Transformer Block\n\n\n0.706\n\n\n\n\n0.664\n\n\n\n\n0.554\n\n\n\n\n0.641\n\n\n\n\nw/o Gated Multimodal Fusion\n\n\n0.720\n\n\n\n\n0.622\n\n\n\n\n0.534\n\n\n\n\n0.625\n\n\n\n\nOurs (Fusion Tower)\n\n\n0.741\n\n\n\n\n0.644\n\n\n\n\n0.586\n\n\n\n\n0.657",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Method</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">V</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">A</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">D</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Avg</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">w/o Prosody Injection</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.608</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.634</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.514</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.585</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">w/o Conformer Blocks</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.592</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.661</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.499</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.584</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">w/o Attentive Statistics Pooling</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.627</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.654</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.556</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.612</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Ours (Speech Tower)</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.639</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.669</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.541</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.616</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">w/o Affect Prior Gating</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.776</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.447</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.406</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.543</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">w/o Attentive Statistics Pooling</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.778</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.426</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.435</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.546</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Ours (Text Tower)</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.784</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.419</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.443</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.549</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">w/o Transformer Block</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.706</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.664</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.554</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.641</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">w/o Gated Multimodal Fusion</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.720</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.622</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.534</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.625</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Ours (Fusion Tower)</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.741</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.644</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.586</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t\" style=\"width:22.8pt;padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.657</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "ablation",
            "text",
            "blocks",
            "ours",
            "avg",
            "conformer",
            "transformer",
            "prior",
            "attentive",
            "statistics",
            "speech",
            "all",
            "metrics",
            "tower",
            "results",
            "gated",
            "block",
            "prosody",
            "fusion",
            "gating",
            "pooling",
            "reported",
            "ccc",
            "affect",
            "method",
            "injection",
            "towers",
            "multimodal"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We conduct a series of ablation studies to validate the necessity our architectural design as shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4 EXPERIMENTAL RESULTS &#8227; InconVAD: A Two-Stage Dual-Tower Framework for Multimodal Emotion Inconsistency Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For the speech tower, removing the Conformer blocks or prosody injection substantially reduces average CCC, highlighting the importance of temporal modeling and prosodic cues. Eliminating ASPool module also leads to a consistent drop, confirming its role in emphasizing salient acoustic features. For text tower, removing the affect prior gating decreases average CCC from 0.549 to 0.543, validating the benefit of injecting lexical affective knowledge. Similarly, discarding ASPool module lowers overall performance. For the Fusion Tower, removing Transformer block or the gated multimodal fusion mechanism degrades the average CCC from 0.657 to 0.641 and 0.625, respectively. These results confirm that modeling mutual interactions and dimension-specific modality weighting are both critical for robust cross-modal integration.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Detecting emotional inconsistency across modalities is a key challenge in affective computing, as speech and text often convey conflicting cues. Existing approaches generally rely on incomplete emotion representations and employ unconditional fusion, which weakens performance when modalities are inconsistent. Moreover, little prior work explicitly addresses inconsistency detection itself. We propose InconVAD, a two-stage framework grounded in the Valence&#8211;Arousal&#8211;Dominance (VAD) space. In the first stage, independent uncertainty-aware models yield robust unimodal predictions. In the second stage, a classifier identifies cross-modal inconsistency and selectively integrates consistent signals. Extensive experiments show that InconVAD surpasses existing methods in both multimodal emotion inconsistency detection and modeling, offering a more reliable and interpretable solution for emotion analysis.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "fusion",
                    "prior",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">With the rapid development of human&#8211;computer interaction systems, accurately modeling emotions across multiple modalities has become a central challenge in affective computing </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. A key difficulty lies in the fact that speech and text do not always convey consistent affective states. Such inconsistencies may reflect complex psychological mechanisms, social strategies, or even clinical conditions </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Detecting and quantifying multimodal emotion inconsistency therefore requires a reliable framework that directly compares emotional representations across modalities, rather than relying solely on unimodal cues or generic fusion strategies.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "multimodal",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Current approaches to cross-modal emotion analysis face two fundamental challenges. First, they often rely on inadequate emotional representation models, such as discrete emotion categories, which fail to capture the nuance and continuity of real-world affective expressions. Existing studies on emotion inconsistency at the categorical level typically reduce the task to comparing emotion labels across modalities. For example, </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduced the Multimodal Cross-Attention Bayesian Network (MCABN), which employed attention mechanisms to identify inconsistencies between images and text on social media, using category-based labels with pseudo-label guidance to resolve conflicts. Similarly, </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> proposed a framework for detecting Acoustic&#8211;Text Emotion Inconsistency (ATEI) in depression diagnosis by categorizing emotions into three classes (positive, negative, neutral). While such label-based methods are computationally efficient and offer clear interpretability, they overlook variations in emotional intensity. As a result, they provide only a coarse quantification of inconsistency, leading to the loss of fine-grained affective information in practical applications that require detailed emotional analysis.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Second, most multimodal emotion recognition methods are built upon the implicit assumption that modalities such as speech and text are emotionally congruent </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. When this assumption is violated, their fusion strategies&#8212;typically averaging or concatenation&#8212;produce vague intermediate representations that dilute the literal sentiment expressed in text and obscure the authentic emotional cues in speech </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib6\" title=\"\">6</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Moreover, these models generally lack mechanisms for uncertainty awareness, treating all inputs as equally reliable and failing to assign greater weight to the clearer or more trustworthy modality when discrepancies arise. The absence of explicit modeling for emotion inconsistency thus remains a critical deficiency in current multimodal emotion recognition research, underscoring the need for frameworks that directly address inconsistency rather than treating it as a byproduct of fusion </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "all",
                    "fusion",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these issues, we propose InconVAD, a two-stage framework grounded in the Valence&#8211;Arousal&#8211;Dominance (VAD) space. In the first stage, modality-specific towers independently predict VAD values with uncertainty-aware estimation, providing robust and comparable unimodal representations. In the second stage, a classifier explicitly detects cross-modal inconsistency, while a gated fusion module selectively integrates predictions only for consistent pairs. This design prevents representation collapse in cases such as sarcasm and preserves modality-specific cues that would otherwise be lost. Extensive experiments demonstrate that InconVAD surpasses existing methods in both multimodal emotion inconsistency detection and modeling, while offering greater interpretability through modality-specific VAD predictions.</span>\n</p>\n\n",
                "matched_terms": [
                    "towers",
                    "multimodal",
                    "gated",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As illustrated in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; InconVAD: A Two-Stage Dual-Tower Framework for Multimodal Emotion Inconsistency Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the proposed </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">InconVAD</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> framework operates in two stages. Stage 1 employs two unimodal towers for VAD pretraining: the speech tower and the text tower process raw speech and text inputs, respectively, and output modality-specific representations\n</span>\n  <math alttext=\"\\mathbf{h}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119841;</mi>\n        <mi mathsize=\"0.900em\">s</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{h}_{s}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\mathbf{h}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119841;</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{h}_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, together with VAD means </span>\n  <math alttext=\"\\boldsymbol{\\mu}_{s},\\boldsymbol{\\mu}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#120641;</mi>\n          <mi mathsize=\"0.900em\">s</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">,</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">&#120641;</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n        </msub>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{\\mu}_{s},\\boldsymbol{\\mu}_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and log-variances </span>\n  <math alttext=\"\\log\\boldsymbol{\\sigma}^{2}_{s},\\log\\boldsymbol{\\sigma}^{2}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">log</mi>\n          <mo lspace=\"0.167em\">&#8289;</mo>\n          <msubsup>\n            <mi mathsize=\"0.900em\">&#120648;</mi>\n            <mi mathsize=\"0.900em\">s</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msubsup>\n        </mrow>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mrow>\n          <mi mathsize=\"0.900em\">log</mi>\n          <mo lspace=\"0.167em\">&#8289;</mo>\n          <msubsup>\n            <mi mathsize=\"0.900em\">&#120648;</mi>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msubsup>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\log\\boldsymbol{\\sigma}^{2}_{s},\\log\\boldsymbol{\\sigma}^{2}_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nStage 2 comprises an inconsistency detection head and a gated fusion tower. It takes the outputs of Stage&#160;1, where the detection head predicts an inconsistency score </span>\n  <math alttext=\"p_{\\text{inc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mtext mathsize=\"0.900em\">inc</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">p_{\\text{inc}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the fusion tower, activated only for consistent pairs, generates a fused representation </span>\n  <math alttext=\"\\mathbf{h}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119841;</mi>\n        <mi mathsize=\"0.900em\">f</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{h}_{f}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and the final VAD prediction </span>\n  <math alttext=\"\\mathbf{y}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119858;</mi>\n        <mi mathsize=\"0.900em\">f</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{y}_{f}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe following subsections describe each component in detail.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "tower",
                    "fusion",
                    "towers",
                    "gated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The speech tower extracts both acoustic and prosodic cues for reliable VAD estimation.\nWe employ a pre-trained Wav2Vec2-base model </span>\n  <math alttext=\"f_{SE}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">f</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">S</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">E</mi>\n          </mrow>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">f_{SE}(\\cdot)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to generate frame-level acoustic embeddings </span>\n  <math alttext=\"\\mathbf{H}_{s}\\in\\mathbb{R}^{T_{s}\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119815;</mi>\n          <mi mathsize=\"0.900em\">s</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">T</mi>\n              <mi mathsize=\"0.900em\">s</mi>\n            </msub>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{s}\\in\\mathbb{R}^{T_{s}\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and a prosody extractor </span>\n  <math alttext=\"f_{PE}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">f</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">P</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">E</mi>\n          </mrow>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">f_{PE}(\\cdot)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to compute pitch and energy features </span>\n  <math alttext=\"\\mathbf{p}\\in\\mathbb{R}^{T_{s}\\times 2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119849;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">T</mi>\n              <mi mathsize=\"0.900em\">s</mi>\n            </msub>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mn mathsize=\"0.900em\">2</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{p}\\in\\mathbb{R}^{T_{s}\\times 2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThese complementary features are concatenated and projected through a linear layer to form the input </span>\n  <math alttext=\"\\mathbf{H}_{\\mathrm{in}}\\in\\mathbb{R}^{T_{s}\\times D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119815;</mi>\n          <mi mathsize=\"0.900em\">in</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">T</mi>\n              <mi mathsize=\"0.900em\">s</mi>\n            </msub>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msup>\n              <mi mathsize=\"0.900em\">D</mi>\n              <mo mathsize=\"0.900em\">&#8242;</mo>\n            </msup>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{\\mathrm{in}}\\in\\mathbb{R}^{T_{s}\\times D^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, ensuring that prosodic variation is explicitly incorporated into the acoustic space.\nThe sequence is then processed by two Conformer blocks </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which integrate multi-head self-attention, convolutional modules, and Macaron-style feed-forward layers to capture both local dynamics and long-range dependencies.\nThe resulting contextualized features </span>\n  <math alttext=\"\\mathbf{H}^{\\prime}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">&#119815;</mi>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}^{\\prime}_{s}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are aggregated by an ASPool module </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, producing a fixed-dimensional utterance-level embedding </span>\n  <math alttext=\"\\mathbf{h}_{s}\\in\\mathbb{R}^{D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119841;</mi>\n          <mi mathsize=\"0.900em\">s</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <msup>\n            <mi mathsize=\"0.900em\">D</mi>\n            <mo mathsize=\"0.900em\">&#8242;</mo>\n          </msup>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{h}_{s}\\in\\mathbb{R}^{D^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFinally, prediction heads output per-dimension means </span>\n  <math alttext=\"\\boldsymbol{\\mu}_{s}\\in\\mathbb{R}^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p1.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#120641;</mi>\n          <mi mathsize=\"0.900em\">s</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mn mathsize=\"0.900em\">3</mn>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{\\mu}_{s}\\in\\mathbb{R}^{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and log-variances </span>\n  <math alttext=\"\\log\\boldsymbol{\\sigma}^{2}_{s}\\in\\mathbb{R}^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p1.m9\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">log</mi>\n          <mo lspace=\"0.167em\">&#8289;</mo>\n          <msubsup>\n            <mi mathsize=\"0.900em\">&#120648;</mi>\n            <mi mathsize=\"0.900em\">s</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msubsup>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mn mathsize=\"0.900em\">3</mn>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\log\\boldsymbol{\\sigma}^{2}_{s}\\in\\mathbb{R}^{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, yielding uncertainty-aware unimodal estimates:</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "blocks",
                    "tower",
                    "prosody",
                    "conformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The text tower captures semantic and lexical-level affective cues, complementing the speech tower.\nA RoBERTa-base encoder </span>\n  <math alttext=\"f_{TE}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">f</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">E</mi>\n          </mrow>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">f_{TE}(\\cdot)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> maps tokenized inputs </span>\n  <math alttext=\"\\mathbf{x}\\in\\mathcal{V}_{\\rm text}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119857;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msubsup>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119985;</mi>\n          <mi mathsize=\"0.900em\">text</mi>\n          <mi mathsize=\"0.900em\">L</mi>\n        </msubsup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{x}\\in\\mathcal{V}_{\\rm text}^{L}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to contextual embeddings </span>\n  <math alttext=\"\\mathbf{H}_{t}\\in\\mathbb{R}^{T_{t}\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119815;</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">T</mi>\n              <mi mathsize=\"0.900em\">t</mi>\n            </msub>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{t}\\in\\mathbb{R}^{T_{t}\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which encode rich semantic and syntactic information.\nTo explicitly inject affective knowledge, we employ the NRC VAD Lexicon v2 </span>\n  <math alttext=\"f_{\\mathrm{Prior}}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">f</mi>\n          <mi mathsize=\"0.900em\">Prior</mi>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">f_{\\mathrm{Prior}}(\\cdot)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to derive token-level prior vectors </span>\n  <math alttext=\"\\mathbf{p}_{t}\\in\\mathbb{R}^{T_{t}\\times 3}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119849;</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">T</mi>\n              <mi mathsize=\"0.900em\">t</mi>\n            </msub>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mn mathsize=\"0.900em\">3</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{p}_{t}\\in\\mathbb{R}^{T_{t}\\times 3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, representing valence, arousal, and dominance values.\nThese priors are integrated with the encoder outputs using a FiLM layer </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, producing gated representations </span>\n  <math alttext=\"\\mathbf{H}_{t}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS2.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">&#119815;</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{t}^{\\prime}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> that remain dimensionally consistent while incorporating explicit affective supervision.\nAn ASPool module then aggregates </span>\n  <math alttext=\"\\mathbf{H}_{t}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS2.p1.m7\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">&#119815;</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{t}^{\\prime}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into an utterance-level embedding </span>\n  <math alttext=\"\\mathbf{h}_{t}\\in\\mathbb{R}^{D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS2.p1.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119841;</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <msup>\n            <mi mathsize=\"0.900em\">D</mi>\n            <mo mathsize=\"0.900em\">&#8242;</mo>\n          </msup>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{h}_{t}\\in\\mathbb{R}^{D^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is passed to prediction heads to produce </span>\n  <math alttext=\"\\boldsymbol{\\mu}_{t}\\in\\mathbb{R}^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS2.p1.m9\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#120641;</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mn mathsize=\"0.900em\">3</mn>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{\\mu}_{t}\\in\\mathbb{R}^{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\log\\boldsymbol{\\sigma}^{2}_{t}\\in\\mathbb{R}^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS2.p1.m10\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">log</mi>\n          <mo lspace=\"0.167em\">&#8289;</mo>\n          <msubsup>\n            <mi mathsize=\"0.900em\">&#120648;</mi>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msubsup>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mn mathsize=\"0.900em\">3</mn>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\log\\boldsymbol{\\sigma}^{2}_{t}\\in\\mathbb{R}^{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThis process yields modality-specific textual predictions that align with the speech tower outputs in the shared VAD space:</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "tower",
                    "prior",
                    "gated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To optimize the unimodal towers, we adopt a heteroscedastic regression framework, where the prediction uncertainty is explicitly modeled through variance estimation. The training objective is the Gaussian Negative Log-Likelihood (NLL) of the ground-truth labels </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For a given modality </span>\n  <math alttext=\"m\\in\\{s,t\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">m</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mi mathsize=\"0.900em\">s</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">t</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">m\\in\\{s,t\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (speech or text) and dimension </span>\n  <math alttext=\"k\\in\\{V,A,D\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">k</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mi mathsize=\"0.900em\">V</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">A</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">D</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">k\\in\\{V,A,D\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the loss is defined as:</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "towers",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The inconsistency classifier is designed to assess cross-modal inconsistency without modifying the unimodal feature extractors trained in Phase&#160;A. It operates on speech and text representations, </span>\n  <math alttext=\"\\mathbf{H}_{s}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">&#119815;</mi>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{s}^{\\prime}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\mathbf{H}_{t}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">&#119815;</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{t}^{\\prime}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and determines whether they convey consistent affective states.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our fusion module is an end-to-end architecture that integrates speech and text features to predict VAD emotional dimensions.\nBuilding on the outputs of the speech and text towers, the projected sequences are passed into a cross-modal fusion module.\nInspired by </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we design a Transformer block that jointly models intra- and inter-modal dependencies.\nSpecifically, multi-head self-attention (MHSA) is applied to capture intra-modal relationships (</span>\n  <math alttext=\"\\text{speech}\\!\\to\\!\\text{speech}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mtext mathsize=\"0.900em\">speech</mtext>\n        <mo lspace=\"0.108em\" mathsize=\"0.900em\" rspace=\"0.108em\" stretchy=\"false\">&#8594;</mo>\n        <mtext mathsize=\"0.900em\">speech</mtext>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\text{speech}\\!\\to\\!\\text{speech}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\text{text}\\!\\to\\!\\text{text}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mtext mathsize=\"0.900em\">text</mtext>\n        <mo lspace=\"0.108em\" mathsize=\"0.900em\" rspace=\"0.108em\" stretchy=\"false\">&#8594;</mo>\n        <mtext mathsize=\"0.900em\">text</mtext>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\text{text}\\!\\to\\!\\text{text}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), while multi-head cross-attention (MHCA) enables cross-modal interactions (</span>\n  <math alttext=\"\\text{speech}\\!\\to\\!\\text{text}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mtext mathsize=\"0.900em\">speech</mtext>\n        <mo lspace=\"0.108em\" mathsize=\"0.900em\" rspace=\"0.108em\" stretchy=\"false\">&#8594;</mo>\n        <mtext mathsize=\"0.900em\">text</mtext>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\text{speech}\\!\\to\\!\\text{text}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\text{text}\\!\\to\\!\\text{speech}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mtext mathsize=\"0.900em\">text</mtext>\n        <mo lspace=\"0.108em\" mathsize=\"0.900em\" rspace=\"0.108em\" stretchy=\"false\">&#8594;</mo>\n        <mtext mathsize=\"0.900em\">speech</mtext>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\text{text}\\!\\to\\!\\text{speech}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nThe outputs are subsequently processed with LayerNorm (LN) and feed-forward networks (FFN) to obtain modality-specific contextual representations </span>\n  <math alttext=\"\\mathbf{f}_{s},\\mathbf{f}_{t}\\in\\mathbb{R}^{L\\times D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119839;</mi>\n            <mi mathsize=\"0.900em\">s</mi>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119839;</mi>\n            <mi mathsize=\"0.900em\">t</mi>\n          </msub>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">L</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msup>\n              <mi mathsize=\"0.900em\">D</mi>\n              <mo mathsize=\"0.900em\">&#8242;</mo>\n            </msup>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{f}_{s},\\mathbf{f}_{t}\\in\\mathbb{R}^{L\\times D^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "block",
                    "fusion",
                    "transformer",
                    "towers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To dynamically integrate information across modalities, we utilize the gated multimodal fusion mechanism.\nEach modality is first projected through a learnable weight matrix and then normalized via a softmax function applied along the modality axis.\nThis produces element-wise gates </span>\n  <math alttext=\"\\mathbf{g}_{s},\\mathbf{g}_{t}\\in\\mathbb{R}^{T\\times 1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119840;</mi>\n            <mi mathsize=\"0.900em\">s</mi>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119840;</mi>\n            <mi mathsize=\"0.900em\">t</mi>\n          </msub>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mn mathsize=\"0.900em\">1</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{g}_{s},\\mathbf{g}_{t}\\in\\mathbb{R}^{T\\times 1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which adaptively control the contribution of each modality at every time step.\nThe final fused representation </span>\n  <math alttext=\"\\mathbf{h}_{f}\\in\\mathbb{R}^{T\\times D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119841;</mi>\n          <mi mathsize=\"0.900em\">f</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msup>\n              <mi mathsize=\"0.900em\">D</mi>\n              <mo mathsize=\"0.900em\">&#8242;</mo>\n            </msup>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{h}_{f}\\in\\mathbb{R}^{T\\times D^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is obtained as the weighted combination of modality-specific features.\nThe entire process can be formulated as:</span>\n</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "gated",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"\\mathbf{W}_{s},\\mathbf{W}_{t}\\in\\mathbb{R}^{D^{\\prime}\\times D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119830;</mi>\n            <mi mathsize=\"0.900em\">s</mi>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119830;</mi>\n            <mi mathsize=\"0.900em\">t</mi>\n          </msub>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <msup>\n              <mi mathsize=\"0.900em\">D</mi>\n              <mo mathsize=\"0.900em\">&#8242;</mo>\n            </msup>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msup>\n              <mi mathsize=\"0.900em\">D</mi>\n              <mo mathsize=\"0.900em\">&#8242;</mo>\n            </msup>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{W}_{s},\\mathbf{W}_{t}\\in\\mathbb{R}^{D^{\\prime}\\times D^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are learnable projections,\n</span>\n  <math alttext=\"[\\mathbf{g}_{s},\\mathbf{g}_{t}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119840;</mi>\n          <mi mathsize=\"0.900em\">s</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">,</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119840;</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n        </msub>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">[\\mathbf{g}_{s},\\mathbf{g}_{t}]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are learned gates for speech and text, and </span>\n  <math alttext=\"\\odot\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#8857;</mo>\n      <annotation encoding=\"application/x-tex\">\\odot</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the element-wise product with broadcasting along the feature dimension.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"\\mu_{\\text{agree}}^{k}=\\tfrac{\\mu_{s}^{k}/{\\sigma_{s}^{k}}^{2}+\\mu_{t}^{k}/{\\sigma_{t}^{k}}^{2}}{1/{\\sigma_{s}^{k}}^{2}+1/{\\sigma_{t}^{k}}^{2}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msubsup>\n          <mi mathsize=\"0.900em\">&#956;</mi>\n          <mtext mathsize=\"0.900em\">agree</mtext>\n          <mi mathsize=\"0.900em\">k</mi>\n        </msubsup>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mfrac>\n          <mrow>\n            <mrow>\n              <msubsup>\n                <mi mathsize=\"0.900em\">&#956;</mi>\n                <mi mathsize=\"0.900em\">s</mi>\n                <mi mathsize=\"0.900em\">k</mi>\n              </msubsup>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n              <mmultiscripts>\n                <mi mathsize=\"0.900em\">&#963;</mi>\n                <mi mathsize=\"0.900em\">s</mi>\n                <mi mathsize=\"0.900em\">k</mi>\n                <mrow/>\n                <mn mathsize=\"0.900em\">2</mn>\n              </mmultiscripts>\n            </mrow>\n            <mo mathsize=\"0.900em\">+</mo>\n            <mrow>\n              <msubsup>\n                <mi mathsize=\"0.900em\">&#956;</mi>\n                <mi mathsize=\"0.900em\">t</mi>\n                <mi mathsize=\"0.900em\">k</mi>\n              </msubsup>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n              <mmultiscripts>\n                <mi mathsize=\"0.900em\">&#963;</mi>\n                <mi mathsize=\"0.900em\">t</mi>\n                <mi mathsize=\"0.900em\">k</mi>\n                <mrow/>\n                <mn mathsize=\"0.900em\">2</mn>\n              </mmultiscripts>\n            </mrow>\n          </mrow>\n          <mrow>\n            <mrow>\n              <mn mathsize=\"0.900em\">1</mn>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n              <mmultiscripts>\n                <mi mathsize=\"0.900em\">&#963;</mi>\n                <mi mathsize=\"0.900em\">s</mi>\n                <mi mathsize=\"0.900em\">k</mi>\n                <mrow/>\n                <mn mathsize=\"0.900em\">2</mn>\n              </mmultiscripts>\n            </mrow>\n            <mo mathsize=\"0.900em\">+</mo>\n            <mrow>\n              <mn mathsize=\"0.900em\">1</mn>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n              <mmultiscripts>\n                <mi mathsize=\"0.900em\">&#963;</mi>\n                <mi mathsize=\"0.900em\">t</mi>\n                <mi mathsize=\"0.900em\">k</mi>\n                <mrow/>\n                <mn mathsize=\"0.900em\">2</mn>\n              </mmultiscripts>\n            </mrow>\n          </mrow>\n        </mfrac>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mu_{\\text{agree}}^{k}=\\tfrac{\\mu_{s}^{k}/{\\sigma_{s}^{k}}^{2}+\\mu_{t}^{k}/{\\sigma_{t}^{k}}^{2}}{1/{\\sigma_{s}^{k}}^{2}+1/{\\sigma_{t}^{k}}^{2}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"{\\sigma_{\\text{agree}}^{k}}^{2}=\\tfrac{1}{1/{\\sigma_{s}^{k}}^{2}+1/{\\sigma_{t}^{k}}^{2}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mmultiscripts>\n          <mi mathsize=\"0.900em\">&#963;</mi>\n          <mtext mathsize=\"0.900em\">agree</mtext>\n          <mi mathsize=\"0.900em\">k</mi>\n          <mrow/>\n          <mn mathsize=\"0.900em\">2</mn>\n        </mmultiscripts>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mfrac>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mrow>\n            <mrow>\n              <mn mathsize=\"0.900em\">1</mn>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n              <mmultiscripts>\n                <mi mathsize=\"0.900em\">&#963;</mi>\n                <mi mathsize=\"0.900em\">s</mi>\n                <mi mathsize=\"0.900em\">k</mi>\n                <mrow/>\n                <mn mathsize=\"0.900em\">2</mn>\n              </mmultiscripts>\n            </mrow>\n            <mo mathsize=\"0.900em\">+</mo>\n            <mrow>\n              <mn mathsize=\"0.900em\">1</mn>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n              <mmultiscripts>\n                <mi mathsize=\"0.900em\">&#963;</mi>\n                <mi mathsize=\"0.900em\">t</mi>\n                <mi mathsize=\"0.900em\">k</mi>\n                <mrow/>\n                <mn mathsize=\"0.900em\">2</mn>\n              </mmultiscripts>\n            </mrow>\n          </mrow>\n        </mfrac>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">{\\sigma_{\\text{agree}}^{k}}^{2}=\\tfrac{1}{1/{\\sigma_{s}^{k}}^{2}+1/{\\sigma_{t}^{k}}^{2}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe overall training objective of the fusion tower combines the supervised and selective terms:</span>\n</p>\n\n",
                "matched_terms": [
                    "tower",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For Phase&#160;A, we use the IEMOCAP </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset for the speech tower and the EmoBank </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset for the text tower, both with dimensional Valence&#8211;Arousal&#8211;Dominance (VAD) annotations. To ensure label comparability across datasets, we apply a parametric Beta Cumulative Distribution Function (CDF) transform that maps each original label </span>\n  <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">v</mi>\n      <annotation encoding=\"application/x-tex\">v</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into an aligned value </span>\n  <math alttext=\"v^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">v</mi>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msup>\n      <annotation encoding=\"application/x-tex\">v^{\\prime}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in a shared target distribution. A source value </span>\n  <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">v</mi>\n      <annotation encoding=\"application/x-tex\">v</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is first normalized to </span>\n  <math alttext=\"[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n        <mn mathsize=\"0.900em\">0</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">[0,1]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, then mapped to its quantile </span>\n  <math alttext=\"u=F_{src}(v)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">u</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <msub>\n            <mi mathsize=\"0.900em\">F</mi>\n            <mrow>\n              <mi mathsize=\"0.900em\">s</mi>\n              <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n              <mi mathsize=\"0.900em\">r</mi>\n              <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n              <mi mathsize=\"0.900em\">c</mi>\n            </mrow>\n          </msub>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mi mathsize=\"0.900em\">v</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">u=F_{src}(v)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> using the source CDF, and finally aligned by applying the target inverse CDF, </span>\n  <math alttext=\"v^{\\prime}=F_{tgt}^{-1}(u)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msup>\n          <mi mathsize=\"0.900em\">v</mi>\n          <mo mathsize=\"0.900em\">&#8242;</mo>\n        </msup>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <msubsup>\n            <mi mathsize=\"0.900em\">F</mi>\n            <mrow>\n              <mi mathsize=\"0.900em\">t</mi>\n              <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n              <mi mathsize=\"0.900em\">g</mi>\n              <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n              <mi mathsize=\"0.900em\">t</mi>\n            </mrow>\n            <mrow>\n              <mo mathsize=\"0.900em\">&#8722;</mo>\n              <mn mathsize=\"0.900em\">1</mn>\n            </mrow>\n          </msubsup>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mi mathsize=\"0.900em\">u</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">v^{\\prime}=F_{tgt}^{-1}(u)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The aligned labels </span>\n  <math alttext=\"v^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">v</mi>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msup>\n      <annotation encoding=\"application/x-tex\">v^{\\prime}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are used as training targets, while during evaluation the model predictions </span>\n  <math alttext=\"\\hat{v}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">v</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\hat{v}^{\\prime}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are mapped back through the inverse transform to obtain </span>\n  <math alttext=\"\\hat{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\">\n    <semantics>\n      <mover accent=\"true\">\n        <mi mathsize=\"0.900em\">v</mi>\n        <mo mathsize=\"0.900em\">^</mo>\n      </mover>\n      <annotation encoding=\"application/x-tex\">\\hat{v}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for comparison against the original labels </span>\n  <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m10\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">v</mi>\n      <annotation encoding=\"application/x-tex\">v</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The Beta-CDF process can be formulated as:</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "tower",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For Phase B, to train the inconsistency classifier, we use IEMOCAP </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset and the EmoV-DB </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset to construct binary-labeled data pairs. Consistency pairs (label </span>\n  <math alttext=\"y=1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">y=1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) include speech-text pairs from IEMOCAP and neutral emotion speech-text pairs from EmoV-DB. Inconsistent pairs (</span>\n  <math alttext=\"y=0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">y=0</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) are generated from EmoV-DB by pairing neutral transcripts with non-neutral speech recordings of the same utterance ID. In addition, to train the fusion tower, we use only consistency pairs (</span>\n  <math alttext=\"y=1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">y=1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), as cross-modal fusion is meaningful only when the two modalities are emotionally aligned. The tower is trained to integrate the unimodal predictions into a single fused VAD output </span>\n  <math alttext=\"\\mathbf{y}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119858;</mi>\n        <mi mathsize=\"0.900em\">f</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{y}_{f}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, providing a unified estimate rather than separate outputs for each modality.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tower",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In Phase&#160;A, both towers use pretrained backbones, Wav2Vec2-base for speech and RoBERTa-base for text (hidden size 768), followed by projection layers of dimension 256. Training is performed with a batch size of 16 for up to 50 epochs with early stopping (patience = 5). Optimization uses AdamW with learning rates of </span>\n  <math alttext=\"2\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">2</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">5</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">2\\times 10^{-5}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for the backbone and </span>\n  <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for the heads, combined with a cosine schedule and 10% warm-up. Weight decay is set to 0.01. The minimum variance of </span>\n  <math alttext=\"2\\times 10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">2</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">3</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">2\\times 10^{-3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is used by the heteroscedastic Gaussian NLL loss function. We use the concordance correlation coefficient (CCC) as the evaluation metric. To avoid data leakage, we use a speaker-independent split for IEMOCAP. All utterances from each of the ten speakers go to a single partition with an 8/1/1 train/validation/test ratio via group-based splitting. For EmoBank, we follow the official train/validation/test split annotated in the corpus.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "all",
                    "ccc",
                    "towers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For Phase&#160;B inconsistency detection, both the speech and text towers are kept frozen, and optimization is performed solely on the classifier head. Each pair of data forms the input after modality-specific linear projections to a 256-dimensional space. We use a batch size of 32 and train for up to 50 epochs with early stopping (patience = 5). Optimization uses AdamW on classifier parameters, with a learning rate of </span>\n  <math alttext=\"1\\times 10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">3</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\times 10^{-3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and weight decay </span>\n  <math alttext=\"0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.01</mn>\n      <annotation encoding=\"application/x-tex\">0.01</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The loss combines binary cross-entropy and a margin term (margin </span>\n  <math alttext=\"m=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">m</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.9</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">m=0.9</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"\\lambda=0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#955;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.15</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\lambda=0.15</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). For fusion tower, we keep the batch size at 16 and train for up to 50 epochs. Optimization uses AdamW with learning rate </span>\n  <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, weight decay 0.01, and a cosine schedule with 10% warm-up. We use CCC as the evaluation metric. For data split, we use the same speaker-independent split as in Phase&#160;A for IEMOCAP dataset, while EmoV-DB utterances are partitioned into 8/1/1 train/validation/test sets.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "ccc",
                    "tower",
                    "fusion",
                    "towers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In Phase&#160;A, our unimodal speech and text towers obtain average CCCs of 0.616 on IMEOCAP dataset and 0.549 on Emobank dataset, respectively. The fusion tower achieves 0.657, surpassing the existing state-of-the-art fusion models, including Dimensional MTL&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Two-stage SVM &#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, RL-MT &#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, MFCNN14 &#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and W2v2-b + BERT-b + L &#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, as shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Datasets &#8227; 3 EXPERIMENTAL SETUPS &#8227; InconVAD: A Two-Stage Dual-Tower Framework for Multimodal Emotion Inconsistency Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The consistent gains across Valence, Arousal, and Dominance highlight the complementary strengths of speech and text tower, and the effectiveness of our transformer blocks and gated fusion mechanism.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "blocks",
                    "tower",
                    "fusion",
                    "transformer",
                    "towers",
                    "gated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For the inconsistency detection task, our classifier achieves the best performance across reported metrics. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4 EXPERIMENTAL RESULTS &#8227; InconVAD: A Two-Stage Dual-Tower Framework for Multimodal Emotion Inconsistency Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, On the test set, it attains an accuracy of 92.3% and an F1-score of 92.2%, surpassing prior methods (SVM </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib28\" title=\"\">28</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, ATEI </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20140v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). Precision and recall are likewise competitive, confirming that the leakage-free training protocol and composite loss design enable clear separation between consistent and inconsistent pairs. The decision threshold </span>\n  <math alttext=\"\\tau^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">&#964;</mi>\n        <mo mathsize=\"0.900em\">&#8727;</mo>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\tau^{*}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> was fixed based on validation by maximizing Youden&#8217;s </span>\n  <math alttext=\"J\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">J</mi>\n      <annotation encoding=\"application/x-tex\">J</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, ensuring fair evaluation.</span>\n</p>\n\n",
                "matched_terms": [
                    "prior",
                    "metrics",
                    "reported"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this study, we propose InconVAD, a cross-modal emotion inconsistency detection framework grounded in a shared three-dimensional VAD emotion space. The framework produces interpretable and comparable VAD predictions from both speech and text, enabling effective inconsistency detection across modalities. This work establishes a foundation for building more emotionally intelligent and trustworthy human&#8211;computer interaction systems in real-world applications. Furthermore, our study highlights the importance of explicitly modeling cross-modal inconsistencies rather than assuming unimodal agreement, paving the way for more reliable multimodal affective computing systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "multimodal"
                ]
            }
        ]
    }
}