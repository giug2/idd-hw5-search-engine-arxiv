{
    "S3.T2.fig1": {
        "source_file": "Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation",
        "caption": "Table 1: \nPerformance of Ming-Flash-Omni on Vision-to-Text Benchmarks compared to leading models.* denotes metrics tested using the official benchmark prompts.",
        "body": "Type\nBenchmark\n\n \n\n\nMing-Flash\n\nOmni\n\n\n\n \n\n\nQwen3-Omni\n\n30B-A3B\n\n\n\n \n\n\nQwen3-VL\n\n30B-A3B\n\n\n\n \n\n\nInternVL3.5\n\n30B-A3B\n\n\n\n\nGeneral\nMMStar\n68.3\n68.5\n72.1\n72\n\n\nAI2D\n85.2\n85.2\n86.9\n86.8\n\n\nHallusionBench\n61.1\n59.7\n61.5\n53.8\n\n\nCV-bench\n81.6\n-\n-\n77.3\n\n\nMathVista_MINI\n81.9\n80.0\n81.9\n80.9\n\n\n\nCRPE\n78.4\n-\n\n80.0*\n\n77.6\n\n\nOCR\nChartQA\n88.4\n87.5\n87.1\n87.4\n\n\nDocVQA\n94.8\n95.0\n95.2\n94.2\n\n\nOCRBench\n879\n860\n903\n880\n\n\nTextVQA\n82.6\n81.7\n81.7\n80.5\n\n\nComplex instruction\nMIA-Bench\n93.8\n94.5\n94.4\n-\n\n\nMulti-image\nMMTBench_val_mi\n68.0\n-\n65.7*\n-\n\n\nMuirBench\n61.5\n-\n62.9\n-\n\n\nLlava_interleave_Bench\n63.3\n-\n51.1*\n-\n\n\nVideo\nMVBench\n74.6\n-\n72.3\n72.1\n\n\nVideoMME\n70.9\n70.5\n74.5\n68.7\n\n\nVideoMME_w_subtitle\n73.0\n-\n-\n71.8\n\n\nLongVideoBench\n61.7\n-\n-\n63.8",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Type</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Benchmark</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Ming-Flash</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Omni</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Qwen3-Omni</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">30B-A3B</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Qwen3-VL</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">30B-A3B</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">InternVL3.5</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">30B-A3B</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"5\"><span class=\"ltx_text\" style=\"font-size:90%;\">General</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">MMStar</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">68.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">68.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">72.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">72</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">AI2D</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">85.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">85.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">86.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">86.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">HallusionBench</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">61.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">59.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">61.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">53.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">CV-bench</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">81.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">77.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">MathVista_MINI</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">81.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">80.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">81.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">80.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">CRPE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">78.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">80.0</span><span class=\"ltx_text\" style=\"font-size:90%;\">*</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">77.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text\" style=\"font-size:90%;\">OCR</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ChartQA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">88.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">87.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">87.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">87.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">DocVQA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">94.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">95.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">95.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">94.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">OCRBench</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">879</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">860</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">903</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">880</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">TextVQA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">82.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">81.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">81.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">80.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Complex instruction</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">MIA-Bench</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">93.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">94.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">94.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\"><span class=\"ltx_text\" style=\"font-size:90%;\">Multi-image</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">MMTBench_val_mi</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">68.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">65.7*</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">MuirBench</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">61.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">62.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Llava_interleave_Bench</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">63.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">51.1*</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text\" style=\"font-size:90%;\">Video</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">MVBench</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">74.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">72.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">72.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">VideoMME</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">74.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">68.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">VideoMME_w_subtitle</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">73.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">LongVideoBench</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">61.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">63.8</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "leading",
            "llavainterleavebench",
            "instruction",
            "mmstar",
            "benchmark",
            "tested",
            "longvideobench",
            "mingflashomni",
            "miabench",
            "video",
            "denotes",
            "visiontotext",
            "textvqa",
            "qwen3omni",
            "general",
            "ocrbench",
            "complex",
            "compared",
            "crpe",
            "docvqa",
            "mmtbenchvalmi",
            "hallusionbench",
            "mvbench",
            "metrics",
            "30ba3b",
            "chartqa",
            "performance",
            "videommewsubtitle",
            "muirbench",
            "qwen3vl",
            "ocr",
            "mathvistamini",
            "ai2d",
            "omni",
            "official",
            "videomme",
            "internvl35",
            "benchmarks",
            "models",
            "multiimage",
            "cvbench",
            "mingflash",
            "prompts",
            "type"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion total parameters, of which only 6.1 billion are active per token. This architecture enables <span class=\"ltx_text ltx_font_bold\">highly efficient scaling</span> (dramatically improving computational efficiency while significantly expanding model capacity) and empowers <span class=\"ltx_text ltx_font_bold\">stronger unified multimodal intelligence</span> across vision, speech, and language, representing a key step toward Artificial General Intelligence (AGI). Compared to its predecessor, the upgraded version exhibits substantial improvements across multimodal understanding and generation. We significantly advance speech recognition capabilities, achieving state-of-the-art performance in <span class=\"ltx_text ltx_font_bold\">contextual ASR</span> and highly competitive results in <span class=\"ltx_text ltx_font_bold\">dialect-aware ASR</span>. In image generation, Ming-Flash-Omni introduces <span class=\"ltx_text ltx_font_bold\">high-fidelity text rendering</span> and demonstrates marked gains in <span class=\"ltx_text ltx_font_bold\">scene consistency</span> and <span class=\"ltx_text ltx_font_bold\">identity preservation</span> during image editing. Furthermore, Ming-Flash-Omni introduces <span class=\"ltx_text ltx_font_bold\">generative segmentation</span>, a capability that not only achieves strong standalone segmentation performance but also enhances spatial control in image generation and improves editing consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in text-to-image generation and generative segmentation, and sets new records on all 12 contextual ASR benchmarks, all within a single unified architecture.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "benchmarks",
                    "general",
                    "compared",
                    "mingflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>, which builds upon the  <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> architecture with a redesigned foundation and targeted enhancements across multimodal understanding and generation. At its core,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> adopts Ling-Flash-2.0 <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lingv2_flash</span></cite> (a scaled-up, highly sparse Mixture-of-Experts architecture) where an increased sparsity ratio enables substantial model capacity while maintaining bounded inference latency, striking a favorable trade-off between performance and efficiency.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the understanding side, the model introduces two key advances. First,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> upgrade the positional encoding to VideoRoPE <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wei2025videorope</span></cite>, a refined variant specifically designed to better capture temporal dynamics in video sequences, thereby enhancing the model&#8217;s ability to understand complex visual events. Second,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> focus on improving the context-aware ASR capability itself, enhancing the model&#8217;s ability to leverage surrounding linguistic context during speech recognition and thereby achieving more accurate transcription in context-dependent scenarios.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "complex",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These architectural innovations empower <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> to deliver exceptional cross-modal performance in both comprehension and generation tasks. Specifically, in the image perception task, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> attained performance comparable to that of Qwen3-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen3</span>)</cite>. <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> also delivers superior performance in end-to-end speech understanding and generation.For instance, it achieves SOTA on all 12 metrics on ContextASR-Bench.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "metrics",
                    "performance",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remainder of this paper is organized as follows. Section 2 presents the detailed architecture of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>. Sections 3 describes the pretraining and post-training datasets.\nSection 4 reports the evaluation results and compare <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> with recent multimodal models. Sections 5 is conclusion.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> retains the same audio encoder as  <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ai2025ming</span>)</cite>, with optimization efforts focused on improving contextual automatic speech recognition (ASR) performance by incorporating preceding textual context and hotword lists during training. For generation, we replace discrete speech tokens with continuous acoustic latents, avoiding quantization loss and improving fidelity. Specifically, we utilize a fixed, pre-trained audio head based on Qwen2.5 (0.5B parameters), which takes LLM-generated text tokens and downsampled VAE latents as input and autoregressively predicts the conditioning signals for the flow-matching head&#8212;following the paradigm of <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jia2025ditar</span></cite>.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Crucially, this training cultivates a more fundamental, generalizable skill: <span class=\"ltx_text ltx_font_bold\">fine-grained spatio-semantic control</span>, which indirectly resolves the compositionality problem in pure text-to-image generation.\n<span class=\"ltx_text ltx_font_bold\">In our evaluation on the GenEval benchmark, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieved a score of 0.90, surpassing leading non-Reinforcement Learning (non-RL) methods.</span>\nThis result suggests that the foundational skill of spatio-semantic control can effectively generalize to pure text-to-image generation tasks.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "mingflashomni",
                    "leading"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Multi-Image Fusion and Style Transfer.</span> The model can deconstruct and recombine elements from multiple images (e.g., ID from A, background from B) using distinct <span class=\"ltx_text ltx_font_bold\">Concept Vectors</span>. This complex fusion directly relies on the delineation skill acquired through our core training paradigm.</p>\n\n",
                "matched_terms": [
                    "multiimage",
                    "complex"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to large language models (LLMs), training multimodal foundation models presents several key challenges, primarily stemming from <span class=\"ltx_text ltx_font_bold\">data heterogeneity</span> and <span class=\"ltx_text ltx_font_bold\">model heterogeneity</span>. First, data heterogeneity arises from the need to dynamically switch between diverse input modalities (text, images, audio, and video) during training. These modalities exhibit significant differences in tensor shape, most notably in the form of dynamic batch sizes and variable-length sequences. This variability complicates the design of a unified parallel computation layout. As a result, computational workloads become unevenly distributed across processing ranks, leading to load imbalance. Moreover, the frequent allocation and deallocation of GPU memory buffers for inputs of varying shapes induce severe memory fragmentation, substantially degrading training efficiency and hardware utilization. Second, in contrast to large language models (LLMs), which are predominantly based on homogeneous, decoder-only Transformer architectures, multimodal foundation models typically employ modality-specific encoders at the input stage, introducing model heterogeneity. Although these encoders are relatively lightweight in terms of parameter count, they are highly sensitive to parallelization strategies. If not carefully partitioned across devices, they can induce substantial pipeline bubbles (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, idle computation cycles) during pipeline-parallel execution, thereby constraining overall training throughput.</p>\n\n",
                "matched_terms": [
                    "models",
                    "compared",
                    "video",
                    "leading"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We have collected a large and diverse set of training data to enable models to process and understand information from multiple modalities, including text, images, audio and videos.\nThe majority of this data comes from <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ai2025ming</span>)</cite>. In addition, we develop several data processing pipelines to ensure data quality, diversity and deduplication. Establishing an effective multimodal data strategy is essential for the joint multi-modal training, as it facilitates seamless alignment of knowledge across diverse modalities.\nWe categorize the training data based on the core modalities they are designed to enhance, including image, audio, video, and text. The detailed sources and construction methods for each type of data are elaborated in this section.</p>\n\n",
                "matched_terms": [
                    "models",
                    "type",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OCR Data:</span>\nText recognition and document understanding capabilities are crucial for MLLM. We construct a large-scale heterogeneous training dataset with millions of samples, consisting of three data sources: open-source data, expert-collaborative pseudo-labeled data, and human-annotated enhancement data. The expert-collaborative pseudo-labeled data is generated by diagnosing model weaknesses, and using expert models to label targeted data.\nIn addition, to enhance the model&#8217;s capability in text&#8211;visual analysis and logical reasoning, we incorporate the Chain-of-Thought (CoT) paradigm into the training data. We incorporate the open-source ChartQA-PoT dataset to enhance the model&#8217;s numerical reasoning ability on charts and pioneeringly use executable Python code as the intermediate reasoning representation.</p>\n\n",
                "matched_terms": [
                    "ocr",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Reasoning Data:</span>\nIn the reasoning training of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>, we enrich the CoT data to enhance the model&#8217;s reasoning capabilities. These data are primarily constructed around three key themes: mathematical logic, spatial reasoning, and GUI reasoning.\nWe design an efficient CoT generation and filtering pipeline to construct high-quality, well-structured reasoning data that enhances the model&#8217;s multi-step reasoning capability. (1) We sample long CoT data using state-of-the-art multimodal reasoning models (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GEMINI</span>)</cite>) to build an initial CoT pool. (2) We evaluate the accuracy and quality of the synthesized CoT data and filter out the low-quality data. Based on this pipeline, we construct 1.5M multimodal long CoT samples, with a maximum length of 16K tokens.\nBesides, to overcome the limitations of the &#8220;direct answer&#8221; pattern in text-based QA data, we use reinforcement learning models to generate multi-step reasoning traces and final answers.\nExperimental results demonstrate that this data significantly improves <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>&#8217;s performance on complex reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "complex",
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Preference alignment Data:</span>\nTo further enhance user interaction and response quality, we introduce preference alignment data, focusing on three main aspects:\n(1) Instruction intent understanding: To enhance the model&#8217;s ability to understand the true intent behind user instructions, we design a novel instruction intent reasoning paradigm where the model first determines if the instruction is clear. For clear instructions, it provides structured reasoning and final answers. For ambiguous ones, it infers the likely intent, generates clarification prompts, and aligns responses accordingly. By incorporating experience-alignment training, we significantly improve the model&#8217;s intent understanding and enhance the overall user experience.\n(2) Multi-turn conversation: Users often engage in repeated questioning on the same context. To maintain semantic consistency across turns, we decompose complex instructions into multi-turn conversations and generate high-quality responses using expert-annotated LLM pipelines <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">laban2025llms</span>)</cite>. This ensures context retention and reduces performance degradation in multi-turn scenarios.\n(3) Complex multimodal instruction following: Users often provide sequential, interdependent commands. We design a multimodal instruction generation pipeline to generate SFT and DPO-style data to cover basic and complex instruction types <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2024iopo</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qian2024mia</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ding2025mm</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "instruction",
                    "complex",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text generation data:</span>\nWe build a Chinese-English text generation dataset across three difficulty levels:\n(1) Monotonic background text rendering: Text is rendered directly by setting background color, font type, size, color, and position. (2) Text rendering on existing images: texts are rendered on suitable smooth regions obtained by Felzenszwalb algorithm. (3) Text-image integrated rendering: Using the SOTA LLMs to generate text rendering prompts, the advanced generation models (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, Qwen-image <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2025qwen</span>)</cite> and Nano Banana) are used for image generation, followed by OCR for consistency checks, resulting in a high-quality dataset.</p>\n\n",
                "matched_terms": [
                    "ocr",
                    "models",
                    "type",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Video streaming conversation constitutes a fundamental capability for MLLMs in video understanding. However, acquiring large-scale streaming multi-turn conversation data is prohibitively expensive. In this work, we propose a pipeline to systematically synthesize diverse, balanced, and high-quality multi-turn conversation video datasets.\nWe collect 8.2M videos from the internet, ranging from 90 seconds to 10 minutes, and first filter out low-quality videos with high speech density, high shot density, irregular aspect ratios, or low resolution. We then filter out low-information, incoherent, or overly simple videos using SOTA MLLMs. To ensure a balanced dataset, we use advanced embedding models (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, Qwen3-Embedding <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2025qwen3</span>)</cite> and M3-Embedding <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024bge</span>)</cite>) to extract embeddings and then cluster the videos to suppress high-frequency data while preserving long-tail content. Finally, we use SOTA video understanding models to generate high-quality video conversations. This produces 1.2M conversation turns across 5-minute average videos, balanced across various task categories.</p>\n\n",
                "matched_terms": [
                    "models",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present the evaluation details and quantitative examples of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> on both public and in-house benchmarks.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "mingflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The details of the public benchmarks are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S7\" title=\"7 Public Benchmarks &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. As shown in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:table_1_image2text</span><math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T9\" title=\"Table 9 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, our holistic assessment covers more than 50 rigorously curated public benchmarks across the following seven distinct multi-modal dimensions: Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding), Text <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Generation), Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Editing), Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Segmentation), Audio <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding), Text <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Audio (Generation), and Video <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding).</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multi-Dialect and Multi-Domain Audio Understanding Benchmark</span>. To extend audio understanding benchmarks into multi-dialect and multi-domain settings, we constructed two specialized datasets. The multi-dialect dataset was created from 15 regions, while the multi-domain one was curated from six domains. All samples were manually verified for quality by trained annotators. The final datasets comprise 51,986 multi-dialect samples and 10,397 multi-domain samples, with the latter distributed across: Noisy (8,145), Chat (443), Government (462), Health (450), Knowledge (421), and Local Services (476).</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video Streaming Multi-turn Benchmark</span>.\nThe evaluation of video streaming multi-turn dialogue capabilities requires quantifying not only the model&#8217;s understand capability but also assessing its interactive experience, including proactivity and naturalness. Previous streaming dialogue datasets, such as StreamBench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2024streamingbench</span>)</cite> and OvO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">niu2025ovo</span>)</cite>, have primarily focused on the understanding aspect while lacking a thorough evaluation of the interactive experience.\nTo address this gap, we introduce StreamingMultiturnBench.\nTo construct StreamingMultiturnBench, we manually selected 380 videos, carefully ensuring coverage of multiple key domains including life recording, education, TV shows, video games, and documentaries. Then we use SOTA closed-source modelfor machine annotation. Subsequently, a team of 10 human annotators revise and double-check the dialogue content to ensure it aligns with human conversational preferences. This process yielded 2,200 video question-answer pairs.\nDuring evaluation, we use advanced closed-source model, <em class=\"ltx_emph ltx_font_italic\">e.g.</em> GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chatgpt4o</span>)</cite>, to compare the model&#8217;s output against the human-annotated answers, scoring it on a scale of 1 to 5 across the six dimensions: accuracy, completeness, relevance, naturalness, conciseness, and proactivity. The final score is the average for each dimension. To align our metrics with other video benchmarks, we linearly scale the results to a 100-point scale. We commit to open-sourcing and publicly maintaining this benchmark to ensure reproducibility.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "metrics",
                    "video",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct comprehensive evaluations of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> against state-of-the-art MLLMs on over 50 different multimodal benchmarks, as illustrated in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:table_1_image2text</span><math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T9\" title=\"Table 9 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. Extensive experiments demonstrate that <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves comparable performance with leading MLLMs.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "mingflashomni",
                    "performance",
                    "leading"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Vision <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding)</span> As shown in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:table_1_image2text</span>, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates strong and competitive performance across a wide range of vision&#8211;language benchmarks. Specifically, on general-purpose understanding task, it achieves performance on par with leading omni-models (most notably scoring 81.9% on MathVista), though it still exhibits a slight gap compared to state-of-the-art vision&#8211;language models. Similarly, on OCR-centric benchmarks, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> attains state-of-the-art results among omni-modal models, yet remains marginally behind the best proprietary counterparts. In multi-image understanding, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> outperforms the leading open-source vision&#8211;language model Qwen3-VL-30B-A3B on MMT-Bench (68.0) and LLaVA-Interleave Bench (63.3), while showing a minor performance gap on MuirBench, suggesting opportunities for further improvement. In video understanding, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves state-of-the-art performance on MVBench with a score of 74.6, demonstrating robust general video reasoning capabilities. It further exhibits specialized proficiency in processing linguistic content within videos, attaining a leading score of 73.0 on VideoMME with subtitles. Although its performance on long-form video understanding is slightly lower, its consistently strong results across most metrics underscore its advanced video comprehension capabilities.</p>\n\n",
                "matched_terms": [
                    "videomme",
                    "performance",
                    "leading",
                    "benchmarks",
                    "models",
                    "multiimage",
                    "muirbench",
                    "general",
                    "compared",
                    "mingflashomni",
                    "mvbench",
                    "metrics",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Generation)</span>. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T2\" title=\"Table 2 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, our experimental results demonstrate that the generation quality of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> is on par with state-of-the-art diffusion models.\nNotably, on the Geneval benchmark, our model surpasses all non-Reinforcement Learning methods, demonstrating exceptional controllability. This advantage is particularly pronounced in the \"Position\" and \"Color.\" sub-categories.\nOn the DPG-Bench benchmark, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves an overall score of 83.08, a performance level comparable to pure image generation models like SD3-Medium (84.08) and leading unified models like OmniGen2 (83.57).</p>\n\n",
                "matched_terms": [
                    "performance",
                    "leading",
                    "models",
                    "benchmark",
                    "mingflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Editing)</span>. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T6\" title=\"Table 6 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>,\n<span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates impressive image editing performance, surpassing all other unified models.\nSpecifically, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> supports editing instructions in Chinese, achieving performance comparable to that with English instructions. Compared to Qwen-Image-Edit which utilizes a 20B DiT head, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves comparable semantic consistency and perceptual quality with a much more efficient 2B DiT head&#8212;only one-tenth the parameters. This efficiency also translates to remarkable inference speeds, typically between 1-2 seconds per generation.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "mingflashomni",
                    "models",
                    "compared"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Segmentation)</span>. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T6\" title=\"Table 6 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> is capable of performing segmentation tasks, achieving performance comparable to that of specialized models designed explicitly for this purpose. Compared to other unified MLLMs, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates a significant advantage in segmentation.\nFor instance, Qwen-Image-Edit often struggles to accurately localize the target object, while Nano-banana frequently misinterprets user intent during inference.\nIn contrast, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> exhibits superior robustness and a more accurate understanding of spatial and semantic instructions.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "mingflashomni",
                    "models",
                    "compared"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding)</span>. Our model sets a new state-of-the-art (SOTA) on all 12 sub-tasks of the ContextASR-Bench (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T6\" title=\"Table 6 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>), underscoring its superior ability to leverage context&#8212;a vital skill for real-world applications like multi-turn dialogue and hotword enhancement. It also exhibits highly competitive performance across various ASR benchmarks, with notable strengths in dialect recognition (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T8\" title=\"Table 8 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>). In audio question answering, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> surpasses all open-source audio-centric and other Omni models, with the exception of Qwen3-Omni-Flash-Instruct<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen3omnitechnicalreport</span>)</cite> (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T8\" title=\"Table 8 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>). Taken together, these findings demonstrate the robust and versatile audio understanding capabilities of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "benchmarks",
                    "models",
                    "omni",
                    "mingflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p7.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Audio (Generation)</span>. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T6\" title=\"Table 6 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, leveraging advancements in speech representation and model architecture, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves SOTA performance among open-source models on the test-zh subset of the SEED-TTS-Eval benchmark<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite>. Furthermore, its WER on the test-en subset is surpassed only by that of Qwen3-Omni.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "models",
                    "performance",
                    "qwen3omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video + Audio <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p8.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Video Streaming Conversation)</span>.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T9\" title=\"Table 9 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, benefiting from the introduction of high-quality and diverse streaming video multiturn data, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> has achieved significant improvements in all dimensions compared to <span class=\"ltx_text ltx_font_typewriter\">Ming-Lite-Omni</span>. <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> also outperforms Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen2</span>)</cite> in the dimensions of accuracy, completeness, relevance, and conciseness, providing better experience in streaming video conversation scenarios.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "compared",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present several visualization examples to better illustrate the capabilities of  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>. First, as shown in the figure,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates strong visual understanding across multiple dimensions: it leverages rich world knowledge to accurately infer geographic locations from visual cues; excels at multi-image understanding and generates creative, coherent text grounded in multiple images; solves complex mathematical problems through clear, step-by-step reasoning; and exhibits robust document understanding by accurately parsing intricate formulas and answering questions about sophisticated charts and diagrams within images. Turning to speech recognition,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves strong performance on contextual ASR tasks. By leveraging contextual information, it effectively resolves many challenging cases where conventional ASR systems tend to fail&#8212;such as ambiguous homophones, domain-specific terminology, or noisy conversational speech. Moreover, this version also supports multiple Chinese dialects, significantly broadening its applicability in real-world multilingual and regional speech scenarios. Lastly, we visualize the capabilities of <span class=\"ltx_text ltx_font_italic\">Text/Image\n&#8594;Image</span> generation tasks in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.F4\" title=\"Figure 4 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.F5\" title=\"Figure 5 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, covering a wide range of applications including image generation, image editing, image segmentation, multi-image editing, ID photo generation, ID photo editing, and background replacement.\nAs can be seen,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> not only supports a broader set of generative capabilities but also achieves higher output quality and greater controllability compared to previous versions.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "multiimage",
                    "complex",
                    "compared",
                    "mingflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>, built upon Ling-Flash-2.0 with 100 billion parameters, where only 6.1B parameters are activated per token. <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates advanced multimodal perception and generation capabilities with improved computational efficiency while scaling model capacity. It achieves SOTA performance across a broad spectrum of tasks, including multi-image and video processing, image generation, generative segmentation, Contextual Automatic Speech Recognition (ContextASR), and multi-dialect recognition, outperforming omni models of comparable scale. We believe the open-sourcing of our models and code will facilitate the development of AGI by advancing multimodal intelligence research and enabling broader real-world applications.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "models",
                    "multiimage",
                    "omni",
                    "video",
                    "mingflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:120%;\">Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding)</span>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. Our evaluation of the image-to-text understanding capabilities primarily encompasses the following six tasks: 1) general image understanding capabilities evaluated on MMStar </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024we</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, AI2D </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kembhavi2016diagram</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, HallusionBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Guan_2024_hallusionbench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, CV-Bench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tong2024cambrian1</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, MathVista </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lu2024mathvista</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, and CRPE </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024crpe</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. 2) OCR capabilities evaluated on ChartQA </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">masry-etal-2022-chartqa</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, DocVQA </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">docvqa</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, OCRBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Liu_2024</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, and TextVQA-VAL </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Eval_textvqa</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. 3) multi-image capabilities evaluated on MMTBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ying2024mmtbench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, MuirBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024muirbench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, and LLaVA-interleave Bench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2024llava_interleave_bench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. 4) Complex instruction capabilities evaluated on MIA-Bench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qian2024mia</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. 5) video understanding capabilities evaluated on MVBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2024mvbench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, VideoMME </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fu2024videomme</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, and LongVideoBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2024longvideobench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "mmstar",
                    "longvideobench",
                    "miabench",
                    "video",
                    "general",
                    "ocrbench",
                    "complex",
                    "docvqa",
                    "crpe",
                    "hallusionbench",
                    "mvbench",
                    "chartqa",
                    "muirbench",
                    "ocr",
                    "ai2d",
                    "multiimage",
                    "cvbench",
                    "videomme"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:120%;\">Video <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p7.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding)</span>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. Our evaluation of the video-to-text understanding capabilities contains the following four benchmarks: MVBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2024mvbench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, VideoMME </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fu2024video</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\"> and LongVideoBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2024longvideobench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "longvideobench",
                    "mvbench",
                    "videomme",
                    "video"
                ]
            }
        ]
    },
    "S3.T2.fig2": {
        "source_file": "Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation",
        "caption": "Table 2: \nPerformance of Ming-Flash-Omni on Text-to-Image Generation Benchmarks compared to leading models. Gen. denotes models for pure image generation, while Uni. denotes models capable of both image understanding and generation. Note that the global best performance is highlighted by an underline, and the local best result in Gen. or Uni. is marked with bold.",
        "body": "Type\nModel\nGenEval\nDPG-Bench\n\n\n\n\n\n\n\n\n\n\n\n\n1-Obj.\n2-Obj.\nCount\nColors\nPosit.\nColor.\nAVG\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLlamaGen\n\n\n\n\n0.71\n\n\n\n\n0.34\n\n\n\n\n0.21\n\n\n\n\n0.58\n\n\n\n\n0.07\n\n\n\n\n0.04\n\n\n\n\n0.32\n\n\n\n\n-\n\n\n\n\n\n\n\n\n\n\n\nLDM\n\n\n\n\n0.92\n\n\n\n\n0.29\n\n\n\n\n0.23\n\n\n\n\n0.70\n\n\n\n\n0.02\n\n\n\n\n0.05\n\n\n\n\n0.37\n\n\n\n\n-\n\n\n\n\n\n\n\n\n\n\n\nSDv1.5\n\n\n\n\n0.97\n\n\n\n\n0.38\n\n\n\n\n0.35\n\n\n\n\n0.76\n\n\n\n\n0.04\n\n\n\n\n0.06\n\n\n\n\n0.43\n\n\n\n\n-\n\n\n\n\n\n\n\n\n\n\n\nPixArt-\\alpha\n\n\n\n\n0.98\n\n\n\n\n0.50\n\n\n\n\n0.44\n\n\n\n\n0.80\n\n\n\n\n0.08\n\n\n\n\n0.07\n\n\n\n\n0.48\n\n\n\n\n-\n\n\n\n\nGen.\n\n\nSDv2.1\n\n\n\n\n0.98\n\n\n\n\n0.51\n\n\n\n\n0.44\n\n\n\n\n0.85\n\n\n\n\n0.07\n\n\n\n\n0.17\n\n\n\n\n0.50\n\n\n\n\n68.09\n\n\n\n\n\n\n\n\n\n\n\nEmu3-Gen\n\n\n\n\n0.98\n\n\n\n\n0.71\n\n\n\n\n0.34\n\n\n\n\n0.81\n\n\n\n\n0.17\n\n\n\n\n0.21\n\n\n\n\n0.54\n\n\n\n\n80.60\n\n\n\n\n\n\n\n\n\n\n\nSDXL\n\n\n\n\n0.98\n\n\n\n\n0.74\n\n\n\n\n0.39\n\n\n\n\n0.85\n\n\n\n\n0.15\n\n\n\n\n0.23\n\n\n\n\n0.55\n\n\n\n\n74.65\n\n\n\n\n\n\n\n\n\n\n\nDALL-E 3\n\n\n\n\n0.96\n\n\n\n\n0.87\n\n\n\n\n0.47\n\n\n\n\n0.83\n\n\n\n\n0.43\n\n\n\n\n0.45\n\n\n\n\n0.67\n\n\n\n\n-\n\n\n\n\n\n\n\n\n\n\n\nSD3-Medium\n\n\n\n\n0.99\n\n\n\n\n0.94\n\n\n\n\n0.72\n\n\n\n\n0.89\n\n\n\n\n0.33\n\n\n\n\n0.60\n\n\n\n\n0.74\n\n\n\n\n84.08\n\n\n\n\n\n\n\n\n\n\n\nLWM\n\n\n\n\n0.93\n\n\n\n\n0.41\n\n\n\n\n0.46\n\n\n\n\n0.79\n\n\n\n\n0.09\n\n\n\n\n0.15\n\n\n\n\n0.47\n\n\n\n\n-\n\n\n\n\n\n\n\n\n\n\n\nSEED-X\n\n\n\n\n0.97\n\n\n\n\n0.58\n\n\n\n\n0.26\n\n\n\n\n0.80\n\n\n\n\n0.19\n\n\n\n\n0.14\n\n\n\n\n0.49\n\n\n\n\n-\n\n\n\n\n\n\n\n\n\n\n\nShow-o\n\n\n\n\n0.95\n\n\n\n\n0.52\n\n\n\n\n0.49\n\n\n\n\n0.82\n\n\n\n\n0.11\n\n\n\n\n0.28\n\n\n\n\n0.53\n\n\n\n\n-\n\n\n\n\nUni.\n\n\nTokenFlow-XL\n\n\n\n\n0.95\n\n\n\n\n0.60\n\n\n\n\n0.41\n\n\n\n\n0.81\n\n\n\n\n0.16\n\n\n\n\n0.24\n\n\n\n\n0.55\n\n\n\n\n-\n\n\n\n\n\n\n\n\n\n\n\nJanus\n\n\n\n\n0.97\n\n\n\n\n0.68\n\n\n\n\n0.30\n\n\n\n\n0.84\n\n\n\n\n0.46\n\n\n\n\n0.42\n\n\n\n\n0.61\n\n\n\n\n79.68\n\n\n\n\n\n\n\n\n\n\n\nJanusFlow\n\n\n\n\n0.97\n\n\n\n\n0.59\n\n\n\n\n0.45\n\n\n\n\n0.83\n\n\n\n\n0.53\n\n\n\n\n0.42\n\n\n\n\n0.63\n\n\n\n\n80.09\n\n\n\n\n\n\n\n\n\n\n\nJanusPro-7B\n\n\n\n\n0.99\n\n\n\n\n0.89\n\n\n\n\n0.59\n\n\n\n\n0.90\n\n\n\n\n0.79\n\n\n\n\n0.66\n\n\n\n\n0.80\n\n\n\n\n84.19\n\n\n\n\n\n\n\n\n\n\n\nUniWorld-V1\n\n\n\n\n0.98\n\n\n\n\n0.93\n\n\n\n\n0.81\n\n\n\n\n0.89\n\n\n\n\n0.74\n\n\n\n\n0.71\n\n\n\n\n0.84\n\n\n\n\n81.38\n\n\n\n\n\n\n\n\n\n\n\nOmniGen2\n\n\n\n\n0.99\n\n\n\n\n0.96\n\n\n\n\n0.74\n\n\n\n\n0.98\n\n\n\n\n0.71\n\n\n\n\n0.75\n\n\n\n\n0.86\n\n\n\n\n83.57\n\n\n\n\n\n\n\n\n\n\n\nBAGEL\n\n\n\n\n0.99\n\n\n\n\n0.94\n\n\n\n\n0.81\n\n\n\n\n0.88\n\n\n\n\n0.64\n\n\n\n\n0.63\n\n\n\n\n0.82\n\n\n\n\n-\n\n\n\n\n\n\n\n\n\n\n\nQwen-Image\n\n\n\n\n0.99\n\n\n\n\n0.92\n\n\n\n\n0.89\n\n\n\n\n0.88\n\n\n\n\n0.76\n\n\n\n\n0.77\n\n\n\n\n0.87\n\n\n\n\n88.32\n\n\n\n\n\n\n\n\n\n\n\nQwen-Image-RL\n\n\n\n\n1.00\n\n\n\n\n0.95\n\n\n\n\n0.93\n\n\n\n\n0.92\n\n\n\n\n0.87\n\n\n\n\n0.83\n\n\n\n\n0.91\n\n\n\n\n-\n\n\n\n\n\n\n\n\n\n\n\nMing-Flash-Omni\n\n\n\n\n0.99\n\n\n\n\n0.94\n\n\n\n\n0.80\n\n\n\n\n0.91\n\n\n\n\n0.95\n\n\n\n\n0.77\n\n\n\n\n0.90\n\n\n\n\n83.08",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Type</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r ltx_border_t\" colspan=\"7\"><span class=\"ltx_text\" style=\"font-size:90%;\">GenEval</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">DPG-Bench</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:17.1pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1-Obj.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2-Obj.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Count</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Colors</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Posit.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Color.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">AVG</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"/>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:17.1pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LlamaGen</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.71</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.34</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.21</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.58</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.07</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.04</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.32</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:17.1pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LDM</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.92</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.29</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.23</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.70</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.02</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.05</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.37</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:17.1pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SDv1.5</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.97</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.38</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.35</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.76</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.04</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.06</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.43</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:17.1pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">PixArt-</span><math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m1\" intent=\":literal\"><semantics><mi mathsize=\"0.900em\">&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.98</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.50</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.44</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.80</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.08</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.07</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.48</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_align_top\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Gen.</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SDv2.1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.98</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.51</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.44</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.85</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.07</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.17</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.50</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">68.09</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:17.1pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Emu3-Gen</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.98</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.71</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.34</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.81</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.17</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.21</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.54</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">80.60</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:17.1pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SDXL</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.98</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.74</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.39</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.85</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.15</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.23</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.55</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">74.65</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:17.1pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">DALL-E 3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.96</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.87</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.47</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.83</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.43</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.45</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.67</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:17.1pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SD3-Medium</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.99</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.94</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.72</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.89</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.33</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.60</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.74</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">84.08</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:17.1pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LWM</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.93</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.41</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.46</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.79</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.09</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.15</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.47</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:17.1pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SEED-X</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.97</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.58</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.26</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.80</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.19</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.14</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.49</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:17.1pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Show-o</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.95</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.52</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.49</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.82</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.11</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.28</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.53</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_align_top\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Uni.</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">TokenFlow-XL</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.95</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.60</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.41</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.81</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.16</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.24</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.55</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:17.1pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Janus</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.97</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.68</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.30</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.84</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.46</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.42</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.61</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">79.68</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:17.1pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">JanusFlow</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.97</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.59</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.45</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.83</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.53</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.42</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.63</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">80.09</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:17.1pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">JanusPro-7B</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.99</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.89</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.59</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.90</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.79</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.66</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.80</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">84.19</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:17.1pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">UniWorld-V1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.98</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.93</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.81</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.89</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.74</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.71</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.84</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">81.38</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:17.1pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">OmniGen2</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.99</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.96</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.74</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.98</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.71</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.75</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.86</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">83.57</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:17.1pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">BAGEL</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.99</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.94</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.81</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.88</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.64</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.63</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.82</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:17.1pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen-Image</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.99</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.92</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.89</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.88</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.76</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.77</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.87</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">88.32</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#ECECEC;\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#ECECEC;\">\n<span class=\"ltx_p\" style=\"width:17.1pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#ECECEC;\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen-Image-RL</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#ECECEC;\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.00</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#ECECEC;\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.95</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#ECECEC;\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.93</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#ECECEC;\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.92</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#ECECEC;\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.87</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#ECECEC;\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.83</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#ECECEC;\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.91</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#ECECEC;\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:17.1pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Ming-Flash-Omni</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.99</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.94</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.80</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.91</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.95</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.77</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.90</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">83.08</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:17.1pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\"/>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "both",
            "leading",
            "seedx",
            "gen",
            "uni",
            "count",
            "sdxl",
            "pure",
            "mingflashomni",
            "underline",
            "1obj",
            "avg",
            "januspro7b",
            "denotes",
            "uni",
            "best",
            "2obj",
            "posit",
            "capable",
            "qwenimagerl",
            "image",
            "note",
            "compared",
            "colors",
            "marked",
            "llamagen",
            "dalle",
            "bold",
            "sdv15",
            "global",
            "while",
            "emu3gen",
            "lwm",
            "local",
            "tokenflowxl",
            "omnigen2",
            "performance",
            "dpgbench",
            "generation",
            "showo",
            "color",
            "sd3medium",
            "janus",
            "qwenimage",
            "sdv21",
            "gen",
            "highlighted",
            "result",
            "texttoimage",
            "janusflow",
            "benchmarks",
            "models",
            "understanding",
            "ldm",
            "uniworldv1",
            "pixartalpha",
            "bagel",
            "geneval",
            "model",
            "type"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion total parameters, of which only 6.1 billion are active per token. This architecture enables <span class=\"ltx_text ltx_font_bold\">highly efficient scaling</span> (dramatically improving computational efficiency while significantly expanding model capacity) and empowers <span class=\"ltx_text ltx_font_bold\">stronger unified multimodal intelligence</span> across vision, speech, and language, representing a key step toward Artificial General Intelligence (AGI). Compared to its predecessor, the upgraded version exhibits substantial improvements across multimodal understanding and generation. We significantly advance speech recognition capabilities, achieving state-of-the-art performance in <span class=\"ltx_text ltx_font_bold\">contextual ASR</span> and highly competitive results in <span class=\"ltx_text ltx_font_bold\">dialect-aware ASR</span>. In image generation, Ming-Flash-Omni introduces <span class=\"ltx_text ltx_font_bold\">high-fidelity text rendering</span> and demonstrates marked gains in <span class=\"ltx_text ltx_font_bold\">scene consistency</span> and <span class=\"ltx_text ltx_font_bold\">identity preservation</span> during image editing. Furthermore, Ming-Flash-Omni introduces <span class=\"ltx_text ltx_font_bold\">generative segmentation</span>, a capability that not only achieves strong standalone segmentation performance but also enhances spatial control in image generation and improves editing consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in text-to-image generation and generative segmentation, and sets new records on all 12 contextual ASR benchmarks, all within a single unified architecture.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "texttoimage",
                    "performance",
                    "benchmarks",
                    "understanding",
                    "generation",
                    "image",
                    "compared",
                    "marked",
                    "mingflashomni",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Driven by advances in Large Language Models (LLMs) and extensive training on large-scale multimodal datasets, Multi-modal Large Language Models (MLLMs) have demonstrated remarkable perceptual capabilities in both vision <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024internvl</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bai2025qwen2</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2025kimi</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen3</span>)</cite> and audio <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ding2025kimi</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen2</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen3</span>)</cite>, as well as generative capabilities in these two modalities <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">huang2025step</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ding2025kimi</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chatgpt4o</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tong2024metamorph</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pan2025transfer</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen3</span>)</cite>. Nevertheless, effectively integrating comprehension and generation across multiple modalities into a unified model remains challenging. While humans naturally learn by combining multiple modalities, leveraging their complementary strengths and interactions to enhance overall learning efficiency, building a unified Omni-MLLM is hindered by representational disparities and modality imbalances.</p>\n\n",
                "matched_terms": [
                    "both",
                    "models",
                    "generation",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>, which builds upon the  <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> architecture with a redesigned foundation and targeted enhancements across multimodal understanding and generation. At its core,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> adopts Ling-Flash-2.0 <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lingv2_flash</span></cite> (a scaled-up, highly sparse Mixture-of-Experts architecture) where an increased sparsity ratio enables substantial model capacity while maintaining bounded inference latency, striking a favorable trade-off between performance and efficiency.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "understanding",
                    "generation",
                    "mingflashomni",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the understanding side, the model introduces two key advances. First,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> upgrade the positional encoding to VideoRoPE <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wei2025videorope</span></cite>, a refined variant specifically designed to better capture temporal dynamics in video sequences, thereby enhancing the model&#8217;s ability to understand complex visual events. Second,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> focus on improving the context-aware ASR capability itself, enhancing the model&#8217;s ability to leverage surrounding linguistic context during speech recognition and thereby achieving more accurate transcription in context-dependent scenarios.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "understanding",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the generation side, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> introduces three key advancements: 1) in speech synthesis, discrete acoustic tokens are replaced with continuous representations, effectively circumventing quantization-induced artifacts and yielding more natural and expressive TTS outputs; 2) the model supports generative semantic segmentation, enabling pixel-level semantic content generation conditioned on multimodal inputs; and 3) it enables fine-grained controllable image generation with improved identity preservation and in-image text generation capabilities.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mingflashomni",
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These architectural innovations empower <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> to deliver exceptional cross-modal performance in both comprehension and generation tasks. Specifically, in the image perception task, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> attained performance comparable to that of Qwen3-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen3</span>)</cite>. <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> also delivers superior performance in end-to-end speech understanding and generation.For instance, it achieves SOTA on all 12 metrics on ContextASR-Bench.</p>\n\n",
                "matched_terms": [
                    "both",
                    "performance",
                    "understanding",
                    "generation",
                    "image",
                    "mingflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remainder of this paper is organized as follows. Section 2 presents the detailed architecture of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>. Sections 3 describes the pretraining and post-training datasets.\nSection 4 reports the evaluation results and compare <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> with recent multimodal models. Sections 5 is conclusion.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S2.F2\" title=\"Figure 2 &#8227; 2 Ming-Flash-Omni &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> retains the unified two-stage pipeline of <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ai2025ming</span></cite>, where perception supports multimodal understanding and generation targets speech and image synthesis, while markedly advancing long-context modeling, reasoning, and controllable generation. At the core is Ling&#8209;Flash&#8209;2.0 <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lingv2_flash</span></cite>, a sparse MoE LM (100B; 6.1B per token) with a dual balancing scheme that stabilizes training and improves efficiency. On the perception side, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> employs VideoRoPE <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wei2025videorope</span></cite> for temporal modeling, context&#8209;aware ASR for more reliable speech understanding, and a think mode for deeper multi&#8209;step reasoning. On the generation side, we replace discrete speech tokens with continuous acoustic latents, avoiding quantization loss and improving fidelity; for images, we upgrade to a synergistic training paradigm that enables generative segmentation-as-editing, facilitating fine-grained and controllable generation. Overall, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> advances the unified model with stable expert routing and scalable long&#8209;context modeling, yielding more reliable multimodal understanding and controllable generation.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "generation",
                    "image",
                    "mingflashomni",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The cornerstone of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> is enhanced multimodal understanding. We retain the established visual and audio encoders (Qwen2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bai2025qwen25vltechnicalreport</span>)</cite> and Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>)</cite>) and feed their projected embeddings, concatenated with tokenized text, into Ling&#8209;flash&#8209;2.0, a sparse MoE language model with distinct routers per modality. Beyond this, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> incorporates VideoRoPE to maintain temporal coherence over long-range frame sequences, thus emphasizing temporal modeling. Furthermore, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> adopt a context&#8209;aware ASR training paradigm that conditions decoding on task or domain context, addressing common shortcomings of conventional ASR in real&#8209;world, multi&#8209;domain scenarios (limited world knowledge and unreliable proper&#8209;noun recognition) and yielding more accurate, context&#8209;consistent transcripts. To stabilize training in the more sparse Ling&#8209;flash&#8209;2.0, we employ a hybrid expert&#8209;balancing scheme that combines an auxiliary load&#8209;balancing loss (as in <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span>) with per&#8209;router bias updates (as in Ling&#8209;flash&#8209;2.0), promoting uniform expert activation and improved convergence.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "understanding",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> retains the same audio encoder as  <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ai2025ming</span>)</cite>, with optimization efforts focused on improving contextual automatic speech recognition (ASR) performance by incorporating preceding textual context and hotword lists during training. For generation, we replace discrete speech tokens with continuous acoustic latents, avoiding quantization loss and improving fidelity. Specifically, we utilize a fixed, pre-trained audio head based on Qwen2.5 (0.5B parameters), which takes LLM-generated text tokens and downsampled VAE latents as input and autoregressively predicts the conditioning signals for the flow-matching head&#8212;following the paradigm of <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jia2025ditar</span></cite>.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "generation",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A core challenge in building unified multimodal models is the effective fusion of image understanding and generation. While our <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> model injects hierarchical semantics via multi-scale query tokens, its language pathway remains frozen during training to prevent interference from the generative objective. This freezing approach, while ensuring stability, creates a critical bottleneck: an inherent discrepancy between the learning objectives of understanding and generation. Consequently, even with injected hierarchical semantics, fine-grained visual knowledge&#8212;such as object attributes and spatial relationships&#8212;cannot be efficiently transferred to high-precision generation and editing tasks, limiting the model&#8217;s final quality and controllability.</p>\n\n",
                "matched_terms": [
                    "models",
                    "understanding",
                    "generation",
                    "image",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome this bottleneck,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> propose a synergistic training paradigm that reframes image segmentation as a generative editing task. Instead of producing an abstract binary mask (e.g., for &#8220;segment the banana&#8221;), the model performs a semantics-preserving edit (e.g., &#8220;color the banana purple&#8221;). This reformulation forcibly binds the learning objectives of understanding and generation: successful generation requires a precise understanding of the object&#8217;s contours and boundaries. Understanding thus becomes a <span class=\"ltx_text ltx_font_italic\">prerequisite</span> for editing, and the edit&#8217;s quality provides a direct supervisory signal for the model&#8217;s comprehension, fundamentally unifying their optimization objectives.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "generation",
                    "image",
                    "mingflashomni",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Crucially, this training cultivates a more fundamental, generalizable skill: <span class=\"ltx_text ltx_font_bold\">fine-grained spatio-semantic control</span>, which indirectly resolves the compositionality problem in pure text-to-image generation.\n<span class=\"ltx_text ltx_font_bold\">In our evaluation on the GenEval benchmark, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieved a score of 0.90, surpassing leading non-Reinforcement Learning (non-RL) methods.</span>\nThis result suggests that the foundational skill of spatio-semantic control can effectively generalize to pure text-to-image generation tasks.</p>\n\n",
                "matched_terms": [
                    "result",
                    "texttoimage",
                    "leading",
                    "generation",
                    "geneval",
                    "pure",
                    "mingflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Identity (ID) Preservation.</span> We use a VAE-encoded identity vector and a composite loss (global semantic and local pixel) to maintain subject consistency. The model&#8217;s learned boundary perception is crucial for accurately isolating non-edited regions, ensuring high faithfulness.</p>\n\n",
                "matched_terms": [
                    "local",
                    "global"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training procedure of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> retains a two-stage pipeline: perception and generation.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After perception, we freeze the perception MLLM and optimize only the image generator, while leveraging the pre-trained audio generator from  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">MingUniAudio2025</span></cite>. For image generation, the training procedure contains two sequential stages. In the first stage, we pre-train a diffusion-based image generator using a flow matching objective, while keeping the perception MLLM frozen. The generator is equipped with multi-scale learnable queries to capture hierarchical visual semantics from textual inputs. In the second stage, we extend the model to support image editing by conditioning the denoising process on reference images: the VAE-encoded representations of input images are concatenated with the noisy latent to enforce structural and semantic consistency with the original content. Additionally, input word-level captions are encoded via ByteT5 embeddings to enrich textual conditioning.</p>\n\n",
                "matched_terms": [
                    "model",
                    "generation",
                    "image",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to large language models (LLMs), training multimodal foundation models presents several key challenges, primarily stemming from <span class=\"ltx_text ltx_font_bold\">data heterogeneity</span> and <span class=\"ltx_text ltx_font_bold\">model heterogeneity</span>. First, data heterogeneity arises from the need to dynamically switch between diverse input modalities (text, images, audio, and video) during training. These modalities exhibit significant differences in tensor shape, most notably in the form of dynamic batch sizes and variable-length sequences. This variability complicates the design of a unified parallel computation layout. As a result, computational workloads become unevenly distributed across processing ranks, leading to load imbalance. Moreover, the frequent allocation and deallocation of GPU memory buffers for inputs of varying shapes induce severe memory fragmentation, substantially degrading training efficiency and hardware utilization. Second, in contrast to large language models (LLMs), which are predominantly based on homogeneous, decoder-only Transformer architectures, multimodal foundation models typically employ modality-specific encoders at the input stage, introducing model heterogeneity. Although these encoders are relatively lightweight in terms of parameter count, they are highly sensitive to parallelization strategies. If not carefully partitioned across devices, they can induce substantial pipeline bubbles (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, idle computation cycles) during pipeline-parallel execution, thereby constraining overall training throughput.</p>\n\n",
                "matched_terms": [
                    "result",
                    "leading",
                    "models",
                    "count",
                    "compared",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We have collected a large and diverse set of training data to enable models to process and understand information from multiple modalities, including text, images, audio and videos.\nThe majority of this data comes from <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ai2025ming</span>)</cite>. In addition, we develop several data processing pipelines to ensure data quality, diversity and deduplication. Establishing an effective multimodal data strategy is essential for the joint multi-modal training, as it facilitates seamless alignment of knowledge across diverse modalities.\nWe categorize the training data based on the core modalities they are designed to enhance, including image, audio, video, and text. The detailed sources and construction methods for each type of data are elaborated in this section.</p>\n\n",
                "matched_terms": [
                    "models",
                    "image",
                    "type"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Image data serves as the cornerstone of our multi-modal corpus.\nFollowing <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ai2025ming</span>)</cite>, we integrate both image-understanding and image-generation datasets to enable the MLLM to acquire unified perception and generation capabilities. Additionally, we further design novel pipelines to synthesize high-quality datasets across diverse dimensions to improve model&#8217;s capabilities and user interaction quality.</p>\n\n",
                "matched_terms": [
                    "both",
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OCR Data:</span>\nText recognition and document understanding capabilities are crucial for MLLM. We construct a large-scale heterogeneous training dataset with millions of samples, consisting of three data sources: open-source data, expert-collaborative pseudo-labeled data, and human-annotated enhancement data. The expert-collaborative pseudo-labeled data is generated by diagnosing model weaknesses, and using expert models to label targeted data.\nIn addition, to enhance the model&#8217;s capability in text&#8211;visual analysis and logical reasoning, we incorporate the Chain-of-Thought (CoT) paradigm into the training data. We incorporate the open-source ChartQA-PoT dataset to enhance the model&#8217;s numerical reasoning ability on charts and pioneeringly use executable Python code as the intermediate reasoning representation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "understanding",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Reasoning Data:</span>\nIn the reasoning training of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>, we enrich the CoT data to enhance the model&#8217;s reasoning capabilities. These data are primarily constructed around three key themes: mathematical logic, spatial reasoning, and GUI reasoning.\nWe design an efficient CoT generation and filtering pipeline to construct high-quality, well-structured reasoning data that enhances the model&#8217;s multi-step reasoning capability. (1) We sample long CoT data using state-of-the-art multimodal reasoning models (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GEMINI</span>)</cite>) to build an initial CoT pool. (2) We evaluate the accuracy and quality of the synthesized CoT data and filter out the low-quality data. Based on this pipeline, we construct 1.5M multimodal long CoT samples, with a maximum length of 16K tokens.\nBesides, to overcome the limitations of the &#8220;direct answer&#8221; pattern in text-based QA data, we use reinforcement learning models to generate multi-step reasoning traces and final answers.\nExperimental results demonstrate that this data significantly improves <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>&#8217;s performance on complex reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "generation",
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Preference alignment Data:</span>\nTo further enhance user interaction and response quality, we introduce preference alignment data, focusing on three main aspects:\n(1) Instruction intent understanding: To enhance the model&#8217;s ability to understand the true intent behind user instructions, we design a novel instruction intent reasoning paradigm where the model first determines if the instruction is clear. For clear instructions, it provides structured reasoning and final answers. For ambiguous ones, it infers the likely intent, generates clarification prompts, and aligns responses accordingly. By incorporating experience-alignment training, we significantly improve the model&#8217;s intent understanding and enhance the overall user experience.\n(2) Multi-turn conversation: Users often engage in repeated questioning on the same context. To maintain semantic consistency across turns, we decompose complex instructions into multi-turn conversations and generate high-quality responses using expert-annotated LLM pipelines <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">laban2025llms</span>)</cite>. This ensures context retention and reduces performance degradation in multi-turn scenarios.\n(3) Complex multimodal instruction following: Users often provide sequential, interdependent commands. We design a multimodal instruction generation pipeline to generate SFT and DPO-style data to cover basic and complex instruction types <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2024iopo</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qian2024mia</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ding2025mm</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "understanding",
                    "generation",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Structured Data:</span>\nStructured data enhances MLLM capabilities in querying fine-grained knowledge associated with specific visual entities.\nTo enhance the MLLM&#8217;s ability to handle knowledge-intensive and information-seeking queries, we design two pipelines to generate large-scale entity-relation and encyclopedic QA data.\n(1) <em class=\"ltx_emph ltx_font_italic\">Entity-relation QA data:</em> We design a multi-stage pipeline to construct a high-quality entity-relation QA corpus. The pipeline integrates scene graph extraction, visual grounding, entity&#8211;relation verification, and diverse QA generation to produce pairs covering entities, attributes, and relations. To ensure quality, we filter source datasets for complex multi-entity scenes and apply two-stage validation before and after QA generation.\n(2) <em class=\"ltx_emph ltx_font_italic\">Encyclopedia QA Data:</em> We design an automated pipeline to generate encyclopedia entity QA data from Chinese Baidu Encyclopedia and English Wikidata. It constructs &lt;image, entity, knowledge&gt; triplets by extracting entities from images or retrieving images for knowledge-base entities, filtering out invalid ones, and validating image&#8211;text consistency. These triplets are then converted into VQA data using LLMs.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Image generation data enhances MLLM capabilities beyond traditional image understanding tasks. In addition to the image generation data used in <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ai2025ming</span>)</cite>, we specifically integrate image segmentation data, text generation data and portrait preservation data to further improve the user experience.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image segmentation data:</span> To improve the model&#8217;s generative segmentation capability, we construct two types of data: (1) We use the open-source referring segmentation datasets RefCOCO/+/g <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">refcoco</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">refcocog</span>)</cite> to construct image editing data. The original image serves as the reference, and binary masks highlight target regions with specified colors to create edited images. (2) For semantic and panoptic segmentation, samples are built from COCO-Panoptic <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Eval_mscoco</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kirillov2019panoptic</span>)</cite>, where each class or instance is assigned a unique color via a predefined colormap to generate edited images.</p>\n\n",
                "matched_terms": [
                    "colors",
                    "image",
                    "color"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Portrait preservation data:</span> The portrait preservation data consist of two data sources: (1) ID Photo Dataset: We collected and constructed 200k paired lifestyle-ID photos. And filter the data using four criteria, <em class=\"ltx_emph ltx_font_italic\">e.g.</em>, face similarity, face size and confidence, face angle and manual review.\n(2) Landmark Check-In Portrait Dataset: We collect 20K high-quality portraits from 225 landmarks.The original images serve as edited images, while using advanced segmentation model like SAM2 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ravi2024sam</span>)</cite> to segments the foreground person and places them onto 1,000 manually collected daily background scenes as pre-edited images. And then use LLM generate diverse prompts for each landmark.</p>\n\n",
                "matched_terms": [
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text generation data:</span>\nWe build a Chinese-English text generation dataset across three difficulty levels:\n(1) Monotonic background text rendering: Text is rendered directly by setting background color, font type, size, color, and position. (2) Text rendering on existing images: texts are rendered on suitable smooth regions obtained by Felzenszwalb algorithm. (3) Text-image integrated rendering: Using the SOTA LLMs to generate text rendering prompts, the advanced generation models (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, Qwen-image <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2025qwen</span>)</cite> and Nano Banana) are used for image generation, followed by OCR for consistency checks, resulting in a high-quality dataset.</p>\n\n",
                "matched_terms": [
                    "models",
                    "generation",
                    "color",
                    "image",
                    "qwenimage",
                    "type"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For audio data, we mainly use the data from <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ai2025ming</span>)</cite>. In addition, we incorporate the following three datasets to further enhance the model&#8217;s audio understanding and generation capabilities.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Video streaming conversation constitutes a fundamental capability for MLLMs in video understanding. However, acquiring large-scale streaming multi-turn conversation data is prohibitively expensive. In this work, we propose a pipeline to systematically synthesize diverse, balanced, and high-quality multi-turn conversation video datasets.\nWe collect 8.2M videos from the internet, ranging from 90 seconds to 10 minutes, and first filter out low-quality videos with high speech density, high shot density, irregular aspect ratios, or low resolution. We then filter out low-information, incoherent, or overly simple videos using SOTA MLLMs. To ensure a balanced dataset, we use advanced embedding models (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, Qwen3-Embedding <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2025qwen3</span>)</cite> and M3-Embedding <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024bge</span>)</cite>) to extract embeddings and then cluster the videos to suppress high-frequency data while preserving long-tail content. Finally, we use SOTA video understanding models to generate high-quality video conversations. This produces 1.2M conversation turns across 5-minute average videos, balanced across various task categories.</p>\n\n",
                "matched_terms": [
                    "models",
                    "understanding",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present the evaluation details and quantitative examples of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> on both public and in-house benchmarks.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "both",
                    "mingflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The details of the public benchmarks are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S7\" title=\"7 Public Benchmarks &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. As shown in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:table_1_image2text</span><math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T9\" title=\"Table 9 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, our holistic assessment covers more than 50 rigorously curated public benchmarks across the following seven distinct multi-modal dimensions: Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding), Text <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Generation), Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Editing), Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Segmentation), Audio <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding), Text <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Audio (Generation), and Video <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding).</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "generation",
                    "image",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multi-Dialect and Multi-Domain Audio Understanding Benchmark</span>. To extend audio understanding benchmarks into multi-dialect and multi-domain settings, we constructed two specialized datasets. The multi-dialect dataset was created from 15 regions, while the multi-domain one was curated from six domains. All samples were manually verified for quality by trained annotators. The final datasets comprise 51,986 multi-dialect samples and 10,397 multi-domain samples, with the latter distributed across: Noisy (8,145), Chat (443), Government (462), Health (450), Knowledge (421), and Local Services (476).</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "local",
                    "understanding",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video Streaming Multi-turn Benchmark</span>.\nThe evaluation of video streaming multi-turn dialogue capabilities requires quantifying not only the model&#8217;s understand capability but also assessing its interactive experience, including proactivity and naturalness. Previous streaming dialogue datasets, such as StreamBench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2024streamingbench</span>)</cite> and OvO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">niu2025ovo</span>)</cite>, have primarily focused on the understanding aspect while lacking a thorough evaluation of the interactive experience.\nTo address this gap, we introduce StreamingMultiturnBench.\nTo construct StreamingMultiturnBench, we manually selected 380 videos, carefully ensuring coverage of multiple key domains including life recording, education, TV shows, video games, and documentaries. Then we use SOTA closed-source modelfor machine annotation. Subsequently, a team of 10 human annotators revise and double-check the dialogue content to ensure it aligns with human conversational preferences. This process yielded 2,200 video question-answer pairs.\nDuring evaluation, we use advanced closed-source model, <em class=\"ltx_emph ltx_font_italic\">e.g.</em> GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chatgpt4o</span>)</cite>, to compare the model&#8217;s output against the human-annotated answers, scoring it on a scale of 1 to 5 across the six dimensions: accuracy, completeness, relevance, naturalness, conciseness, and proactivity. The final score is the average for each dimension. To align our metrics with other video benchmarks, we linearly scale the results to a 100-point scale. We commit to open-sourcing and publicly maintaining this benchmark to ensure reproducibility.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "understanding",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct comprehensive evaluations of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> against state-of-the-art MLLMs on over 50 different multimodal benchmarks, as illustrated in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:table_1_image2text</span><math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T9\" title=\"Table 9 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. Extensive experiments demonstrate that <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves comparable performance with leading MLLMs.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "mingflashomni",
                    "performance",
                    "leading"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Vision <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding)</span> As shown in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:table_1_image2text</span>, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates strong and competitive performance across a wide range of vision&#8211;language benchmarks. Specifically, on general-purpose understanding task, it achieves performance on par with leading omni-models (most notably scoring 81.9% on MathVista), though it still exhibits a slight gap compared to state-of-the-art vision&#8211;language models. Similarly, on OCR-centric benchmarks, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> attains state-of-the-art results among omni-modal models, yet remains marginally behind the best proprietary counterparts. In multi-image understanding, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> outperforms the leading open-source vision&#8211;language model Qwen3-VL-30B-A3B on MMT-Bench (68.0) and LLaVA-Interleave Bench (63.3), while showing a minor performance gap on MuirBench, suggesting opportunities for further improvement. In video understanding, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves state-of-the-art performance on MVBench with a score of 74.6, demonstrating robust general video reasoning capabilities. It further exhibits specialized proficiency in processing linguistic content within videos, attaining a leading score of 73.0 on VideoMME with subtitles. Although its performance on long-form video understanding is slightly lower, its consistently strong results across most metrics underscore its advanced video comprehension capabilities.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "leading",
                    "benchmarks",
                    "best",
                    "models",
                    "understanding",
                    "compared",
                    "mingflashomni",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Generation)</span>. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T2\" title=\"Table 2 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, our experimental results demonstrate that the generation quality of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> is on par with state-of-the-art diffusion models.\nNotably, on the Geneval benchmark, our model surpasses all non-Reinforcement Learning methods, demonstrating exceptional controllability. This advantage is particularly pronounced in the \"Position\" and \"Color.\" sub-categories.\nOn the DPG-Bench benchmark, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves an overall score of 83.08, a performance level comparable to pure image generation models like SD3-Medium (84.08) and leading unified models like OmniGen2 (83.57).</p>\n\n",
                "matched_terms": [
                    "omnigen2",
                    "performance",
                    "leading",
                    "dpgbench",
                    "models",
                    "generation",
                    "sd3medium",
                    "color",
                    "image",
                    "geneval",
                    "pure",
                    "mingflashomni",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Editing)</span>. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T6\" title=\"Table 6 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>,\n<span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates impressive image editing performance, surpassing all other unified models.\nSpecifically, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> supports editing instructions in Chinese, achieving performance comparable to that with English instructions. Compared to Qwen-Image-Edit which utilizes a 20B DiT head, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves comparable semantic consistency and perceptual quality with a much more efficient 2B DiT head&#8212;only one-tenth the parameters. This efficiency also translates to remarkable inference speeds, typically between 1-2 seconds per generation.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "models",
                    "generation",
                    "image",
                    "compared",
                    "mingflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Segmentation)</span>. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T6\" title=\"Table 6 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> is capable of performing segmentation tasks, achieving performance comparable to that of specialized models designed explicitly for this purpose. Compared to other unified MLLMs, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates a significant advantage in segmentation.\nFor instance, Qwen-Image-Edit often struggles to accurately localize the target object, while Nano-banana frequently misinterprets user intent during inference.\nIn contrast, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> exhibits superior robustness and a more accurate understanding of spatial and semantic instructions.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "models",
                    "understanding",
                    "capable",
                    "image",
                    "compared",
                    "mingflashomni",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding)</span>. Our model sets a new state-of-the-art (SOTA) on all 12 sub-tasks of the ContextASR-Bench (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T6\" title=\"Table 6 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>), underscoring its superior ability to leverage context&#8212;a vital skill for real-world applications like multi-turn dialogue and hotword enhancement. It also exhibits highly competitive performance across various ASR benchmarks, with notable strengths in dialect recognition (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T8\" title=\"Table 8 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>). In audio question answering, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> surpasses all open-source audio-centric and other Omni models, with the exception of Qwen3-Omni-Flash-Instruct<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen3omnitechnicalreport</span>)</cite> (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T8\" title=\"Table 8 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>). Taken together, these findings demonstrate the robust and versatile audio understanding capabilities of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "benchmarks",
                    "models",
                    "understanding",
                    "mingflashomni",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p7.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Audio (Generation)</span>. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T6\" title=\"Table 6 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, leveraging advancements in speech representation and model architecture, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves SOTA performance among open-source models on the test-zh subset of the SEED-TTS-Eval benchmark<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite>. Furthermore, its WER on the test-en subset is surpassed only by that of Qwen3-Omni.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "models",
                    "generation",
                    "mingflashomni",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video + Audio <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p8.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Video Streaming Conversation)</span>.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T9\" title=\"Table 9 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, benefiting from the introduction of high-quality and diverse streaming video multiturn data, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> has achieved significant improvements in all dimensions compared to <span class=\"ltx_text ltx_font_typewriter\">Ming-Lite-Omni</span>. <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> also outperforms Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen2</span>)</cite> in the dimensions of accuracy, completeness, relevance, and conciseness, providing better experience in streaming video conversation scenarios.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "compared"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present several visualization examples to better illustrate the capabilities of  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>. First, as shown in the figure,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates strong visual understanding across multiple dimensions: it leverages rich world knowledge to accurately infer geographic locations from visual cues; excels at multi-image understanding and generates creative, coherent text grounded in multiple images; solves complex mathematical problems through clear, step-by-step reasoning; and exhibits robust document understanding by accurately parsing intricate formulas and answering questions about sophisticated charts and diagrams within images. Turning to speech recognition,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves strong performance on contextual ASR tasks. By leveraging contextual information, it effectively resolves many challenging cases where conventional ASR systems tend to fail&#8212;such as ambiguous homophones, domain-specific terminology, or noisy conversational speech. Moreover, this version also supports multiple Chinese dialects, significantly broadening its applicability in real-world multilingual and regional speech scenarios. Lastly, we visualize the capabilities of <span class=\"ltx_text ltx_font_italic\">Text/Image\n&#8594;Image</span> generation tasks in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.F4\" title=\"Figure 4 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.F5\" title=\"Figure 5 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, covering a wide range of applications including image generation, image editing, image segmentation, multi-image editing, ID photo generation, ID photo editing, and background replacement.\nAs can be seen,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> not only supports a broader set of generative capabilities but also achieves higher output quality and greater controllability compared to previous versions.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "understanding",
                    "generation",
                    "image",
                    "compared",
                    "mingflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>, built upon Ling-Flash-2.0 with 100 billion parameters, where only 6.1B parameters are activated per token. <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates advanced multimodal perception and generation capabilities with improved computational efficiency while scaling model capacity. It achieves SOTA performance across a broad spectrum of tasks, including multi-image and video processing, image generation, generative segmentation, Contextual Automatic Speech Recognition (ContextASR), and multi-dialect recognition, outperforming omni models of comparable scale. We believe the open-sourcing of our models and code will facilitate the development of AGI by advancing multimodal intelligence research and enabling broader real-world applications.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "models",
                    "generation",
                    "image",
                    "mingflashomni",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:120%;\">Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding)</span>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. Our evaluation of the image-to-text understanding capabilities primarily encompasses the following six tasks: 1) general image understanding capabilities evaluated on MMStar </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024we</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, AI2D </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kembhavi2016diagram</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, HallusionBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Guan_2024_hallusionbench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, CV-Bench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tong2024cambrian1</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, MathVista </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lu2024mathvista</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, and CRPE </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024crpe</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. 2) OCR capabilities evaluated on ChartQA </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">masry-etal-2022-chartqa</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, DocVQA </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">docvqa</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, OCRBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Liu_2024</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, and TextVQA-VAL </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Eval_textvqa</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. 3) multi-image capabilities evaluated on MMTBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ying2024mmtbench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, MuirBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024muirbench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, and LLaVA-interleave Bench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2024llava_interleave_bench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. 4) Complex instruction capabilities evaluated on MIA-Bench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qian2024mia</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. 5) video understanding capabilities evaluated on MVBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2024mvbench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, VideoMME </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fu2024videomme</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, and LongVideoBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2024longvideobench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:120%;\">Text <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Generation)</span>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. We incorporate text-to-image generation capabilities to enable our MLLM with unified perception-generation abilities, which are evaluated on GenEval </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ghosh2024geneval</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, DPG-Bench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hu2024ella</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, and FID.</span>\n</p>\n\n",
                "matched_terms": [
                    "texttoimage",
                    "dpgbench",
                    "generation",
                    "image",
                    "geneval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:120%;\">Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Segmentation)</span>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">.\nWe evaluate the segmentation capability of our MLLM on the standard referring expression segmentation (RES) benchmarks RefCOCO/+ </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">refcoco</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\"> and RefCOCOg </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">refcocog</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:120%;\">Audio <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding)</span>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. Our evaluation of the audio-to-text understanding capabilities mainly includes the following three tasks: 1) Fundamental audio understanding capabilities evaluated on a broad range of public benchmarks, including public Chinese benchmarks like Aishell1 </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">AISHELL1</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\"> and Wenetspeech </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">WenetSpeech</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, and public English benchmarks like Librispeech </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Librispeech</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\"> and Voxpopuli </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2021voxpopuli</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. And 2) audio question-answering capabilities evaluated on various benchmarks across five specific tasks, such as AlpacaEval and CommonEval from VoiceBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024voicebenchbenchmarkingllmbasedvoice</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\"> for open-ended QA tasks, and SD-QA for knowledge-based QA tasks. Finally 3) evaluates the model&#8217;s ability to utilize context on ContextASR-Bench</span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025contextasrbenchmassivecontextualspeech</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:120%;\">Video <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p7.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding)</span>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. Our evaluation of the video-to-text understanding capabilities contains the following four benchmarks: MVBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2024mvbench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, VideoMME </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fu2024video</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\"> and LongVideoBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2024longvideobench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "understanding"
                ]
            }
        ]
    },
    "S3.T6": {
        "source_file": "Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation",
        "caption": "Table 3: \nPerformance of Ming-Flash-Omni on Image-to-Image Editing Benchmarks compared to leading models. All metrics are evaluated by GPT-4.1. Edit. denotes models specifically trained for image editing, while Unified. denotes models capable of image understanding, generation and editing.",
        "body": "Model\nPerformance\n\n\n\nSpeech-English\nDialogue-English\nSpeech-Mandarin\nDialogue-Mandarin\n\n\n\nWER | NE-WER | NE-FNR\nWER | NE-WER | NE-FNR\nWER | NE-WER | NE-FNR\nWER | NE-WER | NE-FNR\n\n\nQwen2-Audio\n11.49 | 27.27 | 35.08\n13.99 | 33.02 | 32.92\n9.92 | 24.10 | 30.02\n7.00 | 22.76 | 26.17\n\n\nBaichuan-Audio\n7.52 |   5.87 |   4.55\n5.66 | 10.01 |   3.64\n2.16 |   6.65 |   2.35\n2.96 | 11.48 |   3.94\n\n\nKimi-Audio\n2.90 |   6.68 |    8.01\n4.67 | 13.50 | 11.31\n1.95 | 11.13 | 15.28\n2.90 | 15.91 | 16.68\n\n\nBaichuan-Omni-1.5\n8.16 |   7.69 |   6.53\n9.91 | 14.40 |    5.54\n2.98 |   8.39 |   4.71\n5.00 | 16.83 |   7.84\n\n\nQwen2.5-Omni-3B\n3.99 |   7.80 |   9.69\n4.83 | 14.36 | 12.85\n2.13 | 10.55 | 14.11\n3.12 | 15.07 | 15.17\n\n\nQwen2.5-Omni-7B\n3.96 |   7.38 |    8.72\n5.32 | 11.83 |    9.24\n1.84 |   9.80 | 12.19\n2.40 | 14.06 | 13.17\n\n\nMing-Flash-Omni\n\n2.85 |   2.63 |   2.36\n\n\n3.76 |    7.06 |    1.90\n\n\n1.31 |   5.62 |   1.59\n\n\n1.78 |   8.47 |   0.85",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Performance</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech-English</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dialogue-English</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech-Mandarin</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dialogue-Mandarin</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">WER | NE-WER | NE-FNR</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">WER | NE-WER | NE-FNR</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">WER | NE-WER | NE-FNR</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">WER | NE-WER | NE-FNR</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2-Audio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.49 | 27.27 | 35.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.99 | 33.02 | 32.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.92 | 24.10 | 30.02</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.00 | 22.76 | 26.17</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Baichuan-Audio</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.52 |   5.87 |   4.55</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.66 | 10.01 |   3.64</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.16 |   6.65 |   2.35</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.96 | 11.48 |   3.94</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Kimi-Audio</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.90 |   6.68 |    8.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.67 | 13.50 | 11.31</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.95 | 11.13 | 15.28</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.90 | 15.91 | 16.68</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Baichuan-Omni-1.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.16 |   7.69 |   6.53</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.91 | 14.40 |    5.54</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.98 |   8.39 |   4.71</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.00 | 16.83 |   7.84</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2.5-Omni-3B</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.99 |   7.80 |   9.69</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.83 | 14.36 | 12.85</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.13 | 10.55 | 14.11</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.12 | 15.07 | 15.17</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2.5-Omni-7B</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.96 |   7.38 |    8.72</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.32 | 11.83 |    9.24</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.84 |   9.80 | 12.19</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.40 | 14.06 | 13.17</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">Ming-Flash-Omni</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"/><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.85</span><span class=\"ltx_text\" style=\"font-size:90%;\"> |   </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.63</span><span class=\"ltx_text\" style=\"font-size:90%;\"> |   </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.36</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"/><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.76</span><span class=\"ltx_text\" style=\"font-size:90%;\"> |    </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.06</span><span class=\"ltx_text\" style=\"font-size:90%;\"> |    </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.90</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.31</span><span class=\"ltx_text\" style=\"font-size:90%;\"> |   </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.62</span><span class=\"ltx_text\" style=\"font-size:90%;\"> |   </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.59</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.78</span><span class=\"ltx_text\" style=\"font-size:90%;\"> |   </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">8.47</span><span class=\"ltx_text\" style=\"font-size:90%;\"> |   </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.85</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "gpt41",
            "leading",
            "wer",
            "kimiaudio",
            "dialoguemandarin",
            "qwen25omni3b",
            "qwen2audio",
            "editing",
            "mingflashomni",
            "unified",
            "denotes",
            "newer",
            "capable",
            "speechenglish",
            "image",
            "compared",
            "baichuanomni15",
            "metrics",
            "while",
            "speechmandarin",
            "performance",
            "imagetoimage",
            "nefnr",
            "evaluated",
            "generation",
            "baichuanaudio",
            "trained",
            "qwen25omni7b",
            "specifically",
            "edit",
            "dialogueenglish",
            "all",
            "benchmarks",
            "models",
            "understanding",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Editing)</span>. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T6\" title=\"Table 6 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>,\n<span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates impressive image editing performance, surpassing all other unified models.\nSpecifically, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> supports editing instructions in Chinese, achieving performance comparable to that with English instructions. Compared to Qwen-Image-Edit which utilizes a 20B DiT head, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves comparable semantic consistency and perceptual quality with a much more efficient 2B DiT head&#8212;only one-tenth the parameters. This efficiency also translates to remarkable inference speeds, typically between 1-2 seconds per generation.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Segmentation)</span>. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T6\" title=\"Table 6 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> is capable of performing segmentation tasks, achieving performance comparable to that of specialized models designed explicitly for this purpose. Compared to other unified MLLMs, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates a significant advantage in segmentation.\nFor instance, Qwen-Image-Edit often struggles to accurately localize the target object, while Nano-banana frequently misinterprets user intent during inference.\nIn contrast, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> exhibits superior robustness and a more accurate understanding of spatial and semantic instructions.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding)</span>. Our model sets a new state-of-the-art (SOTA) on all 12 sub-tasks of the ContextASR-Bench (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T6\" title=\"Table 6 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>), underscoring its superior ability to leverage context&#8212;a vital skill for real-world applications like multi-turn dialogue and hotword enhancement. It also exhibits highly competitive performance across various ASR benchmarks, with notable strengths in dialect recognition (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T8\" title=\"Table 8 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>). In audio question answering, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> surpasses all open-source audio-centric and other Omni models, with the exception of Qwen3-Omni-Flash-Instruct<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen3omnitechnicalreport</span>)</cite> (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T8\" title=\"Table 8 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>). Taken together, these findings demonstrate the robust and versatile audio understanding capabilities of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p7.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Audio (Generation)</span>. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T6\" title=\"Table 6 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, leveraging advancements in speech representation and model architecture, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves SOTA performance among open-source models on the test-zh subset of the SEED-TTS-Eval benchmark<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite>. Furthermore, its WER on the test-en subset is surpassed only by that of Qwen3-Omni.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion total parameters, of which only 6.1 billion are active per token. This architecture enables <span class=\"ltx_text ltx_font_bold\">highly efficient scaling</span> (dramatically improving computational efficiency while significantly expanding model capacity) and empowers <span class=\"ltx_text ltx_font_bold\">stronger unified multimodal intelligence</span> across vision, speech, and language, representing a key step toward Artificial General Intelligence (AGI). Compared to its predecessor, the upgraded version exhibits substantial improvements across multimodal understanding and generation. We significantly advance speech recognition capabilities, achieving state-of-the-art performance in <span class=\"ltx_text ltx_font_bold\">contextual ASR</span> and highly competitive results in <span class=\"ltx_text ltx_font_bold\">dialect-aware ASR</span>. In image generation, Ming-Flash-Omni introduces <span class=\"ltx_text ltx_font_bold\">high-fidelity text rendering</span> and demonstrates marked gains in <span class=\"ltx_text ltx_font_bold\">scene consistency</span> and <span class=\"ltx_text ltx_font_bold\">identity preservation</span> during image editing. Furthermore, Ming-Flash-Omni introduces <span class=\"ltx_text ltx_font_bold\">generative segmentation</span>, a capability that not only achieves strong standalone segmentation performance but also enhances spatial control in image generation and improves editing consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in text-to-image generation and generative segmentation, and sets new records on all 12 contextual ASR benchmarks, all within a single unified architecture.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "all",
                    "benchmarks",
                    "understanding",
                    "generation",
                    "image",
                    "compared",
                    "editing",
                    "mingflashomni",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Driven by advances in Large Language Models (LLMs) and extensive training on large-scale multimodal datasets, Multi-modal Large Language Models (MLLMs) have demonstrated remarkable perceptual capabilities in both vision <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024internvl</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bai2025qwen2</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2025kimi</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen3</span>)</cite> and audio <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ding2025kimi</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen2</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen3</span>)</cite>, as well as generative capabilities in these two modalities <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">huang2025step</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ding2025kimi</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chatgpt4o</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tong2024metamorph</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pan2025transfer</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen3</span>)</cite>. Nevertheless, effectively integrating comprehension and generation across multiple modalities into a unified model remains challenging. While humans naturally learn by combining multiple modalities, leveraging their complementary strengths and interactions to enhance overall learning efficiency, building a unified Omni-MLLM is hindered by representational disparities and modality imbalances.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "generation",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>, which builds upon the  <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> architecture with a redesigned foundation and targeted enhancements across multimodal understanding and generation. At its core,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> adopts Ling-Flash-2.0 <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lingv2_flash</span></cite> (a scaled-up, highly sparse Mixture-of-Experts architecture) where an increased sparsity ratio enables substantial model capacity while maintaining bounded inference latency, striking a favorable trade-off between performance and efficiency.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "understanding",
                    "generation",
                    "mingflashomni",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the understanding side, the model introduces two key advances. First,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> upgrade the positional encoding to VideoRoPE <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wei2025videorope</span></cite>, a refined variant specifically designed to better capture temporal dynamics in video sequences, thereby enhancing the model&#8217;s ability to understand complex visual events. Second,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> focus on improving the context-aware ASR capability itself, enhancing the model&#8217;s ability to leverage surrounding linguistic context during speech recognition and thereby achieving more accurate transcription in context-dependent scenarios.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "understanding",
                    "model",
                    "specifically"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the generation side, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> introduces three key advancements: 1) in speech synthesis, discrete acoustic tokens are replaced with continuous representations, effectively circumventing quantization-induced artifacts and yielding more natural and expressive TTS outputs; 2) the model supports generative semantic segmentation, enabling pixel-level semantic content generation conditioned on multimodal inputs; and 3) it enables fine-grained controllable image generation with improved identity preservation and in-image text generation capabilities.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mingflashomni",
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These architectural innovations empower <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> to deliver exceptional cross-modal performance in both comprehension and generation tasks. Specifically, in the image perception task, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> attained performance comparable to that of Qwen3-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen3</span>)</cite>. <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> also delivers superior performance in end-to-end speech understanding and generation.For instance, it achieves SOTA on all 12 metrics on ContextASR-Bench.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "all",
                    "understanding",
                    "generation",
                    "image",
                    "specifically",
                    "mingflashomni",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remainder of this paper is organized as follows. Section 2 presents the detailed architecture of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>. Sections 3 describes the pretraining and post-training datasets.\nSection 4 reports the evaluation results and compare <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> with recent multimodal models. Sections 5 is conclusion.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S2.F2\" title=\"Figure 2 &#8227; 2 Ming-Flash-Omni &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> retains the unified two-stage pipeline of <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ai2025ming</span></cite>, where perception supports multimodal understanding and generation targets speech and image synthesis, while markedly advancing long-context modeling, reasoning, and controllable generation. At the core is Ling&#8209;Flash&#8209;2.0 <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lingv2_flash</span></cite>, a sparse MoE LM (100B; 6.1B per token) with a dual balancing scheme that stabilizes training and improves efficiency. On the perception side, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> employs VideoRoPE <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wei2025videorope</span></cite> for temporal modeling, context&#8209;aware ASR for more reliable speech understanding, and a think mode for deeper multi&#8209;step reasoning. On the generation side, we replace discrete speech tokens with continuous acoustic latents, avoiding quantization loss and improving fidelity; for images, we upgrade to a synergistic training paradigm that enables generative segmentation-as-editing, facilitating fine-grained and controllable generation. Overall, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> advances the unified model with stable expert routing and scalable long&#8209;context modeling, yielding more reliable multimodal understanding and controllable generation.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "generation",
                    "image",
                    "mingflashomni",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The cornerstone of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> is enhanced multimodal understanding. We retain the established visual and audio encoders (Qwen2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bai2025qwen25vltechnicalreport</span>)</cite> and Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>)</cite>) and feed their projected embeddings, concatenated with tokenized text, into Ling&#8209;flash&#8209;2.0, a sparse MoE language model with distinct routers per modality. Beyond this, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> incorporates VideoRoPE to maintain temporal coherence over long-range frame sequences, thus emphasizing temporal modeling. Furthermore, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> adopt a context&#8209;aware ASR training paradigm that conditions decoding on task or domain context, addressing common shortcomings of conventional ASR in real&#8209;world, multi&#8209;domain scenarios (limited world knowledge and unreliable proper&#8209;noun recognition) and yielding more accurate, context&#8209;consistent transcripts. To stabilize training in the more sparse Ling&#8209;flash&#8209;2.0, we employ a hybrid expert&#8209;balancing scheme that combines an auxiliary load&#8209;balancing loss (as in <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span>) with per&#8209;router bias updates (as in Ling&#8209;flash&#8209;2.0), promoting uniform expert activation and improved convergence.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "understanding",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> retains the same audio encoder as  <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ai2025ming</span>)</cite>, with optimization efforts focused on improving contextual automatic speech recognition (ASR) performance by incorporating preceding textual context and hotword lists during training. For generation, we replace discrete speech tokens with continuous acoustic latents, avoiding quantization loss and improving fidelity. Specifically, we utilize a fixed, pre-trained audio head based on Qwen2.5 (0.5B parameters), which takes LLM-generated text tokens and downsampled VAE latents as input and autoregressively predicts the conditioning signals for the flow-matching head&#8212;following the paradigm of <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jia2025ditar</span></cite>.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "generation",
                    "specifically",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is worth noting that the continuous features used for our generation tasks are derived from our unified continuous speech tokenizer, which is based on a VAE-GAN architecture, with the overall training objective comprising two main components: a generator loss and a discriminator loss. The generator loss consists of (i) a multi-scale mel-spectrogram reconstruction loss, (ii) an adversarial loss, (iii) a feature matching loss, and (iv) a KL divergence regularization term. The discriminator, in turn, is trained primarily with the adversarial loss. For more implementation details, please refer to  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">MingUniAudio2025</span></cite>.</p>\n\n",
                "matched_terms": [
                    "trained",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A core challenge in building unified multimodal models is the effective fusion of image understanding and generation. While our <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> model injects hierarchical semantics via multi-scale query tokens, its language pathway remains frozen during training to prevent interference from the generative objective. This freezing approach, while ensuring stability, creates a critical bottleneck: an inherent discrepancy between the learning objectives of understanding and generation. Consequently, even with injected hierarchical semantics, fine-grained visual knowledge&#8212;such as object attributes and spatial relationships&#8212;cannot be efficiently transferred to high-precision generation and editing tasks, limiting the model&#8217;s final quality and controllability.</p>\n\n",
                "matched_terms": [
                    "models",
                    "understanding",
                    "generation",
                    "image",
                    "editing",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome this bottleneck,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> propose a synergistic training paradigm that reframes image segmentation as a generative editing task. Instead of producing an abstract binary mask (e.g., for &#8220;segment the banana&#8221;), the model performs a semantics-preserving edit (e.g., &#8220;color the banana purple&#8221;). This reformulation forcibly binds the learning objectives of understanding and generation: successful generation requires a precise understanding of the object&#8217;s contours and boundaries. Understanding thus becomes a <span class=\"ltx_text ltx_font_italic\">prerequisite</span> for editing, and the edit&#8217;s quality provides a direct supervisory signal for the model&#8217;s comprehension, fundamentally unifying their optimization objectives.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "generation",
                    "image",
                    "editing",
                    "mingflashomni",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Crucially, this training cultivates a more fundamental, generalizable skill: <span class=\"ltx_text ltx_font_bold\">fine-grained spatio-semantic control</span>, which indirectly resolves the compositionality problem in pure text-to-image generation.\n<span class=\"ltx_text ltx_font_bold\">In our evaluation on the GenEval benchmark, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieved a score of 0.90, surpassing leading non-Reinforcement Learning (non-RL) methods.</span>\nThis result suggests that the foundational skill of spatio-semantic control can effectively generalize to pure text-to-image generation tasks.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "generation",
                    "leading"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training procedure of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> retains a two-stage pipeline: perception and generation.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After perception, we freeze the perception MLLM and optimize only the image generator, while leveraging the pre-trained audio generator from  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">MingUniAudio2025</span></cite>. For image generation, the training procedure contains two sequential stages. In the first stage, we pre-train a diffusion-based image generator using a flow matching objective, while keeping the perception MLLM frozen. The generator is equipped with multi-scale learnable queries to capture hierarchical visual semantics from textual inputs. In the second stage, we extend the model to support image editing by conditioning the denoising process on reference images: the VAE-encoded representations of input images are concatenated with the noisy latent to enforce structural and semantic consistency with the original content. Additionally, input word-level captions are encoded via ByteT5 embeddings to enrich textual conditioning.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image",
                    "editing",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to large language models (LLMs), training multimodal foundation models presents several key challenges, primarily stemming from <span class=\"ltx_text ltx_font_bold\">data heterogeneity</span> and <span class=\"ltx_text ltx_font_bold\">model heterogeneity</span>. First, data heterogeneity arises from the need to dynamically switch between diverse input modalities (text, images, audio, and video) during training. These modalities exhibit significant differences in tensor shape, most notably in the form of dynamic batch sizes and variable-length sequences. This variability complicates the design of a unified parallel computation layout. As a result, computational workloads become unevenly distributed across processing ranks, leading to load imbalance. Moreover, the frequent allocation and deallocation of GPU memory buffers for inputs of varying shapes induce severe memory fragmentation, substantially degrading training efficiency and hardware utilization. Second, in contrast to large language models (LLMs), which are predominantly based on homogeneous, decoder-only Transformer architectures, multimodal foundation models typically employ modality-specific encoders at the input stage, introducing model heterogeneity. Although these encoders are relatively lightweight in terms of parameter count, they are highly sensitive to parallelization strategies. If not carefully partitioned across devices, they can induce substantial pipeline bubbles (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, idle computation cycles) during pipeline-parallel execution, thereby constraining overall training throughput.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "compared",
                    "leading"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> is trained on an enhanced version of the Megatron-LM <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shoeybi2019megatron</span></cite> framework with two key extensions tailored for multimodal workloads:</p>\n\n",
                "matched_terms": [
                    "trained",
                    "mingflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We have collected a large and diverse set of training data to enable models to process and understand information from multiple modalities, including text, images, audio and videos.\nThe majority of this data comes from <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ai2025ming</span>)</cite>. In addition, we develop several data processing pipelines to ensure data quality, diversity and deduplication. Establishing an effective multimodal data strategy is essential for the joint multi-modal training, as it facilitates seamless alignment of knowledge across diverse modalities.\nWe categorize the training data based on the core modalities they are designed to enhance, including image, audio, video, and text. The detailed sources and construction methods for each type of data are elaborated in this section.</p>\n\n",
                "matched_terms": [
                    "models",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Image data serves as the cornerstone of our multi-modal corpus.\nFollowing <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ai2025ming</span>)</cite>, we integrate both image-understanding and image-generation datasets to enable the MLLM to acquire unified perception and generation capabilities. Additionally, we further design novel pipelines to synthesize high-quality datasets across diverse dimensions to improve model&#8217;s capabilities and user interaction quality.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OCR Data:</span>\nText recognition and document understanding capabilities are crucial for MLLM. We construct a large-scale heterogeneous training dataset with millions of samples, consisting of three data sources: open-source data, expert-collaborative pseudo-labeled data, and human-annotated enhancement data. The expert-collaborative pseudo-labeled data is generated by diagnosing model weaknesses, and using expert models to label targeted data.\nIn addition, to enhance the model&#8217;s capability in text&#8211;visual analysis and logical reasoning, we incorporate the Chain-of-Thought (CoT) paradigm into the training data. We incorporate the open-source ChartQA-PoT dataset to enhance the model&#8217;s numerical reasoning ability on charts and pioneeringly use executable Python code as the intermediate reasoning representation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "understanding",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Reasoning Data:</span>\nIn the reasoning training of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>, we enrich the CoT data to enhance the model&#8217;s reasoning capabilities. These data are primarily constructed around three key themes: mathematical logic, spatial reasoning, and GUI reasoning.\nWe design an efficient CoT generation and filtering pipeline to construct high-quality, well-structured reasoning data that enhances the model&#8217;s multi-step reasoning capability. (1) We sample long CoT data using state-of-the-art multimodal reasoning models (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GEMINI</span>)</cite>) to build an initial CoT pool. (2) We evaluate the accuracy and quality of the synthesized CoT data and filter out the low-quality data. Based on this pipeline, we construct 1.5M multimodal long CoT samples, with a maximum length of 16K tokens.\nBesides, to overcome the limitations of the &#8220;direct answer&#8221; pattern in text-based QA data, we use reinforcement learning models to generate multi-step reasoning traces and final answers.\nExperimental results demonstrate that this data significantly improves <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>&#8217;s performance on complex reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "generation",
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Preference alignment Data:</span>\nTo further enhance user interaction and response quality, we introduce preference alignment data, focusing on three main aspects:\n(1) Instruction intent understanding: To enhance the model&#8217;s ability to understand the true intent behind user instructions, we design a novel instruction intent reasoning paradigm where the model first determines if the instruction is clear. For clear instructions, it provides structured reasoning and final answers. For ambiguous ones, it infers the likely intent, generates clarification prompts, and aligns responses accordingly. By incorporating experience-alignment training, we significantly improve the model&#8217;s intent understanding and enhance the overall user experience.\n(2) Multi-turn conversation: Users often engage in repeated questioning on the same context. To maintain semantic consistency across turns, we decompose complex instructions into multi-turn conversations and generate high-quality responses using expert-annotated LLM pipelines <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">laban2025llms</span>)</cite>. This ensures context retention and reduces performance degradation in multi-turn scenarios.\n(3) Complex multimodal instruction following: Users often provide sequential, interdependent commands. We design a multimodal instruction generation pipeline to generate SFT and DPO-style data to cover basic and complex instruction types <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2024iopo</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qian2024mia</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ding2025mm</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "understanding",
                    "generation",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Structured Data:</span>\nStructured data enhances MLLM capabilities in querying fine-grained knowledge associated with specific visual entities.\nTo enhance the MLLM&#8217;s ability to handle knowledge-intensive and information-seeking queries, we design two pipelines to generate large-scale entity-relation and encyclopedic QA data.\n(1) <em class=\"ltx_emph ltx_font_italic\">Entity-relation QA data:</em> We design a multi-stage pipeline to construct a high-quality entity-relation QA corpus. The pipeline integrates scene graph extraction, visual grounding, entity&#8211;relation verification, and diverse QA generation to produce pairs covering entities, attributes, and relations. To ensure quality, we filter source datasets for complex multi-entity scenes and apply two-stage validation before and after QA generation.\n(2) <em class=\"ltx_emph ltx_font_italic\">Encyclopedia QA Data:</em> We design an automated pipeline to generate encyclopedia entity QA data from Chinese Baidu Encyclopedia and English Wikidata. It constructs &lt;image, entity, knowledge&gt; triplets by extracting entities from images or retrieving images for knowledge-base entities, filtering out invalid ones, and validating image&#8211;text consistency. These triplets are then converted into VQA data using LLMs.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Image generation data enhances MLLM capabilities beyond traditional image understanding tasks. In addition to the image generation data used in <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ai2025ming</span>)</cite>, we specifically integrate image segmentation data, text generation data and portrait preservation data to further improve the user experience.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image",
                    "specifically",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image segmentation data:</span> To improve the model&#8217;s generative segmentation capability, we construct two types of data: (1) We use the open-source referring segmentation datasets RefCOCO/+/g <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">refcoco</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">refcocog</span>)</cite> to construct image editing data. The original image serves as the reference, and binary masks highlight target regions with specified colors to create edited images. (2) For semantic and panoptic segmentation, samples are built from COCO-Panoptic <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Eval_mscoco</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kirillov2019panoptic</span>)</cite>, where each class or instance is assigned a unique color via a predefined colormap to generate edited images.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Portrait preservation data:</span> The portrait preservation data consist of two data sources: (1) ID Photo Dataset: We collected and constructed 200k paired lifestyle-ID photos. And filter the data using four criteria, <em class=\"ltx_emph ltx_font_italic\">e.g.</em>, face similarity, face size and confidence, face angle and manual review.\n(2) Landmark Check-In Portrait Dataset: We collect 20K high-quality portraits from 225 landmarks.The original images serve as edited images, while using advanced segmentation model like SAM2 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ravi2024sam</span>)</cite> to segments the foreground person and places them onto 1,000 manually collected daily background scenes as pre-edited images. And then use LLM generate diverse prompts for each landmark.</p>\n\n",
                "matched_terms": [
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text generation data:</span>\nWe build a Chinese-English text generation dataset across three difficulty levels:\n(1) Monotonic background text rendering: Text is rendered directly by setting background color, font type, size, color, and position. (2) Text rendering on existing images: texts are rendered on suitable smooth regions obtained by Felzenszwalb algorithm. (3) Text-image integrated rendering: Using the SOTA LLMs to generate text rendering prompts, the advanced generation models (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, Qwen-image <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2025qwen</span>)</cite> and Nano Banana) are used for image generation, followed by OCR for consistency checks, resulting in a high-quality dataset.</p>\n\n",
                "matched_terms": [
                    "models",
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For audio data, we mainly use the data from <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ai2025ming</span>)</cite>. In addition, we incorporate the following three datasets to further enhance the model&#8217;s audio understanding and generation capabilities.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">TTS Data:</span>\nThe diversity of TTS data is essential for fully leveraging the pretrained language model&#8217;s capabilities in audio generation. In addition to the open-source data used in <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span>, we develop a data generation pipeline to create large-scale TTS data. Specifically, (1) we crawl extensive audio data using keywords expanded from handcrafted seeds through domain-specific lexical variations, (2) apply VAD <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gao2023funasrfundamentalendtoendspeech</span>)</cite> to segment well-conditioned short clips, and (3) iteratively train an audio labeler&#8212;initially on high-quality data, then using its predictions to label the corpus and refine its accuracy. Based on the short audio clips and the audio labeler, we acquire a large number of high-quality audio clips with labels from different domains.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "specifically"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Video streaming conversation constitutes a fundamental capability for MLLMs in video understanding. However, acquiring large-scale streaming multi-turn conversation data is prohibitively expensive. In this work, we propose a pipeline to systematically synthesize diverse, balanced, and high-quality multi-turn conversation video datasets.\nWe collect 8.2M videos from the internet, ranging from 90 seconds to 10 minutes, and first filter out low-quality videos with high speech density, high shot density, irregular aspect ratios, or low resolution. We then filter out low-information, incoherent, or overly simple videos using SOTA MLLMs. To ensure a balanced dataset, we use advanced embedding models (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, Qwen3-Embedding <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2025qwen3</span>)</cite> and M3-Embedding <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024bge</span>)</cite>) to extract embeddings and then cluster the videos to suppress high-frequency data while preserving long-tail content. Finally, we use SOTA video understanding models to generate high-quality video conversations. This produces 1.2M conversation turns across 5-minute average videos, balanced across various task categories.</p>\n\n",
                "matched_terms": [
                    "models",
                    "understanding",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present the evaluation details and quantitative examples of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> on both public and in-house benchmarks.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "mingflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The details of the public benchmarks are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S7\" title=\"7 Public Benchmarks &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. As shown in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:table_1_image2text</span><math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T9\" title=\"Table 9 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, our holistic assessment covers more than 50 rigorously curated public benchmarks across the following seven distinct multi-modal dimensions: Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding), Text <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Generation), Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Editing), Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Segmentation), Audio <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding), Text <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Audio (Generation), and Video <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding).</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "understanding",
                    "generation",
                    "image",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multi-Dialect and Multi-Domain Audio Understanding Benchmark</span>. To extend audio understanding benchmarks into multi-dialect and multi-domain settings, we constructed two specialized datasets. The multi-dialect dataset was created from 15 regions, while the multi-domain one was curated from six domains. All samples were manually verified for quality by trained annotators. The final datasets comprise 51,986 multi-dialect samples and 10,397 multi-domain samples, with the latter distributed across: Noisy (8,145), Chat (443), Government (462), Health (450), Knowledge (421), and Local Services (476).</p>\n\n",
                "matched_terms": [
                    "all",
                    "benchmarks",
                    "understanding",
                    "trained",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video Streaming Multi-turn Benchmark</span>.\nThe evaluation of video streaming multi-turn dialogue capabilities requires quantifying not only the model&#8217;s understand capability but also assessing its interactive experience, including proactivity and naturalness. Previous streaming dialogue datasets, such as StreamBench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2024streamingbench</span>)</cite> and OvO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">niu2025ovo</span>)</cite>, have primarily focused on the understanding aspect while lacking a thorough evaluation of the interactive experience.\nTo address this gap, we introduce StreamingMultiturnBench.\nTo construct StreamingMultiturnBench, we manually selected 380 videos, carefully ensuring coverage of multiple key domains including life recording, education, TV shows, video games, and documentaries. Then we use SOTA closed-source modelfor machine annotation. Subsequently, a team of 10 human annotators revise and double-check the dialogue content to ensure it aligns with human conversational preferences. This process yielded 2,200 video question-answer pairs.\nDuring evaluation, we use advanced closed-source model, <em class=\"ltx_emph ltx_font_italic\">e.g.</em> GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chatgpt4o</span>)</cite>, to compare the model&#8217;s output against the human-annotated answers, scoring it on a scale of 1 to 5 across the six dimensions: accuracy, completeness, relevance, naturalness, conciseness, and proactivity. The final score is the average for each dimension. To align our metrics with other video benchmarks, we linearly scale the results to a 100-point scale. We commit to open-sourcing and publicly maintaining this benchmark to ensure reproducibility.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "understanding",
                    "model",
                    "metrics",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct comprehensive evaluations of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> against state-of-the-art MLLMs on over 50 different multimodal benchmarks, as illustrated in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:table_1_image2text</span><math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T9\" title=\"Table 9 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. Extensive experiments demonstrate that <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves comparable performance with leading MLLMs.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "mingflashomni",
                    "performance",
                    "leading"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Vision <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding)</span> As shown in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:table_1_image2text</span>, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates strong and competitive performance across a wide range of vision&#8211;language benchmarks. Specifically, on general-purpose understanding task, it achieves performance on par with leading omni-models (most notably scoring 81.9% on MathVista), though it still exhibits a slight gap compared to state-of-the-art vision&#8211;language models. Similarly, on OCR-centric benchmarks, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> attains state-of-the-art results among omni-modal models, yet remains marginally behind the best proprietary counterparts. In multi-image understanding, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> outperforms the leading open-source vision&#8211;language model Qwen3-VL-30B-A3B on MMT-Bench (68.0) and LLaVA-Interleave Bench (63.3), while showing a minor performance gap on MuirBench, suggesting opportunities for further improvement. In video understanding, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves state-of-the-art performance on MVBench with a score of 74.6, demonstrating robust general video reasoning capabilities. It further exhibits specialized proficiency in processing linguistic content within videos, attaining a leading score of 73.0 on VideoMME with subtitles. Although its performance on long-form video understanding is slightly lower, its consistently strong results across most metrics underscore its advanced video comprehension capabilities.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "leading",
                    "benchmarks",
                    "models",
                    "understanding",
                    "compared",
                    "specifically",
                    "mingflashomni",
                    "model",
                    "metrics",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Generation)</span>. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T2\" title=\"Table 2 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, our experimental results demonstrate that the generation quality of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> is on par with state-of-the-art diffusion models.\nNotably, on the Geneval benchmark, our model surpasses all non-Reinforcement Learning methods, demonstrating exceptional controllability. This advantage is particularly pronounced in the \"Position\" and \"Color.\" sub-categories.\nOn the DPG-Bench benchmark, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves an overall score of 83.08, a performance level comparable to pure image generation models like SD3-Medium (84.08) and leading unified models like OmniGen2 (83.57).</p>\n\n",
                "matched_terms": [
                    "performance",
                    "leading",
                    "all",
                    "models",
                    "generation",
                    "image",
                    "mingflashomni",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video + Audio <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p8.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Video Streaming Conversation)</span>.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T9\" title=\"Table 9 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, benefiting from the introduction of high-quality and diverse streaming video multiturn data, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> has achieved significant improvements in all dimensions compared to <span class=\"ltx_text ltx_font_typewriter\">Ming-Lite-Omni</span>. <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> also outperforms Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen2</span>)</cite> in the dimensions of accuracy, completeness, relevance, and conciseness, providing better experience in streaming video conversation scenarios.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "compared",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present several visualization examples to better illustrate the capabilities of  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>. First, as shown in the figure,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates strong visual understanding across multiple dimensions: it leverages rich world knowledge to accurately infer geographic locations from visual cues; excels at multi-image understanding and generates creative, coherent text grounded in multiple images; solves complex mathematical problems through clear, step-by-step reasoning; and exhibits robust document understanding by accurately parsing intricate formulas and answering questions about sophisticated charts and diagrams within images. Turning to speech recognition,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves strong performance on contextual ASR tasks. By leveraging contextual information, it effectively resolves many challenging cases where conventional ASR systems tend to fail&#8212;such as ambiguous homophones, domain-specific terminology, or noisy conversational speech. Moreover, this version also supports multiple Chinese dialects, significantly broadening its applicability in real-world multilingual and regional speech scenarios. Lastly, we visualize the capabilities of <span class=\"ltx_text ltx_font_italic\">Text/Image\n&#8594;Image</span> generation tasks in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.F4\" title=\"Figure 4 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.F5\" title=\"Figure 5 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, covering a wide range of applications including image generation, image editing, image segmentation, multi-image editing, ID photo generation, ID photo editing, and background replacement.\nAs can be seen,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> not only supports a broader set of generative capabilities but also achieves higher output quality and greater controllability compared to previous versions.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "understanding",
                    "generation",
                    "image",
                    "compared",
                    "editing",
                    "mingflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>, built upon Ling-Flash-2.0 with 100 billion parameters, where only 6.1B parameters are activated per token. <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates advanced multimodal perception and generation capabilities with improved computational efficiency while scaling model capacity. It achieves SOTA performance across a broad spectrum of tasks, including multi-image and video processing, image generation, generative segmentation, Contextual Automatic Speech Recognition (ContextASR), and multi-dialect recognition, outperforming omni models of comparable scale. We believe the open-sourcing of our models and code will facilitate the development of AGI by advancing multimodal intelligence research and enabling broader real-world applications.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "models",
                    "generation",
                    "image",
                    "mingflashomni",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:120%;\">Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding)</span>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. Our evaluation of the image-to-text understanding capabilities primarily encompasses the following six tasks: 1) general image understanding capabilities evaluated on MMStar </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024we</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, AI2D </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kembhavi2016diagram</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, HallusionBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Guan_2024_hallusionbench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, CV-Bench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tong2024cambrian1</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, MathVista </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lu2024mathvista</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, and CRPE </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024crpe</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. 2) OCR capabilities evaluated on ChartQA </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">masry-etal-2022-chartqa</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, DocVQA </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">docvqa</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, OCRBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Liu_2024</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, and TextVQA-VAL </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Eval_textvqa</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. 3) multi-image capabilities evaluated on MMTBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ying2024mmtbench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, MuirBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024muirbench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, and LLaVA-interleave Bench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2024llava_interleave_bench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. 4) Complex instruction capabilities evaluated on MIA-Bench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qian2024mia</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. 5) video understanding capabilities evaluated on MVBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2024mvbench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, VideoMME </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fu2024videomme</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, and LongVideoBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2024longvideobench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "evaluated",
                    "image",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:120%;\">Text <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Generation)</span>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. We incorporate text-to-image generation capabilities to enable our MLLM with unified perception-generation abilities, which are evaluated on GenEval </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ghosh2024geneval</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, DPG-Bench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hu2024ella</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, and FID.</span>\n</p>\n\n",
                "matched_terms": [
                    "generation",
                    "evaluated",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:120%;\">Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Editing)</span>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. Our evaluation of image-to-image editing capabilities is conducted on the GEdit-Bench benchmark </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2025step1x</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "imagetoimage",
                    "image",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:120%;\">Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Segmentation)</span>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">.\nWe evaluate the segmentation capability of our MLLM on the standard referring expression segmentation (RES) benchmarks RefCOCO/+ </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">refcoco</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\"> and RefCOCOg </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">refcocog</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:120%;\">Audio <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding)</span>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. Our evaluation of the audio-to-text understanding capabilities mainly includes the following three tasks: 1) Fundamental audio understanding capabilities evaluated on a broad range of public benchmarks, including public Chinese benchmarks like Aishell1 </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">AISHELL1</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\"> and Wenetspeech </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">WenetSpeech</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, and public English benchmarks like Librispeech </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Librispeech</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\"> and Voxpopuli </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2021voxpopuli</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. And 2) audio question-answering capabilities evaluated on various benchmarks across five specific tasks, such as AlpacaEval and CommonEval from VoiceBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024voicebenchbenchmarkingllmbasedvoice</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\"> for open-ended QA tasks, and SD-QA for knowledge-based QA tasks. Finally 3) evaluates the model&#8217;s ability to utilize context on ContextASR-Bench</span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025contextasrbenchmassivecontextualspeech</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "evaluated",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:120%;\">Text <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Audio (Generation)</span>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. We incorporate text-to-audio generation capabilities to enable our MLLM with unified audio perception-generation abilities, which are evaluated on Seed-TTS-Eval </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Seed_TTS</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "evaluated",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:120%;\">Video <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p7.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding)</span>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. Our evaluation of the video-to-text understanding capabilities contains the following four benchmarks: MVBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2024mvbench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, VideoMME </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fu2024video</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\"> and LongVideoBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2024longvideobench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "understanding"
                ]
            }
        ]
    },
    "S3.T8": {
        "source_file": "Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation",
        "caption": "Table 7: \nPerformance of Ming-Flash-Omni on PUBLIC and IN-HOUSE Audio Understanding Benchmarks.",
        "body": "Models\nMean\nOpen-ended QA\nKnowledge\nMulti-Choice QA\nInstruction\nSafety\n\n\n\n\n\n\n\nAlpacaEval\nCommonEval\nSD-QA\nMMSU\nOpenBookQA\nIFEval\nAdvBench\n\n\nStep-Audio-chat\n57.10\n\n\n79.80\n\n\n\n\n59.80\n\n\n\n\n46.84\n\n\n\n\n31.87\n\n\n\n\n29.19\n\n\n\n\n65.77\n\n\n\n\n86.73\n\n\n\n\nQwen2-Audio-chat\n54.70\n\n\n73.80\n\n\n\n\n68.00\n\n\n\n\n35.35\n\n\n\n\n35.43\n\n\n\n\n49.01\n\n\n\n\n22.57\n\n\n\n\n98.85\n\n\n\n\nBaichuan-Audio\n62.50\n\n\n80.00\n\n\n\n\n67.80\n\n\n\n\n49.64\n\n\n\n\n48.80\n\n\n\n\n63.30\n\n\n\n\n41.32\n\n\n\n\n86.73\n\n\n\n\nGLM-4-Voice\n57.20\n\n\n81.20\n\n\n\n\n69.60\n\n\n\n\n43.31\n\n\n\n\n40.11\n\n\n\n\n52.97\n\n\n\n\n24.91\n\n\n\n\n88.08\n\n\n\n\nKimi-Audio\n76.90\n\n\n89.20\n\n\n\n\n79.40\n\n\n\n\n63.12\n\n\n\n\n62.17\n\n\n\n\n83.52\n\n\n\n\n61.10\n\n\n\n\n100.00\n\n\n\n\nMegrez-3B-Omni\n46.20\n\n\n70.00\n\n\n\n\n59.00\n\n\n\n\n25.95\n\n\n\n\n27.03\n\n\n\n\n28.35\n\n\n\n\n25.71\n\n\n\n\n87.69\n\n\n\n\nDiVA\n55.70\n\n\n73.40\n\n\n\n\n70.80\n\n\n\n\n57.05\n\n\n\n\n25.76\n\n\n\n\n25.49\n\n\n\n\n39.15\n\n\n\n\n98.27\n\n\n\n\nQwen2.5-Omni\n74.10\n\n\n89.80\n\n\n\n\n78.60\n\n\n\n\n55.71\n\n\n\n\n61.32\n\n\n\n\n81.10\n\n\n\n\n52.87\n\n\n\n\n99.42\n\n\n\n\nQwen3-Omni-Flash-Instruct\n85.40\n\n\n95.40\n\n\n\n\n91.00\n\n\n\n\n76.80\n\n\n\n\n68.40\n\n\n\n\n91.40\n\n\n\n\n75.20\n\n\n\n\n99.40\n\n\n\n\nBaichuan-Omni-1.5\n71.10\n\n\n90.00\n\n\n\n\n81.00\n\n\n\n\n43.40\n\n\n\n\n57.25\n\n\n\n\n74.51\n\n\n\n\n54.54\n\n\n\n\n97.31\n\n\n\n\nMiniCPM-o\n71.70\n\n\n88.40\n\n\n\n\n83.00\n\n\n\n\n50.72\n\n\n\n\n54.78\n\n\n\n\n78.02\n\n\n\n\n49.25\n\n\n\n\n97.69\n\n\n\n\nMing-Flash-Omni\n83.50\n\n\n94.80\n\n\n\n\n92.60\n\n\n\n\n71.72\n\n\n\n\n68.70\n\n\n\n\n84.84\n\n\n\n\n72.49\n\n\n\n\n99.62",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Models</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Mean</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r ltx_border_t\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Open-ended QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Knowledge</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r ltx_border_t\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Multi-Choice QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Instruction</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Safety</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_align_top\"><span class=\"ltx_text\" style=\"font-size:90%;\">AlpacaEval</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">CommonEval</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">SD-QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top\"><span class=\"ltx_text\" style=\"font-size:90%;\">MMSU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">OpenBookQA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">IFEval</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top\"><span class=\"ltx_text\" style=\"font-size:90%;\">AdvBench</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Step-Audio-chat</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.10</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">79.80</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">59.80</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">46.84</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.87</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.19</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">65.77</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">86.73</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_align_top\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2-Audio-chat</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">54.70</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">73.80</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">68.00</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">35.35</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">35.43</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">49.01</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">22.57</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">98.85</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_align_top\"><span class=\"ltx_text\" style=\"font-size:90%;\">Baichuan-Audio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">62.50</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">80.00</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">67.80</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">49.64</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">48.80</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">63.30</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.32</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">86.73</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_align_top\"><span class=\"ltx_text\" style=\"font-size:90%;\">GLM-4-Voice</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.20</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">81.20</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">69.60</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">43.31</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">40.11</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">52.97</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.91</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">88.08</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_align_top\"><span class=\"ltx_text\" style=\"font-size:90%;\">Kimi-Audio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">76.90</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">89.20</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">79.40</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">63.12</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">62.17</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">83.52</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">61.10</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">100.00</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Megrez-3B-Omni</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">46.20</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.00</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">59.00</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.95</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.03</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.35</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.71</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">87.69</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_align_top\"><span class=\"ltx_text\" style=\"font-size:90%;\">DiVA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">55.70</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">73.40</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.80</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.05</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.76</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.49</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.15</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">98.27</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_align_top\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2.5-Omni</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">74.10</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">89.80</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">78.60</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">55.71</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">61.32</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">81.10</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">52.87</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">99.42</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_align_top\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen3-Omni-Flash-Instruct</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">85.40</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">95.40</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">91.00</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">76.80</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">68.40</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">91.40</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">75.20</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">99.40</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_align_top\"><span class=\"ltx_text\" style=\"font-size:90%;\">Baichuan-Omni-1.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.10</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">90.00</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">81.00</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">43.40</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.25</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">74.51</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">54.54</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">97.31</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_align_top\"><span class=\"ltx_text\" style=\"font-size:90%;\">MiniCPM-o</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.70</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">88.40</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">83.00</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">50.72</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">54.78</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">78.02</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">49.25</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">97.69</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Ming-Flash-Omni</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">83.50</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">94.80</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">92.60</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.72</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">68.70</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">84.84</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">72.49</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">99.62</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\"/>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "inhouse",
            "stepaudiochat",
            "qwen25omni",
            "kimiaudio",
            "sdqa",
            "instruction",
            "mmsu",
            "mingflashomni",
            "ifeval",
            "qwen2audiochat",
            "multichoice",
            "baichuanomni15",
            "audio",
            "glm4voice",
            "knowledge",
            "alpacaeval",
            "qwen3omniflashinstruct",
            "openended",
            "performance",
            "baichuanaudio",
            "advbench",
            "commoneval",
            "minicpmo",
            "openbookqa",
            "public",
            "megrez3bomni",
            "benchmarks",
            "safety",
            "models",
            "understanding",
            "diva",
            "mean"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding)</span>. Our model sets a new state-of-the-art (SOTA) on all 12 sub-tasks of the ContextASR-Bench (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T6\" title=\"Table 6 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>), underscoring its superior ability to leverage context&#8212;a vital skill for real-world applications like multi-turn dialogue and hotword enhancement. It also exhibits highly competitive performance across various ASR benchmarks, with notable strengths in dialect recognition (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T8\" title=\"Table 8 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>). In audio question answering, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> surpasses all open-source audio-centric and other Omni models, with the exception of Qwen3-Omni-Flash-Instruct<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen3omnitechnicalreport</span>)</cite> (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T8\" title=\"Table 8 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>). Taken together, these findings demonstrate the robust and versatile audio understanding capabilities of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion total parameters, of which only 6.1 billion are active per token. This architecture enables <span class=\"ltx_text ltx_font_bold\">highly efficient scaling</span> (dramatically improving computational efficiency while significantly expanding model capacity) and empowers <span class=\"ltx_text ltx_font_bold\">stronger unified multimodal intelligence</span> across vision, speech, and language, representing a key step toward Artificial General Intelligence (AGI). Compared to its predecessor, the upgraded version exhibits substantial improvements across multimodal understanding and generation. We significantly advance speech recognition capabilities, achieving state-of-the-art performance in <span class=\"ltx_text ltx_font_bold\">contextual ASR</span> and highly competitive results in <span class=\"ltx_text ltx_font_bold\">dialect-aware ASR</span>. In image generation, Ming-Flash-Omni introduces <span class=\"ltx_text ltx_font_bold\">high-fidelity text rendering</span> and demonstrates marked gains in <span class=\"ltx_text ltx_font_bold\">scene consistency</span> and <span class=\"ltx_text ltx_font_bold\">identity preservation</span> during image editing. Furthermore, Ming-Flash-Omni introduces <span class=\"ltx_text ltx_font_bold\">generative segmentation</span>, a capability that not only achieves strong standalone segmentation performance but also enhances spatial control in image generation and improves editing consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in text-to-image generation and generative segmentation, and sets new records on all 12 contextual ASR benchmarks, all within a single unified architecture.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "mingflashomni",
                    "understanding",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Driven by advances in Large Language Models (LLMs) and extensive training on large-scale multimodal datasets, Multi-modal Large Language Models (MLLMs) have demonstrated remarkable perceptual capabilities in both vision <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024internvl</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bai2025qwen2</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2025kimi</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen3</span>)</cite> and audio <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ding2025kimi</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen2</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen3</span>)</cite>, as well as generative capabilities in these two modalities <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">huang2025step</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ding2025kimi</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chatgpt4o</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tong2024metamorph</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pan2025transfer</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen3</span>)</cite>. Nevertheless, effectively integrating comprehension and generation across multiple modalities into a unified model remains challenging. While humans naturally learn by combining multiple modalities, leveraging their complementary strengths and interactions to enhance overall learning efficiency, building a unified Omni-MLLM is hindered by representational disparities and modality imbalances.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>, which builds upon the  <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> architecture with a redesigned foundation and targeted enhancements across multimodal understanding and generation. At its core,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> adopts Ling-Flash-2.0 <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lingv2_flash</span></cite> (a scaled-up, highly sparse Mixture-of-Experts architecture) where an increased sparsity ratio enables substantial model capacity while maintaining bounded inference latency, striking a favorable trade-off between performance and efficiency.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "understanding",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the understanding side, the model introduces two key advances. First,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> upgrade the positional encoding to VideoRoPE <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wei2025videorope</span></cite>, a refined variant specifically designed to better capture temporal dynamics in video sequences, thereby enhancing the model&#8217;s ability to understand complex visual events. Second,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> focus on improving the context-aware ASR capability itself, enhancing the model&#8217;s ability to leverage surrounding linguistic context during speech recognition and thereby achieving more accurate transcription in context-dependent scenarios.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These architectural innovations empower <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> to deliver exceptional cross-modal performance in both comprehension and generation tasks. Specifically, in the image perception task, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> attained performance comparable to that of Qwen3-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen3</span>)</cite>. <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> also delivers superior performance in end-to-end speech understanding and generation.For instance, it achieves SOTA on all 12 metrics on ContextASR-Bench.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "understanding",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remainder of this paper is organized as follows. Section 2 presents the detailed architecture of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>. Sections 3 describes the pretraining and post-training datasets.\nSection 4 reports the evaluation results and compare <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> with recent multimodal models. Sections 5 is conclusion.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S2.F2\" title=\"Figure 2 &#8227; 2 Ming-Flash-Omni &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> retains the unified two-stage pipeline of <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ai2025ming</span></cite>, where perception supports multimodal understanding and generation targets speech and image synthesis, while markedly advancing long-context modeling, reasoning, and controllable generation. At the core is Ling&#8209;Flash&#8209;2.0 <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lingv2_flash</span></cite>, a sparse MoE LM (100B; 6.1B per token) with a dual balancing scheme that stabilizes training and improves efficiency. On the perception side, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> employs VideoRoPE <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wei2025videorope</span></cite> for temporal modeling, context&#8209;aware ASR for more reliable speech understanding, and a think mode for deeper multi&#8209;step reasoning. On the generation side, we replace discrete speech tokens with continuous acoustic latents, avoiding quantization loss and improving fidelity; for images, we upgrade to a synergistic training paradigm that enables generative segmentation-as-editing, facilitating fine-grained and controllable generation. Overall, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> advances the unified model with stable expert routing and scalable long&#8209;context modeling, yielding more reliable multimodal understanding and controllable generation.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The cornerstone of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> is enhanced multimodal understanding. We retain the established visual and audio encoders (Qwen2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bai2025qwen25vltechnicalreport</span>)</cite> and Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>)</cite>) and feed their projected embeddings, concatenated with tokenized text, into Ling&#8209;flash&#8209;2.0, a sparse MoE language model with distinct routers per modality. Beyond this, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> incorporates VideoRoPE to maintain temporal coherence over long-range frame sequences, thus emphasizing temporal modeling. Furthermore, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> adopt a context&#8209;aware ASR training paradigm that conditions decoding on task or domain context, addressing common shortcomings of conventional ASR in real&#8209;world, multi&#8209;domain scenarios (limited world knowledge and unreliable proper&#8209;noun recognition) and yielding more accurate, context&#8209;consistent transcripts. To stabilize training in the more sparse Ling&#8209;flash&#8209;2.0, we employ a hybrid expert&#8209;balancing scheme that combines an auxiliary load&#8209;balancing loss (as in <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span>) with per&#8209;router bias updates (as in Ling&#8209;flash&#8209;2.0), promoting uniform expert activation and improved convergence.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mingflashomni",
                    "understanding",
                    "knowledge"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> retains the same audio encoder as  <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ai2025ming</span>)</cite>, with optimization efforts focused on improving contextual automatic speech recognition (ASR) performance by incorporating preceding textual context and hotword lists during training. For generation, we replace discrete speech tokens with continuous acoustic latents, avoiding quantization loss and improving fidelity. Specifically, we utilize a fixed, pre-trained audio head based on Qwen2.5 (0.5B parameters), which takes LLM-generated text tokens and downsampled VAE latents as input and autoregressively predicts the conditioning signals for the flow-matching head&#8212;following the paradigm of <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jia2025ditar</span></cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mingflashomni",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A core challenge in building unified multimodal models is the effective fusion of image understanding and generation. While our <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> model injects hierarchical semantics via multi-scale query tokens, its language pathway remains frozen during training to prevent interference from the generative objective. This freezing approach, while ensuring stability, creates a critical bottleneck: an inherent discrepancy between the learning objectives of understanding and generation. Consequently, even with injected hierarchical semantics, fine-grained visual knowledge&#8212;such as object attributes and spatial relationships&#8212;cannot be efficiently transferred to high-precision generation and editing tasks, limiting the model&#8217;s final quality and controllability.</p>\n\n",
                "matched_terms": [
                    "models",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome this bottleneck,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> propose a synergistic training paradigm that reframes image segmentation as a generative editing task. Instead of producing an abstract binary mask (e.g., for &#8220;segment the banana&#8221;), the model performs a semantics-preserving edit (e.g., &#8220;color the banana purple&#8221;). This reformulation forcibly binds the learning objectives of understanding and generation: successful generation requires a precise understanding of the object&#8217;s contours and boundaries. Understanding thus becomes a <span class=\"ltx_text ltx_font_italic\">prerequisite</span> for editing, and the edit&#8217;s quality provides a direct supervisory signal for the model&#8217;s comprehension, fundamentally unifying their optimization objectives.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to large language models (LLMs), training multimodal foundation models presents several key challenges, primarily stemming from <span class=\"ltx_text ltx_font_bold\">data heterogeneity</span> and <span class=\"ltx_text ltx_font_bold\">model heterogeneity</span>. First, data heterogeneity arises from the need to dynamically switch between diverse input modalities (text, images, audio, and video) during training. These modalities exhibit significant differences in tensor shape, most notably in the form of dynamic batch sizes and variable-length sequences. This variability complicates the design of a unified parallel computation layout. As a result, computational workloads become unevenly distributed across processing ranks, leading to load imbalance. Moreover, the frequent allocation and deallocation of GPU memory buffers for inputs of varying shapes induce severe memory fragmentation, substantially degrading training efficiency and hardware utilization. Second, in contrast to large language models (LLMs), which are predominantly based on homogeneous, decoder-only Transformer architectures, multimodal foundation models typically employ modality-specific encoders at the input stage, introducing model heterogeneity. Although these encoders are relatively lightweight in terms of parameter count, they are highly sensitive to parallelization strategies. If not carefully partitioned across devices, they can induce substantial pipeline bubbles (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, idle computation cycles) during pipeline-parallel execution, thereby constraining overall training throughput.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We have collected a large and diverse set of training data to enable models to process and understand information from multiple modalities, including text, images, audio and videos.\nThe majority of this data comes from <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ai2025ming</span>)</cite>. In addition, we develop several data processing pipelines to ensure data quality, diversity and deduplication. Establishing an effective multimodal data strategy is essential for the joint multi-modal training, as it facilitates seamless alignment of knowledge across diverse modalities.\nWe categorize the training data based on the core modalities they are designed to enhance, including image, audio, video, and text. The detailed sources and construction methods for each type of data are elaborated in this section.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "audio",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OCR Data:</span>\nText recognition and document understanding capabilities are crucial for MLLM. We construct a large-scale heterogeneous training dataset with millions of samples, consisting of three data sources: open-source data, expert-collaborative pseudo-labeled data, and human-annotated enhancement data. The expert-collaborative pseudo-labeled data is generated by diagnosing model weaknesses, and using expert models to label targeted data.\nIn addition, to enhance the model&#8217;s capability in text&#8211;visual analysis and logical reasoning, we incorporate the Chain-of-Thought (CoT) paradigm into the training data. We incorporate the open-source ChartQA-PoT dataset to enhance the model&#8217;s numerical reasoning ability on charts and pioneeringly use executable Python code as the intermediate reasoning representation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Reasoning Data:</span>\nIn the reasoning training of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>, we enrich the CoT data to enhance the model&#8217;s reasoning capabilities. These data are primarily constructed around three key themes: mathematical logic, spatial reasoning, and GUI reasoning.\nWe design an efficient CoT generation and filtering pipeline to construct high-quality, well-structured reasoning data that enhances the model&#8217;s multi-step reasoning capability. (1) We sample long CoT data using state-of-the-art multimodal reasoning models (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GEMINI</span>)</cite>) to build an initial CoT pool. (2) We evaluate the accuracy and quality of the synthesized CoT data and filter out the low-quality data. Based on this pipeline, we construct 1.5M multimodal long CoT samples, with a maximum length of 16K tokens.\nBesides, to overcome the limitations of the &#8220;direct answer&#8221; pattern in text-based QA data, we use reinforcement learning models to generate multi-step reasoning traces and final answers.\nExperimental results demonstrate that this data significantly improves <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>&#8217;s performance on complex reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Preference alignment Data:</span>\nTo further enhance user interaction and response quality, we introduce preference alignment data, focusing on three main aspects:\n(1) Instruction intent understanding: To enhance the model&#8217;s ability to understand the true intent behind user instructions, we design a novel instruction intent reasoning paradigm where the model first determines if the instruction is clear. For clear instructions, it provides structured reasoning and final answers. For ambiguous ones, it infers the likely intent, generates clarification prompts, and aligns responses accordingly. By incorporating experience-alignment training, we significantly improve the model&#8217;s intent understanding and enhance the overall user experience.\n(2) Multi-turn conversation: Users often engage in repeated questioning on the same context. To maintain semantic consistency across turns, we decompose complex instructions into multi-turn conversations and generate high-quality responses using expert-annotated LLM pipelines <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">laban2025llms</span>)</cite>. This ensures context retention and reduces performance degradation in multi-turn scenarios.\n(3) Complex multimodal instruction following: Users often provide sequential, interdependent commands. We design a multimodal instruction generation pipeline to generate SFT and DPO-style data to cover basic and complex instruction types <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2024iopo</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qian2024mia</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ding2025mm</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "performance",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For audio data, we mainly use the data from <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ai2025ming</span>)</cite>. In addition, we incorporate the following three datasets to further enhance the model&#8217;s audio understanding and generation capabilities.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Context ASR Data:</span>\nCurrent ASR systems face challenges in recognizing homophones or phonetically similar words when the context is limited, pronunciations are unclear, or accents are present. ContextASR addresses these issues by leveraging the preceding context. We propose to synthesize a large-scale dataset using LLMs to endow models with ContextASR capabilities.\nWe extract named entities and construct context passages using LLMs based on existing ASR text, producing 3 million Chinese and English samples in the format &lt;audio, text, context, entity_list&gt;. During training, we further filter and sample the data to reduce keyword density, remove keywords that are absent from the text, and generate negative samples to enhance the model&#8217;s discriminative ability.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Video streaming conversation constitutes a fundamental capability for MLLMs in video understanding. However, acquiring large-scale streaming multi-turn conversation data is prohibitively expensive. In this work, we propose a pipeline to systematically synthesize diverse, balanced, and high-quality multi-turn conversation video datasets.\nWe collect 8.2M videos from the internet, ranging from 90 seconds to 10 minutes, and first filter out low-quality videos with high speech density, high shot density, irregular aspect ratios, or low resolution. We then filter out low-information, incoherent, or overly simple videos using SOTA MLLMs. To ensure a balanced dataset, we use advanced embedding models (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, Qwen3-Embedding <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2025qwen3</span>)</cite> and M3-Embedding <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024bge</span>)</cite>) to extract embeddings and then cluster the videos to suppress high-frequency data while preserving long-tail content. Finally, we use SOTA video understanding models to generate high-quality video conversations. This produces 1.2M conversation turns across 5-minute average videos, balanced across various task categories.</p>\n\n",
                "matched_terms": [
                    "models",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present the evaluation details and quantitative examples of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> on both public and in-house benchmarks.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "public",
                    "mingflashomni",
                    "inhouse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The details of the public benchmarks are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S7\" title=\"7 Public Benchmarks &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. As shown in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:table_1_image2text</span><math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T9\" title=\"Table 9 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, our holistic assessment covers more than 50 rigorously curated public benchmarks across the following seven distinct multi-modal dimensions: Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding), Text <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Generation), Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Editing), Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Segmentation), Audio <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding), Text <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Audio (Generation), and Video <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding).</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "public",
                    "audio",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to public benchmarks, we also establish three in-house benchmarks to comprehensively evaluate multiple capabilities of MLLMs, including:</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "public",
                    "inhouse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multi-Dialect and Multi-Domain Audio Understanding Benchmark</span>. To extend audio understanding benchmarks into multi-dialect and multi-domain settings, we constructed two specialized datasets. The multi-dialect dataset was created from 15 regions, while the multi-domain one was curated from six domains. All samples were manually verified for quality by trained annotators. The final datasets comprise 51,986 multi-dialect samples and 10,397 multi-domain samples, with the latter distributed across: Noisy (8,145), Chat (443), Government (462), Health (450), Knowledge (421), and Local Services (476).</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "audio",
                    "understanding",
                    "benchmarks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video Streaming Multi-turn Benchmark</span>.\nThe evaluation of video streaming multi-turn dialogue capabilities requires quantifying not only the model&#8217;s understand capability but also assessing its interactive experience, including proactivity and naturalness. Previous streaming dialogue datasets, such as StreamBench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2024streamingbench</span>)</cite> and OvO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">niu2025ovo</span>)</cite>, have primarily focused on the understanding aspect while lacking a thorough evaluation of the interactive experience.\nTo address this gap, we introduce StreamingMultiturnBench.\nTo construct StreamingMultiturnBench, we manually selected 380 videos, carefully ensuring coverage of multiple key domains including life recording, education, TV shows, video games, and documentaries. Then we use SOTA closed-source modelfor machine annotation. Subsequently, a team of 10 human annotators revise and double-check the dialogue content to ensure it aligns with human conversational preferences. This process yielded 2,200 video question-answer pairs.\nDuring evaluation, we use advanced closed-source model, <em class=\"ltx_emph ltx_font_italic\">e.g.</em> GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chatgpt4o</span>)</cite>, to compare the model&#8217;s output against the human-annotated answers, scoring it on a scale of 1 to 5 across the six dimensions: accuracy, completeness, relevance, naturalness, conciseness, and proactivity. The final score is the average for each dimension. To align our metrics with other video benchmarks, we linearly scale the results to a 100-point scale. We commit to open-sourcing and publicly maintaining this benchmark to ensure reproducibility.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct comprehensive evaluations of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> against state-of-the-art MLLMs on over 50 different multimodal benchmarks, as illustrated in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:table_1_image2text</span><math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T9\" title=\"Table 9 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. Extensive experiments demonstrate that <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves comparable performance with leading MLLMs.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "mingflashomni",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Vision <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding)</span> As shown in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:table_1_image2text</span>, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates strong and competitive performance across a wide range of vision&#8211;language benchmarks. Specifically, on general-purpose understanding task, it achieves performance on par with leading omni-models (most notably scoring 81.9% on MathVista), though it still exhibits a slight gap compared to state-of-the-art vision&#8211;language models. Similarly, on OCR-centric benchmarks, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> attains state-of-the-art results among omni-modal models, yet remains marginally behind the best proprietary counterparts. In multi-image understanding, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> outperforms the leading open-source vision&#8211;language model Qwen3-VL-30B-A3B on MMT-Bench (68.0) and LLaVA-Interleave Bench (63.3), while showing a minor performance gap on MuirBench, suggesting opportunities for further improvement. In video understanding, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves state-of-the-art performance on MVBench with a score of 74.6, demonstrating robust general video reasoning capabilities. It further exhibits specialized proficiency in processing linguistic content within videos, attaining a leading score of 73.0 on VideoMME with subtitles. Although its performance on long-form video understanding is slightly lower, its consistently strong results across most metrics underscore its advanced video comprehension capabilities.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "benchmarks",
                    "models",
                    "understanding",
                    "mingflashomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Generation)</span>. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T2\" title=\"Table 2 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, our experimental results demonstrate that the generation quality of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> is on par with state-of-the-art diffusion models.\nNotably, on the Geneval benchmark, our model surpasses all non-Reinforcement Learning methods, demonstrating exceptional controllability. This advantage is particularly pronounced in the \"Position\" and \"Color.\" sub-categories.\nOn the DPG-Bench benchmark, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves an overall score of 83.08, a performance level comparable to pure image generation models like SD3-Medium (84.08) and leading unified models like OmniGen2 (83.57).</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Editing)</span>. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T6\" title=\"Table 6 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>,\n<span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates impressive image editing performance, surpassing all other unified models.\nSpecifically, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> supports editing instructions in Chinese, achieving performance comparable to that with English instructions. Compared to Qwen-Image-Edit which utilizes a 20B DiT head, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves comparable semantic consistency and perceptual quality with a much more efficient 2B DiT head&#8212;only one-tenth the parameters. This efficiency also translates to remarkable inference speeds, typically between 1-2 seconds per generation.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Segmentation)</span>. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T6\" title=\"Table 6 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> is capable of performing segmentation tasks, achieving performance comparable to that of specialized models designed explicitly for this purpose. Compared to other unified MLLMs, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates a significant advantage in segmentation.\nFor instance, Qwen-Image-Edit often struggles to accurately localize the target object, while Nano-banana frequently misinterprets user intent during inference.\nIn contrast, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> exhibits superior robustness and a more accurate understanding of spatial and semantic instructions.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "understanding",
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p7.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Audio (Generation)</span>. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T6\" title=\"Table 6 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, leveraging advancements in speech representation and model architecture, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves SOTA performance among open-source models on the test-zh subset of the SEED-TTS-Eval benchmark<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite>. Furthermore, its WER on the test-en subset is surpassed only by that of Qwen3-Omni.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mingflashomni",
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video + Audio <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p8.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Video Streaming Conversation)</span>.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T9\" title=\"Table 9 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, benefiting from the introduction of high-quality and diverse streaming video multiturn data, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> has achieved significant improvements in all dimensions compared to <span class=\"ltx_text ltx_font_typewriter\">Ming-Lite-Omni</span>. <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> also outperforms Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen2</span>)</cite> in the dimensions of accuracy, completeness, relevance, and conciseness, providing better experience in streaming video conversation scenarios.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mingflashomni",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present several visualization examples to better illustrate the capabilities of  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>. First, as shown in the figure,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates strong visual understanding across multiple dimensions: it leverages rich world knowledge to accurately infer geographic locations from visual cues; excels at multi-image understanding and generates creative, coherent text grounded in multiple images; solves complex mathematical problems through clear, step-by-step reasoning; and exhibits robust document understanding by accurately parsing intricate formulas and answering questions about sophisticated charts and diagrams within images. Turning to speech recognition,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves strong performance on contextual ASR tasks. By leveraging contextual information, it effectively resolves many challenging cases where conventional ASR systems tend to fail&#8212;such as ambiguous homophones, domain-specific terminology, or noisy conversational speech. Moreover, this version also supports multiple Chinese dialects, significantly broadening its applicability in real-world multilingual and regional speech scenarios. Lastly, we visualize the capabilities of <span class=\"ltx_text ltx_font_italic\">Text/Image\n&#8594;Image</span> generation tasks in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.F4\" title=\"Figure 4 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.F5\" title=\"Figure 5 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, covering a wide range of applications including image generation, image editing, image segmentation, multi-image editing, ID photo generation, ID photo editing, and background replacement.\nAs can be seen,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> not only supports a broader set of generative capabilities but also achieves higher output quality and greater controllability compared to previous versions.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "mingflashomni",
                    "understanding",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>, built upon Ling-Flash-2.0 with 100 billion parameters, where only 6.1B parameters are activated per token. <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates advanced multimodal perception and generation capabilities with improved computational efficiency while scaling model capacity. It achieves SOTA performance across a broad spectrum of tasks, including multi-image and video processing, image generation, generative segmentation, Contextual Automatic Speech Recognition (ContextASR), and multi-dialect recognition, outperforming omni models of comparable scale. We believe the open-sourcing of our models and code will facilitate the development of AGI by advancing multimodal intelligence research and enabling broader real-world applications.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:120%;\">Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding)</span>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. Our evaluation of the image-to-text understanding capabilities primarily encompasses the following six tasks: 1) general image understanding capabilities evaluated on MMStar </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024we</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, AI2D </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kembhavi2016diagram</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, HallusionBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Guan_2024_hallusionbench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, CV-Bench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tong2024cambrian1</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, MathVista </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lu2024mathvista</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, and CRPE </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024crpe</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. 2) OCR capabilities evaluated on ChartQA </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">masry-etal-2022-chartqa</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, DocVQA </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">docvqa</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, OCRBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Liu_2024</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, and TextVQA-VAL </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Eval_textvqa</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. 3) multi-image capabilities evaluated on MMTBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ying2024mmtbench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, MuirBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024muirbench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, and LLaVA-interleave Bench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2024llava_interleave_bench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. 4) Complex instruction capabilities evaluated on MIA-Bench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qian2024mia</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. 5) video understanding capabilities evaluated on MVBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2024mvbench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, VideoMME </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fu2024videomme</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, and LongVideoBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2024longvideobench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:120%;\">Audio <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding)</span>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. Our evaluation of the audio-to-text understanding capabilities mainly includes the following three tasks: 1) Fundamental audio understanding capabilities evaluated on a broad range of public benchmarks, including public Chinese benchmarks like Aishell1 </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">AISHELL1</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\"> and Wenetspeech </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">WenetSpeech</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, and public English benchmarks like Librispeech </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Librispeech</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\"> and Voxpopuli </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2021voxpopuli</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. And 2) audio question-answering capabilities evaluated on various benchmarks across five specific tasks, such as AlpacaEval and CommonEval from VoiceBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024voicebenchbenchmarkingllmbasedvoice</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\"> for open-ended QA tasks, and SD-QA for knowledge-based QA tasks. Finally 3) evaluates the model&#8217;s ability to utilize context on ContextASR-Bench</span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025contextasrbenchmassivecontextualspeech</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "alpacaeval",
                    "public",
                    "openended",
                    "benchmarks",
                    "understanding",
                    "sdqa",
                    "commoneval",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:120%;\">Video <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p7.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding)</span>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">. Our evaluation of the video-to-text understanding capabilities contains the following four benchmarks: MVBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2024mvbench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">, VideoMME </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fu2024video</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\"> and LongVideoBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:120%;\">(</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2024longvideobench</span>\n    <span class=\"ltx_text\" style=\"font-size:120%;\">)</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:120%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "understanding"
                ]
            }
        ]
    },
    "S3.T9": {
        "source_file": "Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation",
        "caption": "Table 9: \nPerformance of Ming-Flash-Omni on StreamingMultiturnBench compared to leading omni-MLLMs.",
        "body": "Model\n\n\n\n\nAccuracy\n\n\n\n\nCompleteness\n\n\n\n\nRelevance\n\n\n\n\nConciseness\n\n\n\n\nNaturalness\n\n\n\n\nProactivity\n\n\n\n\nAverage\n\n\n\n\n\n\nQwen2.5-Omni\n\n\n\n\n54.03\n\n\n\n\n53.68\n\n\n\n\n78.00\n\n\n\n\n77.38\n\n\n\n\n98.28\n\n\n\n\n46.25\n\n\n\n\n67.93\n\n\n\n\n\n\nMing-Lite-Omni\n\n\n\n\n44.63\n\n\n\n\n40.58\n\n\n\n\n69.28\n\n\n\n\n93.88\n\n\n\n\n95.65\n\n\n\n\n28.78\n\n\n\n\n62.13\n\n\n\n\n\n\nMing-Flash-Omni\n\n\n\n\n57.03\n\n\n\n\n57.10\n\n\n\n\n80.40\n\n\n\n\n94.13\n\n\n\n\n99.40\n\n\n\n\n41.38\n\n\n\n\n71.57",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:93.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Model</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:31.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Accuracy</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Completeness</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Relevance</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Conciseness</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:37.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Naturalness</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:37.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Proactivity</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:31.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Average</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:93.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2.5-Omni</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:31.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">54.03</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">53.68</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">78.00</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">77.38</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:37.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">98.28</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:37.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">46.25</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:31.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">67.93</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:93.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Ming-Lite-Omni</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:31.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">44.63</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">40.58</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">69.28</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">93.88</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:37.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">95.65</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:37.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.78</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:31.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">62.13</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:93.9pt;\"><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Ming-Flash-Omni</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:31.3pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">57.03</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">57.10</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:39.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">80.40</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">94.13</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:37.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">99.40</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:37.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.38</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:31.3pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">71.57</span></span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "relevance",
            "performance",
            "leading",
            "completeness",
            "conciseness",
            "model",
            "accuracy",
            "qwen25omni",
            "average",
            "mingliteomni",
            "compared",
            "omnimllms",
            "proactivity",
            "naturalness",
            "mingflashomni",
            "streamingmultiturnbench"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The details of the public benchmarks are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S7\" title=\"7 Public Benchmarks &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. As shown in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:table_1_image2text</span><math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T9\" title=\"Table 9 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, our holistic assessment covers more than 50 rigorously curated public benchmarks across the following seven distinct multi-modal dimensions: Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding), Text <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Generation), Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Editing), Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Segmentation), Audio <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding), Text <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Audio (Generation), and Video <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding).</p>\n\n",
            "<p class=\"ltx_p\">We conduct comprehensive evaluations of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> against state-of-the-art MLLMs on over 50 different multimodal benchmarks, as illustrated in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:table_1_image2text</span><math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T9\" title=\"Table 9 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. Extensive experiments demonstrate that <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves comparable performance with leading MLLMs.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video + Audio <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p8.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Video Streaming Conversation)</span>.\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T9\" title=\"Table 9 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, benefiting from the introduction of high-quality and diverse streaming video multiturn data, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> has achieved significant improvements in all dimensions compared to <span class=\"ltx_text ltx_font_typewriter\">Ming-Lite-Omni</span>. <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> also outperforms Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen2</span>)</cite> in the dimensions of accuracy, completeness, relevance, and conciseness, providing better experience in streaming video conversation scenarios.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion total parameters, of which only 6.1 billion are active per token. This architecture enables <span class=\"ltx_text ltx_font_bold\">highly efficient scaling</span> (dramatically improving computational efficiency while significantly expanding model capacity) and empowers <span class=\"ltx_text ltx_font_bold\">stronger unified multimodal intelligence</span> across vision, speech, and language, representing a key step toward Artificial General Intelligence (AGI). Compared to its predecessor, the upgraded version exhibits substantial improvements across multimodal understanding and generation. We significantly advance speech recognition capabilities, achieving state-of-the-art performance in <span class=\"ltx_text ltx_font_bold\">contextual ASR</span> and highly competitive results in <span class=\"ltx_text ltx_font_bold\">dialect-aware ASR</span>. In image generation, Ming-Flash-Omni introduces <span class=\"ltx_text ltx_font_bold\">high-fidelity text rendering</span> and demonstrates marked gains in <span class=\"ltx_text ltx_font_bold\">scene consistency</span> and <span class=\"ltx_text ltx_font_bold\">identity preservation</span> during image editing. Furthermore, Ming-Flash-Omni introduces <span class=\"ltx_text ltx_font_bold\">generative segmentation</span>, a capability that not only achieves strong standalone segmentation performance but also enhances spatial control in image generation and improves editing consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in text-to-image generation and generative segmentation, and sets new records on all 12 contextual ASR benchmarks, all within a single unified architecture.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "model",
                    "compared",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>, which builds upon the  <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> architecture with a redesigned foundation and targeted enhancements across multimodal understanding and generation. At its core,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> adopts Ling-Flash-2.0 <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lingv2_flash</span></cite> (a scaled-up, highly sparse Mixture-of-Experts architecture) where an increased sparsity ratio enables substantial model capacity while maintaining bounded inference latency, striking a favorable trade-off between performance and efficiency.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the understanding side, the model introduces two key advances. First,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> upgrade the positional encoding to VideoRoPE <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wei2025videorope</span></cite>, a refined variant specifically designed to better capture temporal dynamics in video sequences, thereby enhancing the model&#8217;s ability to understand complex visual events. Second,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> focus on improving the context-aware ASR capability itself, enhancing the model&#8217;s ability to leverage surrounding linguistic context during speech recognition and thereby achieving more accurate transcription in context-dependent scenarios.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the generation side, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> introduces three key advancements: 1) in speech synthesis, discrete acoustic tokens are replaced with continuous representations, effectively circumventing quantization-induced artifacts and yielding more natural and expressive TTS outputs; 2) the model supports generative semantic segmentation, enabling pixel-level semantic content generation conditioned on multimodal inputs; and 3) it enables fine-grained controllable image generation with improved identity preservation and in-image text generation capabilities.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These architectural innovations empower <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> to deliver exceptional cross-modal performance in both comprehension and generation tasks. Specifically, in the image perception task, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> attained performance comparable to that of Qwen3-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen3</span>)</cite>. <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> also delivers superior performance in end-to-end speech understanding and generation.For instance, it achieves SOTA on all 12 metrics on ContextASR-Bench.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S2.F2\" title=\"Figure 2 &#8227; 2 Ming-Flash-Omni &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> retains the unified two-stage pipeline of <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ai2025ming</span></cite>, where perception supports multimodal understanding and generation targets speech and image synthesis, while markedly advancing long-context modeling, reasoning, and controllable generation. At the core is Ling&#8209;Flash&#8209;2.0 <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lingv2_flash</span></cite>, a sparse MoE LM (100B; 6.1B per token) with a dual balancing scheme that stabilizes training and improves efficiency. On the perception side, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> employs VideoRoPE <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wei2025videorope</span></cite> for temporal modeling, context&#8209;aware ASR for more reliable speech understanding, and a think mode for deeper multi&#8209;step reasoning. On the generation side, we replace discrete speech tokens with continuous acoustic latents, avoiding quantization loss and improving fidelity; for images, we upgrade to a synergistic training paradigm that enables generative segmentation-as-editing, facilitating fine-grained and controllable generation. Overall, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> advances the unified model with stable expert routing and scalable long&#8209;context modeling, yielding more reliable multimodal understanding and controllable generation.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The cornerstone of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> is enhanced multimodal understanding. We retain the established visual and audio encoders (Qwen2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bai2025qwen25vltechnicalreport</span>)</cite> and Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>)</cite>) and feed their projected embeddings, concatenated with tokenized text, into Ling&#8209;flash&#8209;2.0, a sparse MoE language model with distinct routers per modality. Beyond this, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> incorporates VideoRoPE to maintain temporal coherence over long-range frame sequences, thus emphasizing temporal modeling. Furthermore, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> adopt a context&#8209;aware ASR training paradigm that conditions decoding on task or domain context, addressing common shortcomings of conventional ASR in real&#8209;world, multi&#8209;domain scenarios (limited world knowledge and unreliable proper&#8209;noun recognition) and yielding more accurate, context&#8209;consistent transcripts. To stabilize training in the more sparse Ling&#8209;flash&#8209;2.0, we employ a hybrid expert&#8209;balancing scheme that combines an auxiliary load&#8209;balancing loss (as in <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span>) with per&#8209;router bias updates (as in Ling&#8209;flash&#8209;2.0), promoting uniform expert activation and improved convergence.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> retains the same audio encoder as  <span class=\"ltx_text ltx_font_typewriter\">Ming-Omni</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ai2025ming</span>)</cite>, with optimization efforts focused on improving contextual automatic speech recognition (ASR) performance by incorporating preceding textual context and hotword lists during training. For generation, we replace discrete speech tokens with continuous acoustic latents, avoiding quantization loss and improving fidelity. Specifically, we utilize a fixed, pre-trained audio head based on Qwen2.5 (0.5B parameters), which takes LLM-generated text tokens and downsampled VAE latents as input and autoregressively predicts the conditioning signals for the flow-matching head&#8212;following the paradigm of <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jia2025ditar</span></cite>.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome this bottleneck,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> propose a synergistic training paradigm that reframes image segmentation as a generative editing task. Instead of producing an abstract binary mask (e.g., for &#8220;segment the banana&#8221;), the model performs a semantics-preserving edit (e.g., &#8220;color the banana purple&#8221;). This reformulation forcibly binds the learning objectives of understanding and generation: successful generation requires a precise understanding of the object&#8217;s contours and boundaries. Understanding thus becomes a <span class=\"ltx_text ltx_font_italic\">prerequisite</span> for editing, and the edit&#8217;s quality provides a direct supervisory signal for the model&#8217;s comprehension, fundamentally unifying their optimization objectives.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Crucially, this training cultivates a more fundamental, generalizable skill: <span class=\"ltx_text ltx_font_bold\">fine-grained spatio-semantic control</span>, which indirectly resolves the compositionality problem in pure text-to-image generation.\n<span class=\"ltx_text ltx_font_bold\">In our evaluation on the GenEval benchmark, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieved a score of 0.90, surpassing leading non-Reinforcement Learning (non-RL) methods.</span>\nThis result suggests that the foundational skill of spatio-semantic control can effectively generalize to pure text-to-image generation tasks.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "leading"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to large language models (LLMs), training multimodal foundation models presents several key challenges, primarily stemming from <span class=\"ltx_text ltx_font_bold\">data heterogeneity</span> and <span class=\"ltx_text ltx_font_bold\">model heterogeneity</span>. First, data heterogeneity arises from the need to dynamically switch between diverse input modalities (text, images, audio, and video) during training. These modalities exhibit significant differences in tensor shape, most notably in the form of dynamic batch sizes and variable-length sequences. This variability complicates the design of a unified parallel computation layout. As a result, computational workloads become unevenly distributed across processing ranks, leading to load imbalance. Moreover, the frequent allocation and deallocation of GPU memory buffers for inputs of varying shapes induce severe memory fragmentation, substantially degrading training efficiency and hardware utilization. Second, in contrast to large language models (LLMs), which are predominantly based on homogeneous, decoder-only Transformer architectures, multimodal foundation models typically employ modality-specific encoders at the input stage, introducing model heterogeneity. Although these encoders are relatively lightweight in terms of parameter count, they are highly sensitive to parallelization strategies. If not carefully partitioned across devices, they can induce substantial pipeline bubbles (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, idle computation cycles) during pipeline-parallel execution, thereby constraining overall training throughput.</p>\n\n",
                "matched_terms": [
                    "model",
                    "compared",
                    "leading"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Reasoning Data:</span>\nIn the reasoning training of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>, we enrich the CoT data to enhance the model&#8217;s reasoning capabilities. These data are primarily constructed around three key themes: mathematical logic, spatial reasoning, and GUI reasoning.\nWe design an efficient CoT generation and filtering pipeline to construct high-quality, well-structured reasoning data that enhances the model&#8217;s multi-step reasoning capability. (1) We sample long CoT data using state-of-the-art multimodal reasoning models (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GEMINI</span>)</cite>) to build an initial CoT pool. (2) We evaluate the accuracy and quality of the synthesized CoT data and filter out the low-quality data. Based on this pipeline, we construct 1.5M multimodal long CoT samples, with a maximum length of 16K tokens.\nBesides, to overcome the limitations of the &#8220;direct answer&#8221; pattern in text-based QA data, we use reinforcement learning models to generate multi-step reasoning traces and final answers.\nExperimental results demonstrate that this data significantly improves <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>&#8217;s performance on complex reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "accuracy",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Preference alignment Data:</span>\nTo further enhance user interaction and response quality, we introduce preference alignment data, focusing on three main aspects:\n(1) Instruction intent understanding: To enhance the model&#8217;s ability to understand the true intent behind user instructions, we design a novel instruction intent reasoning paradigm where the model first determines if the instruction is clear. For clear instructions, it provides structured reasoning and final answers. For ambiguous ones, it infers the likely intent, generates clarification prompts, and aligns responses accordingly. By incorporating experience-alignment training, we significantly improve the model&#8217;s intent understanding and enhance the overall user experience.\n(2) Multi-turn conversation: Users often engage in repeated questioning on the same context. To maintain semantic consistency across turns, we decompose complex instructions into multi-turn conversations and generate high-quality responses using expert-annotated LLM pipelines <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">laban2025llms</span>)</cite>. This ensures context retention and reduces performance degradation in multi-turn scenarios.\n(3) Complex multimodal instruction following: Users often provide sequential, interdependent commands. We design a multimodal instruction generation pipeline to generate SFT and DPO-style data to cover basic and complex instruction types <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2024iopo</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qian2024mia</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ding2025mm</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video Streaming Multi-turn Benchmark</span>.\nThe evaluation of video streaming multi-turn dialogue capabilities requires quantifying not only the model&#8217;s understand capability but also assessing its interactive experience, including proactivity and naturalness. Previous streaming dialogue datasets, such as StreamBench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2024streamingbench</span>)</cite> and OvO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">niu2025ovo</span>)</cite>, have primarily focused on the understanding aspect while lacking a thorough evaluation of the interactive experience.\nTo address this gap, we introduce StreamingMultiturnBench.\nTo construct StreamingMultiturnBench, we manually selected 380 videos, carefully ensuring coverage of multiple key domains including life recording, education, TV shows, video games, and documentaries. Then we use SOTA closed-source modelfor machine annotation. Subsequently, a team of 10 human annotators revise and double-check the dialogue content to ensure it aligns with human conversational preferences. This process yielded 2,200 video question-answer pairs.\nDuring evaluation, we use advanced closed-source model, <em class=\"ltx_emph ltx_font_italic\">e.g.</em> GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chatgpt4o</span>)</cite>, to compare the model&#8217;s output against the human-annotated answers, scoring it on a scale of 1 to 5 across the six dimensions: accuracy, completeness, relevance, naturalness, conciseness, and proactivity. The final score is the average for each dimension. To align our metrics with other video benchmarks, we linearly scale the results to a 100-point scale. We commit to open-sourcing and publicly maintaining this benchmark to ensure reproducibility.</p>\n\n",
                "matched_terms": [
                    "relevance",
                    "completeness",
                    "conciseness",
                    "accuracy",
                    "average",
                    "proactivity",
                    "naturalness",
                    "model",
                    "streamingmultiturnbench"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Vision <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding)</span> As shown in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:table_1_image2text</span>, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates strong and competitive performance across a wide range of vision&#8211;language benchmarks. Specifically, on general-purpose understanding task, it achieves performance on par with leading omni-models (most notably scoring 81.9% on MathVista), though it still exhibits a slight gap compared to state-of-the-art vision&#8211;language models. Similarly, on OCR-centric benchmarks, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> attains state-of-the-art results among omni-modal models, yet remains marginally behind the best proprietary counterparts. In multi-image understanding, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> outperforms the leading open-source vision&#8211;language model Qwen3-VL-30B-A3B on MMT-Bench (68.0) and LLaVA-Interleave Bench (63.3), while showing a minor performance gap on MuirBench, suggesting opportunities for further improvement. In video understanding, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves state-of-the-art performance on MVBench with a score of 74.6, demonstrating robust general video reasoning capabilities. It further exhibits specialized proficiency in processing linguistic content within videos, attaining a leading score of 73.0 on VideoMME with subtitles. Although its performance on long-form video understanding is slightly lower, its consistently strong results across most metrics underscore its advanced video comprehension capabilities.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "leading",
                    "compared",
                    "mingflashomni",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Generation)</span>. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T2\" title=\"Table 2 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, our experimental results demonstrate that the generation quality of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> is on par with state-of-the-art diffusion models.\nNotably, on the Geneval benchmark, our model surpasses all non-Reinforcement Learning methods, demonstrating exceptional controllability. This advantage is particularly pronounced in the \"Position\" and \"Color.\" sub-categories.\nOn the DPG-Bench benchmark, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves an overall score of 83.08, a performance level comparable to pure image generation models like SD3-Medium (84.08) and leading unified models like OmniGen2 (83.57).</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "model",
                    "performance",
                    "leading"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Editing)</span>. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T6\" title=\"Table 6 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>,\n<span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates impressive image editing performance, surpassing all other unified models.\nSpecifically, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> supports editing instructions in Chinese, achieving performance comparable to that with English instructions. Compared to Qwen-Image-Edit which utilizes a 20B DiT head, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves comparable semantic consistency and perceptual quality with a much more efficient 2B DiT head&#8212;only one-tenth the parameters. This efficiency also translates to remarkable inference speeds, typically between 1-2 seconds per generation.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "compared",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Image (Segmentation)</span>. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T6\" title=\"Table 6 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> is capable of performing segmentation tasks, achieving performance comparable to that of specialized models designed explicitly for this purpose. Compared to other unified MLLMs, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates a significant advantage in segmentation.\nFor instance, Qwen-Image-Edit often struggles to accurately localize the target object, while Nano-banana frequently misinterprets user intent during inference.\nIn contrast, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> exhibits superior robustness and a more accurate understanding of spatial and semantic instructions.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "compared",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text (Understanding)</span>. Our model sets a new state-of-the-art (SOTA) on all 12 sub-tasks of the ContextASR-Bench (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T6\" title=\"Table 6 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>), underscoring its superior ability to leverage context&#8212;a vital skill for real-world applications like multi-turn dialogue and hotword enhancement. It also exhibits highly competitive performance across various ASR benchmarks, with notable strengths in dialect recognition (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T8\" title=\"Table 8 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>). In audio question answering, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> surpasses all open-source audio-centric and other Omni models, with the exception of Qwen3-Omni-Flash-Instruct<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2025qwen3omnitechnicalreport</span>)</cite> (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T8\" title=\"Table 8 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>). Taken together, these findings demonstrate the robust and versatile audio understanding capabilities of <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p7.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Audio (Generation)</span>. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.T6\" title=\"Table 6 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, leveraging advancements in speech representation and model architecture, <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves SOTA performance among open-source models on the test-zh subset of the SEED-TTS-Eval benchmark<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite>. Furthermore, its WER on the test-en subset is surpassed only by that of Qwen3-Omni.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present several visualization examples to better illustrate the capabilities of  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>. First, as shown in the figure,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates strong visual understanding across multiple dimensions: it leverages rich world knowledge to accurately infer geographic locations from visual cues; excels at multi-image understanding and generates creative, coherent text grounded in multiple images; solves complex mathematical problems through clear, step-by-step reasoning; and exhibits robust document understanding by accurately parsing intricate formulas and answering questions about sophisticated charts and diagrams within images. Turning to speech recognition,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> achieves strong performance on contextual ASR tasks. By leveraging contextual information, it effectively resolves many challenging cases where conventional ASR systems tend to fail&#8212;such as ambiguous homophones, domain-specific terminology, or noisy conversational speech. Moreover, this version also supports multiple Chinese dialects, significantly broadening its applicability in real-world multilingual and regional speech scenarios. Lastly, we visualize the capabilities of <span class=\"ltx_text ltx_font_italic\">Text/Image\n&#8594;Image</span> generation tasks in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.F4\" title=\"Figure 4 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24821v2#S3.F5\" title=\"Figure 5 &#8227; 3.4 Text Data &#8227; 3 Data Construction &#8227; Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, covering a wide range of applications including image generation, image editing, image segmentation, multi-image editing, ID photo generation, ID photo editing, and background replacement.\nAs can be seen,  <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> not only supports a broader set of generative capabilities but also achieves higher output quality and greater controllability compared to previous versions.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "compared",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span>, built upon Ling-Flash-2.0 with 100 billion parameters, where only 6.1B parameters are activated per token. <span class=\"ltx_text ltx_font_typewriter\">Ming-Flash-Omni</span> demonstrates advanced multimodal perception and generation capabilities with improved computational efficiency while scaling model capacity. It achieves SOTA performance across a broad spectrum of tasks, including multi-image and video processing, image generation, generative segmentation, Contextual Automatic Speech Recognition (ContextASR), and multi-dialect recognition, outperforming omni models of comparable scale. We believe the open-sourcing of our models and code will facilitate the development of AGI by advancing multimodal intelligence research and enabling broader real-world applications.</p>\n\n",
                "matched_terms": [
                    "mingflashomni",
                    "model",
                    "performance"
                ]
            }
        ]
    }
}