{
    "S4.T1": {
        "caption": "Table 1: Objective evaluation results. Bold numbers indicate the best performance, while underlined numbers denote the second-best performance across all models. Note that low-frequency replacement post-processing was applied to the audio reconstructed by the VAE.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Music</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sound Effect</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" rowspan=\"2\">\n<span class=\"ltx_rule\" style=\"width:100%;height:1.2pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span><span class=\"ltx_text\" style=\"font-size:90%;\">\n</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">4kHz &#8194;&#8196;&#8202;8kHz</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">4kHz</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">8kHz</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">4kHz</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">8kHz</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">LSD &#8595;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">LSD &#8595;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">LSD &#8595;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">FD &#8595;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">LSD &#8595;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">FD &#8595;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">LSD &#8595;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">FD &#8595;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">LSD &#8595;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">FD &#8595;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<span class=\"ltx_rule\" style=\"width:100%;height:1.2pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span><span class=\"ltx_text\" style=\"font-size:90%;\">\nUnprocessed</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.49</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.68</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">138.09</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">106.46</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.30</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">110.25</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.39</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">64.08</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">VAE (recon)</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.81</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.13</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.92</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.06</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">17.30</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.13</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.47</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.08</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.53</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">AudioSR </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib1\" title=\"\">1</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.46</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.26</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.09</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">32.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.88</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.69</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.54</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FlashSR </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib4\" title=\"\">4</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.15</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.76</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">37.79</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.69</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">32.08</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.81</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.32</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">36.13</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">SAGA-SR</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">1.64</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">23.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">20.44</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">1.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">26.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.43</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">21.86</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">w/o text</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">1.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">1.11</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.63</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">30.14</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.34</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.60</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">29.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.43</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.94</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">w/o roll-off</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.57</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.43</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.16</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">35.99</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.78</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">23.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.19</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">33.07</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.75</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">22.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_rule\" style=\"width:100%;height:1.2pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "lsd",
            "models",
            "while",
            "lowfrequency",
            "evaluation",
            "unprocessed",
            "4khz",
            "applied",
            "flashsr",
            "sagasr",
            "audio",
            "effect",
            "objective",
            "sound",
            "postprocessing",
            "text",
            "performance",
            "across",
            "indicate",
            "bold",
            "denote",
            "note",
            "results",
            "numbers",
            "underlined",
            "vae",
            "speech",
            "rolloff",
            "audiosr",
            "secondbest",
            "recon",
            "reconstructed",
            "best",
            "all",
            "music",
            "8khz",
            "method",
            "replacement"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Objective Evaluation &#8227; 4 Results &#8227; SAGA-SR: Semantically and Acoustically Guided Audio Super-Resolution\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the results of the objective evaluation. SAGA-SR achieves state-of-the-art performance across all metrics and test cases, demonstrating the effectiveness of the proposed method. Compared to its variant without the spectral roll-off embedding, SAGA-SR consistently achieves better performance across all metrics and test cases. This indicates that the spectral roll-off embedding enhances the model&#8217;s ability to handle varying cutoff frequencies and guides it to generate outputs with the desired high-frequency energy. When compared with its variant without the text embedding, SAGA-SR achieves superior performance in the speech SR task. In the music and sound effect SR tasks, SAGA-SR and its variant achieve comparable performance in terms of LSD. On the other hand, SAGA-SR consistently outperforms its variant in FD. These results demonstrate that, while the spectral roll-off embedding improves spectral alignment with ground-truth audio, the text embedding is essential for generating outputs that are more plausible and perceptually aligned with the reference audio.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Versatile audio super-resolution (SR) aims to predict high-frequency components from low-resolution audio across diverse domains such as speech, music, and sound effects. Existing diffusion-based SR methods often fail to produce semantically aligned outputs and struggle with consistent high-frequency reconstruction. In this paper, we propose SAGA-SR, a versatile audio SR model that combines semantic and acoustic guidance. Based on a DiT backbone trained with a flow matching objective, SAGA-SR is conditioned on text and spectral roll-off embeddings. Due to the effective guidance provided by its conditioning, SAGA-SR robustly upsamples audio from arbitrary input sampling rates between 4 kHz and 32 kHz to 44.1 kHz. Both objective and subjective evaluations show that SAGA-SR achieves state-of-the-art performance across all test cases. Sound examples and code for the proposed model are available online<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://jakeoneijk.github.io/saga-sr-project\" title=\"\">http://jakeoneijk.github.io/saga-sr-project</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "sagasr",
                    "across",
                    "audio",
                    "objective",
                    "all",
                    "speech",
                    "music",
                    "sound",
                    "rolloff",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio super-resolution (SR) aims to reconstruct a high-resolution audio signal from its corresponding low-resolution audio signal. To enhance listening experiences, it can be applied to diverse audio types, including historical recordings, low-bandwidth telephone audio, and audio generated by deep learning models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib1\" title=\"\">1</a>]</cite>. Previous audio SR methods <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib3\" title=\"\">3</a>]</cite> based on deep neural networks have achieved promising performance in constrained settings, including fixed upsampling ratios and restricted domains such as speech or music. However, these constraints limit their applicability in diverse and complex real-world scenarios.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "music",
                    "speech",
                    "applied",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this issue, several recent works <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib4\" title=\"\">4</a>]</cite> have focused on versatile audio super-resolution, which is the task of upsampling general-domain audio, including music, speech, and sound effects, from varying input sampling rates to full bandwidth, such as 44.1 kHz or 48 kHz. AudioSR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib1\" title=\"\">1</a>]</cite> employs a latent diffusion model (LDM) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib5\" title=\"\">5</a>]</cite> with a Transformer-UNet backbone <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib6\" title=\"\">6</a>]</cite> to capture the complex distributions of general audio signals. FlashSR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib4\" title=\"\">4</a>]</cite> employs diffusion distillation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib7\" title=\"\">7</a>]</cite> to train the Student LDM using AudioSR as the Teacher LDM and proposes the SR Vocoder to further enhance AudioSR&#8217;s performance.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "music",
                    "speech",
                    "sound",
                    "audiosr",
                    "flashsr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although previous methods have achieved notable success, there is still room for improvement. There are two key challenges hindering the performance of versatile audio SR models. First, to generate natural high-resolution audio, an audio SR model needs to capture semantic information from low-resolution input and effectively incorporate it into the reconstruction process. Previous methods often fail to predict semantically-aligned high-frequency components, resulting in unnatural artifacts, such as excessive sibilance <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib4\" title=\"\">4</a>]</cite>. Second, unlike speech SR, versatile audio SR handles a much broader range of audio domains, which exhibit high diversity in high-frequency energy distributions. This results in difficulties for models in consistently reconstructing high-frequency content, especially when the input has a low cutoff-frequency (e.g., 4 kHz).</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "results",
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present SAGA-SR, a versatile audio super-resolution model that leverages semantic and acoustic conditions. SAGA-SR is based on a DiT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib8\" title=\"\">8</a>]</cite> backbone trained with a flow matching objective <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib9\" title=\"\">9</a>]</cite>, incorporating two key conditions. First, inspired by recent works <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib11\" title=\"\">11</a>]</cite> in the computer vision domain, we utilize text embeddings for semantic guidance. Specifically, we employ an audio-language model to generate text captions from audio, enabling more efficient training and inference. Second, we introduce spectral roll-off embeddings, which provide relative high-frequency energy information for both the input and target audio. Guided by both semantic and acoustic conditions, SAGA-SR can robustly upsample music, speech, and sound effects from any sampling rate between 4 kHz and 32 kHz to 44.1 kHz, and achieves state-of-the-art performance on both objective and subjective evaluations.</p>\n\n",
                "matched_terms": [
                    "sagasr",
                    "audio",
                    "objective",
                    "music",
                    "speech",
                    "sound",
                    "rolloff",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#S2.F1\" title=\"Figure 1 &#8227; 2 Method &#8227; SAGA-SR: Semantically and Acoustically Guided Audio Super-Resolution\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the overall architecture of SAGA-SR. Let <math alttext=\"x_{h}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>h</mi></msub><annotation encoding=\"application/x-tex\">x_{h}</annotation></semantics></math>, <math alttext=\"x_{l}\\in\\mathbb{R}^{2\\times L}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>x</mi><mi>l</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">x_{l}\\in\\mathbb{R}^{2\\times L}</annotation></semantics></math> represent the high-resolution and low-resolution audio, respectively, where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m3\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the number of samples. Each audio sample is compressed into latent representations <math alttext=\"z_{h}\\in\\mathbb{R}^{64\\times L/2048}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>z</mi><mi>h</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mrow><mn>64</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi></mrow><mo>/</mo><mn>2048</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">z_{h}\\in\\mathbb{R}^{64\\times L/2048}</annotation></semantics></math> and <math alttext=\"z_{l}\\in\\mathbb{R}^{64\\times L/2048}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>z</mi><mi>l</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mrow><mn>64</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi></mrow><mo>/</mo><mn>2048</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">z_{l}\\in\\mathbb{R}^{64\\times L/2048}</annotation></semantics></math> by the pre-trained VAE encoder from <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib12\" title=\"\">12</a>]</cite>. The text <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m6\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> describes the audio content independent of its resolution. Roll-off frequencies <math alttext=\"f_{h}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m7\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>h</mi></msub><annotation encoding=\"application/x-tex\">f_{h}</annotation></semantics></math>, <math alttext=\"f_{l}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m8\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>l</mi></msub><mo>&#8712;</mo><mi>&#8477;</mi></mrow><annotation encoding=\"application/x-tex\">f_{l}\\in\\mathbb{R}</annotation></semantics></math> are extracted from <math alttext=\"x_{h}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m9\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>h</mi></msub><annotation encoding=\"application/x-tex\">x_{h}</annotation></semantics></math> and <math alttext=\"x_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m10\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">x_{l}</annotation></semantics></math>, respectively. DiT estimates <math alttext=\"z_{h}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m11\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>h</mi></msub><annotation encoding=\"application/x-tex\">z_{h}</annotation></semantics></math> from <math alttext=\"z_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m12\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">z_{l}</annotation></semantics></math>, <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m13\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math>, <math alttext=\"f_{h}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m14\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>h</mi></msub><annotation encoding=\"application/x-tex\">f_{h}</annotation></semantics></math>, and <math alttext=\"f_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m15\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">f_{l}</annotation></semantics></math>. The predicted latent is then converted into an audio signal by the pre-trained VAE decoder, followed by low-frequency replacement post-processing <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib1\" title=\"\">1</a>]</cite> to ensure consistency in low-frequency information. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#S2.SS1\" title=\"2.1 DiT Model &#8227; 2 Method &#8227; SAGA-SR: Semantically and Acoustically Guided Audio Super-Resolution\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a>, we present the training and inference procedures of DiT. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#S2.SS2\" title=\"2.2 Conditioning &#8227; 2 Method &#8227; SAGA-SR: Semantically and Acoustically Guided Audio Super-Resolution\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, we describe how each condition is processed to be incorporated into DiT.</p>\n\n",
                "matched_terms": [
                    "rolloff",
                    "sagasr",
                    "audio",
                    "lowfrequency",
                    "vae",
                    "postprocessing",
                    "replacement",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike previous works <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib4\" title=\"\">4</a>]</cite> that employ a Transformer-UNet architecture, we use DiT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib8\" title=\"\">8</a>]</cite>, which is widely adopted in image and audio generation models. We adapt the DiT architecture proposed in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib12\" title=\"\">12</a>]</cite> and train it using the conditional flow matching objective <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib9\" title=\"\">9</a>]</cite>. The flow matching objective regresses onto a target vector field that generates a probability path, transforming a simple distribution into an approximation of the data distribution. We use a linear interpolation path between the noise and the data as follows:</p>\n\n",
                "matched_terms": [
                    "objective",
                    "models",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text embedding.</span>\nAudio super-resolution models are typically trained with high-resolution audio-only data, since low-resolution audio can be simulated from it. Compared to audio-only data, audio-text data is expensive to curate at scale. Furthermore, relying on user-provided text during inference can limit practical applicability and potentially degrade generation quality. To address these issues, we employ Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib16\" title=\"\">16</a>]</cite> to generate text captions from audio. During training, captions generated from high-resolution audio are used for efficiency, while at inference, they are derived from low-resolution audio. Text embeddings are extracted from the generated captions using a pretrained T5-base encoder <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib17\" title=\"\">17</a>]</cite> and provided to the DiT through cross-attention. A dropout rate of 10% is applied to the text embeddings.</p>\n\n",
                "matched_terms": [
                    "models",
                    "while",
                    "audio",
                    "text",
                    "applied"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Spectral roll-off embedding.</span>\nWe compute the roll-off frequency from the STFT spectrogram using an open-source method<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://librosa.org/doc/0.11.0/generated/librosa.feature.spectral_rolloff.html\" title=\"\">https://librosa.org/doc/0.11.0/generated/librosa.feature.spectral_rolloff.html</a></span></span></span>. Instead of computing it frame-wise as in the original implementation, we sum over the time axis of the magnitude spectrogram to obtain a single roll-off frequency value for each audio sample, which is then normalized to <math alttext=\"[0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">[0,1)</annotation></semantics></math> using the min&#8211;max normalization. The normalized roll-off frequency value is projected into learnable Fourier embeddings. We extract the spectral roll-off embeddings from both low-resolution and high-resolution audio.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "rolloff"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The spectral roll-off embeddings are conditioned into the DiT through two mechanisms. First, they are concatenated with the text embeddings along the sequence dimension before cross-attention. Second, the input and target roll-off embeddings are concatenated along the channel dimension, projected by linear layers, summed with the timestep sinusoidal embeddings <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib18\" title=\"\">18</a>]</cite>, and then prepended to the input of DiT. The input roll-off embeddings provide the DiT with information about the cutoff frequency of the input audio, improving its ability to handle varying input sampling rates, while the target roll-off embeddings guide the amount of high-frequency energy to generate. During inference, the target normalized roll-off frequency serves as conditioning, enabling the user to control the high-frequency energy in the generated audio. Because it is represented as a single scalar in <math alttext=\"[0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">[0,1)</annotation></semantics></math>, it is straightforward to manipulate.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text",
                    "while",
                    "rolloff"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training dataset configuration and data simulation method are consistent with previous works <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib4\" title=\"\">4</a>]</cite>. We train on the FreeSound <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib19\" title=\"\">19</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://labs.freesound.org/\" title=\"\">https://labs.freesound.org/</a></span></span></span>, MedleyDB <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib20\" title=\"\">20</a>]</cite>, MUSDB18-HQ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib21\" title=\"\">21</a>]</cite>, MoisesDB <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib22\" title=\"\">22</a>]</cite>, and OpenSLR<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openslr.org/\" title=\"\">https://openslr.org/</a></span></span></span> speech dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib23\" title=\"\">23</a>]</cite>, with a total audio duration of around 3,800 hours. All audio was resampled to 44.1 kHz and randomly segmented into 5.94-second clips for training. To simulate low-high resolution audio pairs, we apply low-pass filtering to the high-resolution audio. The cutoff frequency is uniformly sampled between 2 kHz and 16 kHz. The low-pass filter type is randomly selected from Chebyshev, Butterworth, Bessel, and Elliptic, with the filter order chosen between 2 and 10.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio",
                    "all",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The DiT was trained for 26,000 steps using the AdamW optimizer with <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math> and <math alttext=\"\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.999</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.999</annotation></semantics></math>. We use a batch size of 256 and a learning rate of <math alttext=\"1.0\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1.0</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1.0\\times 10^{-5}</annotation></semantics></math>. An InverseLR scheduler <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib12\" title=\"\">12</a>]</cite> is applied with an inverse gamma of <math alttext=\"10^{6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><msup><mn>10</mn><mn>6</mn></msup><annotation encoding=\"application/x-tex\">10^{6}</annotation></semantics></math>, a power of 0.5, and a warmup factor of 0.99. To compute the roll-off frequency, we extract the STFT spectrogram using a Hann window of 2048 and a hop size of 512. The roll-off percentage is set to 0.985.</p>\n\n",
                "matched_terms": [
                    "rolloff",
                    "applied"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Comparison.</span>\nWe compare SAGA-SR against state-of-the-art models, AudioSR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib1\" title=\"\">1</a>]</cite> and FlashSR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib4\" title=\"\">4</a>]</cite>. The official implementations and checkpoints are used for all comparison models. In addition, we conducted an ablation study to evaluate the effectiveness of the text embedding and the spectral roll-off embedding. We train two SAGA-SR variants, one without the text embedding and another without the spectral roll-off embedding. Both models are trained under the same settings as SAGA-SR.</p>\n\n",
                "matched_terms": [
                    "models",
                    "sagasr",
                    "all",
                    "rolloff",
                    "text",
                    "audiosr",
                    "flashsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dataset.</span>\nFor both objective and subjective evaluations, we adopt the VCTK test set (speech) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib24\" title=\"\">24</a>]</cite>, FMA-small (music) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib25\" title=\"\">25</a>]</cite>, and ESC50 fold-5 (sound effects) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib26\" title=\"\">26</a>]</cite>. We selected 400 samples from each dataset.</p>\n\n",
                "matched_terms": [
                    "sound",
                    "objective",
                    "music",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span>\nFor objective evaluation, each sound category is evaluated at cutoff frequencies of 4 kHz and 8 kHz. Log-Spectral Distance (LSD) is used as the evaluation metric, following previous studies <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib24\" title=\"\">24</a>]</cite>. Although the LSD metric is widely used in audio super-resolution tasks, it has been found that it does not always align with perceptual quality <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib4\" title=\"\">4</a>]</cite>. To complement LSD, we also adopt the Fr&#233;chet Distance (FD) based on OpenL3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib27\" title=\"\">27</a>]</cite> for evaluating the music and sound effects categories. FD compares the statistics of embeddings from generated audio with those from ground-truth audio.</p>\n\n",
                "matched_terms": [
                    "lsd",
                    "audio",
                    "objective",
                    "evaluation",
                    "music",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For subjective evaluation, a listening test was conducted with 25 participants. For all sound categories, the cutoff frequency was set to 4 kHz. During the test, participants were provided with the low-resolution input audio as a low anchor and asked to rate the perceptual quality of the outputs from each model on a scale from 1 to 5. We evaluated AudioSR, FlashSR, and the proposed SAGA-SR.</p>\n\n",
                "matched_terms": [
                    "sagasr",
                    "audio",
                    "all",
                    "evaluation",
                    "sound",
                    "audiosr",
                    "flashsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#S5.T2\" title=\"Table 2 &#8227; 5 Conclusions &#8227; SAGA-SR: Semantically and Acoustically Guided Audio Super-Resolution\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the results of the subjective evaluation. SAGA-SR achieves the highest scores across all test cases. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#S4.F2\" title=\"Figure 2 &#8227; 4.2 Subjective Evaluation &#8227; 4 Results &#8227; SAGA-SR: Semantically and Acoustically Guided Audio Super-Resolution\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows a comparison of the STFT spectrograms for audio generated by different models. We found that AudioSR and FlashSR exhibit high variance in their outputs and often produce audio lacking sufficient high-frequency content. In contrast, SAGA-SR demonstrates better consistency in reconstructing high-frequency components, due to the explicit guidance provided by the spectral roll-off embedding. Moreover, we observed that AudioSR and FlashSR often generate audio that is not semantically aligned with the low-resolution input audio. Specifically, AudioSR tends to produce outputs with excessive sibilance, while FlashSR often fails to generate detailed harmonic structures, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#S4.F2\" title=\"Figure 2 &#8227; 4.2 Subjective Evaluation &#8227; 4 Results &#8227; SAGA-SR: Semantically and Acoustically Guided Audio Super-Resolution\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. By incorporating text embeddings, SAGA-SR is able to generate semantically aligned audio with realistic harmonic detail.</p>\n\n",
                "matched_terms": [
                    "models",
                    "sagasr",
                    "across",
                    "while",
                    "audio",
                    "all",
                    "evaluation",
                    "results",
                    "rolloff",
                    "text",
                    "audiosr",
                    "flashsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#S4.F3\" title=\"Figure 3 &#8227; 4.2 Subjective Evaluation &#8227; 4 Results &#8227; SAGA-SR: Semantically and Acoustically Guided Audio Super-Resolution\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, SAGA-SR allows the user to control the high-frequency energy of the generated audio by adjusting the target normalized roll-off frequency, a single scalar condition. This control not only reduces the variance in perceptual quality but also influences acoustic properties such as timbre.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "sagasr",
                    "rolloff"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose SAGA-SR, a versatile audio super-resolution model that integrates semantic and acoustic conditioning into a DiT backbone trained with a flow matching objective. Text embeddings improve semantic alignment, while spectral roll-off embeddings enhance robustness and controllability in high-frequency reconstruction. Both objective and subjective evaluations show that SAGA-SR outperforms previous methods across all tasks and metrics.</p>\n\n",
                "matched_terms": [
                    "sagasr",
                    "across",
                    "while",
                    "audio",
                    "objective",
                    "all",
                    "rolloff",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite its effectiveness, SAGA-SR has limitations that warrant future work. First, upsampling audio with multiple overlapping sources remains challenging. Future work could scale data and model capacity or develop improved captioning methods to accurately describe all sound sources. Second, low-frequency replacement post-processing may introduce unnatural connections between high and low frequency components. A potential direction is to develop a VAE conditioned on the low-resolution waveform to achieve smoother integration.</p>\n\n",
                "matched_terms": [
                    "sagasr",
                    "audio",
                    "lowfrequency",
                    "all",
                    "sound",
                    "vae",
                    "postprocessing",
                    "replacement"
                ]
            }
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Subjective evaluation results.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<span class=\"ltx_rule\" style=\"width:100%;height:1.2pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span><span class=\"ltx_text\" style=\"font-size:90%;\">\nMethod</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Speech</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Music</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">Sound Effect</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<span class=\"ltx_rule\" style=\"width:100%;height:1.2pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span><span class=\"ltx_text\" style=\"font-size:90%;\">\nUnprocessed</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.81</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.66</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.77</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Ground Truth</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.23</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.18</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">AudioSR </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib1\" title=\"\">1</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.26</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.03</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FlashSR </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib4\" title=\"\">4</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.46</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.34</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">SAGA-SR</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.65</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.88</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_rule\" style=\"width:100%;height:1.2pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "sagasr",
            "unprocessed",
            "effect",
            "truth",
            "evaluation",
            "music",
            "method",
            "results",
            "speech",
            "sound",
            "subjective",
            "audiosr",
            "flashsr",
            "ground"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#S5.T2\" title=\"Table 2 &#8227; 5 Conclusions &#8227; SAGA-SR: Semantically and Acoustically Guided Audio Super-Resolution\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the results of the subjective evaluation. SAGA-SR achieves the highest scores across all test cases. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#S4.F2\" title=\"Figure 2 &#8227; 4.2 Subjective Evaluation &#8227; 4 Results &#8227; SAGA-SR: Semantically and Acoustically Guided Audio Super-Resolution\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows a comparison of the STFT spectrograms for audio generated by different models. We found that AudioSR and FlashSR exhibit high variance in their outputs and often produce audio lacking sufficient high-frequency content. In contrast, SAGA-SR demonstrates better consistency in reconstructing high-frequency components, due to the explicit guidance provided by the spectral roll-off embedding. Moreover, we observed that AudioSR and FlashSR often generate audio that is not semantically aligned with the low-resolution input audio. Specifically, AudioSR tends to produce outputs with excessive sibilance, while FlashSR often fails to generate detailed harmonic structures, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#S4.F2\" title=\"Figure 2 &#8227; 4.2 Subjective Evaluation &#8227; 4 Results &#8227; SAGA-SR: Semantically and Acoustically Guided Audio Super-Resolution\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. By incorporating text embeddings, SAGA-SR is able to generate semantically aligned audio with realistic harmonic detail.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Versatile audio super-resolution (SR) aims to predict high-frequency components from low-resolution audio across diverse domains such as speech, music, and sound effects. Existing diffusion-based SR methods often fail to produce semantically aligned outputs and struggle with consistent high-frequency reconstruction. In this paper, we propose SAGA-SR, a versatile audio SR model that combines semantic and acoustic guidance. Based on a DiT backbone trained with a flow matching objective, SAGA-SR is conditioned on text and spectral roll-off embeddings. Due to the effective guidance provided by its conditioning, SAGA-SR robustly upsamples audio from arbitrary input sampling rates between 4 kHz and 32 kHz to 44.1 kHz. Both objective and subjective evaluations show that SAGA-SR achieves state-of-the-art performance across all test cases. Sound examples and code for the proposed model are available online<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://jakeoneijk.github.io/saga-sr-project\" title=\"\">http://jakeoneijk.github.io/saga-sr-project</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "sagasr",
                    "music",
                    "sound",
                    "speech",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio super-resolution (SR) aims to reconstruct a high-resolution audio signal from its corresponding low-resolution audio signal. To enhance listening experiences, it can be applied to diverse audio types, including historical recordings, low-bandwidth telephone audio, and audio generated by deep learning models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib1\" title=\"\">1</a>]</cite>. Previous audio SR methods <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib3\" title=\"\">3</a>]</cite> based on deep neural networks have achieved promising performance in constrained settings, including fixed upsampling ratios and restricted domains such as speech or music. However, these constraints limit their applicability in diverse and complex real-world scenarios.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this issue, several recent works <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib4\" title=\"\">4</a>]</cite> have focused on versatile audio super-resolution, which is the task of upsampling general-domain audio, including music, speech, and sound effects, from varying input sampling rates to full bandwidth, such as 44.1 kHz or 48 kHz. AudioSR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib1\" title=\"\">1</a>]</cite> employs a latent diffusion model (LDM) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib5\" title=\"\">5</a>]</cite> with a Transformer-UNet backbone <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib6\" title=\"\">6</a>]</cite> to capture the complex distributions of general audio signals. FlashSR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib4\" title=\"\">4</a>]</cite> employs diffusion distillation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib7\" title=\"\">7</a>]</cite> to train the Student LDM using AudioSR as the Teacher LDM and proposes the SR Vocoder to further enhance AudioSR&#8217;s performance.</p>\n\n",
                "matched_terms": [
                    "music",
                    "sound",
                    "speech",
                    "audiosr",
                    "flashsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although previous methods have achieved notable success, there is still room for improvement. There are two key challenges hindering the performance of versatile audio SR models. First, to generate natural high-resolution audio, an audio SR model needs to capture semantic information from low-resolution input and effectively incorporate it into the reconstruction process. Previous methods often fail to predict semantically-aligned high-frequency components, resulting in unnatural artifacts, such as excessive sibilance <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib4\" title=\"\">4</a>]</cite>. Second, unlike speech SR, versatile audio SR handles a much broader range of audio domains, which exhibit high diversity in high-frequency energy distributions. This results in difficulties for models in consistently reconstructing high-frequency content, especially when the input has a low cutoff-frequency (e.g., 4 kHz).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present SAGA-SR, a versatile audio super-resolution model that leverages semantic and acoustic conditions. SAGA-SR is based on a DiT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib8\" title=\"\">8</a>]</cite> backbone trained with a flow matching objective <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib9\" title=\"\">9</a>]</cite>, incorporating two key conditions. First, inspired by recent works <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib11\" title=\"\">11</a>]</cite> in the computer vision domain, we utilize text embeddings for semantic guidance. Specifically, we employ an audio-language model to generate text captions from audio, enabling more efficient training and inference. Second, we introduce spectral roll-off embeddings, which provide relative high-frequency energy information for both the input and target audio. Guided by both semantic and acoustic conditions, SAGA-SR can robustly upsample music, speech, and sound effects from any sampling rate between 4 kHz and 32 kHz to 44.1 kHz, and achieves state-of-the-art performance on both objective and subjective evaluations.</p>\n\n",
                "matched_terms": [
                    "sagasr",
                    "music",
                    "sound",
                    "speech",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training dataset configuration and data simulation method are consistent with previous works <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib4\" title=\"\">4</a>]</cite>. We train on the FreeSound <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib19\" title=\"\">19</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://labs.freesound.org/\" title=\"\">https://labs.freesound.org/</a></span></span></span>, MedleyDB <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib20\" title=\"\">20</a>]</cite>, MUSDB18-HQ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib21\" title=\"\">21</a>]</cite>, MoisesDB <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib22\" title=\"\">22</a>]</cite>, and OpenSLR<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openslr.org/\" title=\"\">https://openslr.org/</a></span></span></span> speech dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib23\" title=\"\">23</a>]</cite>, with a total audio duration of around 3,800 hours. All audio was resampled to 44.1 kHz and randomly segmented into 5.94-second clips for training. To simulate low-high resolution audio pairs, we apply low-pass filtering to the high-resolution audio. The cutoff frequency is uniformly sampled between 2 kHz and 16 kHz. The low-pass filter type is randomly selected from Chebyshev, Butterworth, Bessel, and Elliptic, with the filter order chosen between 2 and 10.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Comparison.</span>\nWe compare SAGA-SR against state-of-the-art models, AudioSR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib1\" title=\"\">1</a>]</cite> and FlashSR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib4\" title=\"\">4</a>]</cite>. The official implementations and checkpoints are used for all comparison models. In addition, we conducted an ablation study to evaluate the effectiveness of the text embedding and the spectral roll-off embedding. We train two SAGA-SR variants, one without the text embedding and another without the spectral roll-off embedding. Both models are trained under the same settings as SAGA-SR.</p>\n\n",
                "matched_terms": [
                    "sagasr",
                    "audiosr",
                    "flashsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dataset.</span>\nFor both objective and subjective evaluations, we adopt the VCTK test set (speech) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib24\" title=\"\">24</a>]</cite>, FMA-small (music) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib25\" title=\"\">25</a>]</cite>, and ESC50 fold-5 (sound effects) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib26\" title=\"\">26</a>]</cite>. We selected 400 samples from each dataset.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "music",
                    "subjective",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span>\nFor objective evaluation, each sound category is evaluated at cutoff frequencies of 4 kHz and 8 kHz. Log-Spectral Distance (LSD) is used as the evaluation metric, following previous studies <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib24\" title=\"\">24</a>]</cite>. Although the LSD metric is widely used in audio super-resolution tasks, it has been found that it does not always align with perceptual quality <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib4\" title=\"\">4</a>]</cite>. To complement LSD, we also adopt the Fr&#233;chet Distance (FD) based on OpenL3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#bib.bib27\" title=\"\">27</a>]</cite> for evaluating the music and sound effects categories. FD compares the statistics of embeddings from generated audio with those from ground-truth audio.</p>\n\n",
                "matched_terms": [
                    "music",
                    "evaluation",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For subjective evaluation, a listening test was conducted with 25 participants. For all sound categories, the cutoff frequency was set to 4 kHz. During the test, participants were provided with the low-resolution input audio as a low anchor and asked to rate the perceptual quality of the outputs from each model on a scale from 1 to 5. We evaluated AudioSR, FlashSR, and the proposed SAGA-SR.</p>\n\n",
                "matched_terms": [
                    "sagasr",
                    "evaluation",
                    "sound",
                    "subjective",
                    "audiosr",
                    "flashsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24924v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Objective Evaluation &#8227; 4 Results &#8227; SAGA-SR: Semantically and Acoustically Guided Audio Super-Resolution\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the results of the objective evaluation. SAGA-SR achieves state-of-the-art performance across all metrics and test cases, demonstrating the effectiveness of the proposed method. Compared to its variant without the spectral roll-off embedding, SAGA-SR consistently achieves better performance across all metrics and test cases. This indicates that the spectral roll-off embedding enhances the model&#8217;s ability to handle varying cutoff frequencies and guides it to generate outputs with the desired high-frequency energy. When compared with its variant without the text embedding, SAGA-SR achieves superior performance in the speech SR task. In the music and sound effect SR tasks, SAGA-SR and its variant achieve comparable performance in terms of LSD. On the other hand, SAGA-SR consistently outperforms its variant in FD. These results demonstrate that, while the spectral roll-off embedding improves spectral alignment with ground-truth audio, the text embedding is essential for generating outputs that are more plausible and perceptually aligned with the reference audio.</p>\n\n",
                "matched_terms": [
                    "sagasr",
                    "effect",
                    "music",
                    "evaluation",
                    "method",
                    "results",
                    "speech",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose SAGA-SR, a versatile audio super-resolution model that integrates semantic and acoustic conditioning into a DiT backbone trained with a flow matching objective. Text embeddings improve semantic alignment, while spectral roll-off embeddings enhance robustness and controllability in high-frequency reconstruction. Both objective and subjective evaluations show that SAGA-SR outperforms previous methods across all tasks and metrics.</p>\n\n",
                "matched_terms": [
                    "sagasr",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite its effectiveness, SAGA-SR has limitations that warrant future work. First, upsampling audio with multiple overlapping sources remains challenging. Future work could scale data and model capacity or develop improved captioning methods to accurately describe all sound sources. Second, low-frequency replacement post-processing may introduce unnatural connections between high and low frequency components. A potential direction is to develop a VAE conditioned on the low-resolution waveform to achieve smoother integration.</p>\n\n",
                "matched_terms": [
                    "sound",
                    "sagasr"
                ]
            }
        ]
    }
}