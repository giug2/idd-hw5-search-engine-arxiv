{
    "S3.T1": {
        "source_file": "InstructAudio: Unified speech and music generation with natural language instruction",
        "caption": "Table 1: Comparison of TTS models on instruction control and word error rate performance.",
        "body": "50K Speech\n\n\n+ 20K Music",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">50K Speech</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">+ 20K Music</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "word",
            "control",
            "performance",
            "error",
            "models",
            "rate",
            "speech",
            "tts",
            "comparison",
            "50k",
            "20k",
            "instruction",
            "music"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate our model&#8217;s unified speech and music generation capabilities, we compare against SOTA models in each task. For TTS, we benchmark fundamental capabilities against MaskGCT</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, E2-TTS</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">eskimez2024e2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, F5-TTS</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, ZipVoice</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhu2025zipvoice</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CosyVoice1</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and CosyVoice2</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.2 Compared Method and Evaluation Metrics &#8227; 3 Experiments &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), and instruction-based TTS performance against the current SOTA model CosyVoice2 (Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.2 Compared Method and Evaluation Metrics &#8227; 3 Experiments &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nNotably, since InstructAudio is a purely instruction-controlled model while the Seed-TTS benchmark comprises natural emotional speech samples, we evaluate WER metrics using natural, calm-style text descriptions with randomized speakers as control conditions. For CosyVoice2, which lacks text-based control for timbre attributes (gender, age), we provide reference audio that matches timbre description during inference and map inputs to its supported short-form control text (e.g., \"Please speak very happy\").\nFor music generation, we compare with DiffRhythm+</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2025diffrhythm+</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and ACE-Step</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2025ace</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.2 Compared Method and Evaluation Metrics &#8227; 3 Experiments &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). Note that DiffRhythm+ does not support music synthesis under 90 seconds; therefore, we generate longer sequences and truncate them to create test samples, which may introduce evaluation bias.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Evaluation of TTS:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.2 Compared Method and Evaluation Metrics &#8227; 3 Experiments &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares InstructAudio with mainstream TTS models. MaskGCT, E2-TTS, F5-TTS, and ZipVoice do not support text-based control capabilities. CosyVoice1 and CosyVoice2 only support emotion, style, and accent control, requiring additional prompt speech for timbre control. In contrast, InstructAudio supports comprehensive text-based control including gender, age, emotion, style, and accent, while uniquely enabling text-controlled dialogue synthesis (a capability absent in other models). Although InstructAudio has the largest parameter count (1.3B) among TTS models, it additionally supports music generation. As shown in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.2 Compared Method and Evaluation Metrics &#8227; 3 Experiments &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, mainstream music generation models like DiffRhythm+ and ACE-Step also exceed 1B parameters. Notably, InstructAudio achieves superior performance with the smallest training dataset. On the Seed-TTS WER metric, InstructAudio achieves the best results when conditioned on neutral emotion and calm style text control.\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Control Capability:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.2 Compared Method and Evaluation Metrics &#8227; 3 Experiments &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> evaluates text-based control TTS capabilities by comparing against CosyVoice2, the current SOTA model. For classification control accuracy, InstructAudio supports gender, age, and dialogue control capabilities unavailable in CosyVoice2 (while outperforming CosyVoice2 across all control categories). InstructAudio achieves precise dialogue synthesis control, attaining 90% accuracy in a capability that other models lack entirely. InstructAudio also demonstrates superior speaker and emotion similarity. This advantage stems from CosyVoice2&#8217;s reliance on additional prompt speech (random speaker audio with matching gender and age) for timbre control, which causes emotion leakage that compromises emotion control effectiveness.\nInstructAudio outperforms CosyVoice2 across all distortion and error metrics. While CosyVoice2 achieves a higher MOS score, it notably requires reference audio as input conditioning. Text-only control introduces TTS one-to-many mapping ambiguity, reducing average audio quality and naturalness for InstructAudio outputs.\nHowever, InstructAudio achieves comparable MOS results despite the unfair disadvantage of missing input modality, while comprehensively outperforming CosyVoice2 on all other metrics.\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Evaluation of TTM:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nTable </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.2 Compared Method and Evaluation Metrics &#8227; 3 Experiments &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> evaluates music generation capabilities against current SOTA models ACE-Step and DiffRhythm+. For classification control accuracy, ACE-Step achieves the best performance in genre and instrument categories, while InstructAudio excels in gender, age, rhythm, and atmosphere control. DiffRhythm+ scores poorly on gender and age due to its lack of singer timbre control capabilities. InstructAudio achieves the best SongEval scores across all metrics. ACE-Step obtains the highest QMOS score, indicating superior perceived audio quality, while InstructAudio achieves the best MMOS score. We acknowledge that our music evaluation uses 5-20 second clips matching speech durations, which may disadvantage models like ACE-Step and DiffRhythm+ that are optimized for longer music generation. This comparison demonstrates that InstructAudio maintains competitive music generation capabilities while achieving SOTA text-based control TTS performance.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Text-to-speech (TTS) and text-to-music (TTM) models face significant limitations in instruction-based control. TTS systems usually depend on reference audio for timbre, offer only limited text-level attribute control, and rarely support dialogue generation.\nTTM systems are constrained by input conditioning requirements that depend on expert knowledge annotations. The high heterogeneity of these input control conditions makes them difficult to joint modeling with speech synthesis.\nDespite sharing common acoustic modeling characteristics, these two tasks have long been developed independently, leaving open the challenge of achieving unified modeling through natural language instructions.\nWe introduce InstructAudio, a unified framework that enables instruction-based (natural language descriptions) control of acoustic attributes including timbre (gender, age), paralinguistic (emotion, style, accent), and musical (genre, instrument, rhythm, atmosphere). It supports expressive speech, music, and dialogue generation in English and Chinese.\nThe model employs joint and single diffusion transformer layers with a standardized instruction-phoneme input format, trained on 50K hours of speech and 20K hours of music data, enabling multi-task learning and cross-modal alignment.\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S0.F1\" title=\"Figure 1 &#8227; InstructAudio: Unified speech and music generation with natural language instruction\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> visualizes performance comparisons with mainstream TTS and TTM models, demonstrating that InstructAudio achieves optimal results on most metrics.\nTo our best knowledge, InstructAudio represents the first instruction-controlled framework unifying speech and music generation. Audio samples are available at: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://qiangchunyu.github.io/InstructAudio/\" title=\"\">https://qiangchunyu.github.io/InstructAudio/</a></span>\n</p>\n\n",
                "matched_terms": [
                    "control",
                    "performance",
                    "models",
                    "speech",
                    "tts",
                    "50k",
                    "20k",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nUnified Audio Generation, Natural Language Instruction, TTS, TTM</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Text-based controllable generation of speech and music is an important research topic in the field of audio generation. Recent developments in TTS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speak</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kharitonov2023speak</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qiang2024minimally</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qiang2024high</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi24f_interspeech</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhu2025zipvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">eskimez2024e2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">10889461</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have achieved impressive results through zero-shot voice cloning and controllable generation. TTM </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">copet2023simple</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2025ace</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">suno2024</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">udio2024</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2025diffrhythm+</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> has advanced with models such as MusicGen </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">copet2023simple</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and ACE-Step </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2025ace</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, alongside commercial systems like Suno</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">suno2024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Udio</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">udio2024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Despite these advances, instruction-controlled speech and music generation remains a challenging problem in audio processing.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech",
                    "tts",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Existing TTS models excel at either zero-shot voice cloning </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> or style control</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ji2024controlspeech</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2023prompttts</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, but lack text-based control over multiple acoustic attributes through natural language descriptions. For instance, while CosyVoice </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and ControlSpeech </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ji2024controlspeech</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> support text-based emotion and style control, they require additional reference audio for timbre attributes and cannot handle text-controlled dialogue generation. Similarly, current TTM models exhibit limited control capabilities. DiffRhythm+</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2025diffrhythm+</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> supports text-based control of genre, instrument, rhythm, and atmosphere but lacks singer timbre control (e.g., gender and age). ACE-Step </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2025ace</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, one of the state-of-the-art (SOTA) open-source music generation model, provides text-based control for all acoustic attributes but focuses exclusively on music without unified speech modeling capabilities.\nSpeech and music generation are typically treated as separate tasks (TTS &amp; TTM), overlooking their shared acoustic modeling abilities and control mechanisms.\nThis separation stems from the difficulty of aligning inputs across TTS and TTM tasks, as speech control involves acoustic attributes such as timbre and paralinguistics, while music generation requires musical attributes such as genre, instrumentation, and rhythm.</span>\n</p>\n\n",
                "matched_terms": [
                    "control",
                    "models",
                    "speech",
                    "tts",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Vevo2 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2025vevo2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduced the first unified speech and singing generation framework, demonstrating that joint modeling leverages rich speech data to improve singing quality while utilizing singing&#8217;s expressive characteristics to enhance TTS. However, Vevo2 relies on reference audio for acoustic attribute control rather than text instructions and generates only vocals without instrumental music capabilities.\nUniAudio </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2023uniaudio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> builds upon the VALL-E </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> framework to create a single model capable of executing multiple tasks; however, it requires inconsistent input formats across different tasks and necessitates task-specific fine-tuning. AudioBox </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vyas2023audiobox</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a flow-matching-based unified model supporting multiple tasks, pre-trains on speech, music, and sound effect data but ultimately supports only speech and sound effect generation. AudioLDM 2 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024audioldm</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> proposes a two-stage model applicable to speech, sound, and music generation, yet requires different model architecture hyperparameters for each task.\nCurrent approaches lack text-based (natural language descriptions) control mechanisms, limiting their ability to achieve unified speech and music generation. To address these limitations, we propose InstructAudio, an instruction-controlled unified framework for speech and music generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "tts",
                    "speech",
                    "control",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our method makes three key contributions:\na) We introduce a joint modeling framework for speech and music generation based on a multimodal diffusion transformer (MM-DiT) architecture.\nb) We achieve unified text-based control over TTS and TTM through natural language descriptions (a standardized instruction-phoneme input format), encompassing timbre attributes (gender, age), paralinguistic attributes (emotion, style, accent), and musical attributes (genre, instrument, rhythm, atmosphere), while supporting dialogue speech generation.\nc) Experimental results demonstrate best performance in instruction-based TTS (best WER, speaker similarity, emotion similarity, classification control accuracy, and distortion/error metrics) while maintaining competitive TTM capabilities (best SongEval metrics), validating the effectiveness of our unified approach.</span>\n</p>\n\n",
                "matched_terms": [
                    "control",
                    "performance",
                    "speech",
                    "tts",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As illustrated in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, InstructAudio introduces a novel approach for unified speech and music generation, drawing inspiration from MM-Audio </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025mmaudio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> through its MM-DiT architecture. The model comprises two core components: joint diffusion transformer layers and single diffusion transformer layers.\nDuring training, the instruct encoder, mel-encoder, and mel-decoder remain frozen as pretrained modules. The first-layer joint diffusion transformer takes temporally concatenated instruct embedding and phoneme embedding as text modal conditioning input, and noised mel vae latents as audio modal input (see Section </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S2.SS4\" style=\"font-size:90%;\" title=\"2.4 Multimodal Diffusion Transformer Architecture &#8227; 2 Method &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">2.4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nUnlike existing non-autoregressive architectures</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhu2025zipvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">eskimez2024e2</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, InstructAudio eliminates the need for text-upsampling alignment with audio representations. For TTS, the model achieves multi-attribute control, including gender, age, emotion, style, and accent, through natural language instructions, and also supports two-speaker dialogue generation. In TTM, it similarly enables multi-attribute control covering singer timbre (e.g., gender and age), music genre, instrumentation, melody, and emotional expression.</span>\n</p>\n\n",
                "matched_terms": [
                    "tts",
                    "speech",
                    "control",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To enable the unified generation of speech and singing, we designed a standardized instruction-phoneme input format that aligns both tasks, as illustrated in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This format consists of two primary components: an instruction description and a phoneme sequence. The instruction description is a natural language prompt specifying the desired acoustic attributes of the output. For speech synthesis, these attributes include speaker characteristics such as gender, age, emotion, style, and accent. In dialogue scenarios, we provide separate descriptions for each of the two speakers and prepend special tokens, [S0] and [S1], to their respective text inputs to differentiate the utterances. Similarly, for music generation, the description specifies attributes like the singer&#8217;s gender and age, music genre, instrumentation, melody, and emotion. For both tasks, the input text (for speech) or lyrics (for music) is converted into phonemes using a Grapheme-to-Phoneme (G2P) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qiang2022back</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model.\nThis unified representation allows a single model to seamlessly process input for both speech and music generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "speech",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The Latent Audio Codec extends our previous SecoustiCodec framework </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qiang2025secousticodec</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qiang2025vq</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qiang2024learning</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and comprises three core components: a mel-encoder, mel-decoder, and discriminator. The VAE architecture enables the model to learn continuous and complete distributions in the latent space, significantly enhancing audio representation capabilities. This high compression ratio facilitates efficient MM-DiT training while improving reconstruction quality for both speech and music. Due to space constraints, we omit the analysis of audio representation effects from this work. For comprehensive results, readers are referred to Section 4.5 of our prior work on Kling-Foley</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025kling</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We employ conditional flow matching</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lipman2022flow</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with an MM-DiT architecture based on Stable Diffusion 3</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">esser2024scaling</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, comprising </span>\n  <math alttext=\"N_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">N</mi>\n        <mn mathsize=\"0.900em\">2</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">N_{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Joint Diffusion Transformer layers. To enhance speech and singing voice generation quality, we incorporate </span>\n  <math alttext=\"N_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">N</mi>\n        <mn mathsize=\"0.900em\">1</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">N_{1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> additional Single Diffusion Transformer layers for audio-only processing. The input instruction embeddings </span>\n  <math alttext=\"\\in\\mathbb{R}^{B\\times L_{1}\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi/>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">B</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msub>\n              <mi mathsize=\"0.900em\">L</mi>\n              <mn mathsize=\"0.900em\">1</mn>\n            </msub>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\in\\mathbb{R}^{B\\times L_{1}\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and phoneme embeddings </span>\n  <math alttext=\"\\in\\mathbb{R}^{B\\times L_{2}\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi/>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">B</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msub>\n              <mi mathsize=\"0.900em\">L</mi>\n              <mn mathsize=\"0.900em\">2</mn>\n            </msub>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\in\\mathbb{R}^{B\\times L_{2}\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are temporally concatenated to form the text modality input </span>\n  <math alttext=\"C_{\\text{text}}\\in\\mathbb{R}^{B\\times(L_{1}+L_{2})\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">C</mi>\n          <mtext mathsize=\"0.900em\">text</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">B</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <mrow>\n                <msub>\n                  <mi mathsize=\"0.900em\">L</mi>\n                  <mn mathsize=\"0.900em\">1</mn>\n                </msub>\n                <mo mathsize=\"0.900em\">+</mo>\n                <msub>\n                  <mi mathsize=\"0.900em\">L</mi>\n                  <mn mathsize=\"0.900em\">2</mn>\n                </msub>\n              </mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" rspace=\"0.055em\">)</mo>\n            </mrow>\n            <mo mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">C_{\\text{text}}\\in\\mathbb{R}^{B\\times(L_{1}+L_{2})\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The linear interpolation path </span>\n  <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">x</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">x_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> between Gaussian noise and VAE latents serves as the audio modality input.\nThe two modalities interact through joint attention, where queries, keys, and values from both modalities are concatenated and processed using scaled dot-product attention. The output maintains input dimensionality and is split back into respective modalities. In Single Diffusion Transformer layers, only audio latents are processed, reducing joint attention to self-attention.\nDuring training, flow matching optimizes the objective: </span>\n  <math alttext=\"\\mathbb{E}\\big\\|v_{\\theta}(t,C_{\\text{text}},x_{t})-u(t,x_{t})\\big\\|^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#120124;</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <msup>\n          <mrow>\n            <mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">&#8214;</mo>\n            <mrow>\n              <mrow>\n                <msub>\n                  <mi mathsize=\"0.900em\">v</mi>\n                  <mi mathsize=\"0.900em\">&#952;</mi>\n                </msub>\n                <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n                <mrow>\n                  <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n                  <mi mathsize=\"0.900em\">t</mi>\n                  <mo mathsize=\"0.900em\">,</mo>\n                  <msub>\n                    <mi mathsize=\"0.900em\">C</mi>\n                    <mtext mathsize=\"0.900em\">text</mtext>\n                  </msub>\n                  <mo mathsize=\"0.900em\">,</mo>\n                  <msub>\n                    <mi mathsize=\"0.900em\">x</mi>\n                    <mi mathsize=\"0.900em\">t</mi>\n                  </msub>\n                  <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n                </mrow>\n              </mrow>\n              <mo mathsize=\"0.900em\">&#8722;</mo>\n              <mrow>\n                <mi mathsize=\"0.900em\">u</mi>\n                <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n                <mrow>\n                  <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n                  <mi mathsize=\"0.900em\">t</mi>\n                  <mo mathsize=\"0.900em\">,</mo>\n                  <msub>\n                    <mi mathsize=\"0.900em\">x</mi>\n                    <mi mathsize=\"0.900em\">t</mi>\n                  </msub>\n                  <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n                </mrow>\n              </mrow>\n            </mrow>\n            <mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">&#8214;</mo>\n          </mrow>\n          <mn mathsize=\"0.900em\">2</mn>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbb{E}\\big\\|v_{\\theta}(t,C_{\\text{text}},x_{t})-u(t,x_{t})\\big\\|^{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nwhere </span>\n  <math alttext=\"v_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m8\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">v</mi>\n        <mi mathsize=\"0.900em\">&#952;</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">v_{\\theta}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> represents the learned conditional velocity field, </span>\n  <math alttext=\"u\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m9\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">u</mi>\n      <annotation encoding=\"application/x-tex\">u</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the target conditional vector field, and </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m10\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the time step. During inference, an ODE solver generates the target VAE latents.</span>\n</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We collect 50K hours of speech and 20K hours of music data from internet sources, applying our internal data processing pipeline to generate instruction descriptions and text/lyrics annotations. For speech data, descriptions encompass gender, age, emotion, style, and accent attributes. Music data descriptions include genre, instrument, gender, age, rhythm, and atmosphere characteristics. Audio clips (2-20s) maintain 1:1 Chinese-English and male-female ratios, with 90%+ neutral emotions and 0.5% dialogue data, standardized to 44.1kHz sampling rate.</span>\n</p>\n\n",
                "matched_terms": [
                    "rate",
                    "speech",
                    "50k",
                    "20k",
                    "instruction",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We employ comprehensive objective and subjective metrics to ensure thorough evaluation. Objective metrics include Word Error Rate (WER) using Seed-TTS</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Speaker Similarity</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/resemble-ai/Resemblyzer\" title=\"\">https://github.com/resemble-ai/Resemblyzer</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Emotion Similarity</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">2</sup>\n        <span class=\"ltx_tag ltx_tag_note\">2</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/emotion2vec\" title=\"\">https://huggingface.co/emotion2vec</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Log-Spectral Distance (LSD), Mel-Cepstral Distortion (MCD), Mean Squared Error of Pitch (MSEP), Voiced/Unvoiced Mismatch Rate (MR), SongEval</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yao2025songeval</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> music evaluation benchmark, and classification control accuracy through perceptual consistency assessment. Subjective evaluation employs Quality Mean Opinion Score (QMOS), Naturalness Mean Opinion Score (NMOS), and Musicality Mean Opinion Score (MMOS), conducted by professionally trained evaluators. Classification control accuracy is evaluated through human listening tests, where annotators select the perceptually consistent category from predefined options (e.g., choosing among \"child,\" \"young/middle-aged,\" or \"elderly\" for age attributes).\nFor WER evaluation, we utilize the complete Seed-TTS benchmark test set. For instruction-based TTS tasks, we construct a manually annotated test set of 500 samples with natural language descriptions covering multiple attributes (gender, age, emotion, style, accent), selecting 100 samples for subjective evaluation. Similarly, for music tasks, we create a 500-sample test set with natural language descriptions of musical attributes (gender, age, genre, instrumentation, melody, emotion), with 100 samples selected for subjective evaluation. The distribution across categories within each attribute is uniform. Notably, InstructAudio&#8217;s conditioning input consists of descriptive text corresponding to ground truth, with similarity and distortion/error calculations performed against ground truth.</span>\n</p>\n\n",
                "matched_terms": [
                    "word",
                    "control",
                    "error",
                    "rate",
                    "tts",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">While the text-only control mechanism achieves unified input formatting for TTS and TTM\ntasks, it introduces inherent information loss compared to audio modalities, resulting in one-to-many mapping ambiguity. This leads to averaged audio quality and naturalness compared to reference audio-based methods, as evidenced by lower NMOS scores in both TTS and TTM tasks.\nAdditionally, to enable joint speech-music modeling, we constrain music generation to 5-20 second clips, limiting long-form music generation capabilities. Our evaluation focuses on short segments to match speech durations, which is detrimental to models like DiffRhythm+ that are optimized for full-length music generation. These limitations above, we expect to address in future work.</span>\n</p>\n\n",
                "matched_terms": [
                    "control",
                    "models",
                    "speech",
                    "tts",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper presents InstructAudio, an instruction-controlled unified framework for speech and music generation based on a MM-DiT architecture, demonstrating the effectiveness of joint TTS and TTM modeling. Our main contributions include: (1) introducing the first instruction-controlled unified framework for speech and music generation that eliminates reference audio requirements in attribute control; (2) achieving comprehensive controllability over timbre, paralinguistic, and musical attributes through standardized instruction-phoneme input formatting (natural language descriptions); and (3) demonstrating SOTA performance in instruction-based TTS while maintaining competitive music generation capabilities. Experimental validation confirms our approach&#8217;s effectiveness, achieving optimal results across multiple metrics including WER, similarity measures, and classification control accuracy. InstructAudio demonstrates the viability of unified audio generation frameworks. Future work will explore multimodal control mechanisms to address quality and naturalness issues, support longer music generation, and incorporate sound effect generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "control",
                    "performance",
                    "speech",
                    "tts",
                    "music"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "InstructAudio: Unified speech and music generation with natural language instruction",
        "caption": "Table 2: Performance comparison of instruction-based TTS on control accuracy, similarity, distortion/error metrics, and subjective evaluation.",
        "body": "Model\nClassification Control Accuracy Rate (%)\\uparrow\nSimilarity\\uparrow\nDistortion/Error \\downarrow\nMOS\\uparrow\n\n\nGender\nAge\nEmotion\nStyle\nAccent\nDialog\nSpeaker\nEmotion\nLSD\nMCD\nMSEP\nMR\nQMOS\nNMOS\n\n\nGround Truth\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n1.00\n1.00\n0.00\n0.00\n0.00\n0.00\n\n\n\n\n\nCosyVoice2[du2024cosyvoice2]\n\n\n\n58.33\n65.00\n100.00\n\n0.68\n0.53\n2.57\n7.11\n547.87\n0.46\n3.90  0.11\n3.65  0.22\n\n\nInstructAudio\n100.00\n86.67\n83.33\n86.67\n100.00\n90.00\n0.76\n0.71\n1.88\n5.71\n437.58\n0.33\n3.73  0.24\n3.46  0.32",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"6\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Classification Control Accuracy Rate (%)<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Similarity<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Distortion/Error <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Gender</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Age</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Emotion</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Style</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Accent</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dialog</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speaker</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Emotion</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LSD</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MCD</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MSEP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MR</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">QMOS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">NMOS</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Ground Truth</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">100.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">100.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">100.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">100.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">100.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">100.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice2</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">58.33</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">65.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">100.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.68</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.53</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.57</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.11</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">547.87</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.46</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.90 &#177; 0.11</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.65 &#177; 0.22</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">InstructAudio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">100.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">86.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">83.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">86.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">100.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">90.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.88</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">437.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.73 &#177; 0.24</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.46 &#177; 0.32</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "accent",
            "control",
            "cosyvoice2du2024cosyvoice2",
            "subjective",
            "gender",
            "classification",
            "emotion",
            "uparrow",
            "evaluation",
            "rate",
            "accuracy",
            "mcd",
            "lsd",
            "truth",
            "instructaudio",
            "metrics",
            "nmos",
            "performance",
            "instructionbased",
            "distortionerror",
            "downarrow",
            "comparison",
            "similarity",
            "dialog",
            "ground",
            "msep",
            "speaker",
            "tts",
            "mosuparrow",
            "age",
            "similarityuparrow",
            "model",
            "style",
            "qmos"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate our model&#8217;s unified speech and music generation capabilities, we compare against SOTA models in each task. For TTS, we benchmark fundamental capabilities against MaskGCT</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, E2-TTS</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">eskimez2024e2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, F5-TTS</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, ZipVoice</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhu2025zipvoice</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CosyVoice1</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and CosyVoice2</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.2 Compared Method and Evaluation Metrics &#8227; 3 Experiments &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), and instruction-based TTS performance against the current SOTA model CosyVoice2 (Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.2 Compared Method and Evaluation Metrics &#8227; 3 Experiments &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nNotably, since InstructAudio is a purely instruction-controlled model while the Seed-TTS benchmark comprises natural emotional speech samples, we evaluate WER metrics using natural, calm-style text descriptions with randomized speakers as control conditions. For CosyVoice2, which lacks text-based control for timbre attributes (gender, age), we provide reference audio that matches timbre description during inference and map inputs to its supported short-form control text (e.g., \"Please speak very happy\").\nFor music generation, we compare with DiffRhythm+</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2025diffrhythm+</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and ACE-Step</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2025ace</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.2 Compared Method and Evaluation Metrics &#8227; 3 Experiments &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). Note that DiffRhythm+ does not support music synthesis under 90 seconds; therefore, we generate longer sequences and truncate them to create test samples, which may introduce evaluation bias.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Evaluation of TTS:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.2 Compared Method and Evaluation Metrics &#8227; 3 Experiments &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares InstructAudio with mainstream TTS models. MaskGCT, E2-TTS, F5-TTS, and ZipVoice do not support text-based control capabilities. CosyVoice1 and CosyVoice2 only support emotion, style, and accent control, requiring additional prompt speech for timbre control. In contrast, InstructAudio supports comprehensive text-based control including gender, age, emotion, style, and accent, while uniquely enabling text-controlled dialogue synthesis (a capability absent in other models). Although InstructAudio has the largest parameter count (1.3B) among TTS models, it additionally supports music generation. As shown in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.2 Compared Method and Evaluation Metrics &#8227; 3 Experiments &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, mainstream music generation models like DiffRhythm+ and ACE-Step also exceed 1B parameters. Notably, InstructAudio achieves superior performance with the smallest training dataset. On the Seed-TTS WER metric, InstructAudio achieves the best results when conditioned on neutral emotion and calm style text control.\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Control Capability:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.2 Compared Method and Evaluation Metrics &#8227; 3 Experiments &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> evaluates text-based control TTS capabilities by comparing against CosyVoice2, the current SOTA model. For classification control accuracy, InstructAudio supports gender, age, and dialogue control capabilities unavailable in CosyVoice2 (while outperforming CosyVoice2 across all control categories). InstructAudio achieves precise dialogue synthesis control, attaining 90% accuracy in a capability that other models lack entirely. InstructAudio also demonstrates superior speaker and emotion similarity. This advantage stems from CosyVoice2&#8217;s reliance on additional prompt speech (random speaker audio with matching gender and age) for timbre control, which causes emotion leakage that compromises emotion control effectiveness.\nInstructAudio outperforms CosyVoice2 across all distortion and error metrics. While CosyVoice2 achieves a higher MOS score, it notably requires reference audio as input conditioning. Text-only control introduces TTS one-to-many mapping ambiguity, reducing average audio quality and naturalness for InstructAudio outputs.\nHowever, InstructAudio achieves comparable MOS results despite the unfair disadvantage of missing input modality, while comprehensively outperforming CosyVoice2 on all other metrics.\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Evaluation of TTM:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nTable </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.2 Compared Method and Evaluation Metrics &#8227; 3 Experiments &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> evaluates music generation capabilities against current SOTA models ACE-Step and DiffRhythm+. For classification control accuracy, ACE-Step achieves the best performance in genre and instrument categories, while InstructAudio excels in gender, age, rhythm, and atmosphere control. DiffRhythm+ scores poorly on gender and age due to its lack of singer timbre control capabilities. InstructAudio achieves the best SongEval scores across all metrics. ACE-Step obtains the highest QMOS score, indicating superior perceived audio quality, while InstructAudio achieves the best MMOS score. We acknowledge that our music evaluation uses 5-20 second clips matching speech durations, which may disadvantage models like ACE-Step and DiffRhythm+ that are optimized for longer music generation. This comparison demonstrates that InstructAudio maintains competitive music generation capabilities while achieving SOTA text-based control TTS performance.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Text-to-speech (TTS) and text-to-music (TTM) models face significant limitations in instruction-based control. TTS systems usually depend on reference audio for timbre, offer only limited text-level attribute control, and rarely support dialogue generation.\nTTM systems are constrained by input conditioning requirements that depend on expert knowledge annotations. The high heterogeneity of these input control conditions makes them difficult to joint modeling with speech synthesis.\nDespite sharing common acoustic modeling characteristics, these two tasks have long been developed independently, leaving open the challenge of achieving unified modeling through natural language instructions.\nWe introduce InstructAudio, a unified framework that enables instruction-based (natural language descriptions) control of acoustic attributes including timbre (gender, age), paralinguistic (emotion, style, accent), and musical (genre, instrument, rhythm, atmosphere). It supports expressive speech, music, and dialogue generation in English and Chinese.\nThe model employs joint and single diffusion transformer layers with a standardized instruction-phoneme input format, trained on 50K hours of speech and 20K hours of music data, enabling multi-task learning and cross-modal alignment.\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S0.F1\" title=\"Figure 1 &#8227; InstructAudio: Unified speech and music generation with natural language instruction\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> visualizes performance comparisons with mainstream TTS and TTM models, demonstrating that InstructAudio achieves optimal results on most metrics.\nTo our best knowledge, InstructAudio represents the first instruction-controlled framework unifying speech and music generation. Audio samples are available at: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://qiangchunyu.github.io/InstructAudio/\" title=\"\">https://qiangchunyu.github.io/InstructAudio/</a></span>\n</p>\n\n",
                "matched_terms": [
                    "accent",
                    "control",
                    "performance",
                    "instructionbased",
                    "model",
                    "tts",
                    "age",
                    "emotion",
                    "gender",
                    "instructaudio",
                    "metrics",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Existing TTS models excel at either zero-shot voice cloning </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> or style control</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ji2024controlspeech</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2023prompttts</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, but lack text-based control over multiple acoustic attributes through natural language descriptions. For instance, while CosyVoice </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and ControlSpeech </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ji2024controlspeech</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> support text-based emotion and style control, they require additional reference audio for timbre attributes and cannot handle text-controlled dialogue generation. Similarly, current TTM models exhibit limited control capabilities. DiffRhythm+</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2025diffrhythm+</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> supports text-based control of genre, instrument, rhythm, and atmosphere but lacks singer timbre control (e.g., gender and age). ACE-Step </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2025ace</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, one of the state-of-the-art (SOTA) open-source music generation model, provides text-based control for all acoustic attributes but focuses exclusively on music without unified speech modeling capabilities.\nSpeech and music generation are typically treated as separate tasks (TTS &amp; TTM), overlooking their shared acoustic modeling abilities and control mechanisms.\nThis separation stems from the difficulty of aligning inputs across TTS and TTM tasks, as speech control involves acoustic attributes such as timbre and paralinguistics, while music generation requires musical attributes such as genre, instrumentation, and rhythm.</span>\n</p>\n\n",
                "matched_terms": [
                    "control",
                    "tts",
                    "age",
                    "emotion",
                    "gender",
                    "model",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Vevo2 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2025vevo2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduced the first unified speech and singing generation framework, demonstrating that joint modeling leverages rich speech data to improve singing quality while utilizing singing&#8217;s expressive characteristics to enhance TTS. However, Vevo2 relies on reference audio for acoustic attribute control rather than text instructions and generates only vocals without instrumental music capabilities.\nUniAudio </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2023uniaudio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> builds upon the VALL-E </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> framework to create a single model capable of executing multiple tasks; however, it requires inconsistent input formats across different tasks and necessitates task-specific fine-tuning. AudioBox </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vyas2023audiobox</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a flow-matching-based unified model supporting multiple tasks, pre-trains on speech, music, and sound effect data but ultimately supports only speech and sound effect generation. AudioLDM 2 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024audioldm</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> proposes a two-stage model applicable to speech, sound, and music generation, yet requires different model architecture hyperparameters for each task.\nCurrent approaches lack text-based (natural language descriptions) control mechanisms, limiting their ability to achieve unified speech and music generation. To address these limitations, we propose InstructAudio, an instruction-controlled unified framework for speech and music generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "tts",
                    "instructaudio",
                    "model",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our method makes three key contributions:\na) We introduce a joint modeling framework for speech and music generation based on a multimodal diffusion transformer (MM-DiT) architecture.\nb) We achieve unified text-based control over TTS and TTM through natural language descriptions (a standardized instruction-phoneme input format), encompassing timbre attributes (gender, age), paralinguistic attributes (emotion, style, accent), and musical attributes (genre, instrument, rhythm, atmosphere), while supporting dialogue speech generation.\nc) Experimental results demonstrate best performance in instruction-based TTS (best WER, speaker similarity, emotion similarity, classification control accuracy, and distortion/error metrics) while maintaining competitive TTM capabilities (best SongEval metrics), validating the effectiveness of our unified approach.</span>\n</p>\n\n",
                "matched_terms": [
                    "accent",
                    "control",
                    "performance",
                    "instructionbased",
                    "distortionerror",
                    "accuracy",
                    "speaker",
                    "tts",
                    "age",
                    "similarity",
                    "emotion",
                    "classification",
                    "gender",
                    "metrics",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As illustrated in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, InstructAudio introduces a novel approach for unified speech and music generation, drawing inspiration from MM-Audio </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025mmaudio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> through its MM-DiT architecture. The model comprises two core components: joint diffusion transformer layers and single diffusion transformer layers.\nDuring training, the instruct encoder, mel-encoder, and mel-decoder remain frozen as pretrained modules. The first-layer joint diffusion transformer takes temporally concatenated instruct embedding and phoneme embedding as text modal conditioning input, and noised mel vae latents as audio modal input (see Section </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S2.SS4\" style=\"font-size:90%;\" title=\"2.4 Multimodal Diffusion Transformer Architecture &#8227; 2 Method &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">2.4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nUnlike existing non-autoregressive architectures</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhu2025zipvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">eskimez2024e2</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, InstructAudio eliminates the need for text-upsampling alignment with audio representations. For TTS, the model achieves multi-attribute control, including gender, age, emotion, style, and accent, through natural language instructions, and also supports two-speaker dialogue generation. In TTM, it similarly enables multi-attribute control covering singer timbre (e.g., gender and age), music genre, instrumentation, melody, and emotional expression.</span>\n</p>\n\n",
                "matched_terms": [
                    "accent",
                    "control",
                    "model",
                    "tts",
                    "age",
                    "emotion",
                    "gender",
                    "instructaudio",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To enable the unified generation of speech and singing, we designed a standardized instruction-phoneme input format that aligns both tasks, as illustrated in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This format consists of two primary components: an instruction description and a phoneme sequence. The instruction description is a natural language prompt specifying the desired acoustic attributes of the output. For speech synthesis, these attributes include speaker characteristics such as gender, age, emotion, style, and accent. In dialogue scenarios, we provide separate descriptions for each of the two speakers and prepend special tokens, [S0] and [S1], to their respective text inputs to differentiate the utterances. Similarly, for music generation, the description specifies attributes like the singer&#8217;s gender and age, music genre, instrumentation, melody, and emotion. For both tasks, the input text (for speech) or lyrics (for music) is converted into phonemes using a Grapheme-to-Phoneme (G2P) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qiang2022back</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model.\nThis unified representation allows a single model to seamlessly process input for both speech and music generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "accent",
                    "speaker",
                    "age",
                    "emotion",
                    "gender",
                    "model",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">InstructAudio comprises 1.34 billion parameters with a flow matching feedforward dimension of 1024. The architecture includes 14 joint diffusion transformer layers and 6 single diffusion transformer layers, incorporating RoPE positional encoding </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">su2024roformer</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We employ a Zipformer-based</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhu2025zipvoice</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> phoneme encoder with a feedforward dimension of 512 and utilize Qwen2.5-7B</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen2025qwen25technicalreport</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the instruct encoder. The mel encoder processes 44.1kHz input waveforms and generates embeddings at 43 Hz, achieving 1024&#215; downsampling relative to the input sampling rate. Training is conducted on 32 NVIDIA Tesla A800 80GB GPUs with a batch size of 16 per GPU, using the Adam optimizer </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Kingma2014AdamAM</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with an initial learning rate of 1e-4.</span>\n</p>\n\n",
                "matched_terms": [
                    "rate",
                    "instructaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We collect 50K hours of speech and 20K hours of music data from internet sources, applying our internal data processing pipeline to generate instruction descriptions and text/lyrics annotations. For speech data, descriptions encompass gender, age, emotion, style, and accent attributes. Music data descriptions include genre, instrument, gender, age, rhythm, and atmosphere characteristics. Audio clips (2-20s) maintain 1:1 Chinese-English and male-female ratios, with 90%+ neutral emotions and 0.5% dialogue data, standardized to 44.1kHz sampling rate.</span>\n</p>\n\n",
                "matched_terms": [
                    "accent",
                    "rate",
                    "age",
                    "emotion",
                    "gender",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We employ comprehensive objective and subjective metrics to ensure thorough evaluation. Objective metrics include Word Error Rate (WER) using Seed-TTS</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Speaker Similarity</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/resemble-ai/Resemblyzer\" title=\"\">https://github.com/resemble-ai/Resemblyzer</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Emotion Similarity</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">2</sup>\n        <span class=\"ltx_tag ltx_tag_note\">2</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/emotion2vec\" title=\"\">https://huggingface.co/emotion2vec</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Log-Spectral Distance (LSD), Mel-Cepstral Distortion (MCD), Mean Squared Error of Pitch (MSEP), Voiced/Unvoiced Mismatch Rate (MR), SongEval</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yao2025songeval</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> music evaluation benchmark, and classification control accuracy through perceptual consistency assessment. Subjective evaluation employs Quality Mean Opinion Score (QMOS), Naturalness Mean Opinion Score (NMOS), and Musicality Mean Opinion Score (MMOS), conducted by professionally trained evaluators. Classification control accuracy is evaluated through human listening tests, where annotators select the perceptually consistent category from predefined options (e.g., choosing among \"child,\" \"young/middle-aged,\" or \"elderly\" for age attributes).\nFor WER evaluation, we utilize the complete Seed-TTS benchmark test set. For instruction-based TTS tasks, we construct a manually annotated test set of 500 samples with natural language descriptions covering multiple attributes (gender, age, emotion, style, accent), selecting 100 samples for subjective evaluation. Similarly, for music tasks, we create a 500-sample test set with natural language descriptions of musical attributes (gender, age, genre, instrumentation, melody, emotion), with 100 samples selected for subjective evaluation. The distribution across categories within each attribute is uniform. Notably, InstructAudio&#8217;s conditioning input consists of descriptive text corresponding to ground truth, with similarity and distortion/error calculations performed against ground truth.</span>\n</p>\n\n",
                "matched_terms": [
                    "accent",
                    "control",
                    "subjective",
                    "gender",
                    "classification",
                    "emotion",
                    "evaluation",
                    "rate",
                    "accuracy",
                    "mcd",
                    "lsd",
                    "truth",
                    "nmos",
                    "metrics",
                    "instructionbased",
                    "distortionerror",
                    "similarity",
                    "ground",
                    "msep",
                    "speaker",
                    "tts",
                    "age",
                    "style",
                    "qmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">While the text-only control mechanism achieves unified input formatting for TTS and TTM\ntasks, it introduces inherent information loss compared to audio modalities, resulting in one-to-many mapping ambiguity. This leads to averaged audio quality and naturalness compared to reference audio-based methods, as evidenced by lower NMOS scores in both TTS and TTM tasks.\nAdditionally, to enable joint speech-music modeling, we constrain music generation to 5-20 second clips, limiting long-form music generation capabilities. Our evaluation focuses on short segments to match speech durations, which is detrimental to models like DiffRhythm+ that are optimized for full-length music generation. These limitations above, we expect to address in future work.</span>\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "tts",
                    "nmos",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper presents InstructAudio, an instruction-controlled unified framework for speech and music generation based on a MM-DiT architecture, demonstrating the effectiveness of joint TTS and TTM modeling. Our main contributions include: (1) introducing the first instruction-controlled unified framework for speech and music generation that eliminates reference audio requirements in attribute control; (2) achieving comprehensive controllability over timbre, paralinguistic, and musical attributes through standardized instruction-phoneme input formatting (natural language descriptions); and (3) demonstrating SOTA performance in instruction-based TTS while maintaining competitive music generation capabilities. Experimental validation confirms our approach&#8217;s effectiveness, achieving optimal results across multiple metrics including WER, similarity measures, and classification control accuracy. InstructAudio demonstrates the viability of unified audio generation frameworks. Future work will explore multimodal control mechanisms to address quality and naturalness issues, support longer music generation, and incorporate sound effect generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "control",
                    "performance",
                    "instructionbased",
                    "accuracy",
                    "tts",
                    "similarity",
                    "classification",
                    "instructaudio",
                    "metrics"
                ]
            }
        ]
    },
    "S3.T3": {
        "source_file": "InstructAudio: Unified speech and music generation with natural language instruction",
        "caption": "Table 3: Performance comparison of TTM on control accuracy, SongEval, and subjective evaluation.",
        "body": "50K Speech\n\n\n+ 20K Music",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">50K Speech</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">+ 20K Music</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "control",
            "performance",
            "songeval",
            "evaluation",
            "accuracy",
            "speech",
            "comparison",
            "subjective",
            "50k",
            "20k",
            "ttm",
            "music"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate our model&#8217;s unified speech and music generation capabilities, we compare against SOTA models in each task. For TTS, we benchmark fundamental capabilities against MaskGCT</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, E2-TTS</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">eskimez2024e2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, F5-TTS</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, ZipVoice</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhu2025zipvoice</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CosyVoice1</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and CosyVoice2</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.2 Compared Method and Evaluation Metrics &#8227; 3 Experiments &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), and instruction-based TTS performance against the current SOTA model CosyVoice2 (Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.2 Compared Method and Evaluation Metrics &#8227; 3 Experiments &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nNotably, since InstructAudio is a purely instruction-controlled model while the Seed-TTS benchmark comprises natural emotional speech samples, we evaluate WER metrics using natural, calm-style text descriptions with randomized speakers as control conditions. For CosyVoice2, which lacks text-based control for timbre attributes (gender, age), we provide reference audio that matches timbre description during inference and map inputs to its supported short-form control text (e.g., \"Please speak very happy\").\nFor music generation, we compare with DiffRhythm+</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2025diffrhythm+</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and ACE-Step</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2025ace</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.2 Compared Method and Evaluation Metrics &#8227; 3 Experiments &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). Note that DiffRhythm+ does not support music synthesis under 90 seconds; therefore, we generate longer sequences and truncate them to create test samples, which may introduce evaluation bias.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Evaluation of TTS:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.2 Compared Method and Evaluation Metrics &#8227; 3 Experiments &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares InstructAudio with mainstream TTS models. MaskGCT, E2-TTS, F5-TTS, and ZipVoice do not support text-based control capabilities. CosyVoice1 and CosyVoice2 only support emotion, style, and accent control, requiring additional prompt speech for timbre control. In contrast, InstructAudio supports comprehensive text-based control including gender, age, emotion, style, and accent, while uniquely enabling text-controlled dialogue synthesis (a capability absent in other models). Although InstructAudio has the largest parameter count (1.3B) among TTS models, it additionally supports music generation. As shown in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.2 Compared Method and Evaluation Metrics &#8227; 3 Experiments &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, mainstream music generation models like DiffRhythm+ and ACE-Step also exceed 1B parameters. Notably, InstructAudio achieves superior performance with the smallest training dataset. On the Seed-TTS WER metric, InstructAudio achieves the best results when conditioned on neutral emotion and calm style text control.\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Control Capability:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.2 Compared Method and Evaluation Metrics &#8227; 3 Experiments &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> evaluates text-based control TTS capabilities by comparing against CosyVoice2, the current SOTA model. For classification control accuracy, InstructAudio supports gender, age, and dialogue control capabilities unavailable in CosyVoice2 (while outperforming CosyVoice2 across all control categories). InstructAudio achieves precise dialogue synthesis control, attaining 90% accuracy in a capability that other models lack entirely. InstructAudio also demonstrates superior speaker and emotion similarity. This advantage stems from CosyVoice2&#8217;s reliance on additional prompt speech (random speaker audio with matching gender and age) for timbre control, which causes emotion leakage that compromises emotion control effectiveness.\nInstructAudio outperforms CosyVoice2 across all distortion and error metrics. While CosyVoice2 achieves a higher MOS score, it notably requires reference audio as input conditioning. Text-only control introduces TTS one-to-many mapping ambiguity, reducing average audio quality and naturalness for InstructAudio outputs.\nHowever, InstructAudio achieves comparable MOS results despite the unfair disadvantage of missing input modality, while comprehensively outperforming CosyVoice2 on all other metrics.\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Evaluation of TTM:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nTable </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.2 Compared Method and Evaluation Metrics &#8227; 3 Experiments &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> evaluates music generation capabilities against current SOTA models ACE-Step and DiffRhythm+. For classification control accuracy, ACE-Step achieves the best performance in genre and instrument categories, while InstructAudio excels in gender, age, rhythm, and atmosphere control. DiffRhythm+ scores poorly on gender and age due to its lack of singer timbre control capabilities. InstructAudio achieves the best SongEval scores across all metrics. ACE-Step obtains the highest QMOS score, indicating superior perceived audio quality, while InstructAudio achieves the best MMOS score. We acknowledge that our music evaluation uses 5-20 second clips matching speech durations, which may disadvantage models like ACE-Step and DiffRhythm+ that are optimized for longer music generation. This comparison demonstrates that InstructAudio maintains competitive music generation capabilities while achieving SOTA text-based control TTS performance.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Text-to-speech (TTS) and text-to-music (TTM) models face significant limitations in instruction-based control. TTS systems usually depend on reference audio for timbre, offer only limited text-level attribute control, and rarely support dialogue generation.\nTTM systems are constrained by input conditioning requirements that depend on expert knowledge annotations. The high heterogeneity of these input control conditions makes them difficult to joint modeling with speech synthesis.\nDespite sharing common acoustic modeling characteristics, these two tasks have long been developed independently, leaving open the challenge of achieving unified modeling through natural language instructions.\nWe introduce InstructAudio, a unified framework that enables instruction-based (natural language descriptions) control of acoustic attributes including timbre (gender, age), paralinguistic (emotion, style, accent), and musical (genre, instrument, rhythm, atmosphere). It supports expressive speech, music, and dialogue generation in English and Chinese.\nThe model employs joint and single diffusion transformer layers with a standardized instruction-phoneme input format, trained on 50K hours of speech and 20K hours of music data, enabling multi-task learning and cross-modal alignment.\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S0.F1\" title=\"Figure 1 &#8227; InstructAudio: Unified speech and music generation with natural language instruction\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> visualizes performance comparisons with mainstream TTS and TTM models, demonstrating that InstructAudio achieves optimal results on most metrics.\nTo our best knowledge, InstructAudio represents the first instruction-controlled framework unifying speech and music generation. Audio samples are available at: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://qiangchunyu.github.io/InstructAudio/\" title=\"\">https://qiangchunyu.github.io/InstructAudio/</a></span>\n</p>\n\n",
                "matched_terms": [
                    "control",
                    "performance",
                    "speech",
                    "50k",
                    "20k",
                    "ttm",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Text-based controllable generation of speech and music is an important research topic in the field of audio generation. Recent developments in TTS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speak</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kharitonov2023speak</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qiang2024minimally</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qiang2024high</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi24f_interspeech</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhu2025zipvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">eskimez2024e2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">10889461</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have achieved impressive results through zero-shot voice cloning and controllable generation. TTM </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">copet2023simple</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2025ace</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">suno2024</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">udio2024</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2025diffrhythm+</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> has advanced with models such as MusicGen </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">copet2023simple</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and ACE-Step </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2025ace</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, alongside commercial systems like Suno</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">suno2024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Udio</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">udio2024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Despite these advances, instruction-controlled speech and music generation remains a challenging problem in audio processing.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "ttm",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Existing TTS models excel at either zero-shot voice cloning </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> or style control</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ji2024controlspeech</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2023prompttts</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, but lack text-based control over multiple acoustic attributes through natural language descriptions. For instance, while CosyVoice </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and ControlSpeech </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ji2024controlspeech</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> support text-based emotion and style control, they require additional reference audio for timbre attributes and cannot handle text-controlled dialogue generation. Similarly, current TTM models exhibit limited control capabilities. DiffRhythm+</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2025diffrhythm+</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> supports text-based control of genre, instrument, rhythm, and atmosphere but lacks singer timbre control (e.g., gender and age). ACE-Step </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong2025ace</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, one of the state-of-the-art (SOTA) open-source music generation model, provides text-based control for all acoustic attributes but focuses exclusively on music without unified speech modeling capabilities.\nSpeech and music generation are typically treated as separate tasks (TTS &amp; TTM), overlooking their shared acoustic modeling abilities and control mechanisms.\nThis separation stems from the difficulty of aligning inputs across TTS and TTM tasks, as speech control involves acoustic attributes such as timbre and paralinguistics, while music generation requires musical attributes such as genre, instrumentation, and rhythm.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "ttm",
                    "control",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Vevo2 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2025vevo2</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduced the first unified speech and singing generation framework, demonstrating that joint modeling leverages rich speech data to improve singing quality while utilizing singing&#8217;s expressive characteristics to enhance TTS. However, Vevo2 relies on reference audio for acoustic attribute control rather than text instructions and generates only vocals without instrumental music capabilities.\nUniAudio </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2023uniaudio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> builds upon the VALL-E </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> framework to create a single model capable of executing multiple tasks; however, it requires inconsistent input formats across different tasks and necessitates task-specific fine-tuning. AudioBox </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vyas2023audiobox</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a flow-matching-based unified model supporting multiple tasks, pre-trains on speech, music, and sound effect data but ultimately supports only speech and sound effect generation. AudioLDM 2 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024audioldm</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> proposes a two-stage model applicable to speech, sound, and music generation, yet requires different model architecture hyperparameters for each task.\nCurrent approaches lack text-based (natural language descriptions) control mechanisms, limiting their ability to achieve unified speech and music generation. To address these limitations, we propose InstructAudio, an instruction-controlled unified framework for speech and music generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "control",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our method makes three key contributions:\na) We introduce a joint modeling framework for speech and music generation based on a multimodal diffusion transformer (MM-DiT) architecture.\nb) We achieve unified text-based control over TTS and TTM through natural language descriptions (a standardized instruction-phoneme input format), encompassing timbre attributes (gender, age), paralinguistic attributes (emotion, style, accent), and musical attributes (genre, instrument, rhythm, atmosphere), while supporting dialogue speech generation.\nc) Experimental results demonstrate best performance in instruction-based TTS (best WER, speaker similarity, emotion similarity, classification control accuracy, and distortion/error metrics) while maintaining competitive TTM capabilities (best SongEval metrics), validating the effectiveness of our unified approach.</span>\n</p>\n\n",
                "matched_terms": [
                    "control",
                    "songeval",
                    "performance",
                    "speech",
                    "accuracy",
                    "ttm",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As illustrated in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, InstructAudio introduces a novel approach for unified speech and music generation, drawing inspiration from MM-Audio </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025mmaudio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> through its MM-DiT architecture. The model comprises two core components: joint diffusion transformer layers and single diffusion transformer layers.\nDuring training, the instruct encoder, mel-encoder, and mel-decoder remain frozen as pretrained modules. The first-layer joint diffusion transformer takes temporally concatenated instruct embedding and phoneme embedding as text modal conditioning input, and noised mel vae latents as audio modal input (see Section </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S2.SS4\" style=\"font-size:90%;\" title=\"2.4 Multimodal Diffusion Transformer Architecture &#8227; 2 Method &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">2.4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nUnlike existing non-autoregressive architectures</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhu2025zipvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">eskimez2024e2</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, InstructAudio eliminates the need for text-upsampling alignment with audio representations. For TTS, the model achieves multi-attribute control, including gender, age, emotion, style, and accent, through natural language instructions, and also supports two-speaker dialogue generation. In TTM, it similarly enables multi-attribute control covering singer timbre (e.g., gender and age), music genre, instrumentation, melody, and emotional expression.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "ttm",
                    "control",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To enable the unified generation of speech and singing, we designed a standardized instruction-phoneme input format that aligns both tasks, as illustrated in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18487v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; InstructAudio: Unified speech and music generation with natural language instruction\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This format consists of two primary components: an instruction description and a phoneme sequence. The instruction description is a natural language prompt specifying the desired acoustic attributes of the output. For speech synthesis, these attributes include speaker characteristics such as gender, age, emotion, style, and accent. In dialogue scenarios, we provide separate descriptions for each of the two speakers and prepend special tokens, [S0] and [S1], to their respective text inputs to differentiate the utterances. Similarly, for music generation, the description specifies attributes like the singer&#8217;s gender and age, music genre, instrumentation, melody, and emotion. For both tasks, the input text (for speech) or lyrics (for music) is converted into phonemes using a Grapheme-to-Phoneme (G2P) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qiang2022back</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model.\nThis unified representation allows a single model to seamlessly process input for both speech and music generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The Latent Audio Codec extends our previous SecoustiCodec framework </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qiang2025secousticodec</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qiang2025vq</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qiang2024learning</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and comprises three core components: a mel-encoder, mel-decoder, and discriminator. The VAE architecture enables the model to learn continuous and complete distributions in the latent space, significantly enhancing audio representation capabilities. This high compression ratio facilitates efficient MM-DiT training while improving reconstruction quality for both speech and music. Due to space constraints, we omit the analysis of audio representation effects from this work. For comprehensive results, readers are referred to Section 4.5 of our prior work on Kling-Foley</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025kling</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We collect 50K hours of speech and 20K hours of music data from internet sources, applying our internal data processing pipeline to generate instruction descriptions and text/lyrics annotations. For speech data, descriptions encompass gender, age, emotion, style, and accent attributes. Music data descriptions include genre, instrument, gender, age, rhythm, and atmosphere characteristics. Audio clips (2-20s) maintain 1:1 Chinese-English and male-female ratios, with 90%+ neutral emotions and 0.5% dialogue data, standardized to 44.1kHz sampling rate.</span>\n</p>\n\n",
                "matched_terms": [
                    "20k",
                    "speech",
                    "50k",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We employ comprehensive objective and subjective metrics to ensure thorough evaluation. Objective metrics include Word Error Rate (WER) using Seed-TTS</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Speaker Similarity</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/resemble-ai/Resemblyzer\" title=\"\">https://github.com/resemble-ai/Resemblyzer</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Emotion Similarity</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">2</sup>\n        <span class=\"ltx_tag ltx_tag_note\">2</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/emotion2vec\" title=\"\">https://huggingface.co/emotion2vec</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Log-Spectral Distance (LSD), Mel-Cepstral Distortion (MCD), Mean Squared Error of Pitch (MSEP), Voiced/Unvoiced Mismatch Rate (MR), SongEval</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yao2025songeval</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> music evaluation benchmark, and classification control accuracy through perceptual consistency assessment. Subjective evaluation employs Quality Mean Opinion Score (QMOS), Naturalness Mean Opinion Score (NMOS), and Musicality Mean Opinion Score (MMOS), conducted by professionally trained evaluators. Classification control accuracy is evaluated through human listening tests, where annotators select the perceptually consistent category from predefined options (e.g., choosing among \"child,\" \"young/middle-aged,\" or \"elderly\" for age attributes).\nFor WER evaluation, we utilize the complete Seed-TTS benchmark test set. For instruction-based TTS tasks, we construct a manually annotated test set of 500 samples with natural language descriptions covering multiple attributes (gender, age, emotion, style, accent), selecting 100 samples for subjective evaluation. Similarly, for music tasks, we create a 500-sample test set with natural language descriptions of musical attributes (gender, age, genre, instrumentation, melody, emotion), with 100 samples selected for subjective evaluation. The distribution across categories within each attribute is uniform. Notably, InstructAudio&#8217;s conditioning input consists of descriptive text corresponding to ground truth, with similarity and distortion/error calculations performed against ground truth.</span>\n</p>\n\n",
                "matched_terms": [
                    "control",
                    "evaluation",
                    "accuracy",
                    "subjective",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">While the text-only control mechanism achieves unified input formatting for TTS and TTM\ntasks, it introduces inherent information loss compared to audio modalities, resulting in one-to-many mapping ambiguity. This leads to averaged audio quality and naturalness compared to reference audio-based methods, as evidenced by lower NMOS scores in both TTS and TTM tasks.\nAdditionally, to enable joint speech-music modeling, we constrain music generation to 5-20 second clips, limiting long-form music generation capabilities. Our evaluation focuses on short segments to match speech durations, which is detrimental to models like DiffRhythm+ that are optimized for full-length music generation. These limitations above, we expect to address in future work.</span>\n</p>\n\n",
                "matched_terms": [
                    "control",
                    "evaluation",
                    "speech",
                    "ttm",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper presents InstructAudio, an instruction-controlled unified framework for speech and music generation based on a MM-DiT architecture, demonstrating the effectiveness of joint TTS and TTM modeling. Our main contributions include: (1) introducing the first instruction-controlled unified framework for speech and music generation that eliminates reference audio requirements in attribute control; (2) achieving comprehensive controllability over timbre, paralinguistic, and musical attributes through standardized instruction-phoneme input formatting (natural language descriptions); and (3) demonstrating SOTA performance in instruction-based TTS while maintaining competitive music generation capabilities. Experimental validation confirms our approach&#8217;s effectiveness, achieving optimal results across multiple metrics including WER, similarity measures, and classification control accuracy. InstructAudio demonstrates the viability of unified audio generation frameworks. Future work will explore multimodal control mechanisms to address quality and naturalness issues, support longer music generation, and incorporate sound effect generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "control",
                    "performance",
                    "speech",
                    "accuracy",
                    "ttm",
                    "music"
                ]
            }
        ]
    }
}