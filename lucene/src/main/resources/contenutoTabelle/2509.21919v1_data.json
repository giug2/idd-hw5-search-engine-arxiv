{
    "S2.T1": {
        "caption": "Table 1: Dataset mapping of spatial attributes to human-readable captions and value ranges.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Attribute</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Caption</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Value Range</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"7\"><span class=\"ltx_text\" style=\"font-size:90%;\">Azimuth (<sup class=\"ltx_sup\">&#8728;</sup>)</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">left</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><math alttext=\"(-100,-80)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m2\" intent=\":literal\"><semantics><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mrow><mo mathsize=\"0.900em\">&#8722;</mo><mn mathsize=\"0.900em\">100</mn></mrow><mo mathsize=\"0.900em\">,</mo><mrow><mo mathsize=\"0.900em\">&#8722;</mo><mn mathsize=\"0.900em\">80</mn></mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><annotation encoding=\"application/x-tex\">(-100,-80)</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">front left</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"(-35,-55)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m3\" intent=\":literal\"><semantics><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mrow><mo mathsize=\"0.900em\">&#8722;</mo><mn mathsize=\"0.900em\">35</mn></mrow><mo mathsize=\"0.900em\">,</mo><mrow><mo mathsize=\"0.900em\">&#8722;</mo><mn mathsize=\"0.900em\">55</mn></mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><annotation encoding=\"application/x-tex\">(-35,-55)</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">front (OM)</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"(-10,10)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m4\" intent=\":literal\"><semantics><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mrow><mo mathsize=\"0.900em\">&#8722;</mo><mn mathsize=\"0.900em\">10</mn></mrow><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">10</mn><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><annotation encoding=\"application/x-tex\">(-10,10)</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">front right</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"(35,55)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m5\" intent=\":literal\"><semantics><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mn mathsize=\"0.900em\">35</mn><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">55</mn><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><annotation encoding=\"application/x-tex\">(35,55)</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">right</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"(80,100)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m6\" intent=\":literal\"><semantics><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mn mathsize=\"0.900em\">80</mn><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">100</mn><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><annotation encoding=\"application/x-tex\">(80,100)</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">right back</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"(125,145)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m7\" intent=\":literal\"><semantics><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mn mathsize=\"0.900em\">125</mn><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">145</mn><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><annotation encoding=\"application/x-tex\">(125,145)</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">back</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"(-170,-180),(170,180)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m8\" intent=\":literal\"><semantics><mrow><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mrow><mo mathsize=\"0.900em\">&#8722;</mo><mn mathsize=\"0.900em\">170</mn></mrow><mo mathsize=\"0.900em\">,</mo><mrow><mo mathsize=\"0.900em\">&#8722;</mo><mn mathsize=\"0.900em\">180</mn></mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><mo mathsize=\"0.900em\">,</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mn mathsize=\"0.900em\">170</mn><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">180</mn><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">(-170,-180),(170,180)</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">left back</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"(-125,-145)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m9\" intent=\":literal\"><semantics><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mrow><mo mathsize=\"0.900em\">&#8722;</mo><mn mathsize=\"0.900em\">125</mn></mrow><mo mathsize=\"0.900em\">,</mo><mrow><mo mathsize=\"0.900em\">&#8722;</mo><mn mathsize=\"0.900em\">145</mn></mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><annotation encoding=\"application/x-tex\">(-125,-145)</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\"><span class=\"ltx_text\" style=\"font-size:90%;\">Elevation (<sup class=\"ltx_sup\">&#8728;</sup>)</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Up</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><math alttext=\"(70,90)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m11\" intent=\":literal\"><semantics><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mn mathsize=\"0.900em\">70</mn><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">90</mn><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><annotation encoding=\"application/x-tex\">(70,90)</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Middle (OM)</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"(-10,10)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m12\" intent=\":literal\"><semantics><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mrow><mo mathsize=\"0.900em\">&#8722;</mo><mn mathsize=\"0.900em\">10</mn></mrow><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">10</mn><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><annotation encoding=\"application/x-tex\">(-10,10)</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Down</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"(-70,-90)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m13\" intent=\":literal\"><semantics><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mrow><mo mathsize=\"0.900em\">&#8722;</mo><mn mathsize=\"0.900em\">70</mn></mrow><mo mathsize=\"0.900em\">,</mo><mrow><mo mathsize=\"0.900em\">&#8722;</mo><mn mathsize=\"0.900em\">90</mn></mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><annotation encoding=\"application/x-tex\">(-70,-90)</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text\" style=\"font-size:90%;\">Distance (m)</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Very close</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><math alttext=\"(0.3,0.6)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m14\" intent=\":literal\"><semantics><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mn mathsize=\"0.900em\">0.3</mn><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">0.6</mn><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><annotation encoding=\"application/x-tex\">(0.3,0.6)</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Close</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"(0.5,1)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m15\" intent=\":literal\"><semantics><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mn mathsize=\"0.900em\">0.5</mn><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">1</mn><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><annotation encoding=\"application/x-tex\">(0.5,1)</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Moderate(OM)</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"(1,3)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m16\" intent=\":literal\"><semantics><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mn mathsize=\"0.900em\">1</mn><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">3</mn><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><annotation encoding=\"application/x-tex\">(1,3)</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">Far</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><math alttext=\"(3,10)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m17\" intent=\":literal\"><semantics><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mn mathsize=\"0.900em\">3</mn><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">10</mn><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><annotation encoding=\"application/x-tex\">(3,10)</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "spatial",
            "down",
            "left",
            "−10101010",
            "moderateom",
            "very",
            "far",
            "humanreadable",
            "elevation",
            "caption",
            "−170−180170180170180170180",
            "distance",
            "back",
            "range",
            "value",
            "−35−553555",
            "front",
            "azimuth",
            "middle",
            "ranges",
            "attribute",
            "right",
            "−100−8010080",
            "attributes",
            "−70−907090",
            "−125−145125145",
            "mapping",
            "close",
            "dataset",
            "captions"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Due to the lack of large-scale datasets containing spatial audio with explicit trajectories, we curated a synthetic dataset tailored for text-to-trajectory training. Our work builds upon the AudioTime dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib17\" title=\"\">17</a>]</cite>, which consists of 5,000 mono audio clips with precise timestamp annotations. Since AudioTime includes clips with multiple overlapping events, we separate the data to contain only single-source events, yielding 7,685 clean clips. To simulate spatial trajectory, we randomly assigned start and end positions for each event using azimuth, elevation, and distance categories derived from human-perceptual ranges (see Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#S2.T1\" title=\"Table 1 &#8227; 2.4 Dataset &#8227; 2 Methodology &#8227; Text2Move: Text-to-moving sound generation via trajectory prediction and temporal alignment\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>), similar to &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib2\" title=\"\">2</a>]</cite>. This randomization process was repeated ten times per file, resulting in a corpus of 76,850 spatialized samples (213 hours, 90%/10% train/test split). Each trajectory corresponds to a source moving linearly from its assigned start to end position at a constant speed with a sampling rate of 20Hz. We then simulate the binaural moving audio by convolving the audio source with the HRIR in each frame using an HRTF library&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib18\" title=\"\">18</a>]</cite>. To provide a natural caption about the spatial motion, we used GPT-4 to write human-readable text descriptions according to the audio event and spatial attributes. We encouraged lexical diversity by requiring synonyms for audio events, directions, and distance descriptions. For sanity reasons, we ramdonly (50%) omit captions (OM) where common attributes do not require explicit description. This dataset serves as the foundation for training and evaluating our proposed text-to-moving sound framework.</p>\n\n",
            "<p class=\"ltx_p\">where <math alttext=\"\\hat{y}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\hat{y}_{i}</annotation></semantics></math> is the predicted value, <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> is the ground-truth, and <math alttext=\"\\mathcal{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><annotation encoding=\"application/x-tex\">\\mathcal{R}</annotation></semantics></math> denotes the set of predefined valid ranges for the attributes&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#S2.T1\" title=\"Table 1 &#8227; 2.4 Dataset &#8227; 2 Methodology &#8227; Text2Move: Text-to-moving sound generation via trajectory prediction and temporal alignment\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The operator <math alttext=\"\\Pi_{r}(y_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi mathvariant=\"normal\">&#928;</mi><mi>r</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\Pi_{r}(y_{i})</annotation></semantics></math> projects <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> into its corresponding range <math alttext=\"r\\in\\mathcal{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi></mrow><annotation encoding=\"application/x-tex\">r\\in\\mathcal{R}</annotation></semantics></math>, so the error is measured relative to the nearest boundary within that range. We found the predicted trajectories in general fall within the boundaries of the pre-defined ranges, although it slightly drifts slightly in an acceptable scope.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Human auditory perception is shaped by moving sound sources in 3D space, yet prior work in generative sound modelling has largely been restricted to mono signals or static spatial audio. In this work, we introduce a framework for generating moving sounds given text prompts in a controllable fashion. To enable training, we construct a synthetic dataset that records moving sounds in binaural format, their spatial trajectories, and text captions about the sound event and spatial motion. Using this dataset, we train a text-to-trajectory prediction model that outputs the three-dimensional trajectory of a moving sound source given text prompts. To generate spatial audio, we first fine-tune a pre-trained text-to-audio generative model to output temporally aligned mono sound with the trajectory. The spatial audio is then simulated using the predicted temporally-aligned trajectory. Experimental evaluation demonstrates reasonable spatial understanding of the text-to-trajectory model. This approach could be easily integrated into existing text-to-audio generative workflow and extended to moving sound generation in other spatial audio formats.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "captions",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spatial audio plays an essential role in our everyday life, from navigating the sound source to augmenting immersive experiences for audio-visual media. In recent years, with the development of generative artificial intelligence and multi-modal learning, modelling spatial audio using another modality such as video&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib1\" title=\"\">1</a>]</cite> or text&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib2\" title=\"\">2</a>]</cite> is emerging to gain more attention. In the domain of text-to-spatial sound modelling, existing works focus on introducing end-to-end architectures generating either the four-channel First-order Ambisonics (FOA) waveforms&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib4\" title=\"\">4</a>]</cite> or binaural audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib6\" title=\"\">6</a>]</cite>. However, most of the spatial attributes captured in these works were only static, meaning that the sound source remained still throughout time. In reality, many sound sources are constantly moving while emitting sound waves.</p>\n\n",
                "matched_terms": [
                    "attributes",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A common approach towards moving sound modelling is via object-based audio, where a mono sound source and its spatial-temporal metadata are used to simulate its movements&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib7\" title=\"\">7</a>]</cite>. Such metadata usually captures essential spatial trajectories of the sound source throughout time, such as azimuth, elevation, distance, reverberation, etc. Parallel to the developments in audio generation, research on trajectory and motion prediction from language or vision inputs has advanced in fields such as robotics&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib8\" title=\"\">8</a>]</cite>, autonomous driving&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib9\" title=\"\">9</a>]</cite>, and human motion&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib10\" title=\"\">10</a>]</cite> synthesis. Models in these domains demonstrate that text&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib11\" title=\"\">11</a>]</cite> or vision&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib13\" title=\"\">13</a>]</cite> can provide strong semantic priors for predicting motion patterns in space. Yet, this paradigm has not been fully exploited in the context of audio trajectories, where motion paths can serve as explicit conditioning for spatialization.</p>\n\n",
                "matched_terms": [
                    "elevation",
                    "azimuth",
                    "distance",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce a new framework for moving sound generation. Our approach addresses the research gap by decomposing the problem into two components: (i) a text-to-trajectory prediction model, which outputs the spatial-temporal trajectory of an audio object given text prompts, and (ii) a synchronized text-to-audio pipeline, where a fine-tuned generative model produces temporally aligned mono sound that is later spatialized according to the predicted trajectory. To support training and evaluation, we construct a synthetic dataset consisting of (a) binaural audio simulated using HRTF-based rendering, (b) GPT-augmented textual captions describing sound events and their spatial attributes, and (c) ground-truth trajectories parameterized by pre-defined spatial attributes. To the best of our knowledge, this work is the first to explicitly bridge text, trajectories, and audio in a unified framework for spatial audio generation.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "attributes",
                    "captions",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text encoder.</span> We employ a DistilBERT backbone&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib14\" title=\"\">14</a>]</cite> (hidden size 768) as our primary text encoder. Input captions are tokenized and passed through the transformer to produce contextualized embeddings. We then apply a learnable attention pooling vector and project it to a 512-dimensional semantic vector, which forms as a shared latent space (via Linear&#8201;<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>&#8201;GELU&#8201;<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>&#8201;LayerNorm). This design enables the model to capture semantic attributes from captions that describe the relative spatial movements of the sound source.</p>\n\n",
                "matched_terms": [
                    "attributes",
                    "captions",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Trajectory decoder.</span> The decoder is a transformer model that operates over the temporal dimension using a 4-layer transformer encoder, each configured with 8 attention heads, hidden size 512, feed-forward expansion factor 4, and dropout 0.1. At each timestep, the input is the concatenation of the 512-dimensional text embedding and the temporal embedding, as well as the 64-dimensional per-step features, which are then projected into a shared hidden space before being processed by self-attention layers. A lightweight regression head (two linear layers with GELU and LayerNorm) maps the hidden states to per-step outputs: azimuth, elevation, and distance. To ensure physically meaningful predictions, azimuth and elevation are constrained using <math alttext=\"\\tanh\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m1\" intent=\":literal\"><semantics><mi>tanh</mi><annotation encoding=\"application/x-tex\">\\tanh</annotation></semantics></math> activations scaled by fixed multipliers (<math alttext=\"\\pm 180^{\\circ}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m2\" intent=\":literal\"><semantics><mrow><mo>&#177;</mo><msup><mn>180</mn><mo>&#8728;</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\pm 180^{\\circ}</annotation></semantics></math> and <math alttext=\"\\pm 90^{\\circ}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><mo>&#177;</mo><msup><mn>90</mn><mo>&#8728;</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\pm 90^{\\circ}</annotation></semantics></math>), while distance is constrained with a softplus transformation to enforce positivity.</p>\n\n",
                "matched_terms": [
                    "elevation",
                    "azimuth",
                    "distance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Loss function.</span> Our training optimizes two objectives. The first is a trajectory loss that computes masked mean absolute error over the predicted and ground-truth curves, using circular L1 error for angular dimensions and standard L1 for distance. At each time step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, the model predicts azimuth <math alttext=\"\\hat{a}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m2\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>a</mi><mo>^</mo></mover><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\hat{a}_{t}</annotation></semantics></math>, elevation <math alttext=\"\\hat{e}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m3\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>e</mi><mo>^</mo></mover><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\hat{e}_{t}</annotation></semantics></math>, and distance <math alttext=\"\\hat{d}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>d</mi><mo>^</mo></mover><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\hat{d}_{t}</annotation></semantics></math>, while the ground truth is <math alttext=\"a_{t},e_{t},d_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m5\" intent=\":literal\"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>,</mo><msub><mi>e</mi><mi>t</mi></msub><mo>,</mo><msub><mi>d</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">a_{t},e_{t},d_{t}</annotation></semantics></math>. We introduce a validity mask <math alttext=\"m_{t}\\in\\{0,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m6\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>t</mi></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m_{t}\\in\\{0,1\\}</annotation></semantics></math> to restrict the computation to valid steps, and define the circular error as <math alttext=\"\\Delta^{\\circ}(x,y)=\\min\\big(|x-y|,\\,360-|x-y|\\big)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m7\" intent=\":literal\"><semantics><mrow><mrow><msup><mi mathvariant=\"normal\">&#916;</mi><mo>&#8728;</mo></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>min</mi><mo>&#8289;</mo><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">(</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mi>x</mi><mo>&#8722;</mo><mi>y</mi></mrow><mo stretchy=\"false\">|</mo></mrow><mo>,</mo><mrow><mn>&#8201;360</mn><mo>&#8722;</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mi>x</mi><mo>&#8722;</mo><mi>y</mi></mrow><mo stretchy=\"false\">|</mo></mrow></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta^{\\circ}(x,y)=\\min\\big(|x-y|,\\,360-|x-y|\\big)</annotation></semantics></math>. The three coordinates are balanced by weights <math alttext=\"w_{\\text{az}},w_{\\text{el}},w_{\\text{ds}}&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m8\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>w</mi><mtext>az</mtext></msub><mo>,</mo><msub><mi>w</mi><mtext>el</mtext></msub><mo>,</mo><msub><mi>w</mi><mtext>ds</mtext></msub></mrow><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">w_{\\text{az}},w_{\\text{el}},w_{\\text{ds}}&gt;0</annotation></semantics></math>. The trajectory loss then penalizes the per-step errors over the full (padded) trajectory:</p>\n\n",
                "matched_terms": [
                    "elevation",
                    "azimuth",
                    "distance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the trajectory-based framework, we also established a simpler baseline model that directly predicts six spatial parameters from text, namely the start and endpoints of azimuth, elevation, and distance. In this formulation, temporal encoder is discarded and the task is simplified to regressing the fixed endpoints (start and end) of a spatial trajectory. We use the same semantic encoder as above followed by a projection layer and a lightweight transformer encoder that models contextual dependencies within the text representation. The resulting embedding is passed to a regression head that outputs six normalized parameters. During training, predictions are optimized using a mean squared error (MSE) loss against the ground truth metadata. We use this design to understand the performance between naively regressing endpoints and our proposed trajectory prediction model.</p>\n\n",
                "matched_terms": [
                    "elevation",
                    "azimuth",
                    "distance",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We trained the text-to-trajectory model with AdamW (<math alttext=\"\\text{lr}=1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><mtext>lr</mtext><mo>=</mo><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{lr}=1\\times 10^{-5}</annotation></semantics></math>, weight decay <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math>) for <math alttext=\"10{,}000\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.p1.m3\" intent=\":literal\"><semantics><mrow><mn>10</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding=\"application/x-tex\">10{,}000</annotation></semantics></math> epochs using a batch size of <math alttext=\"256\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.p1.m4\" intent=\":literal\"><semantics><mn>256</mn><annotation encoding=\"application/x-tex\">256</annotation></semantics></math>, mixed-precision training, and gradient clipping at <math alttext=\"1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.p1.m5\" intent=\":literal\"><semantics><mn>1.0</mn><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math>. The objective combined masked mean absolute error on azimuth, elevation, and distance with an auxiliary start&#8211;end alignment loss. For the temporal alignment model, we froze the latent diffusion backbone and optimized only the temporal adjuster with AdamW (<math alttext=\"\\text{lr}=1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.p1.m6\" intent=\":literal\"><semantics><mrow><mtext>lr</mtext><mo>=</mo><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{lr}=1\\times 10^{-4}</annotation></semantics></math>) for <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.p1.m7\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> epochs, using a masked reconstruction loss applied outside the annotated temporal window.</p>\n\n",
                "matched_terms": [
                    "elevation",
                    "azimuth",
                    "distance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#S3.T2\" title=\"Table 2 &#8227; 3.1 Trajectory Prediction &#8227; 3 Results &#8227; Text2Move: Text-to-moving sound generation via trajectory prediction and temporal alignment\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we show the evaluation results using the test dataset. The naive model predicts near-perfect metadata in most metrics, which demonstrates the capability of our text encoder in understanding spatial semantics. Our proposed trajectory prediction model shows that it is possible to generate temporally aligned spatial motion directly from text. Although the accuracies are lower than the simplified naive adaptation, the MAE suggests a reasonable deviation range (18.53<sup class=\"ltx_sup\">&#8728;</sup> for Azimuth and 28.75<sup class=\"ltx_sup\">&#8728;</sup> for elevation). For distance prediction, we observed larger deviations, but we hypothesize this is because the spatial coordinates vary in ranges in our dataset. Therefore, we report the Range-Aware MAE to account for the drifts in prediction:</p>\n\n",
                "matched_terms": [
                    "elevation",
                    "spatial",
                    "azimuth",
                    "distance",
                    "ranges",
                    "dataset",
                    "range"
                ]
            }
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Prediction results of naive and whole-trajectory model. Metrics include range-aware classification (Accuracy, Macro-F1) and regression (MAE and range-aware MAE).",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Attribute</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Accuracy</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Macro-F1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">MAE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">RA-MAE</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"5\"><span class=\"ltx_text ltx_font_bold\">Naive Model (End Point)</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Azimuth</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">98.2%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">98.5%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.789<sup class=\"ltx_sup\">&#8728;</sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.339<sup class=\"ltx_sup\">&#8728;</sup>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Elevation</th>\n<td class=\"ltx_td ltx_align_center\">98.0%</td>\n<td class=\"ltx_td ltx_align_center\">98.1%</td>\n<td class=\"ltx_td ltx_align_center\">6.431<sup class=\"ltx_sup\">&#8728;</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">1.352<sup class=\"ltx_sup\">&#8728;</sup>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Distance</th>\n<td class=\"ltx_td ltx_align_center\">87.5%</td>\n<td class=\"ltx_td ltx_align_center\">87.1%</td>\n<td class=\"ltx_td ltx_align_center\">0.166</td>\n<td class=\"ltx_td ltx_align_center\">0.013</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"5\"><span class=\"ltx_text ltx_font_bold\">Whole-trajectory prediction</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Azimuth</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">75.9%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">75.1%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">18.53<sup class=\"ltx_sup\">&#8728;</sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">15.52<sup class=\"ltx_sup\">&#8728;</sup>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Elevation</th>\n<td class=\"ltx_td ltx_align_center\">61.2%</td>\n<td class=\"ltx_td ltx_align_center\">65.9%</td>\n<td class=\"ltx_td ltx_align_center\">28.75<sup class=\"ltx_sup\">&#8728;</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">21.44<sup class=\"ltx_sup\">&#8728;</sup>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Distance</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">66.7%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">52.1%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1.601</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.365</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "rangeaware",
            "6431∘",
            "include",
            "1552∘",
            "mae",
            "5789∘",
            "2875∘",
            "elevation",
            "wholetrajectory",
            "1352∘",
            "ramae",
            "prediction",
            "distance",
            "accuracy",
            "2144∘",
            "naive",
            "metrics",
            "azimuth",
            "2339∘",
            "1853∘",
            "classification",
            "results",
            "regression",
            "end",
            "attribute",
            "point",
            "model",
            "macrof1"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#S3.T2\" title=\"Table 2 &#8227; 3.1 Trajectory Prediction &#8227; 3 Results &#8227; Text2Move: Text-to-moving sound generation via trajectory prediction and temporal alignment\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we show the evaluation results using the test dataset. The naive model predicts near-perfect metadata in most metrics, which demonstrates the capability of our text encoder in understanding spatial semantics. Our proposed trajectory prediction model shows that it is possible to generate temporally aligned spatial motion directly from text. Although the accuracies are lower than the simplified naive adaptation, the MAE suggests a reasonable deviation range (18.53<sup class=\"ltx_sup\">&#8728;</sup> for Azimuth and 28.75<sup class=\"ltx_sup\">&#8728;</sup> for elevation). For distance prediction, we observed larger deviations, but we hypothesize this is because the spatial coordinates vary in ranges in our dataset. Therefore, we report the Range-Aware MAE to account for the drifts in prediction:</p>\n\n",
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#S3.T2\" title=\"Table 2 &#8227; 3.1 Trajectory Prediction &#8227; 3 Results &#8227; Text2Move: Text-to-moving sound generation via trajectory prediction and temporal alignment\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, both our trajectory predictor and temporal modifier models are able to output highly accurate temporal alignments (below 10&#8201;ms absolute error between the ground-truth and predicted start/end timestamps). The OLR (Overlap Ratio) measures the overlap between the predicted and ground-truth active windows:</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Human auditory perception is shaped by moving sound sources in 3D space, yet prior work in generative sound modelling has largely been restricted to mono signals or static spatial audio. In this work, we introduce a framework for generating moving sounds given text prompts in a controllable fashion. To enable training, we construct a synthetic dataset that records moving sounds in binaural format, their spatial trajectories, and text captions about the sound event and spatial motion. Using this dataset, we train a text-to-trajectory prediction model that outputs the three-dimensional trajectory of a moving sound source given text prompts. To generate spatial audio, we first fine-tune a pre-trained text-to-audio generative model to output temporally aligned mono sound with the trajectory. The spatial audio is then simulated using the predicted temporally-aligned trajectory. Experimental evaluation demonstrates reasonable spatial understanding of the text-to-trajectory model. This approach could be easily integrated into existing text-to-audio generative workflow and extended to moving sound generation in other spatial audio formats.</p>\n\n",
                "matched_terms": [
                    "model",
                    "prediction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A common approach towards moving sound modelling is via object-based audio, where a mono sound source and its spatial-temporal metadata are used to simulate its movements&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib7\" title=\"\">7</a>]</cite>. Such metadata usually captures essential spatial trajectories of the sound source throughout time, such as azimuth, elevation, distance, reverberation, etc. Parallel to the developments in audio generation, research on trajectory and motion prediction from language or vision inputs has advanced in fields such as robotics&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib8\" title=\"\">8</a>]</cite>, autonomous driving&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib9\" title=\"\">9</a>]</cite>, and human motion&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib10\" title=\"\">10</a>]</cite> synthesis. Models in these domains demonstrate that text&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib11\" title=\"\">11</a>]</cite> or vision&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib13\" title=\"\">13</a>]</cite> can provide strong semantic priors for predicting motion patterns in space. Yet, this paradigm has not been fully exploited in the context of audio trajectories, where motion paths can serve as explicit conditioning for spatialization.</p>\n\n",
                "matched_terms": [
                    "elevation",
                    "azimuth",
                    "distance",
                    "prediction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce a new framework for moving sound generation. Our approach addresses the research gap by decomposing the problem into two components: (i) a text-to-trajectory prediction model, which outputs the spatial-temporal trajectory of an audio object given text prompts, and (ii) a synchronized text-to-audio pipeline, where a fine-tuned generative model produces temporally aligned mono sound that is later spatialized according to the predicted trajectory. To support training and evaluation, we construct a synthetic dataset consisting of (a) binaural audio simulated using HRTF-based rendering, (b) GPT-augmented textual captions describing sound events and their spatial attributes, and (c) ground-truth trajectories parameterized by pre-defined spatial attributes. To the best of our knowledge, this work is the first to explicitly bridge text, trajectories, and audio in a unified framework for spatial audio generation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "prediction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To accurately model a moving sound source, we need to understand its pointwise spatial locations. Our proposed framework learns to predict time-aligned spatial trajectories of a moving sound source from textual descriptions. As shown in Figure<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Text2Move: Text-to-moving sound generation via trajectory prediction and temporal alignment\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our text-to-trajectory prediction model integrates three key components: a text semantic encoder, a temporal encoder, and a trajectory decoder.</p>\n\n",
                "matched_terms": [
                    "model",
                    "prediction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Temporal encoder.</span> Temporal information is injected at both global and local levels. At the global level, the encoder takes in the start and end timestamps <math alttext=\"(t_{0},t_{1})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>0</mn></msub><mo>,</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t_{0},t_{1})</annotation></semantics></math> of an event, which are expanded through Fourier feature mappings with <math alttext=\"F{=}8\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mi>F</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">F{=}8</annotation></semantics></math> log-spaced frequencies to capture multi-scale periodic patterns. These are passed through a lightweight two-layer MLP (32&#8201;<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>&#8201;256&#8201;<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>&#8201;512 with GELU and LayerNorm) to yield a 512-dimensional temporal embedding. At the local level, each step along the trajectory is assigned a normalized index <math alttext=\"\\tau\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m5\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau\\in[0,1]</annotation></semantics></math>, which is similarly encoded with Fourier features and projected into a compact 64-dimensional per-step embedding. This provides the model with explicit knowledge of event duration and temporal alignment without requiring frame-level supervision.</p>\n\n",
                "matched_terms": [
                    "end",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Trajectory decoder.</span> The decoder is a transformer model that operates over the temporal dimension using a 4-layer transformer encoder, each configured with 8 attention heads, hidden size 512, feed-forward expansion factor 4, and dropout 0.1. At each timestep, the input is the concatenation of the 512-dimensional text embedding and the temporal embedding, as well as the 64-dimensional per-step features, which are then projected into a shared hidden space before being processed by self-attention layers. A lightweight regression head (two linear layers with GELU and LayerNorm) maps the hidden states to per-step outputs: azimuth, elevation, and distance. To ensure physically meaningful predictions, azimuth and elevation are constrained using <math alttext=\"\\tanh\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m1\" intent=\":literal\"><semantics><mi>tanh</mi><annotation encoding=\"application/x-tex\">\\tanh</annotation></semantics></math> activations scaled by fixed multipliers (<math alttext=\"\\pm 180^{\\circ}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m2\" intent=\":literal\"><semantics><mrow><mo>&#177;</mo><msup><mn>180</mn><mo>&#8728;</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\pm 180^{\\circ}</annotation></semantics></math> and <math alttext=\"\\pm 90^{\\circ}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><mo>&#177;</mo><msup><mn>90</mn><mo>&#8728;</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\pm 90^{\\circ}</annotation></semantics></math>), while distance is constrained with a softplus transformation to enforce positivity.</p>\n\n",
                "matched_terms": [
                    "elevation",
                    "model",
                    "azimuth",
                    "distance",
                    "regression"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Loss function.</span> Our training optimizes two objectives. The first is a trajectory loss that computes masked mean absolute error over the predicted and ground-truth curves, using circular L1 error for angular dimensions and standard L1 for distance. At each time step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, the model predicts azimuth <math alttext=\"\\hat{a}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m2\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>a</mi><mo>^</mo></mover><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\hat{a}_{t}</annotation></semantics></math>, elevation <math alttext=\"\\hat{e}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m3\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>e</mi><mo>^</mo></mover><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\hat{e}_{t}</annotation></semantics></math>, and distance <math alttext=\"\\hat{d}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>d</mi><mo>^</mo></mover><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\hat{d}_{t}</annotation></semantics></math>, while the ground truth is <math alttext=\"a_{t},e_{t},d_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m5\" intent=\":literal\"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>,</mo><msub><mi>e</mi><mi>t</mi></msub><mo>,</mo><msub><mi>d</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">a_{t},e_{t},d_{t}</annotation></semantics></math>. We introduce a validity mask <math alttext=\"m_{t}\\in\\{0,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m6\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>t</mi></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m_{t}\\in\\{0,1\\}</annotation></semantics></math> to restrict the computation to valid steps, and define the circular error as <math alttext=\"\\Delta^{\\circ}(x,y)=\\min\\big(|x-y|,\\,360-|x-y|\\big)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m7\" intent=\":literal\"><semantics><mrow><mrow><msup><mi mathvariant=\"normal\">&#916;</mi><mo>&#8728;</mo></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>min</mi><mo>&#8289;</mo><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">(</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mi>x</mi><mo>&#8722;</mo><mi>y</mi></mrow><mo stretchy=\"false\">|</mo></mrow><mo>,</mo><mrow><mn>&#8201;360</mn><mo>&#8722;</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mi>x</mi><mo>&#8722;</mo><mi>y</mi></mrow><mo stretchy=\"false\">|</mo></mrow></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta^{\\circ}(x,y)=\\min\\big(|x-y|,\\,360-|x-y|\\big)</annotation></semantics></math>. The three coordinates are balanced by weights <math alttext=\"w_{\\text{az}},w_{\\text{el}},w_{\\text{ds}}&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m8\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>w</mi><mtext>az</mtext></msub><mo>,</mo><msub><mi>w</mi><mtext>el</mtext></msub><mo>,</mo><msub><mi>w</mi><mtext>ds</mtext></msub></mrow><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">w_{\\text{az}},w_{\\text{el}},w_{\\text{ds}}&gt;0</annotation></semantics></math>. The trajectory loss then penalizes the per-step errors over the full (padded) trajectory:</p>\n\n",
                "matched_terms": [
                    "model",
                    "elevation",
                    "azimuth",
                    "distance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the trajectory-based framework, we also established a simpler baseline model that directly predicts six spatial parameters from text, namely the start and endpoints of azimuth, elevation, and distance. In this formulation, temporal encoder is discarded and the task is simplified to regressing the fixed endpoints (start and end) of a spatial trajectory. We use the same semantic encoder as above followed by a projection layer and a lightweight transformer encoder that models contextual dependencies within the text representation. The resulting embedding is passed to a regression head that outputs six normalized parameters. During training, predictions are optimized using a mean squared error (MSE) loss against the ground truth metadata. We use this design to understand the performance between naively regressing endpoints and our proposed trajectory prediction model.</p>\n\n",
                "matched_terms": [
                    "elevation",
                    "azimuth",
                    "model",
                    "distance",
                    "prediction",
                    "regression",
                    "end"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our setup, a pre-trained latent diffusion model (Make-an-Audio 2)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib16\" title=\"\">16</a>]</cite> serves as the text-to-audio backbone. Given a text caption and its structured description, the model generates a latent audio representation that can be used to synthesize spectrograms with a transformer-based Variational Autoencoder model. We augment this with a Temporal Modifier, a lightweight trainable module that refines the latent sequence <math alttext=\"z_{\\text{ldm}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>z</mi><mtext>ldm</mtext></msub><annotation encoding=\"application/x-tex\">z_{\\text{ldm}}</annotation></semantics></math> based on start and end timestamps <math alttext=\"(t_{0},t_{1})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>0</mn></msub><mo>,</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t_{0},t_{1})</annotation></semantics></math>. The Modifier combines convolutional layers for local temporal smoothing with an MLP that encodes the timestamp pair into a global temporal bias. We then use a binary mask to ensure that only regions outside the target temporal window are modified, while leaving the inside region unchanged. The training objective is a masked mean squared error (MSE), applied only to the outside region:</p>\n\n",
                "matched_terms": [
                    "end",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"z_{\\text{vae}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m3\" intent=\":literal\"><semantics><msub><mi>z</mi><mtext>vae</mtext></msub><annotation encoding=\"application/x-tex\">z_{\\text{vae}}</annotation></semantics></math> is the ground-truth latent representation produced from VAE, <math alttext=\"M_{\\text{out},t}=1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>M</mi><mrow><mtext>out</mtext><mo>,</mo><mi>t</mi></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M_{\\text{out},t}=1</annotation></semantics></math> if <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m5\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> is outside <math alttext=\"[t_{0},t_{1}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>t</mi><mn>0</mn></msub><mo>,</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[t_{0},t_{1}]</annotation></semantics></math> and <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m7\" intent=\":literal\"><mn>0</mn></math> otherwise. This formulation encourages the Modifier to match the ground-truth latents in all regions where no sound event is expected, while preserving the original latents inside the event boundaries. As a result, this model learns to output arbitrary-length sound effects that are temporally matched with our trajectory prediction model by explicitly conditioning the Temporal Modifier on the start and end timestamps.</p>\n\n",
                "matched_terms": [
                    "end",
                    "model",
                    "prediction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the lack of large-scale datasets containing spatial audio with explicit trajectories, we curated a synthetic dataset tailored for text-to-trajectory training. Our work builds upon the AudioTime dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib17\" title=\"\">17</a>]</cite>, which consists of 5,000 mono audio clips with precise timestamp annotations. Since AudioTime includes clips with multiple overlapping events, we separate the data to contain only single-source events, yielding 7,685 clean clips. To simulate spatial trajectory, we randomly assigned start and end positions for each event using azimuth, elevation, and distance categories derived from human-perceptual ranges (see Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#S2.T1\" title=\"Table 1 &#8227; 2.4 Dataset &#8227; 2 Methodology &#8227; Text2Move: Text-to-moving sound generation via trajectory prediction and temporal alignment\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>), similar to &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib2\" title=\"\">2</a>]</cite>. This randomization process was repeated ten times per file, resulting in a corpus of 76,850 spatialized samples (213 hours, 90%/10% train/test split). Each trajectory corresponds to a source moving linearly from its assigned start to end position at a constant speed with a sampling rate of 20Hz. We then simulate the binaural moving audio by convolving the audio source with the HRIR in each frame using an HRTF library&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib18\" title=\"\">18</a>]</cite>. To provide a natural caption about the spatial motion, we used GPT-4 to write human-readable text descriptions according to the audio event and spatial attributes. We encouraged lexical diversity by requiring synonyms for audio events, directions, and distance descriptions. For sanity reasons, we ramdonly (50%) omit captions (OM) where common attributes do not require explicit description. This dataset serves as the foundation for training and evaluating our proposed text-to-moving sound framework.</p>\n\n",
                "matched_terms": [
                    "elevation",
                    "end",
                    "azimuth",
                    "distance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We trained the text-to-trajectory model with AdamW (<math alttext=\"\\text{lr}=1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><mtext>lr</mtext><mo>=</mo><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{lr}=1\\times 10^{-5}</annotation></semantics></math>, weight decay <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math>) for <math alttext=\"10{,}000\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.p1.m3\" intent=\":literal\"><semantics><mrow><mn>10</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding=\"application/x-tex\">10{,}000</annotation></semantics></math> epochs using a batch size of <math alttext=\"256\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.p1.m4\" intent=\":literal\"><semantics><mn>256</mn><annotation encoding=\"application/x-tex\">256</annotation></semantics></math>, mixed-precision training, and gradient clipping at <math alttext=\"1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.p1.m5\" intent=\":literal\"><semantics><mn>1.0</mn><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math>. The objective combined masked mean absolute error on azimuth, elevation, and distance with an auxiliary start&#8211;end alignment loss. For the temporal alignment model, we froze the latent diffusion backbone and optimized only the temporal adjuster with AdamW (<math alttext=\"\\text{lr}=1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.p1.m6\" intent=\":literal\"><semantics><mrow><mtext>lr</mtext><mo>=</mo><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{lr}=1\\times 10^{-4}</annotation></semantics></math>) for <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.p1.m7\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> epochs, using a masked reconstruction loss applied outside the annotated temporal window.</p>\n\n",
                "matched_terms": [
                    "model",
                    "elevation",
                    "azimuth",
                    "distance"
                ]
            }
        ]
    },
    "S3.T3": {
        "caption": "Table 3: Temporal alignment performance of our approaches. Start and End MAE measure the average timing error (in seconds) at the predicted onset and offset boundaries. OLR (Overlap Ratio) quantifies the overall temporal overlap between predicted and ground-truth intervals.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Start MAE</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">End MAE</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">OLR</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Trajectory predictor</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0086</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0012</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.8596</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Temporal modifier</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0018</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0024</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.9370</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "predicted",
            "seconds",
            "overall",
            "offset",
            "mae",
            "overlap",
            "onset",
            "error",
            "our",
            "olr",
            "quantifies",
            "modifier",
            "groundtruth",
            "average",
            "predictor",
            "measure",
            "between",
            "ratio",
            "performance",
            "alignment",
            "end",
            "start",
            "intervals",
            "trajectory",
            "timing",
            "boundaries",
            "method",
            "approaches",
            "temporal"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Human auditory perception is shaped by moving sound sources in 3D space, yet prior work in generative sound modelling has largely been restricted to mono signals or static spatial audio. In this work, we introduce a framework for generating moving sounds given text prompts in a controllable fashion. To enable training, we construct a synthetic dataset that records moving sounds in binaural format, their spatial trajectories, and text captions about the sound event and spatial motion. Using this dataset, we train a text-to-trajectory prediction model that outputs the three-dimensional trajectory of a moving sound source given text prompts. To generate spatial audio, we first fine-tune a pre-trained text-to-audio generative model to output temporally aligned mono sound with the trajectory. The spatial audio is then simulated using the predicted temporally-aligned trajectory. Experimental evaluation demonstrates reasonable spatial understanding of the text-to-trajectory model. This approach could be easily integrated into existing text-to-audio generative workflow and extended to moving sound generation in other spatial audio formats.</p>\n\n",
                "matched_terms": [
                    "trajectory",
                    "predicted"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce a new framework for moving sound generation. Our approach addresses the research gap by decomposing the problem into two components: (i) a text-to-trajectory prediction model, which outputs the spatial-temporal trajectory of an audio object given text prompts, and (ii) a synchronized text-to-audio pipeline, where a fine-tuned generative model produces temporally aligned mono sound that is later spatialized according to the predicted trajectory. To support training and evaluation, we construct a synthetic dataset consisting of (a) binaural audio simulated using HRTF-based rendering, (b) GPT-augmented textual captions describing sound events and their spatial attributes, and (c) ground-truth trajectories parameterized by pre-defined spatial attributes. To the best of our knowledge, this work is the first to explicitly bridge text, trajectories, and audio in a unified framework for spatial audio generation.</p>\n\n",
                "matched_terms": [
                    "our",
                    "trajectory",
                    "groundtruth",
                    "predicted"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To accurately model a moving sound source, we need to understand its pointwise spatial locations. Our proposed framework learns to predict time-aligned spatial trajectories of a moving sound source from textual descriptions. As shown in Figure<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Text2Move: Text-to-moving sound generation via trajectory prediction and temporal alignment\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our text-to-trajectory prediction model integrates three key components: a text semantic encoder, a temporal encoder, and a trajectory decoder.</p>\n\n",
                "matched_terms": [
                    "trajectory",
                    "temporal",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Temporal encoder.</span> Temporal information is injected at both global and local levels. At the global level, the encoder takes in the start and end timestamps <math alttext=\"(t_{0},t_{1})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>0</mn></msub><mo>,</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t_{0},t_{1})</annotation></semantics></math> of an event, which are expanded through Fourier feature mappings with <math alttext=\"F{=}8\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mi>F</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">F{=}8</annotation></semantics></math> log-spaced frequencies to capture multi-scale periodic patterns. These are passed through a lightweight two-layer MLP (32&#8201;<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>&#8201;256&#8201;<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>&#8201;512 with GELU and LayerNorm) to yield a 512-dimensional temporal embedding. At the local level, each step along the trajectory is assigned a normalized index <math alttext=\"\\tau\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m5\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau\\in[0,1]</annotation></semantics></math>, which is similarly encoded with Fourier features and projected into a compact 64-dimensional per-step embedding. This provides the model with explicit knowledge of event duration and temporal alignment without requiring frame-level supervision.</p>\n\n",
                "matched_terms": [
                    "start",
                    "alignment",
                    "trajectory",
                    "end",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Trajectory decoder.</span> The decoder is a transformer model that operates over the temporal dimension using a 4-layer transformer encoder, each configured with 8 attention heads, hidden size 512, feed-forward expansion factor 4, and dropout 0.1. At each timestep, the input is the concatenation of the 512-dimensional text embedding and the temporal embedding, as well as the 64-dimensional per-step features, which are then projected into a shared hidden space before being processed by self-attention layers. A lightweight regression head (two linear layers with GELU and LayerNorm) maps the hidden states to per-step outputs: azimuth, elevation, and distance. To ensure physically meaningful predictions, azimuth and elevation are constrained using <math alttext=\"\\tanh\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m1\" intent=\":literal\"><semantics><mi>tanh</mi><annotation encoding=\"application/x-tex\">\\tanh</annotation></semantics></math> activations scaled by fixed multipliers (<math alttext=\"\\pm 180^{\\circ}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m2\" intent=\":literal\"><semantics><mrow><mo>&#177;</mo><msup><mn>180</mn><mo>&#8728;</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\pm 180^{\\circ}</annotation></semantics></math> and <math alttext=\"\\pm 90^{\\circ}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><mo>&#177;</mo><msup><mn>90</mn><mo>&#8728;</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\pm 90^{\\circ}</annotation></semantics></math>), while distance is constrained with a softplus transformation to enforce positivity.</p>\n\n",
                "matched_terms": [
                    "trajectory",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Loss function.</span> Our training optimizes two objectives. The first is a trajectory loss that computes masked mean absolute error over the predicted and ground-truth curves, using circular L1 error for angular dimensions and standard L1 for distance. At each time step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, the model predicts azimuth <math alttext=\"\\hat{a}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m2\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>a</mi><mo>^</mo></mover><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\hat{a}_{t}</annotation></semantics></math>, elevation <math alttext=\"\\hat{e}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m3\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>e</mi><mo>^</mo></mover><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\hat{e}_{t}</annotation></semantics></math>, and distance <math alttext=\"\\hat{d}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>d</mi><mo>^</mo></mover><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\hat{d}_{t}</annotation></semantics></math>, while the ground truth is <math alttext=\"a_{t},e_{t},d_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m5\" intent=\":literal\"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>,</mo><msub><mi>e</mi><mi>t</mi></msub><mo>,</mo><msub><mi>d</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">a_{t},e_{t},d_{t}</annotation></semantics></math>. We introduce a validity mask <math alttext=\"m_{t}\\in\\{0,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m6\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>t</mi></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m_{t}\\in\\{0,1\\}</annotation></semantics></math> to restrict the computation to valid steps, and define the circular error as <math alttext=\"\\Delta^{\\circ}(x,y)=\\min\\big(|x-y|,\\,360-|x-y|\\big)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m7\" intent=\":literal\"><semantics><mrow><mrow><msup><mi mathvariant=\"normal\">&#916;</mi><mo>&#8728;</mo></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>min</mi><mo>&#8289;</mo><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">(</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mi>x</mi><mo>&#8722;</mo><mi>y</mi></mrow><mo stretchy=\"false\">|</mo></mrow><mo>,</mo><mrow><mn>&#8201;360</mn><mo>&#8722;</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mi>x</mi><mo>&#8722;</mo><mi>y</mi></mrow><mo stretchy=\"false\">|</mo></mrow></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta^{\\circ}(x,y)=\\min\\big(|x-y|,\\,360-|x-y|\\big)</annotation></semantics></math>. The three coordinates are balanced by weights <math alttext=\"w_{\\text{az}},w_{\\text{el}},w_{\\text{ds}}&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m8\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>w</mi><mtext>az</mtext></msub><mo>,</mo><msub><mi>w</mi><mtext>el</mtext></msub><mo>,</mo><msub><mi>w</mi><mtext>ds</mtext></msub></mrow><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">w_{\\text{az}},w_{\\text{el}},w_{\\text{ds}}&gt;0</annotation></semantics></math>. The trajectory loss then penalizes the per-step errors over the full (padded) trajectory:</p>\n\n",
                "matched_terms": [
                    "predicted",
                    "groundtruth",
                    "trajectory",
                    "error",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second is a temporal loss that encourages the first and last predicted positions to match the annotated start and end points to ensure accurate temporal alignment:</p>\n\n",
                "matched_terms": [
                    "start",
                    "alignment",
                    "predicted",
                    "end",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the trajectory-based framework, we also established a simpler baseline model that directly predicts six spatial parameters from text, namely the start and endpoints of azimuth, elevation, and distance. In this formulation, temporal encoder is discarded and the task is simplified to regressing the fixed endpoints (start and end) of a spatial trajectory. We use the same semantic encoder as above followed by a projection layer and a lightweight transformer encoder that models contextual dependencies within the text representation. The resulting embedding is passed to a regression head that outputs six normalized parameters. During training, predictions are optimized using a mean squared error (MSE) loss against the ground truth metadata. We use this design to understand the performance between naively regressing endpoints and our proposed trajectory prediction model.</p>\n\n",
                "matched_terms": [
                    "start",
                    "temporal",
                    "trajectory",
                    "performance",
                    "between",
                    "end",
                    "error",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While text-to-audio (T2A)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib15\" title=\"\">15</a>]</cite> diffusion models have demonstrated strong generative capabilities, they often lack precise control over when an event occurs within the generated sound. To address this, we introduce a lightweight temporal alignment mechanism that adjusts latent representations from the diffusion model to respect user-provided timing constraints.</p>\n\n",
                "matched_terms": [
                    "timing",
                    "alignment",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our setup, a pre-trained latent diffusion model (Make-an-Audio 2)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib16\" title=\"\">16</a>]</cite> serves as the text-to-audio backbone. Given a text caption and its structured description, the model generates a latent audio representation that can be used to synthesize spectrograms with a transformer-based Variational Autoencoder model. We augment this with a Temporal Modifier, a lightweight trainable module that refines the latent sequence <math alttext=\"z_{\\text{ldm}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>z</mi><mtext>ldm</mtext></msub><annotation encoding=\"application/x-tex\">z_{\\text{ldm}}</annotation></semantics></math> based on start and end timestamps <math alttext=\"(t_{0},t_{1})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>0</mn></msub><mo>,</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t_{0},t_{1})</annotation></semantics></math>. The Modifier combines convolutional layers for local temporal smoothing with an MLP that encodes the timestamp pair into a global temporal bias. We then use a binary mask to ensure that only regions outside the target temporal window are modified, while leaving the inside region unchanged. The training objective is a masked mean squared error (MSE), applied only to the outside region:</p>\n\n",
                "matched_terms": [
                    "start",
                    "temporal",
                    "modifier",
                    "end",
                    "error",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"z_{\\text{vae}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m3\" intent=\":literal\"><semantics><msub><mi>z</mi><mtext>vae</mtext></msub><annotation encoding=\"application/x-tex\">z_{\\text{vae}}</annotation></semantics></math> is the ground-truth latent representation produced from VAE, <math alttext=\"M_{\\text{out},t}=1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>M</mi><mrow><mtext>out</mtext><mo>,</mo><mi>t</mi></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M_{\\text{out},t}=1</annotation></semantics></math> if <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m5\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> is outside <math alttext=\"[t_{0},t_{1}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>t</mi><mn>0</mn></msub><mo>,</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[t_{0},t_{1}]</annotation></semantics></math> and <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m7\" intent=\":literal\"><mn>0</mn></math> otherwise. This formulation encourages the Modifier to match the ground-truth latents in all regions where no sound event is expected, while preserving the original latents inside the event boundaries. As a result, this model learns to output arbitrary-length sound effects that are temporally matched with our trajectory prediction model by explicitly conditioning the Temporal Modifier on the start and end timestamps.</p>\n\n",
                "matched_terms": [
                    "start",
                    "temporal",
                    "modifier",
                    "groundtruth",
                    "trajectory",
                    "boundaries",
                    "end",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the lack of large-scale datasets containing spatial audio with explicit trajectories, we curated a synthetic dataset tailored for text-to-trajectory training. Our work builds upon the AudioTime dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib17\" title=\"\">17</a>]</cite>, which consists of 5,000 mono audio clips with precise timestamp annotations. Since AudioTime includes clips with multiple overlapping events, we separate the data to contain only single-source events, yielding 7,685 clean clips. To simulate spatial trajectory, we randomly assigned start and end positions for each event using azimuth, elevation, and distance categories derived from human-perceptual ranges (see Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#S2.T1\" title=\"Table 1 &#8227; 2.4 Dataset &#8227; 2 Methodology &#8227; Text2Move: Text-to-moving sound generation via trajectory prediction and temporal alignment\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>), similar to &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib2\" title=\"\">2</a>]</cite>. This randomization process was repeated ten times per file, resulting in a corpus of 76,850 spatialized samples (213 hours, 90%/10% train/test split). Each trajectory corresponds to a source moving linearly from its assigned start to end position at a constant speed with a sampling rate of 20Hz. We then simulate the binaural moving audio by convolving the audio source with the HRIR in each frame using an HRTF library&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#bib.bib18\" title=\"\">18</a>]</cite>. To provide a natural caption about the spatial motion, we used GPT-4 to write human-readable text descriptions according to the audio event and spatial attributes. We encouraged lexical diversity by requiring synonyms for audio events, directions, and distance descriptions. For sanity reasons, we ramdonly (50%) omit captions (OM) where common attributes do not require explicit description. This dataset serves as the foundation for training and evaluating our proposed text-to-moving sound framework.</p>\n\n",
                "matched_terms": [
                    "trajectory",
                    "end",
                    "start",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We trained the text-to-trajectory model with AdamW (<math alttext=\"\\text{lr}=1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><mtext>lr</mtext><mo>=</mo><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{lr}=1\\times 10^{-5}</annotation></semantics></math>, weight decay <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math>) for <math alttext=\"10{,}000\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.p1.m3\" intent=\":literal\"><semantics><mrow><mn>10</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding=\"application/x-tex\">10{,}000</annotation></semantics></math> epochs using a batch size of <math alttext=\"256\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.p1.m4\" intent=\":literal\"><semantics><mn>256</mn><annotation encoding=\"application/x-tex\">256</annotation></semantics></math>, mixed-precision training, and gradient clipping at <math alttext=\"1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.p1.m5\" intent=\":literal\"><semantics><mn>1.0</mn><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math>. The objective combined masked mean absolute error on azimuth, elevation, and distance with an auxiliary start&#8211;end alignment loss. For the temporal alignment model, we froze the latent diffusion backbone and optimized only the temporal adjuster with AdamW (<math alttext=\"\\text{lr}=1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.p1.m6\" intent=\":literal\"><semantics><mrow><mtext>lr</mtext><mo>=</mo><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{lr}=1\\times 10^{-4}</annotation></semantics></math>) for <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS5.p1.m7\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> epochs, using a masked reconstruction loss applied outside the annotated temporal window.</p>\n\n",
                "matched_terms": [
                    "temporal",
                    "alignment",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#S3.T2\" title=\"Table 2 &#8227; 3.1 Trajectory Prediction &#8227; 3 Results &#8227; Text2Move: Text-to-moving sound generation via trajectory prediction and temporal alignment\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we show the evaluation results using the test dataset. The naive model predicts near-perfect metadata in most metrics, which demonstrates the capability of our text encoder in understanding spatial semantics. Our proposed trajectory prediction model shows that it is possible to generate temporally aligned spatial motion directly from text. Although the accuracies are lower than the simplified naive adaptation, the MAE suggests a reasonable deviation range (18.53<sup class=\"ltx_sup\">&#8728;</sup> for Azimuth and 28.75<sup class=\"ltx_sup\">&#8728;</sup> for elevation). For distance prediction, we observed larger deviations, but we hypothesize this is because the spatial coordinates vary in ranges in our dataset. Therefore, we report the Range-Aware MAE to account for the drifts in prediction:</p>\n\n",
                "matched_terms": [
                    "trajectory",
                    "mae",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\hat{y}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\hat{y}_{i}</annotation></semantics></math> is the predicted value, <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> is the ground-truth, and <math alttext=\"\\mathcal{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><annotation encoding=\"application/x-tex\">\\mathcal{R}</annotation></semantics></math> denotes the set of predefined valid ranges for the attributes&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#S2.T1\" title=\"Table 1 &#8227; 2.4 Dataset &#8227; 2 Methodology &#8227; Text2Move: Text-to-moving sound generation via trajectory prediction and temporal alignment\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The operator <math alttext=\"\\Pi_{r}(y_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi mathvariant=\"normal\">&#928;</mi><mi>r</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\Pi_{r}(y_{i})</annotation></semantics></math> projects <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> into its corresponding range <math alttext=\"r\\in\\mathcal{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi></mrow><annotation encoding=\"application/x-tex\">r\\in\\mathcal{R}</annotation></semantics></math>, so the error is measured relative to the nearest boundary within that range. We found the predicted trajectories in general fall within the boundaries of the pre-defined ranges, although it slightly drifts slightly in an acceptable scope.</p>\n\n",
                "matched_terms": [
                    "groundtruth",
                    "error",
                    "boundaries",
                    "predicted"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21919v1#S3.T2\" title=\"Table 2 &#8227; 3.1 Trajectory Prediction &#8227; 3 Results &#8227; Text2Move: Text-to-moving sound generation via trajectory prediction and temporal alignment\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, both our trajectory predictor and temporal modifier models are able to output highly accurate temporal alignments (below 10&#8201;ms absolute error between the ground-truth and predicted start/end timestamps). The OLR (Overlap Ratio) measures the overlap between the predicted and ground-truth active windows:</p>\n\n",
                "matched_terms": [
                    "olr",
                    "predicted",
                    "temporal",
                    "modifier",
                    "groundtruth",
                    "ratio",
                    "trajectory",
                    "predictor",
                    "between",
                    "overlap",
                    "error",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"m_{\\text{pred}}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mtext>pred</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m_{\\text{pred}}(t)</annotation></semantics></math> and <math alttext=\"m_{\\text{gt}}(t)\\in\\{0,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>m</mi><mtext>gt</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m_{\\text{gt}}(t)\\in\\{0,1\\}</annotation></semantics></math> are the predicted and ground-truth activity masks(1 = active). An OLR of <math alttext=\"0.86\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mn>0.86</mn><annotation encoding=\"application/x-tex\">0.86</annotation></semantics></math> from the trajectory predictor indicates a strong temporal alignment, although in some regions it deviates slightly. The temporal modifier on the other hand, obtains an even higher OLR of <math alttext=\"0.94\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mn>0.94</mn><annotation encoding=\"application/x-tex\">0.94</annotation></semantics></math>, hypothetically due to the introduction of binary masks. These results demonstrate that both approaches are able to deliver reliable and accurate temporal control in general.</p>\n\n",
                "matched_terms": [
                    "olr",
                    "alignment",
                    "predicted",
                    "temporal",
                    "modifier",
                    "groundtruth",
                    "trajectory",
                    "predictor",
                    "approaches"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research presents text2move, a hybrid approach towards moving sound generation guided by text prompts. Different from end-to-end methods, our approach disentangles the sound generation process into a few simpler steps: text-to-trajectory prediction, mono sound generation and synchronization, as well as object-based spatialization. This framework takes advantage of pre-trained text-to-mono-audio generation models, and provides high-level controllability of modifying the semantics, dynamics and spatial characteristics of moving sounds. By temporally aligning the predicted trajecotry and generative audio, our approach can deliver accurate spatial audio movements demonstrated from our website&#160;<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://reinliu.github.io/text2move/\" title=\"\">https://reinliu.github.io/text2move/</a>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "predicted"
                ]
            }
        ]
    }
}