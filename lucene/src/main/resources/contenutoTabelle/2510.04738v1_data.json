{
    "S4.T1": {
        "source_file": "Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba",
        "caption": "Table 1: Results on RealEdit benchmark for the speech editing task.",
        "body": "Model\n\nWER (L) ↓\\downarrow\n\n\nWER (M) ↓\\downarrow\n\n\nMOS Naturalness ↑\\uparrow\n\n\nMOS Intelligibility ↑\\uparrow\n\n\n\nVoiceCraft\n8.4\n6.9\n3.77 ± 0.08\n4.20 ± 0.07\n\n\nMAVE (ours)\n7.5\n5.9\n3.90 ± 0.08\n4.25 ± 0.07\n\n\nGround Truth\n6.8\n5.2\n4.00 ± 0.08\n4.31 ± 0.06",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding:-0.25pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.25pt 5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER</span><span class=\"ltx_text\" style=\"font-size:90%;\"> (L) </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.25pt 5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER</span><span class=\"ltx_text\" style=\"font-size:90%;\"> (M) </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.25pt 5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MOS Naturalness</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.25pt 5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MOS Intelligibility</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:-0.25pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">VoiceCraft</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.77 &#177; 0.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.20 &#177; 0.07</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.25pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">MAVE (ours)</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.90 &#177; 0.08</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.25 &#177; 0.07</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding:-0.25pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Ground Truth</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-0.25pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-0.25pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-0.25pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.00 &#177; 0.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-0.25pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.31 &#177; 0.06</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "ours",
            "voicecraft",
            "intelligibility",
            "realedit",
            "wer",
            "↓downarrow",
            "speech",
            "task",
            "ground",
            "results",
            "mos",
            "mave",
            "truth",
            "benchmark",
            "editing",
            "naturalness",
            "model",
            "↑uparrow"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Results on Speech Editing &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the results of both VoiceCraft <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite> and MAVE on RealEdit dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite>, which reflects real-life speech editing cases that are both challenging and diverse. The original dataset contains 100 utterances from LibriTTS (dev-clean and dev-other) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib50\" title=\"\">50</a>]</cite>, 100 utterances from\nYouTube (from Gigaspeech testset) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib6\" title=\"\">6</a>]</cite>, and 110 utterances from the Spotify Podcast dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib8\" title=\"\">8</a>]</cite>. However, the Spotify podcasts dataset is no longer available on the official website, so we have used only the 200 audios that are still publicly available.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">MAVE</span> (<span class=\"ltx_text ltx_font_bold\">M</span>amba with Cross-<span class=\"ltx_text ltx_font_bold\">A</span>ttention for <span class=\"ltx_text ltx_font_bold\">V</span>oice <span class=\"ltx_text ltx_font_bold\">E</span>diting and Synthesis), a novel autoregressive architecture for text-conditioned voice editing and high-fidelity text-to-speech (TTS) synthesis, built on a cross-attentive Mamba backbone. MAVE achieves state-of-the-art performance in speech editing and very competitive results in zero-shot TTS, while not being explicitly trained on the latter task, outperforming leading autoregressive and diffusion models on diverse, real-world audio. By integrating Mamba for efficient audio sequence modeling with cross-attention for precise text-acoustic alignment, MAVE enables context-aware voice editing with exceptional naturalness and speaker consistency. In pairwise human evaluations on a random 40-sample subset of the RealEdit benchmark (400 judgments), 57.2% of listeners rated MAVE-edited speech as perceptually equal to the original, while 24.8% prefered the original and 18.0% MAVE- demonstrating that in the majority of cases edits are indistinguishable from the source. MAVE compares favorably with VoiceCraft and FluentSpeech both on pairwise comparisons and standalone mean opinion score (MOS) evaluations. For zero-shot TTS, MAVE exceeds VoiceCraft in both speaker similarity and naturalness, without requiring multiple inference runs or post-processing. Remarkably, these quality gains come with a significantly lower memory cost and approximately the same latency: MAVE requires <math alttext=\"\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\sim$}}{\\scalebox{0.8}{$\\textstyle\\sim$}}{\\scalebox{0.8}{$\\scriptstyle\\sim$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\sim$}}6\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\times$}}{\\scalebox{0.8}{$\\textstyle\\times$}}{\\scalebox{0.8}{$\\scriptstyle\\times$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\times$}}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"m6\" intent=\":literal\"><semantics><mrow><mpadded depth=\"0.0pt\" height=\"2.9pt\" style=\"width:8.4pt;height:2.9pt;vertical-align:-0.0pt;\" width=\"8.4pt\"><mo>&#8764;</mo></mpadded><mn>6</mn><mpadded depth=\"0.7pt\" height=\"4.7pt\" style=\"width:8.0pt;height:5.4pt;vertical-align:-0.7pt;\" width=\"8.0pt\"><mo>&#215;</mo></mpadded></mrow><annotation encoding=\"application/x-tex\">\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\sim$}}{\\scalebox{0.8}{$\\textstyle\\sim$}}{\\scalebox{0.8}{$\\scriptstyle\\sim$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\sim$}}6\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\times$}}{\\scalebox{0.8}{$\\textstyle\\times$}}{\\scalebox{0.8}{$\\scriptstyle\\times$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\times$}}</annotation></semantics></math> less memory than VoiceCraft during inference on utterances from the RealEdit database (mean duration: 6.21s, A100, FP16, batch size 1). Our results demonstrate that MAVE establishes a new standard for flexible, high-fidelity voice editing and synthesis through the synergistic integration of structured state-space modeling and cross-modal attention.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "realedit",
                    "speech",
                    "task",
                    "results",
                    "mos",
                    "benchmark",
                    "editing",
                    "naturalness",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in neural speech synthesis have enabled compelling text-to-speech (TTS) and voice editing capabilities, yet <em class=\"ltx_emph ltx_font_italic\">precise, context-aware voice editing</em> remains a formidable challenge. Current approaches fall into two paradigms: <span class=\"ltx_text ltx_font_bold\">autoregressive models</span> (AR) such as Transformer-based methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite> and <span class=\"ltx_text ltx_font_bold\">non-autoregressive frameworks</span> (NAR) that are based on flow-matching or diffusion probabilistic models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib11\" title=\"\">11</a>]</cite>). While AR models excel in fidelity, they suffer from <em class=\"ltx_emph ltx_font_italic\">quadratic complexity</em>. Conversely, flow-matching methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib14\" title=\"\">14</a>]</cite> prioritize speed, but struggle with <em class=\"ltx_emph ltx_font_italic\">temporal coherence</em> and <em class=\"ltx_emph ltx_font_italic\">fine-grained prosodic control</em>, particularly in noisy, real-world audio.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these limitations, we propose <span class=\"ltx_text ltx_font_bold\">MAVE</span> (<span class=\"ltx_text ltx_font_bold\">M</span>amba with Cross-<span class=\"ltx_text ltx_font_bold\">A</span>ttention for <span class=\"ltx_text ltx_font_bold\">V</span>oice <span class=\"ltx_text ltx_font_bold\">E</span>diting and Synthesis), a novel <em class=\"ltx_emph ltx_font_italic\">autoregressive</em> architecture for text-conditioned voice editing and zero-shot TTS that synergizes Mamba state-space models with cross-attention mechanisms. Unlike Transformer-based decoders&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib42\" title=\"\">42</a>]</cite>, MAVE replaces self-attention with structured state-space sequences (SSMs), enabling <em class=\"ltx_emph ltx_font_italic\">linear-complexity modeling</em> of dependencies between acoustic tokens. Crucially, our cross-attention module dynamically aligns <em class=\"ltx_emph ltx_font_italic\">augmented text inputs</em> with acoustic tokens, allowing the model to &#8220;edit&#8221; speech by attending to the textual information.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "model",
                    "speech",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To the best of our knowledge, <span class=\"ltx_text ltx_font_bold\">MAVE</span> &#8211; built on a cross-attentive Mamba backbone &#8211; is the first successful application of a structured state-space model to text-conditional speech generation, namely speech editing and zero-shot TTS. On the challenging RealEdit benchmark, MAVE achieves <span class=\"ltx_text ltx_font_bold\">human-parity naturalness</span> in speech editing and surpasses state-of-the-art models like VoiceCraft and FluentSpeech in both speaker similarity and naturalness (MOS) without requiring any post-processing. Moreover, MAVE offers significant efficiency gains, reducing memory usage by <span class=\"ltx_text ltx_font_bold\">six</span> times compared to Transformer-based VoiceCraft, while enabling single-pass generation without silent tokens handling or multiple runs. Our results demonstrate that Mamba-based models enhanced with cross-attention can outperform both autoregressive and diffusion-based approaches in fidelity, efficiency, and robustness, establishing a new direction for scalable and high-quality speech generation.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "realedit",
                    "speech",
                    "results",
                    "mos",
                    "benchmark",
                    "editing",
                    "naturalness",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Neural Codec Language Models for Speech Synthesis</span>\nThe evolution of high-fidelity speech generation has been significantly advanced by Neural Codec Language Models (NCLMs), which discretize speech into symbolic units through Residual Vector Quantization (RVQ) frameworks as shown by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib49\" title=\"\">49</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib10\" title=\"\">10</a>]</cite> and model temporal dependencies via autoregressive sequence learning. Originating in textless NLP research&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib32\" title=\"\">32</a>]</cite>, this paradigm achieved breakthrough performance in speech continuity with AudioLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib4\" title=\"\">4</a>]</cite>. A pivotal advancement emerged through the application of NCLMs to zero-shot text-to-speech synthesis, where models like VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib42\" title=\"\">42</a>]</cite> and Spear-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib23\" title=\"\">23</a>]</cite> reframed synthesis as transcript-conditioned speech continuation conditioned on brief speaker references. These systems substantially outperformed conventional non-NCLM approaches, prompting extensions for cross-lingual TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib52\" title=\"\">52</a>]</cite>, style-controlled synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib29\" title=\"\">29</a>]</cite>, and phoneme alignment refinement&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib39\" title=\"\">39</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The Evolution of Speech Editing</span>\nSpeech editing, defined as the precise modification of targeted speech segments while preserving unaltered regions, has evolved through three distinct generations of methodology. Early approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib21\" title=\"\">21</a>]</cite> relied on concatenating TTS-generated segments with original speech, inevitably introducing prosody mismatches and boundary artifacts due to the absence of contextual conditioning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib31\" title=\"\">31</a>]</cite>. Subsequent research introduced context-aware mechanisms through bidirectional fusion architectures&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib40\" title=\"\">40</a>]</cite> and masked reconstruction objectives&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib43\" title=\"\">43</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib5\" title=\"\">5</a>]</cite>, with Transformer-based systems demonstrating improved contextualization. Most recently, diffusion-based methods like FluentSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib20\" title=\"\">20</a>]</cite> achieved state-of-the-art performance on standard benchmarks through denoising frameworks. However, these approaches collectively suffer from three interrelated limitations. First, Transformer-based systems incur quadratic computational complexity relative to sequence length, restricting practical context windows. Second, evaluation protocols remain constrained to short editing spans&#8212;UniCATS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib11\" title=\"\">11</a>]</cite>, for instance, limits assessments to segments under two seconds, failing to address real-world editing scenarios involving multi-word phrases. Third and most critically, none incorporate mechanisms for leveraging linguistically augmented inputs (e.g., prosody-annotated text) to guide context-aware modifications. MAVE overcomes these constraints through its linear-complexity Mamba architecture and differentiable cross-attention fusion mechanism, enabling robust editing of spans up to 16 words while preserving natural prosody through explicit text augmentation.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "speech",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Unified Frameworks for Voice Editing and Synthesis</span>\nRecent efforts have sought to develop unified models capable of both zero-shot TTS and speech editing, recognizing their shared dependency on context-aware acoustic modeling. These frameworks broadly bifurcate into modular architectures&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib48\" title=\"\">48</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib20\" title=\"\">20</a>]</cite> that employ separate components for distinct tasks, and end-to-end systems including SpeechX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib44\" title=\"\">44</a>]</cite>&#8212;which adapts VALLE through prompt tuning&#8212;and flow-matching approaches like VoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib26\" title=\"\">26</a>]</cite> and UniCATS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib11\" title=\"\">11</a>]</cite>. While representing important conceptual advances, these unified systems exhibit significant practical limitations. SpeechX lacks human evaluation metrics for editing performance, undermining claims of perceptual quality. VoiceBox, despite its broad task coverage, omits formal speech editing evaluation in its methodology, relying solely on demo examples. UniCATS restricts editing capability to brief segments (<math alttext=\"&lt;2\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">&lt;2</annotation></semantics></math> seconds) and employs rule-based prosody markers rather than differentiable conditioning. Crucially, all existing unified frameworks are constrained by a fundamental dichotomy: autoregressive systems (e.g., SpeechX) maintain high fidelity but sacrifice contextual awareness through causal masking and face quadratic time complexity, while non-autoregressive approaches (e.g., VoiceBox) enable bidirectional context modeling at the cost of temporal coherence and prosodic precision. This trade-off between contextual awareness and generation fidelity has remained unresolved in prior art.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "speech",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MAVE in a Nutshell.</span>\nIn summary, MAVE establishes a novel paradigm in text-conditioned speech generation by bridging the efficiency of State Space Models with the flexibility of cross-modal attention. Unlike prior SSM-based approaches such as CM-Mamba&#160;&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib18\" title=\"\">18</a>]</cite>, which require strict alignment between text and audio sequences, our architecture introduces a length-agnostic cross-attention mechanism that enables rich, contextualized phoneme conditioning without artificial upsampling or architectural constraints. By replacing the quadratic self-attention of Transformer-based editors like Voicecraft with linear-time selective state updates, MAVE achieves scalable long-context modeling while preserving prosodic coherence and speaker identity through recurrence. Our framework further unifies three critical capabilities: (1) efficient bidirectional context access via causal token rearrangement, (2) precise linguistic control through differentiable cross-attention on phonemized input, and (3) reference-guided voice consistency without explicit speaker embeddings. As demonstrated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4\" title=\"4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, MAVE is the first model to simultaneously outperform both autoregressive (e.g., VoiceCraft) and non-autoregressive (e.g., FluentSpeech) baselines in human evaluations across speech editing and zero-shot TTS, setting a new standard for unified, high-fidelity, and scalable speech generation. To our knowledge, this is the first successful integration of cross-attention into a Mamba-based audio decoder for unrestricted, long-form speech editing and synthesis.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "speech",
                    "editing",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present <span class=\"ltx_text ltx_font_bold\">MAVE</span>, a novel architecture for text-conditioned speech editing and zero-shot text-to-speech (TTS). The core challenge lies in autoregressively generating long sequences of discrete audio tokens conditioned on a textual transcript, where the quadratic complexity of self-attention in Transformers becomes prohibitive. Audio signals are typically encoded at 50 Hz using residual vector quantization (RVQ), yielding 4 to 8 discrete tokens per frame, which amounts to 200&#8211;400 tokens per second of speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib47\" title=\"\">47</a>]</cite>. In contrast, the corresponding phoneme sequence contains approximately 12 units per second&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib36\" title=\"\">36</a>]</cite>, resulting in a significant sequence length mismatch between modalities. This disparity makes direct application of Transformer-based models inefficient for high-fidelity, long-form audio generation.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "speech",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these issues, we propose MAVE, a hybrid decoder that combines the efficiency of Mamba for audio token modeling with a flexible cross-attention mechanism for text conditioning&#8212;without requiring aligned sequence lengths. Our model supports context-aware speech infilling and zero-shot TTS by leveraging both surrounding audio context and linguistic input.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speech",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech editing, we consider a non-autoregressive infilling task where one or more spans of audio tokens are masked and must be reconstructed. To enable bidirectional context access during autoregressive generation, we adopt the <span class=\"ltx_text ltx_font_bold\">causal masking</span> strategy from CM3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib1\" title=\"\">1</a>]</cite>. Masked spans are replaced with special mask tokens and moved to the end of the sequence, allowing the model to condition on both past and future context while preserving autoregressive training.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "speech",
                    "model",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Having defined the input representation and preprocessing pipeline, we now describe how the model leverages Mamba and cross-attention to generate audio tokens autoregressively conditioned on text and reference context. The task is formulated as <span class=\"ltx_text ltx_font_italic\">text-conditioned autoregressive audio generation</span>, decomposed into two components: (1) modeling intra-audio temporal dependencies, and (2) conditioning on linguistic input.</p>\n\n",
                "matched_terms": [
                    "task",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ the Mamba architecture&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib13\" title=\"\">13</a>]</cite>, a selective State Space Model (SSM), to model long-range dependencies in the audio token sequence. Unlike Transformers, Mamba scales linearly with sequence length and maintains a compressed latent state that effectively captures speaker identity, prosody, and acoustic continuity&#8212;critical for coherent speech editing.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We stack multiple Mamba blocks with intermediate normalization and residual connections, forming the backbone of our audio decoder. Mamba has proven to be very effective in terms of modeling dependencies between audio tokens in previous research <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib12\" title=\"\">12</a>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib37\" title=\"\">37</a>]</cite>, but its effectivness to generate text-conditioned conherent audio are still not well-studied. Many text-conditioned audio generation approaches, such as text-to-speech (TTS) and speech editing, have adopted transformer-based decoder architectures that process concatenated text and audio tokens within a unified sequence <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib42\" title=\"\">42</a>]</cite>. While this design leverages the strong sequence modeling capabilities of transformers, it poses significant challenges when applied to alternative architectures such as Mamba.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike transformers, which benefit from global self-attention, Mamba relies on selective state-space mechanisms that exhibit difficulty in retaining fine-grained information over long sequences. As a result, if we concatenate text and audio tokens and attempt to process them with the same decoder model, distant text tokens&#8212;critical for maintaining linguistic fidelity&#8212;tend to be encoded with degraded or &#8221;fuzzy&#8221; memory <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib41\" title=\"\">41</a>]</cite>, leading to a loss of semantic precision. This limitation is particularly detrimental in speech generation tasks, where high-fidelity retention of textual information is essential to ensure accurate alignment and high-quality output.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the other hand, this limitation is less critical when modeling dependencies among audio tokens, as exact low-level detail preservation is not essential for high-quality speech synthesis. Instead, the model benefits from retaining high-level acoustic features which are sufficient to generate coherent and natural-sounding audio. The compressed, selective memory inherent in SSMs like Mamba is well-suited to this purpose, effectively filtering out perceptually irrelevant noise while preserving meaningful temporal patterns. In our ablation studies, we further demonstrate that conditioning on text via cross-attention&#8212;rather than token concatenation&#8212;is crucial for maintaining linguistic fidelity in Mamba-based architectures. This design ensures that textual information is explicitly attended to throughout generation, mitigating the risk of forgetting distant linguistic context.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MAVE does not rely on explicit speaker embeddings for voice identity preservation. Instead, it leverages <span class=\"ltx_text ltx_font_italic\">in-context learning</span> for TTS task by prepending a short reference utterance encoded into discrete audio tokens using <span class=\"ltx_text ltx_font_typewriter\">X-Codec</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib47\" title=\"\">47</a>]</cite> to the generation sequence. This reference context provides rich acoustic, prosodic, and timbral cues that guide the autoregressive decoder to synthesize speech in the target speaker&#8217;s voice, without requiring speaker labels or fine-tuning.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mave",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During both training and inference, the reference tokens are placed at the beginning of the input sequence, followed by the masked audio context. The Mamba decoder attends to this context through its long-range recurrence, effectively conditioning the generated speech on the speaker&#8217;s voice characteristics. This strategy enables true zero-shot TTS: given only a few seconds of reference audio and a target transcript, the model generates high-fidelity, speaker-consistent speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In speech editing tasks, the reference can be omitted because the surrounding unmasked audio already provides sufficient speaker context. However, for zero-shot TTS or cross-speaker editing, the reference utterance is essential for voice coherence.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, MAVE is a hybrid autoregressive decoder that integrates Mamba blocks for efficient long-sequence audio modeling with cross-attention for flexible text conditioning. By combining token rearrangement, delayed RVQ decoding, and explicit speaker conditioning, it supports high-fidelity speech editing and zero-shot TTS. The network efficiently handles a large disparity between text and audio sequence lengths, offering a scalable alternative to transformer-based methods.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "speech",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Gigaspeech training set&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib6\" title=\"\">6</a>]</cite> is used as the training data, which contains 9k hours\nof audiobooks, podcasts, and YouTube videos at 16kHz audio sampling rate. For speech editing evaluation, we use the real-world benchmark called RealEdit from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite>. For TTS evaluation, we use a subset of libritts <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib51\" title=\"\">51</a>]</cite></p>\n\n",
                "matched_terms": [
                    "realedit",
                    "speech",
                    "editing",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To train MAVE, we used the ScaledAdam optimizer and Eden Scheduler proposed in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib46\" title=\"\">46</a>]</cite> with a base learning rate of 0.01, batch size of 400k frames (i.e. 133.2 minutes), and total training step\nof 50k with gradient accumulation. The training of the 830M MAVE model took about 4 days on 4 NVIDIA A100 GPUs. For inference, we use Nucleus sampling <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib16\" title=\"\">16</a>]</cite> with p = 0.8 and a temperature of 1 for all experiments.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VoiceCraft has been observed to generate prolonged silences during synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite>. To mitigate this issue, the authors proposed reducing the probability of silence tokens and performing multiple generations per sample, selecting the shortest output as the final result. In all of our experiments involving VoiceCraft, we adopted this strategy, generating five samples per input and selecting the shortest one. In contrast we haven&#8217;t noticed this problem in our model, so we produce a single generation with no further processing.</p>\n\n",
                "matched_terms": [
                    "model",
                    "voicecraft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We computed the Word Error Rate (WER) between the generated audio and the target transcript using Whisper-large and Whisper-medium.en <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib34\" title=\"\">34</a>]</cite>. In addition, we conducted a human evaluation to assess naturalness and intelligibility, based on audio samples from 80 randomly selected test cases for each model. Specifically, 20 people of proven English fluency participated in the study, they were divided into 2 groups of 10 people, and each group was asked to evaluate 120 audios (40 from each method) on the naturalness and intelligibility on a 5-point Likert scale (poor=1, excellent=5). For further details we refer to&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS3.SSS1\" title=\"A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.3.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "naturalness",
                    "model",
                    "intelligibility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MAVE demonstrates superior performance compared to VoiceCraft, achieving smaller WER and higher mean opinion scores (MOS) in both naturalness (3.90 vs. 3.77) and intelligibility (4.25 vs. 4.20). Notably, the performance gap between our model and the ground truth recordings is minimal, with differences of less than 0.1 in naturalness MOS and less than 0.07 in intelligibility MOS. This is an indication that MAVE-edits feature strong perceptual correlation to human speech. It is worth noting that even for the original audio, the average naturalness ratings did not exceed 4.0 out of 5.0. This is mostly attributed to the fact that the audio data were collected in the wild and their quality is inferior to studio recordings, with the presence of background noise and other sources of recording artifacts. This also highlights the challenging conditions under which the evaluation was conducted.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "wer",
                    "ground",
                    "speech",
                    "mos",
                    "truth",
                    "intelligibility",
                    "naturalness",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further assess the perceptual quality of our generated speech relative to ground truth, we conducted an additional study in which we asked a group of 10 users of proven English fluency to do a side-by-side comparison on the naturalness of the original unedited audio and our edited audio on 40 samples from the RealEdit dataset. The results are very promising and demonstrate that 57.2% of the times, users reported that both audios sound equally natural, while in 24.8% of cases, the ground truth was judged as more natural, and interestingly 18% of the times, the edited audio was preferred over the original. These findings indicate that in the majority of cases, our generated audios are indistinguishable from the original ones. For further details about the setup, we refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS3.SSS2\" title=\"A.3.2 Comparative studies &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.3.2</span></a></p>\n\n",
                "matched_terms": [
                    "realedit",
                    "ground",
                    "speech",
                    "results",
                    "truth",
                    "naturalness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable a meaningful comparison with diffusion-based speech generation models, we selected FluentSpeech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib20\" title=\"\">20</a>]</cite> as the most relevant open-source baseline. However, due to its significantly smaller model size, a direct comparison with our approach would not be fair. To address this, we leveraged the larger variant of FluentSpeech retrained and evaluated by the authors of VoiceCraft. The authors have not made this model publicly available but have provided 14 synthesized utterances, publishing for each sample: (1) VoiceCraft&#8217;s generation, (2) the enhanced FluentSpeech model&#8217;s generation, and (3) the original audio with original and target transcript.\nWe generated corresponding outputs using MAVE for the same 14 samples and conducted a perceptual listening study to compare the audio quality between the three models. The evaluation consisted of a pairwise, side-by-side comparison, where each participant (20 in total) was presented with two audio samples from different models and asked to judge which exhibited better naturalness and intelligibility. With 14 samples per system and three possible pairings between the models (VoiceCraft vs. FluentSpeech, VoiceCraft vs. Ours, and FluentSpeech vs. Ours), each participant evaluated a total of 42 audio pairs. Further details on the experimental setup, including participant instructions, interface design, and evaluation protocol, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS3.SSS2\" title=\"A.3.2 Comparative studies &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.3.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "ours",
                    "voicecraft",
                    "speech",
                    "intelligibility",
                    "naturalness",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.F2\" title=\"Figure 2 &#8227; 4.1 Results on Speech Editing &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> clearly indicate that MAVE accomplishes superior perceptual quality compared to both FluentSpeech and VoiceCraft. While VoiceCraft already holds an advantage over FluentSpeech as it was preferred in 47.1% vs. 13.2% for naturalness and 50.7% vs. 6.4% for intelligibility, our model surpasses FluentSpeech by an even larger margin, being favored in 62.9% of cases for naturalness and 59.3% for intelligibility. More importantly, our model also outperforms VoiceCraft: it is preferred over VoiceCraft in 33.6% of comparisons for naturalness and 21.4% for intelligibility, while VoiceCraft is preferred over ours in only 17.5% and 12.9%, respectively.</p>\n\n",
                "matched_terms": [
                    "ours",
                    "voicecraft",
                    "results",
                    "intelligibility",
                    "naturalness",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T2\" title=\"Table 2 &#8227; 4.1 Results on Speech Editing &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the results for the zero-shot TTS task. For the evaluation we employed both objective and subjective metrics. The objective evaluation included WER, computed using Whisper Large and Whisper medium.en&#8212;following the methodology commonly adopted in speech editing tasks. Additionally, we measured speaker similarity using the pretrained WavLM-Large model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib7\" title=\"\">7</a>]</cite> to compute cosine similarity between reference and synthesized speaker embeddings. We also report the Mel-Cepstral Distortion (MCD) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib24\" title=\"\">24</a>]</cite> to assess spectral fidelity.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "speech",
                    "task",
                    "results",
                    "editing",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the subjective evaluation, we randomly selected 80 samples from our test set and generated corresponding speech outputs using VoiceCraft and our proposed model. To ensure a balanced assessment, we conducted a listening study involving 20 native or near-native English speakers (C1&#8211;C2 proficiency level). Participants were randomly assigned to two groups of 10, with each group evaluating a total of 120 audio clips: 40 generated by VoiceCraft, 40 produced by MAVE, and 40 ground-truth recordings. Each participant rated the naturalness and intelligibility of the audio samples on a 5-point Likert scale (1 = poor, 5 = excellent). Further details regarding the experimental setup, rating criteria, and instructions provided to participants are available in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS3.SSS1\" title=\"A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.3.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "speech",
                    "intelligibility",
                    "naturalness",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MAVE outperformed VoiceCraft in MOS evaluations across both naturalness (3.48 vs. 3.22) and intelligibility (4.20 vs. 4.01). While the overall gap between our model and the ground truth remains notable, a closer analysis reveals a reduced performance disparity under certain conditions. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T3\" title=\"Table 3 &#8227; 4.2 Results on Zero-shot TTS &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, which breaks down results by the number of words to be generated, with split points selected to ensure approximately balanced sample sizes across ranges, specifically 17, 22, 21, 20 for the different ranges in ascending order. The performance gap narrows significantly in the 8&#8211;15 word range. In this interval, our model achieves a naturalness score of 3.71 compared to the ground truth&#8217;s 3.87, and an intelligibility score of 4.26 versus 4.36, indicating competitive quality for medium-length utterances, while the gap becomes more significant with the increasing length of the target transcript. This trend is consistent with what the model is trained on, as it was trained on the Gigaspeech dataset, which consists of speech with moderate utterance lengths.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "ground",
                    "speech",
                    "results",
                    "mos",
                    "truth",
                    "intelligibility",
                    "naturalness",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we compare our model, with previous SOTA AR model for speech editing, namely VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite>, which uses a transformer backbone. All inference experiments were conducted on NVIDIA A100 GPU. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Efficiency Analysis &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> reports the inference time required to process the entire RealEdit dataset as well as the average and peak memory consumption. It is important to note that these reported numbers are based on a single generation per sample. The results clearly demonstrate the superior memory efficiency of MAVE compared to VoiceCraft with KV caching, requiring approximately six times less GPU memory&#8212;largely due to the selective state mechanism of the Mamba backbone. While VoiceCraft exhibits faster inference speed in this evaluation, this advantage is primarily attributed to the relatively short duration of the target audio segments. On average, our model generates about 85 tokens per sample, corresponding to approximately 1.7 seconds of speech, a regime in which autoregressive overheads are minimal.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "realedit",
                    "speech",
                    "results",
                    "editing",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, theoretical complexity analysis reveals that MAVE scales more favorably with sequence length. For longer generation tasks, our model not only maintains significant memory savings but also surpasses VoiceCraft in computational efficiency, offering better speed performance for extended audio synthesis (see &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS4\" title=\"A.4 Theoretical Analysis of Computational Complexity &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> for more details).</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave",
                    "voicecraft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the ablation study, we considered the masked reconstruction task, in which we used a random subset of the Gigaspeech validation dataset, randomly masked a sequence of words and required the model to reconstruct them (details provided in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS2\" title=\"A.2 Ablation Study Setup &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Efficiency Analysis &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents a comprehensive ablation study comparing different architectural designs under similar conditions, including model size (approximately 830M parameters) and training setup. The results highlight that neither a pure transformer-based encoder-decoder architecture nor a standalone Mamba decoder achieves optimal performance, highlighting the importance of our hybrid design.</p>\n\n",
                "matched_terms": [
                    "results",
                    "task",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The encoder-decoder transformer, with the encoder utilized to get contextualized text tokens, cross-attention to condition the audio decoder on text, and transformer decoder to model audio dependencies, achieves moderate performance but lags behind our model in all metrics, particularly in WER (10.8 vs. <span class=\"ltx_text ltx_font_bold\">7.8</span>). In contrast, the Mamba-only model, where text and audio tokens are concatenated and processed autoregressively, performs significantly worse with a higher WER (13.0).</p>\n\n",
                "matched_terms": [
                    "wer",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Crucially, our MAVE architecture integrates both components, it uses cross-attention to explicitly condition the generation process on textual input while leveraging the Mamba decoder to efficiently model acoustic dependencies. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Efficiency Analysis &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, this hybrid approach yields superior performance across all metrics, achieving the lowest WER and MCD, best fundamental frequency (F0) accuracy, and highest PESQ score. These improvements demonstrate that the strength of MAVE does not arise from either cross-attention or Mamba in isolation, but from the combination of both.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work we introduce MAVE, a novel hybrid architecture for high-fidelity speech synthesis that combines cross-attention and structured state-space modeling via Mamba within a single autoregressive framework. By leveraging Mamba for robust audio modeling and cross-attention for precise text&#8211;audio alignment, MAVE sets a new benchmark for speech editing and zero-shot text-to-speech in terms of both naturalness and efficiency. As future work, we plan to train MAVE on significantly longer audio sequences to further exploit Mamba&#8217;s ability to capture long-range dependencies.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "benchmark",
                    "editing",
                    "naturalness",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development and deployment of speech editing models like MAVE raise important ethical concerns that must be carefully addressed. One primary issue is the potential for misuse in generating deceptive audio content, such as deepfakes, which could be exploited for misinformation, fraud, or impersonation. While our model enables beneficial applications, such as facilitating editing tasks by correcting errors in speech recordings, improving accessibility, or enhancing voice interfaces.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "model",
                    "speech",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To conduct ablation studies, we employed the masked reconstruction task. For dataset construction, we used the evaluation subset of GigaSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib6\" title=\"\">6</a>]</cite>, from which we randomly sampled 500 utterances. We applied several filtering criteria to ensure data quality: (1) utterances with a word error rate (WER) greater than 0.1 when transcribed by Whisper-medium.en&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib35\" title=\"\">35</a>]</cite> were excluded to ensure reliable reference transcripts; and (2) utterances containing fewer than five words were discarded to provide sufficient linguistic context.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study was conducted to evaluate the quality of audio samples generated by our model in comparison to VoiceCraft and ground-truth recordings, to provide insights of how well our model performs against state-of-the-art models and to assess how closely our synthesized speech approaches the quality of natural, human speech.\nWe sampled 80 random examples from the RealEdit benchmark to evaluate the models on the speech editing task, and another 80 from the LibriTTS to evaluate on zero-shot TTS. Then, for each task we generated the synthesized audios using both our model MAVE and VoiceCraft. After that we divided these audios into two groups, each group was assigned 120 audios (40 from each category). We then asked 20 people to assess the quality of the generation, assigning each user to one group, ensuring that each group receives exactly 10 people. Audios are randomly shuffled for each new user. At the beginning of the study, the instructions are provided to the users as illustrated in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F3\" title=\"Figure 3 &#8227; A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Then for each audio sample, participants were first presented with the corresponding transcript and asked to rate the naturalness of the speech as depicted in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F4\" title=\"Figure 4 &#8227; A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. After submitting their naturalness rating, they were then prompted to evaluate the same audio in terms of intelligibility.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "realedit",
                    "speech",
                    "task",
                    "benchmark",
                    "intelligibility",
                    "editing",
                    "naturalness",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further assess the quality of the generated audios, and to enable direct comparison between the different methods, we have conducted 2 comparative user studies.\nThe first study evaluates our model against VoiceCraft and FluentSpeech. We selected the 14 publicly available audio samples generated by both VoiceCraft and FluentSpeech, which are provided on the VoiceCraft demo page&#160;<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://jasonppy.github.io/VoiceCraft_web/\" title=\"\">https://jasonppy.github.io/VoiceCraft_web/</a>. Using the same text prompts, we generated corresponding outputs using MAVE. This resulted in three pairwise comparisons: (1) VoiceCraft vs. FluentSpeech, (2) VoiceCraft vs. Ours, and (3) FluentSpeech vs. Ours), with 14 pairs per comparison, yielding a total of 42 unique audio pairs.</p>\n\n",
                "matched_terms": [
                    "ours",
                    "model",
                    "mave",
                    "voicecraft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We asked 20 people of proven English fluency to evaluate each pair and choose which is better in terms of both naturalness and intelligibility, or to indicate that they are perceptually equal (three options: &#8221;First is better,&#8221; &#8221;Second is better,&#8221; or &#8221;Equal&#8221;) as show in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F5\" title=\"Figure 5 &#8227; A.3.2 Comparative studies &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. To minimize positional bias, the order of the two audios in each pair was randomized across participants, and the presentation order of the pairs themselves was shuffled for each user.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "intelligibility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second comparative study focuses on a direct comparison between MAVE and ground truth audio for the speech editing task. To accomplish this, we have selected 40 random samples from RealEdit data, provided the ground truth before editing and our edited audio, along with their respective transcript. In total, 10 native or near-native English speakers&#8212;were asked to compare the two audios in terms of naturalness, and indicate whether the ground truth sounded better, the MAVE-edit sounded better, or there was no perceptual difference between the two. Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F6\" title=\"Figure 6 &#8227; A.3.2 Comparative studies &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> illustrates the instructions provided to users at the beginning of the study, and Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F7\" title=\"Figure 7 &#8227; A.3.2 Comparative studies &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> provides an example interface of the side-by-side comparison.</p>\n\n",
                "matched_terms": [
                    "realedit",
                    "ground",
                    "speech",
                    "task",
                    "truth",
                    "editing",
                    "naturalness",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Memory efficiency:</span>\nalthough both models can use KV-cache, the decoder-only Transformer\nmust store keys and values for all past text and audio tokens\n(<math alttext=\"\\mathcal{O}(N_{d}H_{0}(L_{x}+L_{y}))\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I3.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>N</mi><mi>d</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>H</mi><mn>0</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>L</mi><mi>x</mi></msub><mo>+</mo><msub><mi>L</mi><mi>y</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}(N_{d}H_{0}(L_{x}+L_{y}))</annotation></semantics></math> per layer) which grows linearly as the generation goes, whereas the\nencoder&#8211;decoder model only stores a fixed encoder cache\n(<math alttext=\"\\mathcal{O}(N_{e}HL_{x})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I3.i2.p1.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>N</mi><mi>e</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>H</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>L</mi><mi>x</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}(N_{e}HL_{x})</annotation></semantics></math>) and the hidden state of Mamba. When <math alttext=\"L_{y}\\gg L_{x}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I3.i2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mi>y</mi></msub><mo>&#8811;</mo><msub><mi>L</mi><mi>x</mi></msub></mrow><annotation encoding=\"application/x-tex\">L_{y}\\gg L_{x}</annotation></semantics></math>, this results in\nsignificantly lower memory usage for the encoder&#8211;decoder model.</p>\n\n",
                "matched_terms": [
                    "results",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The authors acknowledge the use of a large language model (LLM) solely for language editing and grammatical refinement of the current manuscript. All scientific content, analysis, and interpretations presented herein are the sole responsibility of the authors.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "model"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba",
        "caption": "Table 2: Performance analysis of the proposed MAVE on the zero-shot TTS task.",
        "body": "Model\n\nWER (L) ↓\\downarrow\n\n\nWER (M) ↓\\downarrow\n\n\nSIM ↑\\uparrow\n\n\nMCD ↓\\downarrow\n\n\nMOS Naturalness ↑\\uparrow\n\n\nMOS Intelligibility ↑\\uparrow\n\n\n\nVoiceCraft\n9.3\n7.5\n0.55\n4.75\n3.22 ± 0.07\n4.01 ± 0.07\n\n\nMAVE (ours)\n7.4\n6.6\n0.57\n4.73\n3.48 ± 0.08\n4.20 ± 0.06\n\n\nGround Truth\n6.2\n5.4\n0.66\n-\n3.90 ± 0.08\n4.40 ± 0.06",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_text ltx_font_bold\">WER (L)</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_text ltx_font_bold\">WER (M)</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_text ltx_font_bold\">MCD</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_text ltx_font_bold\">MOS Naturalness</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_text ltx_font_bold\">MOS Intelligibility</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">VoiceCraft</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">9.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">7.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">0.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">4.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">3.22 &#177; 0.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">4.01 &#177; 0.07</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">MAVE (ours)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">7.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">6.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">0.57</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">4.73</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">3.48 &#177; 0.08</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">4.20 &#177; 0.06</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">Ground Truth</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">6.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">5.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">0.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">3.90 &#177; 0.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">4.40 &#177; 0.06</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "voicecraft",
            "wer",
            "proposed",
            "↑uparrow",
            "mave",
            "mcd",
            "truth",
            "performance",
            "↓downarrow",
            "sim",
            "analysis",
            "mos",
            "zeroshot",
            "ours",
            "ground",
            "task",
            "tts",
            "intelligibility",
            "naturalness",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T2\" title=\"Table 2 &#8227; 4.1 Results on Speech Editing &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the results for the zero-shot TTS task. For the evaluation we employed both objective and subjective metrics. The objective evaluation included WER, computed using Whisper Large and Whisper medium.en&#8212;following the methodology commonly adopted in speech editing tasks. Additionally, we measured speaker similarity using the pretrained WavLM-Large model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib7\" title=\"\">7</a>]</cite> to compute cosine similarity between reference and synthesized speaker embeddings. We also report the Mel-Cepstral Distortion (MCD) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib24\" title=\"\">24</a>]</cite> to assess spectral fidelity.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">MAVE</span> (<span class=\"ltx_text ltx_font_bold\">M</span>amba with Cross-<span class=\"ltx_text ltx_font_bold\">A</span>ttention for <span class=\"ltx_text ltx_font_bold\">V</span>oice <span class=\"ltx_text ltx_font_bold\">E</span>diting and Synthesis), a novel autoregressive architecture for text-conditioned voice editing and high-fidelity text-to-speech (TTS) synthesis, built on a cross-attentive Mamba backbone. MAVE achieves state-of-the-art performance in speech editing and very competitive results in zero-shot TTS, while not being explicitly trained on the latter task, outperforming leading autoregressive and diffusion models on diverse, real-world audio. By integrating Mamba for efficient audio sequence modeling with cross-attention for precise text-acoustic alignment, MAVE enables context-aware voice editing with exceptional naturalness and speaker consistency. In pairwise human evaluations on a random 40-sample subset of the RealEdit benchmark (400 judgments), 57.2% of listeners rated MAVE-edited speech as perceptually equal to the original, while 24.8% prefered the original and 18.0% MAVE- demonstrating that in the majority of cases edits are indistinguishable from the source. MAVE compares favorably with VoiceCraft and FluentSpeech both on pairwise comparisons and standalone mean opinion score (MOS) evaluations. For zero-shot TTS, MAVE exceeds VoiceCraft in both speaker similarity and naturalness, without requiring multiple inference runs or post-processing. Remarkably, these quality gains come with a significantly lower memory cost and approximately the same latency: MAVE requires <math alttext=\"\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\sim$}}{\\scalebox{0.8}{$\\textstyle\\sim$}}{\\scalebox{0.8}{$\\scriptstyle\\sim$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\sim$}}6\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\times$}}{\\scalebox{0.8}{$\\textstyle\\times$}}{\\scalebox{0.8}{$\\scriptstyle\\times$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\times$}}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"m6\" intent=\":literal\"><semantics><mrow><mpadded depth=\"0.0pt\" height=\"2.9pt\" style=\"width:8.4pt;height:2.9pt;vertical-align:-0.0pt;\" width=\"8.4pt\"><mo>&#8764;</mo></mpadded><mn>6</mn><mpadded depth=\"0.7pt\" height=\"4.7pt\" style=\"width:8.0pt;height:5.4pt;vertical-align:-0.7pt;\" width=\"8.0pt\"><mo>&#215;</mo></mpadded></mrow><annotation encoding=\"application/x-tex\">\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\sim$}}{\\scalebox{0.8}{$\\textstyle\\sim$}}{\\scalebox{0.8}{$\\scriptstyle\\sim$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\sim$}}6\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\times$}}{\\scalebox{0.8}{$\\textstyle\\times$}}{\\scalebox{0.8}{$\\scriptstyle\\times$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\times$}}</annotation></semantics></math> less memory than VoiceCraft during inference on utterances from the RealEdit database (mean duration: 6.21s, A100, FP16, batch size 1). Our results demonstrate that MAVE establishes a new standard for flexible, high-fidelity voice editing and synthesis through the synergistic integration of structured state-space modeling and cross-modal attention.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "performance",
                    "task",
                    "tts",
                    "mos",
                    "naturalness",
                    "zeroshot",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these limitations, we propose <span class=\"ltx_text ltx_font_bold\">MAVE</span> (<span class=\"ltx_text ltx_font_bold\">M</span>amba with Cross-<span class=\"ltx_text ltx_font_bold\">A</span>ttention for <span class=\"ltx_text ltx_font_bold\">V</span>oice <span class=\"ltx_text ltx_font_bold\">E</span>diting and Synthesis), a novel <em class=\"ltx_emph ltx_font_italic\">autoregressive</em> architecture for text-conditioned voice editing and zero-shot TTS that synergizes Mamba state-space models with cross-attention mechanisms. Unlike Transformer-based decoders&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib42\" title=\"\">42</a>]</cite>, MAVE replaces self-attention with structured state-space sequences (SSMs), enabling <em class=\"ltx_emph ltx_font_italic\">linear-complexity modeling</em> of dependencies between acoustic tokens. Crucially, our cross-attention module dynamically aligns <em class=\"ltx_emph ltx_font_italic\">augmented text inputs</em> with acoustic tokens, allowing the model to &#8220;edit&#8221; speech by attending to the textual information.</p>\n\n",
                "matched_terms": [
                    "model",
                    "zeroshot",
                    "mave",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To the best of our knowledge, <span class=\"ltx_text ltx_font_bold\">MAVE</span> &#8211; built on a cross-attentive Mamba backbone &#8211; is the first successful application of a structured state-space model to text-conditional speech generation, namely speech editing and zero-shot TTS. On the challenging RealEdit benchmark, MAVE achieves <span class=\"ltx_text ltx_font_bold\">human-parity naturalness</span> in speech editing and surpasses state-of-the-art models like VoiceCraft and FluentSpeech in both speaker similarity and naturalness (MOS) without requiring any post-processing. Moreover, MAVE offers significant efficiency gains, reducing memory usage by <span class=\"ltx_text ltx_font_bold\">six</span> times compared to Transformer-based VoiceCraft, while enabling single-pass generation without silent tokens handling or multiple runs. Our results demonstrate that Mamba-based models enhanced with cross-attention can outperform both autoregressive and diffusion-based approaches in fidelity, efficiency, and robustness, establishing a new direction for scalable and high-quality speech generation.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "model",
                    "tts",
                    "mos",
                    "naturalness",
                    "zeroshot",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Neural Codec Language Models for Speech Synthesis</span>\nThe evolution of high-fidelity speech generation has been significantly advanced by Neural Codec Language Models (NCLMs), which discretize speech into symbolic units through Residual Vector Quantization (RVQ) frameworks as shown by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib49\" title=\"\">49</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib10\" title=\"\">10</a>]</cite> and model temporal dependencies via autoregressive sequence learning. Originating in textless NLP research&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib32\" title=\"\">32</a>]</cite>, this paradigm achieved breakthrough performance in speech continuity with AudioLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib4\" title=\"\">4</a>]</cite>. A pivotal advancement emerged through the application of NCLMs to zero-shot text-to-speech synthesis, where models like VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib42\" title=\"\">42</a>]</cite> and Spear-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib23\" title=\"\">23</a>]</cite> reframed synthesis as transcript-conditioned speech continuation conditioned on brief speaker references. These systems substantially outperformed conventional non-NCLM approaches, prompting extensions for cross-lingual TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib52\" title=\"\">52</a>]</cite>, style-controlled synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib29\" title=\"\">29</a>]</cite>, and phoneme alignment refinement&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib39\" title=\"\">39</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "model",
                    "tts",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The Evolution of Speech Editing</span>\nSpeech editing, defined as the precise modification of targeted speech segments while preserving unaltered regions, has evolved through three distinct generations of methodology. Early approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib21\" title=\"\">21</a>]</cite> relied on concatenating TTS-generated segments with original speech, inevitably introducing prosody mismatches and boundary artifacts due to the absence of contextual conditioning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib31\" title=\"\">31</a>]</cite>. Subsequent research introduced context-aware mechanisms through bidirectional fusion architectures&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib40\" title=\"\">40</a>]</cite> and masked reconstruction objectives&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib43\" title=\"\">43</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib5\" title=\"\">5</a>]</cite>, with Transformer-based systems demonstrating improved contextualization. Most recently, diffusion-based methods like FluentSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib20\" title=\"\">20</a>]</cite> achieved state-of-the-art performance on standard benchmarks through denoising frameworks. However, these approaches collectively suffer from three interrelated limitations. First, Transformer-based systems incur quadratic computational complexity relative to sequence length, restricting practical context windows. Second, evaluation protocols remain constrained to short editing spans&#8212;UniCATS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib11\" title=\"\">11</a>]</cite>, for instance, limits assessments to segments under two seconds, failing to address real-world editing scenarios involving multi-word phrases. Third and most critically, none incorporate mechanisms for leveraging linguistically augmented inputs (e.g., prosody-annotated text) to guide context-aware modifications. MAVE overcomes these constraints through its linear-complexity Mamba architecture and differentiable cross-attention fusion mechanism, enabling robust editing of spans up to 16 words while preserving natural prosody through explicit text augmentation.</p>\n\n",
                "matched_terms": [
                    "mave",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Unified Frameworks for Voice Editing and Synthesis</span>\nRecent efforts have sought to develop unified models capable of both zero-shot TTS and speech editing, recognizing their shared dependency on context-aware acoustic modeling. These frameworks broadly bifurcate into modular architectures&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib48\" title=\"\">48</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib20\" title=\"\">20</a>]</cite> that employ separate components for distinct tasks, and end-to-end systems including SpeechX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib44\" title=\"\">44</a>]</cite>&#8212;which adapts VALLE through prompt tuning&#8212;and flow-matching approaches like VoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib26\" title=\"\">26</a>]</cite> and UniCATS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib11\" title=\"\">11</a>]</cite>. While representing important conceptual advances, these unified systems exhibit significant practical limitations. SpeechX lacks human evaluation metrics for editing performance, undermining claims of perceptual quality. VoiceBox, despite its broad task coverage, omits formal speech editing evaluation in its methodology, relying solely on demo examples. UniCATS restricts editing capability to brief segments (<math alttext=\"&lt;2\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">&lt;2</annotation></semantics></math> seconds) and employs rule-based prosody markers rather than differentiable conditioning. Crucially, all existing unified frameworks are constrained by a fundamental dichotomy: autoregressive systems (e.g., SpeechX) maintain high fidelity but sacrifice contextual awareness through causal masking and face quadratic time complexity, while non-autoregressive approaches (e.g., VoiceBox) enable bidirectional context modeling at the cost of temporal coherence and prosodic precision. This trade-off between contextual awareness and generation fidelity has remained unresolved in prior art.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts",
                    "performance",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MAVE in a Nutshell.</span>\nIn summary, MAVE establishes a novel paradigm in text-conditioned speech generation by bridging the efficiency of State Space Models with the flexibility of cross-modal attention. Unlike prior SSM-based approaches such as CM-Mamba&#160;&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib18\" title=\"\">18</a>]</cite>, which require strict alignment between text and audio sequences, our architecture introduces a length-agnostic cross-attention mechanism that enables rich, contextualized phoneme conditioning without artificial upsampling or architectural constraints. By replacing the quadratic self-attention of Transformer-based editors like Voicecraft with linear-time selective state updates, MAVE achieves scalable long-context modeling while preserving prosodic coherence and speaker identity through recurrence. Our framework further unifies three critical capabilities: (1) efficient bidirectional context access via causal token rearrangement, (2) precise linguistic control through differentiable cross-attention on phonemized input, and (3) reference-guided voice consistency without explicit speaker embeddings. As demonstrated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4\" title=\"4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, MAVE is the first model to simultaneously outperform both autoregressive (e.g., VoiceCraft) and non-autoregressive (e.g., FluentSpeech) baselines in human evaluations across speech editing and zero-shot TTS, setting a new standard for unified, high-fidelity, and scalable speech generation. To our knowledge, this is the first successful integration of cross-attention into a Mamba-based audio decoder for unrestricted, long-form speech editing and synthesis.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "model",
                    "tts",
                    "zeroshot",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present <span class=\"ltx_text ltx_font_bold\">MAVE</span>, a novel architecture for text-conditioned speech editing and zero-shot text-to-speech (TTS). The core challenge lies in autoregressively generating long sequences of discrete audio tokens conditioned on a textual transcript, where the quadratic complexity of self-attention in Transformers becomes prohibitive. Audio signals are typically encoded at 50 Hz using residual vector quantization (RVQ), yielding 4 to 8 discrete tokens per frame, which amounts to 200&#8211;400 tokens per second of speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib47\" title=\"\">47</a>]</cite>. In contrast, the corresponding phoneme sequence contains approximately 12 units per second&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib36\" title=\"\">36</a>]</cite>, resulting in a significant sequence length mismatch between modalities. This disparity makes direct application of Transformer-based models inefficient for high-fidelity, long-form audio generation.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "mave",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these issues, we propose MAVE, a hybrid decoder that combines the efficiency of Mamba for audio token modeling with a flexible cross-attention mechanism for text conditioning&#8212;without requiring aligned sequence lengths. Our model supports context-aware speech infilling and zero-shot TTS by leveraging both surrounding audio context and linguistic input.</p>\n\n",
                "matched_terms": [
                    "model",
                    "zeroshot",
                    "mave",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech editing, we consider a non-autoregressive infilling task where one or more spans of audio tokens are masked and must be reconstructed. To enable bidirectional context access during autoregressive generation, we adopt the <span class=\"ltx_text ltx_font_bold\">causal masking</span> strategy from CM3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib1\" title=\"\">1</a>]</cite>. Masked spans are replaced with special mask tokens and moved to the end of the sequence, allowing the model to condition on both past and future context while preserving autoregressive training.</p>\n\n",
                "matched_terms": [
                    "task",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Having defined the input representation and preprocessing pipeline, we now describe how the model leverages Mamba and cross-attention to generate audio tokens autoregressively conditioned on text and reference context. The task is formulated as <span class=\"ltx_text ltx_font_italic\">text-conditioned autoregressive audio generation</span>, decomposed into two components: (1) modeling intra-audio temporal dependencies, and (2) conditioning on linguistic input.</p>\n\n",
                "matched_terms": [
                    "task",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MAVE does not rely on explicit speaker embeddings for voice identity preservation. Instead, it leverages <span class=\"ltx_text ltx_font_italic\">in-context learning</span> for TTS task by prepending a short reference utterance encoded into discrete audio tokens using <span class=\"ltx_text ltx_font_typewriter\">X-Codec</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib47\" title=\"\">47</a>]</cite> to the generation sequence. This reference context provides rich acoustic, prosodic, and timbral cues that guide the autoregressive decoder to synthesize speech in the target speaker&#8217;s voice, without requiring speaker labels or fine-tuning.</p>\n\n",
                "matched_terms": [
                    "task",
                    "mave",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During both training and inference, the reference tokens are placed at the beginning of the input sequence, followed by the masked audio context. The Mamba decoder attends to this context through its long-range recurrence, effectively conditioning the generated speech on the speaker&#8217;s voice characteristics. This strategy enables true zero-shot TTS: given only a few seconds of reference audio and a target transcript, the model generates high-fidelity, speaker-consistent speech.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In speech editing tasks, the reference can be omitted because the surrounding unmasked audio already provides sufficient speaker context. However, for zero-shot TTS or cross-speaker editing, the reference utterance is essential for voice coherence.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, MAVE is a hybrid autoregressive decoder that integrates Mamba blocks for efficient long-sequence audio modeling with cross-attention for flexible text conditioning. By combining token rearrangement, delayed RVQ decoding, and explicit speaker conditioning, it supports high-fidelity speech editing and zero-shot TTS. The network efficiently handles a large disparity between text and audio sequence lengths, offering a scalable alternative to transformer-based methods.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "mave",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To train MAVE, we used the ScaledAdam optimizer and Eden Scheduler proposed in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib46\" title=\"\">46</a>]</cite> with a base learning rate of 0.01, batch size of 400k frames (i.e. 133.2 minutes), and total training step\nof 50k with gradient accumulation. The training of the 830M MAVE model took about 4 days on 4 NVIDIA A100 GPUs. For inference, we use Nucleus sampling <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib16\" title=\"\">16</a>]</cite> with p = 0.8 and a temperature of 1 for all experiments.</p>\n\n",
                "matched_terms": [
                    "proposed",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VoiceCraft has been observed to generate prolonged silences during synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite>. To mitigate this issue, the authors proposed reducing the probability of silence tokens and performing multiple generations per sample, selecting the shortest output as the final result. In all of our experiments involving VoiceCraft, we adopted this strategy, generating five samples per input and selecting the shortest one. In contrast we haven&#8217;t noticed this problem in our model, so we produce a single generation with no further processing.</p>\n\n",
                "matched_terms": [
                    "proposed",
                    "model",
                    "voicecraft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Results on Speech Editing &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the results of both VoiceCraft <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite> and MAVE on RealEdit dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite>, which reflects real-life speech editing cases that are both challenging and diverse. The original dataset contains 100 utterances from LibriTTS (dev-clean and dev-other) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib50\" title=\"\">50</a>]</cite>, 100 utterances from\nYouTube (from Gigaspeech testset) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib6\" title=\"\">6</a>]</cite>, and 110 utterances from the Spotify Podcast dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib8\" title=\"\">8</a>]</cite>. However, the Spotify podcasts dataset is no longer available on the official website, so we have used only the 200 audios that are still publicly available.</p>\n\n",
                "matched_terms": [
                    "mave",
                    "voicecraft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We computed the Word Error Rate (WER) between the generated audio and the target transcript using Whisper-large and Whisper-medium.en <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib34\" title=\"\">34</a>]</cite>. In addition, we conducted a human evaluation to assess naturalness and intelligibility, based on audio samples from 80 randomly selected test cases for each model. Specifically, 20 people of proven English fluency participated in the study, they were divided into 2 groups of 10 people, and each group was asked to evaluate 120 audios (40 from each method) on the naturalness and intelligibility on a 5-point Likert scale (poor=1, excellent=5). For further details we refer to&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS3.SSS1\" title=\"A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.3.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "naturalness",
                    "model",
                    "intelligibility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MAVE demonstrates superior performance compared to VoiceCraft, achieving smaller WER and higher mean opinion scores (MOS) in both naturalness (3.90 vs. 3.77) and intelligibility (4.25 vs. 4.20). Notably, the performance gap between our model and the ground truth recordings is minimal, with differences of less than 0.1 in naturalness MOS and less than 0.07 in intelligibility MOS. This is an indication that MAVE-edits feature strong perceptual correlation to human speech. It is worth noting that even for the original audio, the average naturalness ratings did not exceed 4.0 out of 5.0. This is mostly attributed to the fact that the audio data were collected in the wild and their quality is inferior to studio recordings, with the presence of background noise and other sources of recording artifacts. This also highlights the challenging conditions under which the evaluation was conducted.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "performance",
                    "wer",
                    "ground",
                    "mos",
                    "truth",
                    "intelligibility",
                    "naturalness",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further assess the perceptual quality of our generated speech relative to ground truth, we conducted an additional study in which we asked a group of 10 users of proven English fluency to do a side-by-side comparison on the naturalness of the original unedited audio and our edited audio on 40 samples from the RealEdit dataset. The results are very promising and demonstrate that 57.2% of the times, users reported that both audios sound equally natural, while in 24.8% of cases, the ground truth was judged as more natural, and interestingly 18% of the times, the edited audio was preferred over the original. These findings indicate that in the majority of cases, our generated audios are indistinguishable from the original ones. For further details about the setup, we refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS3.SSS2\" title=\"A.3.2 Comparative studies &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.3.2</span></a></p>\n\n",
                "matched_terms": [
                    "ground",
                    "naturalness",
                    "truth"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable a meaningful comparison with diffusion-based speech generation models, we selected FluentSpeech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib20\" title=\"\">20</a>]</cite> as the most relevant open-source baseline. However, due to its significantly smaller model size, a direct comparison with our approach would not be fair. To address this, we leveraged the larger variant of FluentSpeech retrained and evaluated by the authors of VoiceCraft. The authors have not made this model publicly available but have provided 14 synthesized utterances, publishing for each sample: (1) VoiceCraft&#8217;s generation, (2) the enhanced FluentSpeech model&#8217;s generation, and (3) the original audio with original and target transcript.\nWe generated corresponding outputs using MAVE for the same 14 samples and conducted a perceptual listening study to compare the audio quality between the three models. The evaluation consisted of a pairwise, side-by-side comparison, where each participant (20 in total) was presented with two audio samples from different models and asked to judge which exhibited better naturalness and intelligibility. With 14 samples per system and three possible pairings between the models (VoiceCraft vs. FluentSpeech, VoiceCraft vs. Ours, and FluentSpeech vs. Ours), each participant evaluated a total of 42 audio pairs. Further details on the experimental setup, including participant instructions, interface design, and evaluation protocol, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS3.SSS2\" title=\"A.3.2 Comparative studies &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.3.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "ours",
                    "voicecraft",
                    "intelligibility",
                    "naturalness",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.F2\" title=\"Figure 2 &#8227; 4.1 Results on Speech Editing &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> clearly indicate that MAVE accomplishes superior perceptual quality compared to both FluentSpeech and VoiceCraft. While VoiceCraft already holds an advantage over FluentSpeech as it was preferred in 47.1% vs. 13.2% for naturalness and 50.7% vs. 6.4% for intelligibility, our model surpasses FluentSpeech by an even larger margin, being favored in 62.9% of cases for naturalness and 59.3% for intelligibility. More importantly, our model also outperforms VoiceCraft: it is preferred over VoiceCraft in 33.6% of comparisons for naturalness and 21.4% for intelligibility, while VoiceCraft is preferred over ours in only 17.5% and 12.9%, respectively.</p>\n\n",
                "matched_terms": [
                    "ours",
                    "voicecraft",
                    "intelligibility",
                    "naturalness",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zero-shot text-to-speech (TTS) evaluation, we used the LibriTTS dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib50\" title=\"\">50</a>]</cite> and we randomly selected 500 utterances. For each utterance, we extracted a different uterrance with the same speaker ID. The prompt was trimmed to be as close as possible to 3 seconds in duration, with cuts made only at word boundaries. We then filtered the initial 500 samples to retain only those containing more than 8 words in the target text, ensuring sufficient linguistic complexity for meaningful evaluation. This filtering resulted in a final evaluation set of 372 samples.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the subjective evaluation, we randomly selected 80 samples from our test set and generated corresponding speech outputs using VoiceCraft and our proposed model. To ensure a balanced assessment, we conducted a listening study involving 20 native or near-native English speakers (C1&#8211;C2 proficiency level). Participants were randomly assigned to two groups of 10, with each group evaluating a total of 120 audio clips: 40 generated by VoiceCraft, 40 produced by MAVE, and 40 ground-truth recordings. Each participant rated the naturalness and intelligibility of the audio samples on a 5-point Likert scale (1 = poor, 5 = excellent). Further details regarding the experimental setup, rating criteria, and instructions provided to participants are available in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS3.SSS1\" title=\"A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.3.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "proposed",
                    "intelligibility",
                    "naturalness",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MAVE outperformed VoiceCraft in MOS evaluations across both naturalness (3.48 vs. 3.22) and intelligibility (4.20 vs. 4.01). While the overall gap between our model and the ground truth remains notable, a closer analysis reveals a reduced performance disparity under certain conditions. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T3\" title=\"Table 3 &#8227; 4.2 Results on Zero-shot TTS &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, which breaks down results by the number of words to be generated, with split points selected to ensure approximately balanced sample sizes across ranges, specifically 17, 22, 21, 20 for the different ranges in ascending order. The performance gap narrows significantly in the 8&#8211;15 word range. In this interval, our model achieves a naturalness score of 3.71 compared to the ground truth&#8217;s 3.87, and an intelligibility score of 4.26 versus 4.36, indicating competitive quality for medium-length utterances, while the gap becomes more significant with the increasing length of the target transcript. This trend is consistent with what the model is trained on, as it was trained on the Gigaspeech dataset, which consists of speech with moderate utterance lengths.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "performance",
                    "ground",
                    "analysis",
                    "mos",
                    "truth",
                    "intelligibility",
                    "naturalness",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we compare our model, with previous SOTA AR model for speech editing, namely VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite>, which uses a transformer backbone. All inference experiments were conducted on NVIDIA A100 GPU. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Efficiency Analysis &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> reports the inference time required to process the entire RealEdit dataset as well as the average and peak memory consumption. It is important to note that these reported numbers are based on a single generation per sample. The results clearly demonstrate the superior memory efficiency of MAVE compared to VoiceCraft with KV caching, requiring approximately six times less GPU memory&#8212;largely due to the selective state mechanism of the Mamba backbone. While VoiceCraft exhibits faster inference speed in this evaluation, this advantage is primarily attributed to the relatively short duration of the target audio segments. On average, our model generates about 85 tokens per sample, corresponding to approximately 1.7 seconds of speech, a regime in which autoregressive overheads are minimal.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave",
                    "voicecraft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, theoretical complexity analysis reveals that MAVE scales more favorably with sequence length. For longer generation tasks, our model not only maintains significant memory savings but also surpasses VoiceCraft in computational efficiency, offering better speed performance for extended audio synthesis (see &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS4\" title=\"A.4 Theoretical Analysis of Computational Complexity &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> for more details).</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "performance",
                    "analysis",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the ablation study, we considered the masked reconstruction task, in which we used a random subset of the Gigaspeech validation dataset, randomly masked a sequence of words and required the model to reconstruct them (details provided in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS2\" title=\"A.2 Ablation Study Setup &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Efficiency Analysis &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents a comprehensive ablation study comparing different architectural designs under similar conditions, including model size (approximately 830M parameters) and training setup. The results highlight that neither a pure transformer-based encoder-decoder architecture nor a standalone Mamba decoder achieves optimal performance, highlighting the importance of our hybrid design.</p>\n\n",
                "matched_terms": [
                    "task",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The encoder-decoder transformer, with the encoder utilized to get contextualized text tokens, cross-attention to condition the audio decoder on text, and transformer decoder to model audio dependencies, achieves moderate performance but lags behind our model in all metrics, particularly in WER (10.8 vs. <span class=\"ltx_text ltx_font_bold\">7.8</span>). In contrast, the Mamba-only model, where text and audio tokens are concatenated and processed autoregressively, performs significantly worse with a higher WER (13.0).</p>\n\n",
                "matched_terms": [
                    "wer",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Crucially, our MAVE architecture integrates both components, it uses cross-attention to explicitly condition the generation process on textual input while leveraging the Mamba decoder to efficiently model acoustic dependencies. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Efficiency Analysis &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, this hybrid approach yields superior performance across all metrics, achieving the lowest WER and MCD, best fundamental frequency (F0) accuracy, and highest PESQ score. These improvements demonstrate that the strength of MAVE does not arise from either cross-attention or Mamba in isolation, but from the combination of both.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "wer",
                    "mcd",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work we introduce MAVE, a novel hybrid architecture for high-fidelity speech synthesis that combines cross-attention and structured state-space modeling via Mamba within a single autoregressive framework. By leveraging Mamba for robust audio modeling and cross-attention for precise text&#8211;audio alignment, MAVE sets a new benchmark for speech editing and zero-shot text-to-speech in terms of both naturalness and efficiency. As future work, we plan to train MAVE on significantly longer audio sequences to further exploit Mamba&#8217;s ability to capture long-range dependencies.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "zeroshot",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development and deployment of speech editing models like MAVE raise important ethical concerns that must be carefully addressed. One primary issue is the potential for misuse in generating deceptive audio content, such as deepfakes, which could be exploited for misinformation, fraud, or impersonation. While our model enables beneficial applications, such as facilitating editing tasks by correcting errors in speech recordings, improving accessibility, or enhancing voice interfaces.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Configuration for our model, and the variants used in ablation studies are shown in Table &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.T6\" title=\"Table 6 &#8227; A.1 Implementation Details &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. All models were trained under consistent experimental settings, using Scaled Adam optimizer and Eden Secheduler proposed in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib46\" title=\"\">46</a>]</cite>, with a base learning rate of 0.01. Hyperparameters were carefully selected to ensure a fair comparison, with all models constrained to have approximately 830 million parameters.</p>\n\n",
                "matched_terms": [
                    "proposed",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To conduct ablation studies, we employed the masked reconstruction task. For dataset construction, we used the evaluation subset of GigaSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib6\" title=\"\">6</a>]</cite>, from which we randomly sampled 500 utterances. We applied several filtering criteria to ensure data quality: (1) utterances with a word error rate (WER) greater than 0.1 when transcribed by Whisper-medium.en&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib35\" title=\"\">35</a>]</cite> were excluded to ensure reliable reference transcripts; and (2) utterances containing fewer than five words were discarded to provide sufficient linguistic context.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study was conducted to evaluate the quality of audio samples generated by our model in comparison to VoiceCraft and ground-truth recordings, to provide insights of how well our model performs against state-of-the-art models and to assess how closely our synthesized speech approaches the quality of natural, human speech.\nWe sampled 80 random examples from the RealEdit benchmark to evaluate the models on the speech editing task, and another 80 from the LibriTTS to evaluate on zero-shot TTS. Then, for each task we generated the synthesized audios using both our model MAVE and VoiceCraft. After that we divided these audios into two groups, each group was assigned 120 audios (40 from each category). We then asked 20 people to assess the quality of the generation, assigning each user to one group, ensuring that each group receives exactly 10 people. Audios are randomly shuffled for each new user. At the beginning of the study, the instructions are provided to the users as illustrated in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F3\" title=\"Figure 3 &#8227; A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Then for each audio sample, participants were first presented with the corresponding transcript and asked to rate the naturalness of the speech as depicted in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F4\" title=\"Figure 4 &#8227; A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. After submitting their naturalness rating, they were then prompted to evaluate the same audio in terms of intelligibility.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "task",
                    "model",
                    "tts",
                    "intelligibility",
                    "naturalness",
                    "zeroshot",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further assess the quality of the generated audios, and to enable direct comparison between the different methods, we have conducted 2 comparative user studies.\nThe first study evaluates our model against VoiceCraft and FluentSpeech. We selected the 14 publicly available audio samples generated by both VoiceCraft and FluentSpeech, which are provided on the VoiceCraft demo page&#160;<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://jasonppy.github.io/VoiceCraft_web/\" title=\"\">https://jasonppy.github.io/VoiceCraft_web/</a>. Using the same text prompts, we generated corresponding outputs using MAVE. This resulted in three pairwise comparisons: (1) VoiceCraft vs. FluentSpeech, (2) VoiceCraft vs. Ours, and (3) FluentSpeech vs. Ours), with 14 pairs per comparison, yielding a total of 42 unique audio pairs.</p>\n\n",
                "matched_terms": [
                    "ours",
                    "model",
                    "mave",
                    "voicecraft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We asked 20 people of proven English fluency to evaluate each pair and choose which is better in terms of both naturalness and intelligibility, or to indicate that they are perceptually equal (three options: &#8221;First is better,&#8221; &#8221;Second is better,&#8221; or &#8221;Equal&#8221;) as show in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F5\" title=\"Figure 5 &#8227; A.3.2 Comparative studies &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. To minimize positional bias, the order of the two audios in each pair was randomized across participants, and the presentation order of the pairs themselves was shuffled for each user.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "intelligibility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second comparative study focuses on a direct comparison between MAVE and ground truth audio for the speech editing task. To accomplish this, we have selected 40 random samples from RealEdit data, provided the ground truth before editing and our edited audio, along with their respective transcript. In total, 10 native or near-native English speakers&#8212;were asked to compare the two audios in terms of naturalness, and indicate whether the ground truth sounded better, the MAVE-edit sounded better, or there was no perceptual difference between the two. Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F6\" title=\"Figure 6 &#8227; A.3.2 Comparative studies &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> illustrates the instructions provided to users at the beginning of the study, and Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F7\" title=\"Figure 7 &#8227; A.3.2 Comparative studies &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> provides an example interface of the side-by-side comparison.</p>\n\n",
                "matched_terms": [
                    "ground",
                    "task",
                    "truth",
                    "naturalness",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments the number of audio tokens is significantly larger than the\nnumber of text tokens (<math alttext=\"L_{y}\\gg L_{x}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mi>y</mi></msub><mo>&#8811;</mo><msub><mi>L</mi><mi>x</mi></msub></mrow><annotation encoding=\"application/x-tex\">L_{y}\\gg L_{x}</annotation></semantics></math>).\nUnder this regime the proposed encoder&#8211;decoder model with a Mamba decoder\noffers two clear advantages:</p>\n\n",
                "matched_terms": [
                    "proposed",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The authors acknowledge the use of a large language model (LLM) solely for language editing and grammatical refinement of the current manuscript. All scientific content, analysis, and interpretations presented herein are the sole responsibility of the authors.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "model"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba",
        "caption": "Table 3: MOS comparison across different transcript word-length ranges.",
        "body": "Naturalness MOS\nIntelligibility MOS\n\n\nModel\n8–15\n15–22\n23–34\n\n>>34\n8–15\n15–22\n23–34\n\n>>34\n\n\nVoiceCraft\n3.44±0.143.44\\pm 0.14\n3.17±0.153.17\\pm 0.15\n3.17±0.143.17\\pm 0.14\n3.11±0.153.11\\pm 0.15\n4.07±0.074.07\\pm 0.07\n4.00±0.074.00\\pm 0.07\n3.95±0.073.95\\pm 0.07\n4.01±0.154.01\\pm 0.15\n\n\nMAVE (ours)\n3.71±0.15\\textbf{3.71}\\pm 0.15\n3.45±0.16\\textbf{3.45}\\pm 0.16\n3.35±0.15\\textbf{3.35}\\pm 0.15\n3.46±0.16\\textbf{3.46}\\pm 0.16\n4.26±0.14\\textbf{4.26}\\pm 0.14\n4.28±0.13\\textbf{4.28}\\pm 0.13\n4.09±0.12\\textbf{4.09}\\pm 0.12\n4.19±0.12\\textbf{4.19}\\pm 0.12\n\n\nGround Truth\n3.87±0.183.87\\pm 0.18\n3.78±0.163.78\\pm 0.16\n3.93±0.143.93\\pm 0.14\n4.03±0.154.03\\pm 0.15\n4.36±0.144.36\\pm 0.14\n4.38±0.144.38\\pm 0.14\n4.33±0.124.33\\pm 0.12\n4.54±0.114.54\\pm 0.11",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">Naturalness MOS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">Intelligibility MOS</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">8&#8211;15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">15&#8211;22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">23&#8211;34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">8&#8211;15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">15&#8211;22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">23&#8211;34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>34</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">VoiceCraft</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"3.44\\pm 0.14\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m3\" intent=\":literal\"><semantics><mrow><mn>3.44</mn><mo>&#177;</mo><mn>0.14</mn></mrow><annotation encoding=\"application/x-tex\">3.44\\pm 0.14</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"3.17\\pm 0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m4\" intent=\":literal\"><semantics><mrow><mn>3.17</mn><mo>&#177;</mo><mn>0.15</mn></mrow><annotation encoding=\"application/x-tex\">3.17\\pm 0.15</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"3.17\\pm 0.14\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m5\" intent=\":literal\"><semantics><mrow><mn>3.17</mn><mo>&#177;</mo><mn>0.14</mn></mrow><annotation encoding=\"application/x-tex\">3.17\\pm 0.14</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"3.11\\pm 0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m6\" intent=\":literal\"><semantics><mrow><mn>3.11</mn><mo>&#177;</mo><mn>0.15</mn></mrow><annotation encoding=\"application/x-tex\">3.11\\pm 0.15</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"4.07\\pm 0.07\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m7\" intent=\":literal\"><semantics><mrow><mn>4.07</mn><mo>&#177;</mo><mn>0.07</mn></mrow><annotation encoding=\"application/x-tex\">4.07\\pm 0.07</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"4.00\\pm 0.07\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m8\" intent=\":literal\"><semantics><mrow><mn>4.00</mn><mo>&#177;</mo><mn>0.07</mn></mrow><annotation encoding=\"application/x-tex\">4.00\\pm 0.07</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"3.95\\pm 0.07\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m9\" intent=\":literal\"><semantics><mrow><mn>3.95</mn><mo>&#177;</mo><mn>0.07</mn></mrow><annotation encoding=\"application/x-tex\">3.95\\pm 0.07</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"4.01\\pm 0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m10\" intent=\":literal\"><semantics><mrow><mn>4.01</mn><mo>&#177;</mo><mn>0.15</mn></mrow><annotation encoding=\"application/x-tex\">4.01\\pm 0.15</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">MAVE (ours)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"\\textbf{3.71}\\pm 0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m11\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">3.71</mtext><mo>&#177;</mo><mn>0.15</mn></mrow><annotation encoding=\"application/x-tex\">\\textbf{3.71}\\pm 0.15</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"\\textbf{3.45}\\pm 0.16\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m12\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">3.45</mtext><mo>&#177;</mo><mn>0.16</mn></mrow><annotation encoding=\"application/x-tex\">\\textbf{3.45}\\pm 0.16</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"\\textbf{3.35}\\pm 0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m13\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">3.35</mtext><mo>&#177;</mo><mn>0.15</mn></mrow><annotation encoding=\"application/x-tex\">\\textbf{3.35}\\pm 0.15</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"\\textbf{3.46}\\pm 0.16\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m14\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">3.46</mtext><mo>&#177;</mo><mn>0.16</mn></mrow><annotation encoding=\"application/x-tex\">\\textbf{3.46}\\pm 0.16</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"\\textbf{4.26}\\pm 0.14\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m15\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">4.26</mtext><mo>&#177;</mo><mn>0.14</mn></mrow><annotation encoding=\"application/x-tex\">\\textbf{4.26}\\pm 0.14</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"\\textbf{4.28}\\pm 0.13\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m16\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">4.28</mtext><mo>&#177;</mo><mn>0.13</mn></mrow><annotation encoding=\"application/x-tex\">\\textbf{4.28}\\pm 0.13</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"\\textbf{4.09}\\pm 0.12\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m17\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">4.09</mtext><mo>&#177;</mo><mn>0.12</mn></mrow><annotation encoding=\"application/x-tex\">\\textbf{4.09}\\pm 0.12</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"\\textbf{4.19}\\pm 0.12\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m18\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">4.19</mtext><mo>&#177;</mo><mn>0.12</mn></mrow><annotation encoding=\"application/x-tex\">\\textbf{4.19}\\pm 0.12</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">Ground Truth</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"3.87\\pm 0.18\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m19\" intent=\":literal\"><semantics><mrow><mn>3.87</mn><mo>&#177;</mo><mn>0.18</mn></mrow><annotation encoding=\"application/x-tex\">3.87\\pm 0.18</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"3.78\\pm 0.16\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m20\" intent=\":literal\"><semantics><mrow><mn>3.78</mn><mo>&#177;</mo><mn>0.16</mn></mrow><annotation encoding=\"application/x-tex\">3.78\\pm 0.16</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"3.93\\pm 0.14\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m21\" intent=\":literal\"><semantics><mrow><mn>3.93</mn><mo>&#177;</mo><mn>0.14</mn></mrow><annotation encoding=\"application/x-tex\">3.93\\pm 0.14</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"4.03\\pm 0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m22\" intent=\":literal\"><semantics><mrow><mn>4.03</mn><mo>&#177;</mo><mn>0.15</mn></mrow><annotation encoding=\"application/x-tex\">4.03\\pm 0.15</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"4.36\\pm 0.14\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m23\" intent=\":literal\"><semantics><mrow><mn>4.36</mn><mo>&#177;</mo><mn>0.14</mn></mrow><annotation encoding=\"application/x-tex\">4.36\\pm 0.14</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"4.38\\pm 0.14\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m24\" intent=\":literal\"><semantics><mrow><mn>4.38</mn><mo>&#177;</mo><mn>0.14</mn></mrow><annotation encoding=\"application/x-tex\">4.38\\pm 0.14</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"4.33\\pm 0.12\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m25\" intent=\":literal\"><semantics><mrow><mn>4.33</mn><mo>&#177;</mo><mn>0.12</mn></mrow><annotation encoding=\"application/x-tex\">4.33\\pm 0.12</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"4.54\\pm 0.11\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m26\" intent=\":literal\"><semantics><mrow><mn>4.54</mn><mo>&#177;</mo><mn>0.11</mn></mrow><annotation encoding=\"application/x-tex\">4.54\\pm 0.11</annotation></semantics></math></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "438±014438pm",
            "15–22",
            "voicecraft",
            "409±012textbf409pm",
            "335±015textbf335pm",
            "400±007400pm",
            "transcript",
            "317±014317pm",
            "371±015textbf371pm",
            "mave",
            "ranges",
            "393±014393pm",
            "403±015403pm",
            "454±011454pm",
            "different",
            "419±012textbf419pm",
            "truth",
            "407±007407pm",
            "wordlength",
            "311±015311pm",
            "317±015317pm",
            "436±014436pm",
            "8–15",
            "23–34",
            "comparison",
            "mos",
            "344±014344pm",
            "426±014textbf426pm",
            "401±015401pm",
            "345±016textbf345pm",
            "ours",
            "across",
            "378±016378pm",
            "395±007395pm",
            "428±013textbf428pm",
            "ground",
            "346±016textbf346pm",
            "433±012433pm",
            "intelligibility",
            "naturalness",
            "model",
            "387±018387pm"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">MAVE outperformed VoiceCraft in MOS evaluations across both naturalness (3.48 vs. 3.22) and intelligibility (4.20 vs. 4.01). While the overall gap between our model and the ground truth remains notable, a closer analysis reveals a reduced performance disparity under certain conditions. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T3\" title=\"Table 3 &#8227; 4.2 Results on Zero-shot TTS &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, which breaks down results by the number of words to be generated, with split points selected to ensure approximately balanced sample sizes across ranges, specifically 17, 22, 21, 20 for the different ranges in ascending order. The performance gap narrows significantly in the 8&#8211;15 word range. In this interval, our model achieves a naturalness score of 3.71 compared to the ground truth&#8217;s 3.87, and an intelligibility score of 4.26 versus 4.36, indicating competitive quality for medium-length utterances, while the gap becomes more significant with the increasing length of the target transcript. This trend is consistent with what the model is trained on, as it was trained on the Gigaspeech dataset, which consists of speech with moderate utterance lengths.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">MAVE</span> (<span class=\"ltx_text ltx_font_bold\">M</span>amba with Cross-<span class=\"ltx_text ltx_font_bold\">A</span>ttention for <span class=\"ltx_text ltx_font_bold\">V</span>oice <span class=\"ltx_text ltx_font_bold\">E</span>diting and Synthesis), a novel autoregressive architecture for text-conditioned voice editing and high-fidelity text-to-speech (TTS) synthesis, built on a cross-attentive Mamba backbone. MAVE achieves state-of-the-art performance in speech editing and very competitive results in zero-shot TTS, while not being explicitly trained on the latter task, outperforming leading autoregressive and diffusion models on diverse, real-world audio. By integrating Mamba for efficient audio sequence modeling with cross-attention for precise text-acoustic alignment, MAVE enables context-aware voice editing with exceptional naturalness and speaker consistency. In pairwise human evaluations on a random 40-sample subset of the RealEdit benchmark (400 judgments), 57.2% of listeners rated MAVE-edited speech as perceptually equal to the original, while 24.8% prefered the original and 18.0% MAVE- demonstrating that in the majority of cases edits are indistinguishable from the source. MAVE compares favorably with VoiceCraft and FluentSpeech both on pairwise comparisons and standalone mean opinion score (MOS) evaluations. For zero-shot TTS, MAVE exceeds VoiceCraft in both speaker similarity and naturalness, without requiring multiple inference runs or post-processing. Remarkably, these quality gains come with a significantly lower memory cost and approximately the same latency: MAVE requires <math alttext=\"\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\sim$}}{\\scalebox{0.8}{$\\textstyle\\sim$}}{\\scalebox{0.8}{$\\scriptstyle\\sim$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\sim$}}6\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\times$}}{\\scalebox{0.8}{$\\textstyle\\times$}}{\\scalebox{0.8}{$\\scriptstyle\\times$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\times$}}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"m6\" intent=\":literal\"><semantics><mrow><mpadded depth=\"0.0pt\" height=\"2.9pt\" style=\"width:8.4pt;height:2.9pt;vertical-align:-0.0pt;\" width=\"8.4pt\"><mo>&#8764;</mo></mpadded><mn>6</mn><mpadded depth=\"0.7pt\" height=\"4.7pt\" style=\"width:8.0pt;height:5.4pt;vertical-align:-0.7pt;\" width=\"8.0pt\"><mo>&#215;</mo></mpadded></mrow><annotation encoding=\"application/x-tex\">\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\sim$}}{\\scalebox{0.8}{$\\textstyle\\sim$}}{\\scalebox{0.8}{$\\scriptstyle\\sim$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\sim$}}6\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\times$}}{\\scalebox{0.8}{$\\textstyle\\times$}}{\\scalebox{0.8}{$\\scriptstyle\\times$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\times$}}</annotation></semantics></math> less memory than VoiceCraft during inference on utterances from the RealEdit database (mean duration: 6.21s, A100, FP16, batch size 1). Our results demonstrate that MAVE establishes a new standard for flexible, high-fidelity voice editing and synthesis through the synergistic integration of structured state-space modeling and cross-modal attention.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "mos",
                    "mave",
                    "voicecraft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these limitations, we propose <span class=\"ltx_text ltx_font_bold\">MAVE</span> (<span class=\"ltx_text ltx_font_bold\">M</span>amba with Cross-<span class=\"ltx_text ltx_font_bold\">A</span>ttention for <span class=\"ltx_text ltx_font_bold\">V</span>oice <span class=\"ltx_text ltx_font_bold\">E</span>diting and Synthesis), a novel <em class=\"ltx_emph ltx_font_italic\">autoregressive</em> architecture for text-conditioned voice editing and zero-shot TTS that synergizes Mamba state-space models with cross-attention mechanisms. Unlike Transformer-based decoders&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib42\" title=\"\">42</a>]</cite>, MAVE replaces self-attention with structured state-space sequences (SSMs), enabling <em class=\"ltx_emph ltx_font_italic\">linear-complexity modeling</em> of dependencies between acoustic tokens. Crucially, our cross-attention module dynamically aligns <em class=\"ltx_emph ltx_font_italic\">augmented text inputs</em> with acoustic tokens, allowing the model to &#8220;edit&#8221; speech by attending to the textual information.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To the best of our knowledge, <span class=\"ltx_text ltx_font_bold\">MAVE</span> &#8211; built on a cross-attentive Mamba backbone &#8211; is the first successful application of a structured state-space model to text-conditional speech generation, namely speech editing and zero-shot TTS. On the challenging RealEdit benchmark, MAVE achieves <span class=\"ltx_text ltx_font_bold\">human-parity naturalness</span> in speech editing and surpasses state-of-the-art models like VoiceCraft and FluentSpeech in both speaker similarity and naturalness (MOS) without requiring any post-processing. Moreover, MAVE offers significant efficiency gains, reducing memory usage by <span class=\"ltx_text ltx_font_bold\">six</span> times compared to Transformer-based VoiceCraft, while enabling single-pass generation without silent tokens handling or multiple runs. Our results demonstrate that Mamba-based models enhanced with cross-attention can outperform both autoregressive and diffusion-based approaches in fidelity, efficiency, and robustness, establishing a new direction for scalable and high-quality speech generation.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "mos",
                    "naturalness",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MAVE in a Nutshell.</span>\nIn summary, MAVE establishes a novel paradigm in text-conditioned speech generation by bridging the efficiency of State Space Models with the flexibility of cross-modal attention. Unlike prior SSM-based approaches such as CM-Mamba&#160;&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib18\" title=\"\">18</a>]</cite>, which require strict alignment between text and audio sequences, our architecture introduces a length-agnostic cross-attention mechanism that enables rich, contextualized phoneme conditioning without artificial upsampling or architectural constraints. By replacing the quadratic self-attention of Transformer-based editors like Voicecraft with linear-time selective state updates, MAVE achieves scalable long-context modeling while preserving prosodic coherence and speaker identity through recurrence. Our framework further unifies three critical capabilities: (1) efficient bidirectional context access via causal token rearrangement, (2) precise linguistic control through differentiable cross-attention on phonemized input, and (3) reference-guided voice consistency without explicit speaker embeddings. As demonstrated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4\" title=\"4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, MAVE is the first model to simultaneously outperform both autoregressive (e.g., VoiceCraft) and non-autoregressive (e.g., FluentSpeech) baselines in human evaluations across speech editing and zero-shot TTS, setting a new standard for unified, high-fidelity, and scalable speech generation. To our knowledge, this is the first successful integration of cross-attention into a Mamba-based audio decoder for unrestricted, long-form speech editing and synthesis.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave",
                    "voicecraft",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present <span class=\"ltx_text ltx_font_bold\">MAVE</span>, a novel architecture for text-conditioned speech editing and zero-shot text-to-speech (TTS). The core challenge lies in autoregressively generating long sequences of discrete audio tokens conditioned on a textual transcript, where the quadratic complexity of self-attention in Transformers becomes prohibitive. Audio signals are typically encoded at 50 Hz using residual vector quantization (RVQ), yielding 4 to 8 discrete tokens per frame, which amounts to 200&#8211;400 tokens per second of speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib47\" title=\"\">47</a>]</cite>. In contrast, the corresponding phoneme sequence contains approximately 12 units per second&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib36\" title=\"\">36</a>]</cite>, resulting in a significant sequence length mismatch between modalities. This disparity makes direct application of Transformer-based models inefficient for high-fidelity, long-form audio generation.</p>\n\n",
                "matched_terms": [
                    "mave",
                    "transcript"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these issues, we propose MAVE, a hybrid decoder that combines the efficiency of Mamba for audio token modeling with a flexible cross-attention mechanism for text conditioning&#8212;without requiring aligned sequence lengths. Our model supports context-aware speech infilling and zero-shot TTS by leveraging both surrounding audio context and linguistic input.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During both training and inference, the reference tokens are placed at the beginning of the input sequence, followed by the masked audio context. The Mamba decoder attends to this context through its long-range recurrence, effectively conditioning the generated speech on the speaker&#8217;s voice characteristics. This strategy enables true zero-shot TTS: given only a few seconds of reference audio and a target transcript, the model generates high-fidelity, speaker-consistent speech.</p>\n\n",
                "matched_terms": [
                    "model",
                    "transcript"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To train MAVE, we used the ScaledAdam optimizer and Eden Scheduler proposed in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib46\" title=\"\">46</a>]</cite> with a base learning rate of 0.01, batch size of 400k frames (i.e. 133.2 minutes), and total training step\nof 50k with gradient accumulation. The training of the 830M MAVE model took about 4 days on 4 NVIDIA A100 GPUs. For inference, we use Nucleus sampling <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib16\" title=\"\">16</a>]</cite> with p = 0.8 and a temperature of 1 for all experiments.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VoiceCraft has been observed to generate prolonged silences during synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite>. To mitigate this issue, the authors proposed reducing the probability of silence tokens and performing multiple generations per sample, selecting the shortest output as the final result. In all of our experiments involving VoiceCraft, we adopted this strategy, generating five samples per input and selecting the shortest one. In contrast we haven&#8217;t noticed this problem in our model, so we produce a single generation with no further processing.</p>\n\n",
                "matched_terms": [
                    "model",
                    "voicecraft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Results on Speech Editing &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the results of both VoiceCraft <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite> and MAVE on RealEdit dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite>, which reflects real-life speech editing cases that are both challenging and diverse. The original dataset contains 100 utterances from LibriTTS (dev-clean and dev-other) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib50\" title=\"\">50</a>]</cite>, 100 utterances from\nYouTube (from Gigaspeech testset) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib6\" title=\"\">6</a>]</cite>, and 110 utterances from the Spotify Podcast dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib8\" title=\"\">8</a>]</cite>. However, the Spotify podcasts dataset is no longer available on the official website, so we have used only the 200 audios that are still publicly available.</p>\n\n",
                "matched_terms": [
                    "mave",
                    "voicecraft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We computed the Word Error Rate (WER) between the generated audio and the target transcript using Whisper-large and Whisper-medium.en <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib34\" title=\"\">34</a>]</cite>. In addition, we conducted a human evaluation to assess naturalness and intelligibility, based on audio samples from 80 randomly selected test cases for each model. Specifically, 20 people of proven English fluency participated in the study, they were divided into 2 groups of 10 people, and each group was asked to evaluate 120 audios (40 from each method) on the naturalness and intelligibility on a 5-point Likert scale (poor=1, excellent=5). For further details we refer to&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS3.SSS1\" title=\"A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.3.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "model",
                    "transcript",
                    "intelligibility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MAVE demonstrates superior performance compared to VoiceCraft, achieving smaller WER and higher mean opinion scores (MOS) in both naturalness (3.90 vs. 3.77) and intelligibility (4.25 vs. 4.20). Notably, the performance gap between our model and the ground truth recordings is minimal, with differences of less than 0.1 in naturalness MOS and less than 0.07 in intelligibility MOS. This is an indication that MAVE-edits feature strong perceptual correlation to human speech. It is worth noting that even for the original audio, the average naturalness ratings did not exceed 4.0 out of 5.0. This is mostly attributed to the fact that the audio data were collected in the wild and their quality is inferior to studio recordings, with the presence of background noise and other sources of recording artifacts. This also highlights the challenging conditions under which the evaluation was conducted.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "ground",
                    "mos",
                    "truth",
                    "intelligibility",
                    "naturalness",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further assess the perceptual quality of our generated speech relative to ground truth, we conducted an additional study in which we asked a group of 10 users of proven English fluency to do a side-by-side comparison on the naturalness of the original unedited audio and our edited audio on 40 samples from the RealEdit dataset. The results are very promising and demonstrate that 57.2% of the times, users reported that both audios sound equally natural, while in 24.8% of cases, the ground truth was judged as more natural, and interestingly 18% of the times, the edited audio was preferred over the original. These findings indicate that in the majority of cases, our generated audios are indistinguishable from the original ones. For further details about the setup, we refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS3.SSS2\" title=\"A.3.2 Comparative studies &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.3.2</span></a></p>\n\n",
                "matched_terms": [
                    "ground",
                    "naturalness",
                    "comparison",
                    "truth"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable a meaningful comparison with diffusion-based speech generation models, we selected FluentSpeech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib20\" title=\"\">20</a>]</cite> as the most relevant open-source baseline. However, due to its significantly smaller model size, a direct comparison with our approach would not be fair. To address this, we leveraged the larger variant of FluentSpeech retrained and evaluated by the authors of VoiceCraft. The authors have not made this model publicly available but have provided 14 synthesized utterances, publishing for each sample: (1) VoiceCraft&#8217;s generation, (2) the enhanced FluentSpeech model&#8217;s generation, and (3) the original audio with original and target transcript.\nWe generated corresponding outputs using MAVE for the same 14 samples and conducted a perceptual listening study to compare the audio quality between the three models. The evaluation consisted of a pairwise, side-by-side comparison, where each participant (20 in total) was presented with two audio samples from different models and asked to judge which exhibited better naturalness and intelligibility. With 14 samples per system and three possible pairings between the models (VoiceCraft vs. FluentSpeech, VoiceCraft vs. Ours, and FluentSpeech vs. Ours), each participant evaluated a total of 42 audio pairs. Further details on the experimental setup, including participant instructions, interface design, and evaluation protocol, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS3.SSS2\" title=\"A.3.2 Comparative studies &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.3.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "ours",
                    "voicecraft",
                    "comparison",
                    "different",
                    "transcript",
                    "intelligibility",
                    "naturalness",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.F2\" title=\"Figure 2 &#8227; 4.1 Results on Speech Editing &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> clearly indicate that MAVE accomplishes superior perceptual quality compared to both FluentSpeech and VoiceCraft. While VoiceCraft already holds an advantage over FluentSpeech as it was preferred in 47.1% vs. 13.2% for naturalness and 50.7% vs. 6.4% for intelligibility, our model surpasses FluentSpeech by an even larger margin, being favored in 62.9% of cases for naturalness and 59.3% for intelligibility. More importantly, our model also outperforms VoiceCraft: it is preferred over VoiceCraft in 33.6% of comparisons for naturalness and 21.4% for intelligibility, while VoiceCraft is preferred over ours in only 17.5% and 12.9%, respectively.</p>\n\n",
                "matched_terms": [
                    "ours",
                    "voicecraft",
                    "intelligibility",
                    "naturalness",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the subjective evaluation, we randomly selected 80 samples from our test set and generated corresponding speech outputs using VoiceCraft and our proposed model. To ensure a balanced assessment, we conducted a listening study involving 20 native or near-native English speakers (C1&#8211;C2 proficiency level). Participants were randomly assigned to two groups of 10, with each group evaluating a total of 120 audio clips: 40 generated by VoiceCraft, 40 produced by MAVE, and 40 ground-truth recordings. Each participant rated the naturalness and intelligibility of the audio samples on a 5-point Likert scale (1 = poor, 5 = excellent). Further details regarding the experimental setup, rating criteria, and instructions provided to participants are available in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS3.SSS1\" title=\"A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.3.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "intelligibility",
                    "naturalness",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we compare our model, with previous SOTA AR model for speech editing, namely VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite>, which uses a transformer backbone. All inference experiments were conducted on NVIDIA A100 GPU. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Efficiency Analysis &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> reports the inference time required to process the entire RealEdit dataset as well as the average and peak memory consumption. It is important to note that these reported numbers are based on a single generation per sample. The results clearly demonstrate the superior memory efficiency of MAVE compared to VoiceCraft with KV caching, requiring approximately six times less GPU memory&#8212;largely due to the selective state mechanism of the Mamba backbone. While VoiceCraft exhibits faster inference speed in this evaluation, this advantage is primarily attributed to the relatively short duration of the target audio segments. On average, our model generates about 85 tokens per sample, corresponding to approximately 1.7 seconds of speech, a regime in which autoregressive overheads are minimal.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave",
                    "voicecraft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, theoretical complexity analysis reveals that MAVE scales more favorably with sequence length. For longer generation tasks, our model not only maintains significant memory savings but also surpasses VoiceCraft in computational efficiency, offering better speed performance for extended audio synthesis (see &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS4\" title=\"A.4 Theoretical Analysis of Computational Complexity &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> for more details).</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave",
                    "voicecraft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the ablation study, we considered the masked reconstruction task, in which we used a random subset of the Gigaspeech validation dataset, randomly masked a sequence of words and required the model to reconstruct them (details provided in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS2\" title=\"A.2 Ablation Study Setup &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Efficiency Analysis &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents a comprehensive ablation study comparing different architectural designs under similar conditions, including model size (approximately 830M parameters) and training setup. The results highlight that neither a pure transformer-based encoder-decoder architecture nor a standalone Mamba decoder achieves optimal performance, highlighting the importance of our hybrid design.</p>\n\n",
                "matched_terms": [
                    "model",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Crucially, our MAVE architecture integrates both components, it uses cross-attention to explicitly condition the generation process on textual input while leveraging the Mamba decoder to efficiently model acoustic dependencies. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Efficiency Analysis &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, this hybrid approach yields superior performance across all metrics, achieving the lowest WER and MCD, best fundamental frequency (F0) accuracy, and highest PESQ score. These improvements demonstrate that the strength of MAVE does not arise from either cross-attention or Mamba in isolation, but from the combination of both.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work we introduce MAVE, a novel hybrid architecture for high-fidelity speech synthesis that combines cross-attention and structured state-space modeling via Mamba within a single autoregressive framework. By leveraging Mamba for robust audio modeling and cross-attention for precise text&#8211;audio alignment, MAVE sets a new benchmark for speech editing and zero-shot text-to-speech in terms of both naturalness and efficiency. As future work, we plan to train MAVE on significantly longer audio sequences to further exploit Mamba&#8217;s ability to capture long-range dependencies.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development and deployment of speech editing models like MAVE raise important ethical concerns that must be carefully addressed. One primary issue is the potential for misuse in generating deceptive audio content, such as deepfakes, which could be exploited for misinformation, fraud, or impersonation. While our model enables beneficial applications, such as facilitating editing tasks by correcting errors in speech recordings, improving accessibility, or enhancing voice interfaces.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Configuration for our model, and the variants used in ablation studies are shown in Table &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.T6\" title=\"Table 6 &#8227; A.1 Implementation Details &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. All models were trained under consistent experimental settings, using Scaled Adam optimizer and Eden Secheduler proposed in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib46\" title=\"\">46</a>]</cite>, with a base learning rate of 0.01. Hyperparameters were carefully selected to ensure a fair comparison, with all models constrained to have approximately 830 million parameters.</p>\n\n",
                "matched_terms": [
                    "model",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study was conducted to evaluate the quality of audio samples generated by our model in comparison to VoiceCraft and ground-truth recordings, to provide insights of how well our model performs against state-of-the-art models and to assess how closely our synthesized speech approaches the quality of natural, human speech.\nWe sampled 80 random examples from the RealEdit benchmark to evaluate the models on the speech editing task, and another 80 from the LibriTTS to evaluate on zero-shot TTS. Then, for each task we generated the synthesized audios using both our model MAVE and VoiceCraft. After that we divided these audios into two groups, each group was assigned 120 audios (40 from each category). We then asked 20 people to assess the quality of the generation, assigning each user to one group, ensuring that each group receives exactly 10 people. Audios are randomly shuffled for each new user. At the beginning of the study, the instructions are provided to the users as illustrated in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F3\" title=\"Figure 3 &#8227; A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Then for each audio sample, participants were first presented with the corresponding transcript and asked to rate the naturalness of the speech as depicted in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F4\" title=\"Figure 4 &#8227; A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. After submitting their naturalness rating, they were then prompted to evaluate the same audio in terms of intelligibility.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "comparison",
                    "transcript",
                    "intelligibility",
                    "naturalness",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further assess the quality of the generated audios, and to enable direct comparison between the different methods, we have conducted 2 comparative user studies.\nThe first study evaluates our model against VoiceCraft and FluentSpeech. We selected the 14 publicly available audio samples generated by both VoiceCraft and FluentSpeech, which are provided on the VoiceCraft demo page&#160;<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://jasonppy.github.io/VoiceCraft_web/\" title=\"\">https://jasonppy.github.io/VoiceCraft_web/</a>. Using the same text prompts, we generated corresponding outputs using MAVE. This resulted in three pairwise comparisons: (1) VoiceCraft vs. FluentSpeech, (2) VoiceCraft vs. Ours, and (3) FluentSpeech vs. Ours), with 14 pairs per comparison, yielding a total of 42 unique audio pairs.</p>\n\n",
                "matched_terms": [
                    "ours",
                    "voicecraft",
                    "comparison",
                    "different",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We asked 20 people of proven English fluency to evaluate each pair and choose which is better in terms of both naturalness and intelligibility, or to indicate that they are perceptually equal (three options: &#8221;First is better,&#8221; &#8221;Second is better,&#8221; or &#8221;Equal&#8221;) as show in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F5\" title=\"Figure 5 &#8227; A.3.2 Comparative studies &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. To minimize positional bias, the order of the two audios in each pair was randomized across participants, and the presentation order of the pairs themselves was shuffled for each user.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "across",
                    "intelligibility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second comparative study focuses on a direct comparison between MAVE and ground truth audio for the speech editing task. To accomplish this, we have selected 40 random samples from RealEdit data, provided the ground truth before editing and our edited audio, along with their respective transcript. In total, 10 native or near-native English speakers&#8212;were asked to compare the two audios in terms of naturalness, and indicate whether the ground truth sounded better, the MAVE-edit sounded better, or there was no perceptual difference between the two. Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F6\" title=\"Figure 6 &#8227; A.3.2 Comparative studies &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> illustrates the instructions provided to users at the beginning of the study, and Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F7\" title=\"Figure 7 &#8227; A.3.2 Comparative studies &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> provides an example interface of the side-by-side comparison.</p>\n\n",
                "matched_terms": [
                    "ground",
                    "comparison",
                    "truth",
                    "transcript",
                    "naturalness",
                    "mave"
                ]
            }
        ]
    },
    "S4.T5.fig1": {
        "source_file": "Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba",
        "caption": "Table 4: Performance comparison of MAVE (ours) and VoiceCraft on the RealEdit dataset.",
        "body": "Model\n\n\nKV Cache\n\n\n\n\nInference Time\n\n\n\n\nTokens / Sec\n\n\n\n\nAvg / Max Mem. (GB)\n\n\n\n\nMAVE (ours)\n\n\nX-attn\n\n\n\n\n5 min 17 s\n\n\n\n\n53.5\n\n\n\n\n6.2 / 6.5\n\n\n\n\nVoiceCraft\n\n\nYes\n\n\n\n\n4 min 33 s\n\n\n\n\n74.9\n\n\n\n\n37.9 / 40.2\n\n\n\n\nVoiceCraft\n\n\nNo\n\n\n\n\n22 min 31 s\n\n\n\n\n15.1\n\n\n\n\n9.2 / 9.7",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:42.7pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">KV Cache</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:56.9pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">Inference Time</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:56.9pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">Tokens / Sec</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:56.9pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">Avg / Max Mem. (GB)</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">MAVE (ours)</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:42.7pt;\">X-attn</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:56.9pt;\">5 min 17 s</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:56.9pt;\">53.5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:56.9pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">6.2 / 6.5</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">VoiceCraft</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:42.7pt;\">Yes</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:56.9pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">4 min 33 s</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:56.9pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">74.9</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:56.9pt;\">37.9 / 40.2</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">VoiceCraft</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:42.7pt;\">No</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:56.9pt;\">22 min 31 s</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:56.9pt;\">15.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:56.9pt;\">9.2 / 9.7</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "voicecraft",
            "sec",
            "realedit",
            "inference",
            "mem",
            "time",
            "avg",
            "mave",
            "xattn",
            "min",
            "max",
            "cache",
            "performance",
            "yes",
            "comparison",
            "tokens",
            "dataset",
            "ours",
            "model"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">MAVE</span> (<span class=\"ltx_text ltx_font_bold\">M</span>amba with Cross-<span class=\"ltx_text ltx_font_bold\">A</span>ttention for <span class=\"ltx_text ltx_font_bold\">V</span>oice <span class=\"ltx_text ltx_font_bold\">E</span>diting and Synthesis), a novel autoregressive architecture for text-conditioned voice editing and high-fidelity text-to-speech (TTS) synthesis, built on a cross-attentive Mamba backbone. MAVE achieves state-of-the-art performance in speech editing and very competitive results in zero-shot TTS, while not being explicitly trained on the latter task, outperforming leading autoregressive and diffusion models on diverse, real-world audio. By integrating Mamba for efficient audio sequence modeling with cross-attention for precise text-acoustic alignment, MAVE enables context-aware voice editing with exceptional naturalness and speaker consistency. In pairwise human evaluations on a random 40-sample subset of the RealEdit benchmark (400 judgments), 57.2% of listeners rated MAVE-edited speech as perceptually equal to the original, while 24.8% prefered the original and 18.0% MAVE- demonstrating that in the majority of cases edits are indistinguishable from the source. MAVE compares favorably with VoiceCraft and FluentSpeech both on pairwise comparisons and standalone mean opinion score (MOS) evaluations. For zero-shot TTS, MAVE exceeds VoiceCraft in both speaker similarity and naturalness, without requiring multiple inference runs or post-processing. Remarkably, these quality gains come with a significantly lower memory cost and approximately the same latency: MAVE requires <math alttext=\"\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\sim$}}{\\scalebox{0.8}{$\\textstyle\\sim$}}{\\scalebox{0.8}{$\\scriptstyle\\sim$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\sim$}}6\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\times$}}{\\scalebox{0.8}{$\\textstyle\\times$}}{\\scalebox{0.8}{$\\scriptstyle\\times$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\times$}}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"m6\" intent=\":literal\"><semantics><mrow><mpadded depth=\"0.0pt\" height=\"2.9pt\" style=\"width:8.4pt;height:2.9pt;vertical-align:-0.0pt;\" width=\"8.4pt\"><mo>&#8764;</mo></mpadded><mn>6</mn><mpadded depth=\"0.7pt\" height=\"4.7pt\" style=\"width:8.0pt;height:5.4pt;vertical-align:-0.7pt;\" width=\"8.0pt\"><mo>&#215;</mo></mpadded></mrow><annotation encoding=\"application/x-tex\">\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\sim$}}{\\scalebox{0.8}{$\\textstyle\\sim$}}{\\scalebox{0.8}{$\\scriptstyle\\sim$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\sim$}}6\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\times$}}{\\scalebox{0.8}{$\\textstyle\\times$}}{\\scalebox{0.8}{$\\scriptstyle\\times$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\times$}}</annotation></semantics></math> less memory than VoiceCraft during inference on utterances from the RealEdit database (mean duration: 6.21s, A100, FP16, batch size 1). Our results demonstrate that MAVE establishes a new standard for flexible, high-fidelity voice editing and synthesis through the synergistic integration of structured state-space modeling and cross-modal attention.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "performance",
                    "realedit",
                    "inference",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these limitations, we propose <span class=\"ltx_text ltx_font_bold\">MAVE</span> (<span class=\"ltx_text ltx_font_bold\">M</span>amba with Cross-<span class=\"ltx_text ltx_font_bold\">A</span>ttention for <span class=\"ltx_text ltx_font_bold\">V</span>oice <span class=\"ltx_text ltx_font_bold\">E</span>diting and Synthesis), a novel <em class=\"ltx_emph ltx_font_italic\">autoregressive</em> architecture for text-conditioned voice editing and zero-shot TTS that synergizes Mamba state-space models with cross-attention mechanisms. Unlike Transformer-based decoders&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib42\" title=\"\">42</a>]</cite>, MAVE replaces self-attention with structured state-space sequences (SSMs), enabling <em class=\"ltx_emph ltx_font_italic\">linear-complexity modeling</em> of dependencies between acoustic tokens. Crucially, our cross-attention module dynamically aligns <em class=\"ltx_emph ltx_font_italic\">augmented text inputs</em> with acoustic tokens, allowing the model to &#8220;edit&#8221; speech by attending to the textual information.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To the best of our knowledge, <span class=\"ltx_text ltx_font_bold\">MAVE</span> &#8211; built on a cross-attentive Mamba backbone &#8211; is the first successful application of a structured state-space model to text-conditional speech generation, namely speech editing and zero-shot TTS. On the challenging RealEdit benchmark, MAVE achieves <span class=\"ltx_text ltx_font_bold\">human-parity naturalness</span> in speech editing and surpasses state-of-the-art models like VoiceCraft and FluentSpeech in both speaker similarity and naturalness (MOS) without requiring any post-processing. Moreover, MAVE offers significant efficiency gains, reducing memory usage by <span class=\"ltx_text ltx_font_bold\">six</span> times compared to Transformer-based VoiceCraft, while enabling single-pass generation without silent tokens handling or multiple runs. Our results demonstrate that Mamba-based models enhanced with cross-attention can outperform both autoregressive and diffusion-based approaches in fidelity, efficiency, and robustness, establishing a new direction for scalable and high-quality speech generation.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "realedit",
                    "model",
                    "mave",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Neural Codec Language Models for Speech Synthesis</span>\nThe evolution of high-fidelity speech generation has been significantly advanced by Neural Codec Language Models (NCLMs), which discretize speech into symbolic units through Residual Vector Quantization (RVQ) frameworks as shown by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib49\" title=\"\">49</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib10\" title=\"\">10</a>]</cite> and model temporal dependencies via autoregressive sequence learning. Originating in textless NLP research&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib32\" title=\"\">32</a>]</cite>, this paradigm achieved breakthrough performance in speech continuity with AudioLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib4\" title=\"\">4</a>]</cite>. A pivotal advancement emerged through the application of NCLMs to zero-shot text-to-speech synthesis, where models like VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib42\" title=\"\">42</a>]</cite> and Spear-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib23\" title=\"\">23</a>]</cite> reframed synthesis as transcript-conditioned speech continuation conditioned on brief speaker references. These systems substantially outperformed conventional non-NCLM approaches, prompting extensions for cross-lingual TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib52\" title=\"\">52</a>]</cite>, style-controlled synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib29\" title=\"\">29</a>]</cite>, and phoneme alignment refinement&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib39\" title=\"\">39</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The Evolution of Speech Editing</span>\nSpeech editing, defined as the precise modification of targeted speech segments while preserving unaltered regions, has evolved through three distinct generations of methodology. Early approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib21\" title=\"\">21</a>]</cite> relied on concatenating TTS-generated segments with original speech, inevitably introducing prosody mismatches and boundary artifacts due to the absence of contextual conditioning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib31\" title=\"\">31</a>]</cite>. Subsequent research introduced context-aware mechanisms through bidirectional fusion architectures&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib40\" title=\"\">40</a>]</cite> and masked reconstruction objectives&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib43\" title=\"\">43</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib5\" title=\"\">5</a>]</cite>, with Transformer-based systems demonstrating improved contextualization. Most recently, diffusion-based methods like FluentSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib20\" title=\"\">20</a>]</cite> achieved state-of-the-art performance on standard benchmarks through denoising frameworks. However, these approaches collectively suffer from three interrelated limitations. First, Transformer-based systems incur quadratic computational complexity relative to sequence length, restricting practical context windows. Second, evaluation protocols remain constrained to short editing spans&#8212;UniCATS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib11\" title=\"\">11</a>]</cite>, for instance, limits assessments to segments under two seconds, failing to address real-world editing scenarios involving multi-word phrases. Third and most critically, none incorporate mechanisms for leveraging linguistically augmented inputs (e.g., prosody-annotated text) to guide context-aware modifications. MAVE overcomes these constraints through its linear-complexity Mamba architecture and differentiable cross-attention fusion mechanism, enabling robust editing of spans up to 16 words while preserving natural prosody through explicit text augmentation.</p>\n\n",
                "matched_terms": [
                    "mave",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Unified Frameworks for Voice Editing and Synthesis</span>\nRecent efforts have sought to develop unified models capable of both zero-shot TTS and speech editing, recognizing their shared dependency on context-aware acoustic modeling. These frameworks broadly bifurcate into modular architectures&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib48\" title=\"\">48</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib20\" title=\"\">20</a>]</cite> that employ separate components for distinct tasks, and end-to-end systems including SpeechX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib44\" title=\"\">44</a>]</cite>&#8212;which adapts VALLE through prompt tuning&#8212;and flow-matching approaches like VoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib26\" title=\"\">26</a>]</cite> and UniCATS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib11\" title=\"\">11</a>]</cite>. While representing important conceptual advances, these unified systems exhibit significant practical limitations. SpeechX lacks human evaluation metrics for editing performance, undermining claims of perceptual quality. VoiceBox, despite its broad task coverage, omits formal speech editing evaluation in its methodology, relying solely on demo examples. UniCATS restricts editing capability to brief segments (<math alttext=\"&lt;2\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">&lt;2</annotation></semantics></math> seconds) and employs rule-based prosody markers rather than differentiable conditioning. Crucially, all existing unified frameworks are constrained by a fundamental dichotomy: autoregressive systems (e.g., SpeechX) maintain high fidelity but sacrifice contextual awareness through causal masking and face quadratic time complexity, while non-autoregressive approaches (e.g., VoiceBox) enable bidirectional context modeling at the cost of temporal coherence and prosodic precision. This trade-off between contextual awareness and generation fidelity has remained unresolved in prior art.</p>\n\n",
                "matched_terms": [
                    "time",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MAVE in a Nutshell.</span>\nIn summary, MAVE establishes a novel paradigm in text-conditioned speech generation by bridging the efficiency of State Space Models with the flexibility of cross-modal attention. Unlike prior SSM-based approaches such as CM-Mamba&#160;&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib18\" title=\"\">18</a>]</cite>, which require strict alignment between text and audio sequences, our architecture introduces a length-agnostic cross-attention mechanism that enables rich, contextualized phoneme conditioning without artificial upsampling or architectural constraints. By replacing the quadratic self-attention of Transformer-based editors like Voicecraft with linear-time selective state updates, MAVE achieves scalable long-context modeling while preserving prosodic coherence and speaker identity through recurrence. Our framework further unifies three critical capabilities: (1) efficient bidirectional context access via causal token rearrangement, (2) precise linguistic control through differentiable cross-attention on phonemized input, and (3) reference-guided voice consistency without explicit speaker embeddings. As demonstrated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4\" title=\"4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, MAVE is the first model to simultaneously outperform both autoregressive (e.g., VoiceCraft) and non-autoregressive (e.g., FluentSpeech) baselines in human evaluations across speech editing and zero-shot TTS, setting a new standard for unified, high-fidelity, and scalable speech generation. To our knowledge, this is the first successful integration of cross-attention into a Mamba-based audio decoder for unrestricted, long-form speech editing and synthesis.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave",
                    "voicecraft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present <span class=\"ltx_text ltx_font_bold\">MAVE</span>, a novel architecture for text-conditioned speech editing and zero-shot text-to-speech (TTS). The core challenge lies in autoregressively generating long sequences of discrete audio tokens conditioned on a textual transcript, where the quadratic complexity of self-attention in Transformers becomes prohibitive. Audio signals are typically encoded at 50 Hz using residual vector quantization (RVQ), yielding 4 to 8 discrete tokens per frame, which amounts to 200&#8211;400 tokens per second of speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib47\" title=\"\">47</a>]</cite>. In contrast, the corresponding phoneme sequence contains approximately 12 units per second&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib36\" title=\"\">36</a>]</cite>, resulting in a significant sequence length mismatch between modalities. This disparity makes direct application of Transformer-based models inefficient for high-fidelity, long-form audio generation.</p>\n\n",
                "matched_terms": [
                    "mave",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these issues, we propose MAVE, a hybrid decoder that combines the efficiency of Mamba for audio token modeling with a flexible cross-attention mechanism for text conditioning&#8212;without requiring aligned sequence lengths. Our model supports context-aware speech infilling and zero-shot TTS by leveraging both surrounding audio context and linguistic input.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech editing, we consider a non-autoregressive infilling task where one or more spans of audio tokens are masked and must be reconstructed. To enable bidirectional context access during autoregressive generation, we adopt the <span class=\"ltx_text ltx_font_bold\">causal masking</span> strategy from CM3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib1\" title=\"\">1</a>]</cite>. Masked spans are replaced with special mask tokens and moved to the end of the sequence, allowing the model to condition on both past and future context while preserving autoregressive training.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The model autoregressively generates tokens following the last <math alttext=\"M_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><msub><mi>M</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">M_{1}</annotation></semantics></math>, reconstructing the original masked content. After that, to accelerate decoding, we apply the <span class=\"ltx_text ltx_font_bold\">codebook delay pattern</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib9\" title=\"\">9</a>]</cite>. This is mainly to ensure that the generation of a speific code of level <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> at some time step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m5\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> is conditioned on all the higher levels <math alttext=\"[1,...,k-1\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.SS1.p4.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>k</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">[1,...,k-1</annotation></semantics></math>] at the same time step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m7\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "time",
                    "model",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Having defined the input representation and preprocessing pipeline, we now describe how the model leverages Mamba and cross-attention to generate audio tokens autoregressively conditioned on text and reference context. The task is formulated as <span class=\"ltx_text ltx_font_italic\">text-conditioned autoregressive audio generation</span>, decomposed into two components: (1) modeling intra-audio temporal dependencies, and (2) conditioning on linguistic input.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike transformers, which benefit from global self-attention, Mamba relies on selective state-space mechanisms that exhibit difficulty in retaining fine-grained information over long sequences. As a result, if we concatenate text and audio tokens and attempt to process them with the same decoder model, distant text tokens&#8212;critical for maintaining linguistic fidelity&#8212;tend to be encoded with degraded or &#8221;fuzzy&#8221; memory <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib41\" title=\"\">41</a>]</cite>, leading to a loss of semantic precision. This limitation is particularly detrimental in speech generation tasks, where high-fidelity retention of textual information is essential to ensure accurate alignment and high-quality output.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the other hand, this limitation is less critical when modeling dependencies among audio tokens, as exact low-level detail preservation is not essential for high-quality speech synthesis. Instead, the model benefits from retaining high-level acoustic features which are sufficient to generate coherent and natural-sounding audio. The compressed, selective memory inherent in SSMs like Mamba is well-suited to this purpose, effectively filtering out perceptually irrelevant noise while preserving meaningful temporal patterns. In our ablation studies, we further demonstrate that conditioning on text via cross-attention&#8212;rather than token concatenation&#8212;is crucial for maintaining linguistic fidelity in Mamba-based architectures. This design ensures that textual information is explicitly attended to throughout generation, mitigating the risk of forgetting distant linguistic context.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MAVE does not rely on explicit speaker embeddings for voice identity preservation. Instead, it leverages <span class=\"ltx_text ltx_font_italic\">in-context learning</span> for TTS task by prepending a short reference utterance encoded into discrete audio tokens using <span class=\"ltx_text ltx_font_typewriter\">X-Codec</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib47\" title=\"\">47</a>]</cite> to the generation sequence. This reference context provides rich acoustic, prosodic, and timbral cues that guide the autoregressive decoder to synthesize speech in the target speaker&#8217;s voice, without requiring speaker labels or fine-tuning.</p>\n\n",
                "matched_terms": [
                    "mave",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During both training and inference, the reference tokens are placed at the beginning of the input sequence, followed by the masked audio context. The Mamba decoder attends to this context through its long-range recurrence, effectively conditioning the generated speech on the speaker&#8217;s voice characteristics. This strategy enables true zero-shot TTS: given only a few seconds of reference audio and a target transcript, the model generates high-fidelity, speaker-consistent speech.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tokens",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the input context (including text, reference audio, and unmasked audio tokens), the model parametrized by <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.Px1.p1.m1\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> autoregressively predicts the conditional distribution over the RVQ codebooks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\{\\alpha_{k}\\}_{k=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.Px1.p3.m1\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#945;</mi><mi>k</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><annotation encoding=\"application/x-tex\">\\{\\alpha_{k}\\}_{k=1}^{K}</annotation></semantics></math> are tunable hyperparameters with <math alttext=\"\\alpha_{1}\\geq\\alpha_{2}\\geq\\dots\\geq\\alpha_{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.Px1.p3.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#945;</mi><mn>1</mn></msub><mo>&#8805;</mo><msub><mi>&#945;</mi><mn>2</mn></msub><mo>&#8805;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>&#8805;</mo><msub><mi>&#945;</mi><mi>K</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\alpha_{1}\\geq\\alpha_{2}\\geq\\dots\\geq\\alpha_{K}</annotation></semantics></math>, typically set inversely proportional to codebook index. Following <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib28\" title=\"\">28</a>]</cite> we assign a weight of 0.25 to the first three levels and 0.05 to the last five. Additionally, following &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib1\" title=\"\">1</a>]</cite>, we compute the prediction loss over all valid audio tokens in the sequence, excluding special tokens such as mask tokens <math alttext=\"M_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.Px1.p3.m3\" intent=\":literal\"><semantics><msub><mi>M</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">M_{j}</annotation></semantics></math> regardless of whether they belong to masked or unmasked regions. This encourages the model to refine its internal representations throughout the sequence, improving contextual coherence and reconstruction fidelity.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To train MAVE, we used the ScaledAdam optimizer and Eden Scheduler proposed in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib46\" title=\"\">46</a>]</cite> with a base learning rate of 0.01, batch size of 400k frames (i.e. 133.2 minutes), and total training step\nof 50k with gradient accumulation. The training of the 830M MAVE model took about 4 days on 4 NVIDIA A100 GPUs. For inference, we use Nucleus sampling <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib16\" title=\"\">16</a>]</cite> with p = 0.8 and a temperature of 1 for all experiments.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VoiceCraft has been observed to generate prolonged silences during synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite>. To mitigate this issue, the authors proposed reducing the probability of silence tokens and performing multiple generations per sample, selecting the shortest output as the final result. In all of our experiments involving VoiceCraft, we adopted this strategy, generating five samples per input and selecting the shortest one. In contrast we haven&#8217;t noticed this problem in our model, so we produce a single generation with no further processing.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tokens",
                    "voicecraft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Results on Speech Editing &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the results of both VoiceCraft <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite> and MAVE on RealEdit dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite>, which reflects real-life speech editing cases that are both challenging and diverse. The original dataset contains 100 utterances from LibriTTS (dev-clean and dev-other) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib50\" title=\"\">50</a>]</cite>, 100 utterances from\nYouTube (from Gigaspeech testset) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib6\" title=\"\">6</a>]</cite>, and 110 utterances from the Spotify Podcast dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib8\" title=\"\">8</a>]</cite>. However, the Spotify podcasts dataset is no longer available on the official website, so we have used only the 200 audios that are still publicly available.</p>\n\n",
                "matched_terms": [
                    "realedit",
                    "mave",
                    "voicecraft",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MAVE demonstrates superior performance compared to VoiceCraft, achieving smaller WER and higher mean opinion scores (MOS) in both naturalness (3.90 vs. 3.77) and intelligibility (4.25 vs. 4.20). Notably, the performance gap between our model and the ground truth recordings is minimal, with differences of less than 0.1 in naturalness MOS and less than 0.07 in intelligibility MOS. This is an indication that MAVE-edits feature strong perceptual correlation to human speech. It is worth noting that even for the original audio, the average naturalness ratings did not exceed 4.0 out of 5.0. This is mostly attributed to the fact that the audio data were collected in the wild and their quality is inferior to studio recordings, with the presence of background noise and other sources of recording artifacts. This also highlights the challenging conditions under which the evaluation was conducted.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave",
                    "voicecraft",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further assess the perceptual quality of our generated speech relative to ground truth, we conducted an additional study in which we asked a group of 10 users of proven English fluency to do a side-by-side comparison on the naturalness of the original unedited audio and our edited audio on 40 samples from the RealEdit dataset. The results are very promising and demonstrate that 57.2% of the times, users reported that both audios sound equally natural, while in 24.8% of cases, the ground truth was judged as more natural, and interestingly 18% of the times, the edited audio was preferred over the original. These findings indicate that in the majority of cases, our generated audios are indistinguishable from the original ones. For further details about the setup, we refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS3.SSS2\" title=\"A.3.2 Comparative studies &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.3.2</span></a></p>\n\n",
                "matched_terms": [
                    "realedit",
                    "comparison",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable a meaningful comparison with diffusion-based speech generation models, we selected FluentSpeech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib20\" title=\"\">20</a>]</cite> as the most relevant open-source baseline. However, due to its significantly smaller model size, a direct comparison with our approach would not be fair. To address this, we leveraged the larger variant of FluentSpeech retrained and evaluated by the authors of VoiceCraft. The authors have not made this model publicly available but have provided 14 synthesized utterances, publishing for each sample: (1) VoiceCraft&#8217;s generation, (2) the enhanced FluentSpeech model&#8217;s generation, and (3) the original audio with original and target transcript.\nWe generated corresponding outputs using MAVE for the same 14 samples and conducted a perceptual listening study to compare the audio quality between the three models. The evaluation consisted of a pairwise, side-by-side comparison, where each participant (20 in total) was presented with two audio samples from different models and asked to judge which exhibited better naturalness and intelligibility. With 14 samples per system and three possible pairings between the models (VoiceCraft vs. FluentSpeech, VoiceCraft vs. Ours, and FluentSpeech vs. Ours), each participant evaluated a total of 42 audio pairs. Further details on the experimental setup, including participant instructions, interface design, and evaluation protocol, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS3.SSS2\" title=\"A.3.2 Comparative studies &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.3.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "ours",
                    "voicecraft",
                    "comparison",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.F2\" title=\"Figure 2 &#8227; 4.1 Results on Speech Editing &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> clearly indicate that MAVE accomplishes superior perceptual quality compared to both FluentSpeech and VoiceCraft. While VoiceCraft already holds an advantage over FluentSpeech as it was preferred in 47.1% vs. 13.2% for naturalness and 50.7% vs. 6.4% for intelligibility, our model surpasses FluentSpeech by an even larger margin, being favored in 62.9% of cases for naturalness and 59.3% for intelligibility. More importantly, our model also outperforms VoiceCraft: it is preferred over VoiceCraft in 33.6% of comparisons for naturalness and 21.4% for intelligibility, while VoiceCraft is preferred over ours in only 17.5% and 12.9%, respectively.</p>\n\n",
                "matched_terms": [
                    "ours",
                    "model",
                    "mave",
                    "voicecraft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the subjective evaluation, we randomly selected 80 samples from our test set and generated corresponding speech outputs using VoiceCraft and our proposed model. To ensure a balanced assessment, we conducted a listening study involving 20 native or near-native English speakers (C1&#8211;C2 proficiency level). Participants were randomly assigned to two groups of 10, with each group evaluating a total of 120 audio clips: 40 generated by VoiceCraft, 40 produced by MAVE, and 40 ground-truth recordings. Each participant rated the naturalness and intelligibility of the audio samples on a 5-point Likert scale (1 = poor, 5 = excellent). Further details regarding the experimental setup, rating criteria, and instructions provided to participants are available in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS3.SSS1\" title=\"A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.3.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave",
                    "voicecraft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MAVE outperformed VoiceCraft in MOS evaluations across both naturalness (3.48 vs. 3.22) and intelligibility (4.20 vs. 4.01). While the overall gap between our model and the ground truth remains notable, a closer analysis reveals a reduced performance disparity under certain conditions. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T3\" title=\"Table 3 &#8227; 4.2 Results on Zero-shot TTS &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, which breaks down results by the number of words to be generated, with split points selected to ensure approximately balanced sample sizes across ranges, specifically 17, 22, 21, 20 for the different ranges in ascending order. The performance gap narrows significantly in the 8&#8211;15 word range. In this interval, our model achieves a naturalness score of 3.71 compared to the ground truth&#8217;s 3.87, and an intelligibility score of 4.26 versus 4.36, indicating competitive quality for medium-length utterances, while the gap becomes more significant with the increasing length of the target transcript. This trend is consistent with what the model is trained on, as it was trained on the Gigaspeech dataset, which consists of speech with moderate utterance lengths.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "performance",
                    "model",
                    "mave",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we compare our model, with previous SOTA AR model for speech editing, namely VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite>, which uses a transformer backbone. All inference experiments were conducted on NVIDIA A100 GPU. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Efficiency Analysis &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> reports the inference time required to process the entire RealEdit dataset as well as the average and peak memory consumption. It is important to note that these reported numbers are based on a single generation per sample. The results clearly demonstrate the superior memory efficiency of MAVE compared to VoiceCraft with KV caching, requiring approximately six times less GPU memory&#8212;largely due to the selective state mechanism of the Mamba backbone. While VoiceCraft exhibits faster inference speed in this evaluation, this advantage is primarily attributed to the relatively short duration of the target audio segments. On average, our model generates about 85 tokens per sample, corresponding to approximately 1.7 seconds of speech, a regime in which autoregressive overheads are minimal.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "realedit",
                    "model",
                    "inference",
                    "time",
                    "mave",
                    "tokens",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, theoretical complexity analysis reveals that MAVE scales more favorably with sequence length. For longer generation tasks, our model not only maintains significant memory savings but also surpasses VoiceCraft in computational efficiency, offering better speed performance for extended audio synthesis (see &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS4\" title=\"A.4 Theoretical Analysis of Computational Complexity &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> for more details).</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave",
                    "voicecraft",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the ablation study, we considered the masked reconstruction task, in which we used a random subset of the Gigaspeech validation dataset, randomly masked a sequence of words and required the model to reconstruct them (details provided in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS2\" title=\"A.2 Ablation Study Setup &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Efficiency Analysis &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents a comprehensive ablation study comparing different architectural designs under similar conditions, including model size (approximately 830M parameters) and training setup. The results highlight that neither a pure transformer-based encoder-decoder architecture nor a standalone Mamba decoder achieves optimal performance, highlighting the importance of our hybrid design.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The encoder-decoder transformer, with the encoder utilized to get contextualized text tokens, cross-attention to condition the audio decoder on text, and transformer decoder to model audio dependencies, achieves moderate performance but lags behind our model in all metrics, particularly in WER (10.8 vs. <span class=\"ltx_text ltx_font_bold\">7.8</span>). In contrast, the Mamba-only model, where text and audio tokens are concatenated and processed autoregressively, performs significantly worse with a higher WER (13.0).</p>\n\n",
                "matched_terms": [
                    "model",
                    "tokens",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Crucially, our MAVE architecture integrates both components, it uses cross-attention to explicitly condition the generation process on textual input while leveraging the Mamba decoder to efficiently model acoustic dependencies. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Efficiency Analysis &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, this hybrid approach yields superior performance across all metrics, achieving the lowest WER and MCD, best fundamental frequency (F0) accuracy, and highest PESQ score. These improvements demonstrate that the strength of MAVE does not arise from either cross-attention or Mamba in isolation, but from the combination of both.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development and deployment of speech editing models like MAVE raise important ethical concerns that must be carefully addressed. One primary issue is the potential for misuse in generating deceptive audio content, such as deepfakes, which could be exploited for misinformation, fraud, or impersonation. While our model enables beneficial applications, such as facilitating editing tasks by correcting errors in speech recordings, improving accessibility, or enhancing voice interfaces.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Configuration for our model, and the variants used in ablation studies are shown in Table &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.T6\" title=\"Table 6 &#8227; A.1 Implementation Details &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. All models were trained under consistent experimental settings, using Scaled Adam optimizer and Eden Secheduler proposed in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib46\" title=\"\">46</a>]</cite>, with a base learning rate of 0.01. Hyperparameters were carefully selected to ensure a fair comparison, with all models constrained to have approximately 830 million parameters.</p>\n\n",
                "matched_terms": [
                    "model",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study was conducted to evaluate the quality of audio samples generated by our model in comparison to VoiceCraft and ground-truth recordings, to provide insights of how well our model performs against state-of-the-art models and to assess how closely our synthesized speech approaches the quality of natural, human speech.\nWe sampled 80 random examples from the RealEdit benchmark to evaluate the models on the speech editing task, and another 80 from the LibriTTS to evaluate on zero-shot TTS. Then, for each task we generated the synthesized audios using both our model MAVE and VoiceCraft. After that we divided these audios into two groups, each group was assigned 120 audios (40 from each category). We then asked 20 people to assess the quality of the generation, assigning each user to one group, ensuring that each group receives exactly 10 people. Audios are randomly shuffled for each new user. At the beginning of the study, the instructions are provided to the users as illustrated in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F3\" title=\"Figure 3 &#8227; A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Then for each audio sample, participants were first presented with the corresponding transcript and asked to rate the naturalness of the speech as depicted in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F4\" title=\"Figure 4 &#8227; A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. After submitting their naturalness rating, they were then prompted to evaluate the same audio in terms of intelligibility.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "realedit",
                    "comparison",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further assess the quality of the generated audios, and to enable direct comparison between the different methods, we have conducted 2 comparative user studies.\nThe first study evaluates our model against VoiceCraft and FluentSpeech. We selected the 14 publicly available audio samples generated by both VoiceCraft and FluentSpeech, which are provided on the VoiceCraft demo page&#160;<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://jasonppy.github.io/VoiceCraft_web/\" title=\"\">https://jasonppy.github.io/VoiceCraft_web/</a>. Using the same text prompts, we generated corresponding outputs using MAVE. This resulted in three pairwise comparisons: (1) VoiceCraft vs. FluentSpeech, (2) VoiceCraft vs. Ours, and (3) FluentSpeech vs. Ours), with 14 pairs per comparison, yielding a total of 42 unique audio pairs.</p>\n\n",
                "matched_terms": [
                    "ours",
                    "voicecraft",
                    "comparison",
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second comparative study focuses on a direct comparison between MAVE and ground truth audio for the speech editing task. To accomplish this, we have selected 40 random samples from RealEdit data, provided the ground truth before editing and our edited audio, along with their respective transcript. In total, 10 native or near-native English speakers&#8212;were asked to compare the two audios in terms of naturalness, and indicate whether the ground truth sounded better, the MAVE-edit sounded better, or there was no perceptual difference between the two. Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F6\" title=\"Figure 6 &#8227; A.3.2 Comparative studies &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> illustrates the instructions provided to users at the beginning of the study, and Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F7\" title=\"Figure 7 &#8227; A.3.2 Comparative studies &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> provides an example interface of the side-by-side comparison.</p>\n\n",
                "matched_terms": [
                    "realedit",
                    "mave",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments the number of audio tokens is significantly larger than the\nnumber of text tokens (<math alttext=\"L_{y}\\gg L_{x}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mi>y</mi></msub><mo>&#8811;</mo><msub><mi>L</mi><mi>x</mi></msub></mrow><annotation encoding=\"application/x-tex\">L_{y}\\gg L_{x}</annotation></semantics></math>).\nUnder this regime the proposed encoder&#8211;decoder model with a Mamba decoder\noffers two clear advantages:</p>\n\n",
                "matched_terms": [
                    "model",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Memory efficiency:</span>\nalthough both models can use KV-cache, the decoder-only Transformer\nmust store keys and values for all past text and audio tokens\n(<math alttext=\"\\mathcal{O}(N_{d}H_{0}(L_{x}+L_{y}))\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I3.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>N</mi><mi>d</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>H</mi><mn>0</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>L</mi><mi>x</mi></msub><mo>+</mo><msub><mi>L</mi><mi>y</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}(N_{d}H_{0}(L_{x}+L_{y}))</annotation></semantics></math> per layer) which grows linearly as the generation goes, whereas the\nencoder&#8211;decoder model only stores a fixed encoder cache\n(<math alttext=\"\\mathcal{O}(N_{e}HL_{x})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I3.i2.p1.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>N</mi><mi>e</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>H</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>L</mi><mi>x</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}(N_{e}HL_{x})</annotation></semantics></math>) and the hidden state of Mamba. When <math alttext=\"L_{y}\\gg L_{x}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I3.i2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mi>y</mi></msub><mo>&#8811;</mo><msub><mi>L</mi><mi>x</mi></msub></mrow><annotation encoding=\"application/x-tex\">L_{y}\\gg L_{x}</annotation></semantics></math>, this results in\nsignificantly lower memory usage for the encoder&#8211;decoder model.</p>\n\n",
                "matched_terms": [
                    "cache",
                    "model",
                    "tokens"
                ]
            }
        ]
    },
    "S4.T5.fig2": {
        "source_file": "Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba",
        "caption": "Table 5: Ablation study of model architectures by conditioning mechanism.",
        "body": "Decoder\nConditioning\nWER↓\\downarrow\nMCD↓\\downarrow\nF0↓\\downarrow\nPESQ↑\\uparrow\n\n\nMamba (ours)\nX-Attn.\n7.8\n4.29\n0.280\n2.08\n\n\nTrm.\nX-Attn.\n10.8\n4.58\n0.293\n1.99\n\n\nMamba\nConcat.\n13.0\n4.48\n0.284\n2.02",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">Decoder</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">Conditioning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">MCD<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">F0<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">PESQ<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">Mamba (ours)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">X-Attn.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">7.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">4.29</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">0.280</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">2.08</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">Trm.</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">X-Attn.</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">10.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">4.58</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">0.293</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">1.99</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">Mamba</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">Concat.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">13.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">4.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">0.284</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">2.02</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "wer↓downarrow",
            "pesq↑uparrow",
            "ours",
            "ablation",
            "model",
            "study",
            "conditioning",
            "decoder",
            "mechanism",
            "mamba",
            "trm",
            "f0↓downarrow",
            "architectures",
            "mcd↓downarrow",
            "xattn",
            "concat"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">To address these limitations, we propose <span class=\"ltx_text ltx_font_bold\">MAVE</span> (<span class=\"ltx_text ltx_font_bold\">M</span>amba with Cross-<span class=\"ltx_text ltx_font_bold\">A</span>ttention for <span class=\"ltx_text ltx_font_bold\">V</span>oice <span class=\"ltx_text ltx_font_bold\">E</span>diting and Synthesis), a novel <em class=\"ltx_emph ltx_font_italic\">autoregressive</em> architecture for text-conditioned voice editing and zero-shot TTS that synergizes Mamba state-space models with cross-attention mechanisms. Unlike Transformer-based decoders&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib42\" title=\"\">42</a>]</cite>, MAVE replaces self-attention with structured state-space sequences (SSMs), enabling <em class=\"ltx_emph ltx_font_italic\">linear-complexity modeling</em> of dependencies between acoustic tokens. Crucially, our cross-attention module dynamically aligns <em class=\"ltx_emph ltx_font_italic\">augmented text inputs</em> with acoustic tokens, allowing the model to &#8220;edit&#8221; speech by attending to the textual information.</p>\n\n",
                "matched_terms": [
                    "mamba",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To the best of our knowledge, <span class=\"ltx_text ltx_font_bold\">MAVE</span> &#8211; built on a cross-attentive Mamba backbone &#8211; is the first successful application of a structured state-space model to text-conditional speech generation, namely speech editing and zero-shot TTS. On the challenging RealEdit benchmark, MAVE achieves <span class=\"ltx_text ltx_font_bold\">human-parity naturalness</span> in speech editing and surpasses state-of-the-art models like VoiceCraft and FluentSpeech in both speaker similarity and naturalness (MOS) without requiring any post-processing. Moreover, MAVE offers significant efficiency gains, reducing memory usage by <span class=\"ltx_text ltx_font_bold\">six</span> times compared to Transformer-based VoiceCraft, while enabling single-pass generation without silent tokens handling or multiple runs. Our results demonstrate that Mamba-based models enhanced with cross-attention can outperform both autoregressive and diffusion-based approaches in fidelity, efficiency, and robustness, establishing a new direction for scalable and high-quality speech generation.</p>\n\n",
                "matched_terms": [
                    "mamba",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The Evolution of Speech Editing</span>\nSpeech editing, defined as the precise modification of targeted speech segments while preserving unaltered regions, has evolved through three distinct generations of methodology. Early approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib21\" title=\"\">21</a>]</cite> relied on concatenating TTS-generated segments with original speech, inevitably introducing prosody mismatches and boundary artifacts due to the absence of contextual conditioning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib31\" title=\"\">31</a>]</cite>. Subsequent research introduced context-aware mechanisms through bidirectional fusion architectures&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib40\" title=\"\">40</a>]</cite> and masked reconstruction objectives&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib43\" title=\"\">43</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib5\" title=\"\">5</a>]</cite>, with Transformer-based systems demonstrating improved contextualization. Most recently, diffusion-based methods like FluentSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib20\" title=\"\">20</a>]</cite> achieved state-of-the-art performance on standard benchmarks through denoising frameworks. However, these approaches collectively suffer from three interrelated limitations. First, Transformer-based systems incur quadratic computational complexity relative to sequence length, restricting practical context windows. Second, evaluation protocols remain constrained to short editing spans&#8212;UniCATS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib11\" title=\"\">11</a>]</cite>, for instance, limits assessments to segments under two seconds, failing to address real-world editing scenarios involving multi-word phrases. Third and most critically, none incorporate mechanisms for leveraging linguistically augmented inputs (e.g., prosody-annotated text) to guide context-aware modifications. MAVE overcomes these constraints through its linear-complexity Mamba architecture and differentiable cross-attention fusion mechanism, enabling robust editing of spans up to 16 words while preserving natural prosody through explicit text augmentation.</p>\n\n",
                "matched_terms": [
                    "mechanism",
                    "mamba",
                    "architectures",
                    "conditioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Unified Frameworks for Voice Editing and Synthesis</span>\nRecent efforts have sought to develop unified models capable of both zero-shot TTS and speech editing, recognizing their shared dependency on context-aware acoustic modeling. These frameworks broadly bifurcate into modular architectures&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib48\" title=\"\">48</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib20\" title=\"\">20</a>]</cite> that employ separate components for distinct tasks, and end-to-end systems including SpeechX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib44\" title=\"\">44</a>]</cite>&#8212;which adapts VALLE through prompt tuning&#8212;and flow-matching approaches like VoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib26\" title=\"\">26</a>]</cite> and UniCATS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib11\" title=\"\">11</a>]</cite>. While representing important conceptual advances, these unified systems exhibit significant practical limitations. SpeechX lacks human evaluation metrics for editing performance, undermining claims of perceptual quality. VoiceBox, despite its broad task coverage, omits formal speech editing evaluation in its methodology, relying solely on demo examples. UniCATS restricts editing capability to brief segments (<math alttext=\"&lt;2\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">&lt;2</annotation></semantics></math> seconds) and employs rule-based prosody markers rather than differentiable conditioning. Crucially, all existing unified frameworks are constrained by a fundamental dichotomy: autoregressive systems (e.g., SpeechX) maintain high fidelity but sacrifice contextual awareness through causal masking and face quadratic time complexity, while non-autoregressive approaches (e.g., VoiceBox) enable bidirectional context modeling at the cost of temporal coherence and prosodic precision. This trade-off between contextual awareness and generation fidelity has remained unresolved in prior art.</p>\n\n",
                "matched_terms": [
                    "architectures",
                    "conditioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MAVE in a Nutshell.</span>\nIn summary, MAVE establishes a novel paradigm in text-conditioned speech generation by bridging the efficiency of State Space Models with the flexibility of cross-modal attention. Unlike prior SSM-based approaches such as CM-Mamba&#160;&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib18\" title=\"\">18</a>]</cite>, which require strict alignment between text and audio sequences, our architecture introduces a length-agnostic cross-attention mechanism that enables rich, contextualized phoneme conditioning without artificial upsampling or architectural constraints. By replacing the quadratic self-attention of Transformer-based editors like Voicecraft with linear-time selective state updates, MAVE achieves scalable long-context modeling while preserving prosodic coherence and speaker identity through recurrence. Our framework further unifies three critical capabilities: (1) efficient bidirectional context access via causal token rearrangement, (2) precise linguistic control through differentiable cross-attention on phonemized input, and (3) reference-guided voice consistency without explicit speaker embeddings. As demonstrated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4\" title=\"4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, MAVE is the first model to simultaneously outperform both autoregressive (e.g., VoiceCraft) and non-autoregressive (e.g., FluentSpeech) baselines in human evaluations across speech editing and zero-shot TTS, setting a new standard for unified, high-fidelity, and scalable speech generation. To our knowledge, this is the first successful integration of cross-attention into a Mamba-based audio decoder for unrestricted, long-form speech editing and synthesis.</p>\n\n",
                "matched_terms": [
                    "mechanism",
                    "model",
                    "decoder",
                    "conditioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While State Space Models (SSMs) such as Mamba&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib13\" title=\"\">13</a>]</cite> offer linear-time sequence modeling, they face two key challenges in multimodal speech generation: (1) limited support for cross-modal conditioning (such as encoder-decoder architecture of transformers), and (2) potential loss of fine-grained acoustic details over long sequences. Prior work such as CM-Mamba&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib18\" title=\"\">18</a>]</cite> enables cross-attention but requires equal-length modalities, which is infeasible for text-conditioned speech processing. Unlike CM-Mamba, which requires equal-length modalities and thus forces phoneme padding or audio subsampling, our architecture supports arbitrary-length text-to-audio conditioning via explicit Cross-Attention module, attended access to text embeddings, decoupled from the autoregressive audio state evolution.</p>\n\n",
                "matched_terms": [
                    "mamba",
                    "conditioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these issues, we propose MAVE, a hybrid decoder that combines the efficiency of Mamba for audio token modeling with a flexible cross-attention mechanism for text conditioning&#8212;without requiring aligned sequence lengths. Our model supports context-aware speech infilling and zero-shot TTS by leveraging both surrounding audio context and linguistic input.</p>\n\n",
                "matched_terms": [
                    "mechanism",
                    "mamba",
                    "model",
                    "decoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Having defined the input representation and preprocessing pipeline, we now describe how the model leverages Mamba and cross-attention to generate audio tokens autoregressively conditioned on text and reference context. The task is formulated as <span class=\"ltx_text ltx_font_italic\">text-conditioned autoregressive audio generation</span>, decomposed into two components: (1) modeling intra-audio temporal dependencies, and (2) conditioning on linguistic input.</p>\n\n",
                "matched_terms": [
                    "mamba",
                    "model",
                    "conditioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ the Mamba architecture&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib13\" title=\"\">13</a>]</cite>, a selective State Space Model (SSM), to model long-range dependencies in the audio token sequence. Unlike Transformers, Mamba scales linearly with sequence length and maintains a compressed latent state that effectively captures speaker identity, prosody, and acoustic continuity&#8212;critical for coherent speech editing.</p>\n\n",
                "matched_terms": [
                    "mamba",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\overline{A}_{t},\\overline{B}_{t},\\overline{C}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p6.m1\" intent=\":literal\"><semantics><mrow><msub><mover accent=\"true\"><mi>A</mi><mo stretchy=\"true\">&#175;</mo></mover><mi>t</mi></msub><mo>,</mo><msub><mover accent=\"true\"><mi>B</mi><mo stretchy=\"true\">&#175;</mo></mover><mi>t</mi></msub><mo>,</mo><msub><mover accent=\"true\"><mi>C</mi><mo stretchy=\"true\">&#175;</mo></mover><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\overline{A}_{t},\\overline{B}_{t},\\overline{C}_{t}</annotation></semantics></math> are discretized SSM parameters computed from <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p6.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math> via a shared projection. The output <math alttext=\"g_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p6.m3\" intent=\":literal\"><semantics><msub><mi>g</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">g_{t}</annotation></semantics></math> is then passed through a gating mechanism (e.g., SiLU) and combined with a residual branch. This selective mechanism allows Mamba to dynamically attend to relevant past information, making it well-suited for preserving voice and prosody across long contexts.</p>\n\n",
                "matched_terms": [
                    "mechanism",
                    "mamba"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We stack multiple Mamba blocks with intermediate normalization and residual connections, forming the backbone of our audio decoder. Mamba has proven to be very effective in terms of modeling dependencies between audio tokens in previous research <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib12\" title=\"\">12</a>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib37\" title=\"\">37</a>]</cite>, but its effectivness to generate text-conditioned conherent audio are still not well-studied. Many text-conditioned audio generation approaches, such as text-to-speech (TTS) and speech editing, have adopted transformer-based decoder architectures that process concatenated text and audio tokens within a unified sequence <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib42\" title=\"\">42</a>]</cite>. While this design leverages the strong sequence modeling capabilities of transformers, it poses significant challenges when applied to alternative architectures such as Mamba.</p>\n\n",
                "matched_terms": [
                    "mamba",
                    "architectures",
                    "decoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike transformers, which benefit from global self-attention, Mamba relies on selective state-space mechanisms that exhibit difficulty in retaining fine-grained information over long sequences. As a result, if we concatenate text and audio tokens and attempt to process them with the same decoder model, distant text tokens&#8212;critical for maintaining linguistic fidelity&#8212;tend to be encoded with degraded or &#8221;fuzzy&#8221; memory <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib41\" title=\"\">41</a>]</cite>, leading to a loss of semantic precision. This limitation is particularly detrimental in speech generation tasks, where high-fidelity retention of textual information is essential to ensure accurate alignment and high-quality output.</p>\n\n",
                "matched_terms": [
                    "mamba",
                    "model",
                    "decoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the other hand, this limitation is less critical when modeling dependencies among audio tokens, as exact low-level detail preservation is not essential for high-quality speech synthesis. Instead, the model benefits from retaining high-level acoustic features which are sufficient to generate coherent and natural-sounding audio. The compressed, selective memory inherent in SSMs like Mamba is well-suited to this purpose, effectively filtering out perceptually irrelevant noise while preserving meaningful temporal patterns. In our ablation studies, we further demonstrate that conditioning on text via cross-attention&#8212;rather than token concatenation&#8212;is crucial for maintaining linguistic fidelity in Mamba-based architectures. This design ensures that textual information is explicitly attended to throughout generation, mitigating the risk of forgetting distant linguistic context.</p>\n\n",
                "matched_terms": [
                    "ablation",
                    "model",
                    "conditioning",
                    "mamba",
                    "architectures"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To incorporate linguistic information, we first convert the input transcript into a sequence of phonemes using the Montreal Forced Aligner (MFA) toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib30\" title=\"\">30</a>]</cite>. This ensures explicit modeling of pronunciation, especially for homographs. The phoneme sequence is then processed by a 4-layer Transformer encoder to produce contextualized embeddings <math alttext=\"\\mathbf{z}_{\\text{text}}=[\\mathbf{z}_{1},\\dots,\\mathbf{z}_{M}]\\in\\mathbb{R}^{M\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#119859;</mi><mtext>text</mtext></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119859;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119859;</mi><mi>M</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>M</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{z}_{\\text{text}}=[\\mathbf{z}_{1},\\dots,\\mathbf{z}_{M}]\\in\\mathbb{R}^{M\\times d}</annotation></semantics></math>, where <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> is the number of phonemes. As depicted in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S3.F1\" title=\"Figure 1 &#8227; 3 Proposed Method &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, a cross-attention module is inserted after each Mamba block. The query is derived from the Mamba block&#8217;s output, while keys and values come from <math alttext=\"\\mathbf{z}_{\\text{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119859;</mi><mtext>text</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{z}_{\\text{text}}</annotation></semantics></math>. This allows the audio decoder to attend to relevant phonetic content at each generation step, ensuring precise linguistic alignment.</p>\n\n",
                "matched_terms": [
                    "mamba",
                    "decoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During both training and inference, the reference tokens are placed at the beginning of the input sequence, followed by the masked audio context. The Mamba decoder attends to this context through its long-range recurrence, effectively conditioning the generated speech on the speaker&#8217;s voice characteristics. This strategy enables true zero-shot TTS: given only a few seconds of reference audio and a target transcript, the model generates high-fidelity, speaker-consistent speech.</p>\n\n",
                "matched_terms": [
                    "mamba",
                    "model",
                    "decoder",
                    "conditioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, MAVE is a hybrid autoregressive decoder that integrates Mamba blocks for efficient long-sequence audio modeling with cross-attention for flexible text conditioning. By combining token rearrangement, delayed RVQ decoding, and explicit speaker conditioning, it supports high-fidelity speech editing and zero-shot TTS. The network efficiently handles a large disparity between text and audio sequence lengths, offering a scalable alternative to transformer-based methods.</p>\n\n",
                "matched_terms": [
                    "mamba",
                    "decoder",
                    "conditioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We computed the Word Error Rate (WER) between the generated audio and the target transcript using Whisper-large and Whisper-medium.en <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib34\" title=\"\">34</a>]</cite>. In addition, we conducted a human evaluation to assess naturalness and intelligibility, based on audio samples from 80 randomly selected test cases for each model. Specifically, 20 people of proven English fluency participated in the study, they were divided into 2 groups of 10 people, and each group was asked to evaluate 120 audios (40 from each method) on the naturalness and intelligibility on a 5-point Likert scale (poor=1, excellent=5). For further details we refer to&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS3.SSS1\" title=\"A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.3.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "study"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable a meaningful comparison with diffusion-based speech generation models, we selected FluentSpeech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib20\" title=\"\">20</a>]</cite> as the most relevant open-source baseline. However, due to its significantly smaller model size, a direct comparison with our approach would not be fair. To address this, we leveraged the larger variant of FluentSpeech retrained and evaluated by the authors of VoiceCraft. The authors have not made this model publicly available but have provided 14 synthesized utterances, publishing for each sample: (1) VoiceCraft&#8217;s generation, (2) the enhanced FluentSpeech model&#8217;s generation, and (3) the original audio with original and target transcript.\nWe generated corresponding outputs using MAVE for the same 14 samples and conducted a perceptual listening study to compare the audio quality between the three models. The evaluation consisted of a pairwise, side-by-side comparison, where each participant (20 in total) was presented with two audio samples from different models and asked to judge which exhibited better naturalness and intelligibility. With 14 samples per system and three possible pairings between the models (VoiceCraft vs. FluentSpeech, VoiceCraft vs. Ours, and FluentSpeech vs. Ours), each participant evaluated a total of 42 audio pairs. Further details on the experimental setup, including participant instructions, interface design, and evaluation protocol, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS3.SSS2\" title=\"A.3.2 Comparative studies &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.3.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "ours",
                    "model",
                    "study"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.F2\" title=\"Figure 2 &#8227; 4.1 Results on Speech Editing &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> clearly indicate that MAVE accomplishes superior perceptual quality compared to both FluentSpeech and VoiceCraft. While VoiceCraft already holds an advantage over FluentSpeech as it was preferred in 47.1% vs. 13.2% for naturalness and 50.7% vs. 6.4% for intelligibility, our model surpasses FluentSpeech by an even larger margin, being favored in 62.9% of cases for naturalness and 59.3% for intelligibility. More importantly, our model also outperforms VoiceCraft: it is preferred over VoiceCraft in 33.6% of comparisons for naturalness and 21.4% for intelligibility, while VoiceCraft is preferred over ours in only 17.5% and 12.9%, respectively.</p>\n\n",
                "matched_terms": [
                    "ours",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the subjective evaluation, we randomly selected 80 samples from our test set and generated corresponding speech outputs using VoiceCraft and our proposed model. To ensure a balanced assessment, we conducted a listening study involving 20 native or near-native English speakers (C1&#8211;C2 proficiency level). Participants were randomly assigned to two groups of 10, with each group evaluating a total of 120 audio clips: 40 generated by VoiceCraft, 40 produced by MAVE, and 40 ground-truth recordings. Each participant rated the naturalness and intelligibility of the audio samples on a 5-point Likert scale (1 = poor, 5 = excellent). Further details regarding the experimental setup, rating criteria, and instructions provided to participants are available in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS3.SSS1\" title=\"A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.3.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "study"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we compare our model, with previous SOTA AR model for speech editing, namely VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite>, which uses a transformer backbone. All inference experiments were conducted on NVIDIA A100 GPU. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Efficiency Analysis &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> reports the inference time required to process the entire RealEdit dataset as well as the average and peak memory consumption. It is important to note that these reported numbers are based on a single generation per sample. The results clearly demonstrate the superior memory efficiency of MAVE compared to VoiceCraft with KV caching, requiring approximately six times less GPU memory&#8212;largely due to the selective state mechanism of the Mamba backbone. While VoiceCraft exhibits faster inference speed in this evaluation, this advantage is primarily attributed to the relatively short duration of the target audio segments. On average, our model generates about 85 tokens per sample, corresponding to approximately 1.7 seconds of speech, a regime in which autoregressive overheads are minimal.</p>\n\n",
                "matched_terms": [
                    "mechanism",
                    "mamba",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the ablation study, we considered the masked reconstruction task, in which we used a random subset of the Gigaspeech validation dataset, randomly masked a sequence of words and required the model to reconstruct them (details provided in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS2\" title=\"A.2 Ablation Study Setup &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Efficiency Analysis &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents a comprehensive ablation study comparing different architectural designs under similar conditions, including model size (approximately 830M parameters) and training setup. The results highlight that neither a pure transformer-based encoder-decoder architecture nor a standalone Mamba decoder achieves optimal performance, highlighting the importance of our hybrid design.</p>\n\n",
                "matched_terms": [
                    "ablation",
                    "study",
                    "decoder",
                    "mamba",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The encoder-decoder transformer, with the encoder utilized to get contextualized text tokens, cross-attention to condition the audio decoder on text, and transformer decoder to model audio dependencies, achieves moderate performance but lags behind our model in all metrics, particularly in WER (10.8 vs. <span class=\"ltx_text ltx_font_bold\">7.8</span>). In contrast, the Mamba-only model, where text and audio tokens are concatenated and processed autoregressively, performs significantly worse with a higher WER (13.0).</p>\n\n",
                "matched_terms": [
                    "model",
                    "decoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Crucially, our MAVE architecture integrates both components, it uses cross-attention to explicitly condition the generation process on textual input while leveraging the Mamba decoder to efficiently model acoustic dependencies. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Efficiency Analysis &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, this hybrid approach yields superior performance across all metrics, achieving the lowest WER and MCD, best fundamental frequency (F0) accuracy, and highest PESQ score. These improvements demonstrate that the strength of MAVE does not arise from either cross-attention or Mamba in isolation, but from the combination of both.</p>\n\n",
                "matched_terms": [
                    "mamba",
                    "model",
                    "decoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Configuration for our model, and the variants used in ablation studies are shown in Table &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.T6\" title=\"Table 6 &#8227; A.1 Implementation Details &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. All models were trained under consistent experimental settings, using Scaled Adam optimizer and Eden Secheduler proposed in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib46\" title=\"\">46</a>]</cite>, with a base learning rate of 0.01. Hyperparameters were carefully selected to ensure a fair comparison, with all models constrained to have approximately 830 million parameters.</p>\n\n",
                "matched_terms": [
                    "model",
                    "ablation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study was conducted to evaluate the quality of audio samples generated by our model in comparison to VoiceCraft and ground-truth recordings, to provide insights of how well our model performs against state-of-the-art models and to assess how closely our synthesized speech approaches the quality of natural, human speech.\nWe sampled 80 random examples from the RealEdit benchmark to evaluate the models on the speech editing task, and another 80 from the LibriTTS to evaluate on zero-shot TTS. Then, for each task we generated the synthesized audios using both our model MAVE and VoiceCraft. After that we divided these audios into two groups, each group was assigned 120 audios (40 from each category). We then asked 20 people to assess the quality of the generation, assigning each user to one group, ensuring that each group receives exactly 10 people. Audios are randomly shuffled for each new user. At the beginning of the study, the instructions are provided to the users as illustrated in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F3\" title=\"Figure 3 &#8227; A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Then for each audio sample, participants were first presented with the corresponding transcript and asked to rate the naturalness of the speech as depicted in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F4\" title=\"Figure 4 &#8227; A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. After submitting their naturalness rating, they were then prompted to evaluate the same audio in terms of intelligibility.</p>\n\n",
                "matched_terms": [
                    "model",
                    "study"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further assess the quality of the generated audios, and to enable direct comparison between the different methods, we have conducted 2 comparative user studies.\nThe first study evaluates our model against VoiceCraft and FluentSpeech. We selected the 14 publicly available audio samples generated by both VoiceCraft and FluentSpeech, which are provided on the VoiceCraft demo page&#160;<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://jasonppy.github.io/VoiceCraft_web/\" title=\"\">https://jasonppy.github.io/VoiceCraft_web/</a>. Using the same text prompts, we generated corresponding outputs using MAVE. This resulted in three pairwise comparisons: (1) VoiceCraft vs. FluentSpeech, (2) VoiceCraft vs. Ours, and (3) FluentSpeech vs. Ours), with 14 pairs per comparison, yielding a total of 42 unique audio pairs.</p>\n\n",
                "matched_terms": [
                    "ours",
                    "model",
                    "study"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">an <span class=\"ltx_text ltx_font_bold\">encoder&#8211;decoder</span> architecture in which a Transformer encoder\nprocesses the text and a stack of Mamba&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib13\" title=\"\">13</a>]</cite> layers autoregressively\ngenerates the audio. Each Mamba layer is followed by a cross&#8211;attention module\nconditioning on the encoder output.</p>\n\n",
                "matched_terms": [
                    "mamba",
                    "conditioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"M_{d}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I2.i3.p1.m1\" intent=\":literal\"><semantics><msub><mi>M</mi><mi>d</mi></msub><annotation encoding=\"application/x-tex\">M_{d}</annotation></semantics></math>: number of Mamba layers in the encoder&#8211;decoder model, each with\nhidden dimension <math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I2.i3.p1.m2\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math> (assumed equal in encoder and decoder).</p>\n\n",
                "matched_terms": [
                    "mamba",
                    "model",
                    "decoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">dominated by the self&#8211;attention layers.\nDuring generation, each Mamba layer updates its hidden state in\n<math alttext=\"\\mathcal{O}(H)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}(H)</annotation></semantics></math> time per token and attends to the fixed encoder output of\nlength <math alttext=\"L_{x}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>x</mi></msub><annotation encoding=\"application/x-tex\">L_{x}</annotation></semantics></math>.\nThus the per&#8211;token decoder cost is\n<math alttext=\"\\mathcal{O}\\big(M_{d}H(L_{x}+1)\\big)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">(</mo><mrow><msub><mi>M</mi><mi>d</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>H</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>L</mi><mi>x</mi></msub><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}\\big(M_{d}H(L_{x}+1)\\big)</annotation></semantics></math>,\nand generating the entire audio sequence requires</p>\n\n",
                "matched_terms": [
                    "mamba",
                    "decoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Equations&#160;equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.E4\" title=\"In Decoder&#8211;only Transformer. &#8227; A.4 Theoretical Analysis of Computational Complexity &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>&#8211;equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.E6\" title=\"In Encoder&#8211;decoder with Mamba decoder. &#8227; A.4 Theoretical Analysis of Computational Complexity &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>\nhighlight a key difference:\nthe decoder&#8211;only Transformer grows <em class=\"ltx_emph ltx_font_italic\">quadratically</em> with the audio length\n<math alttext=\"L_{y}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>y</mi></msub><annotation encoding=\"application/x-tex\">L_{y}</annotation></semantics></math>,\nwhereas the proposed encoder&#8211;decoder with Mamba decoder grows only\n<em class=\"ltx_emph ltx_font_italic\">linearly</em> with <math alttext=\"L_{y}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>y</mi></msub><annotation encoding=\"application/x-tex\">L_{y}</annotation></semantics></math> (after a one&#8211;time\n<math alttext=\"\\mathcal{O}(N_{e}HL_{x}^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>N</mi><mi>e</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>H</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>L</mi><mi>x</mi><mn>2</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}(N_{e}HL_{x}^{2})</annotation></semantics></math> encoder cost).\nWhen the audio output is much longer than the text prompt\n(<math alttext=\"L_{y}\\gg L_{x}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mi>y</mi></msub><mo>&#8811;</mo><msub><mi>L</mi><mi>x</mi></msub></mrow><annotation encoding=\"application/x-tex\">L_{y}\\gg L_{x}</annotation></semantics></math>), the dominant terms simplify to</p>\n\n",
                "matched_terms": [
                    "mamba",
                    "decoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments the number of audio tokens is significantly larger than the\nnumber of text tokens (<math alttext=\"L_{y}\\gg L_{x}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mi>y</mi></msub><mo>&#8811;</mo><msub><mi>L</mi><mi>x</mi></msub></mrow><annotation encoding=\"application/x-tex\">L_{y}\\gg L_{x}</annotation></semantics></math>).\nUnder this regime the proposed encoder&#8211;decoder model with a Mamba decoder\noffers two clear advantages:</p>\n\n",
                "matched_terms": [
                    "mamba",
                    "model",
                    "decoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Memory efficiency:</span>\nalthough both models can use KV-cache, the decoder-only Transformer\nmust store keys and values for all past text and audio tokens\n(<math alttext=\"\\mathcal{O}(N_{d}H_{0}(L_{x}+L_{y}))\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I3.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>N</mi><mi>d</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>H</mi><mn>0</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>L</mi><mi>x</mi></msub><mo>+</mo><msub><mi>L</mi><mi>y</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}(N_{d}H_{0}(L_{x}+L_{y}))</annotation></semantics></math> per layer) which grows linearly as the generation goes, whereas the\nencoder&#8211;decoder model only stores a fixed encoder cache\n(<math alttext=\"\\mathcal{O}(N_{e}HL_{x})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I3.i2.p1.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>N</mi><mi>e</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>H</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>L</mi><mi>x</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}(N_{e}HL_{x})</annotation></semantics></math>) and the hidden state of Mamba. When <math alttext=\"L_{y}\\gg L_{x}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I3.i2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mi>y</mi></msub><mo>&#8811;</mo><msub><mi>L</mi><mi>x</mi></msub></mrow><annotation encoding=\"application/x-tex\">L_{y}\\gg L_{x}</annotation></semantics></math>, this results in\nsignificantly lower memory usage for the encoder&#8211;decoder model.</p>\n\n",
                "matched_terms": [
                    "mamba",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Better conditioning:</span>\nthe cross&#8211;attention mechanism allows the decoder to efficiently\nincorporate the entire encoded text representation at every step\nwithout revisiting the growing audio history.</p>\n\n",
                "matched_terms": [
                    "mechanism",
                    "decoder",
                    "conditioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These properties make the encoder&#8211;decoder with Mamba decoder a natural choice\nfor high&#8211;fidelity speech synthesis tasks where the output sequence length\ngreatly exceeds that of the input text.</p>\n\n",
                "matched_terms": [
                    "mamba",
                    "decoder"
                ]
            }
        ]
    },
    "A1.T6": {
        "source_file": "Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba",
        "caption": "Table 6: Model architecture details for different configurations.",
        "body": "Model\n#Encoder Layers\n#Decoder Layers\nModel Dimension\n\n\nMAVE\n4\n12\n1808\n\n\nEncoder-decoder Transformer\n4\n12\n1840\n\n\nMamba-only\n–\n16\n2016",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">#Encoder Layers</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">#Decoder Layers</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Model Dimension</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">MAVE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">1808</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1pt;padding-bottom:1pt;\">Encoder-decoder Transformer</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">12</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">1840</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">Mamba-only</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">2016</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "configurations",
            "decoder",
            "encoderdecoder",
            "transformer",
            "different",
            "details",
            "architecture",
            "dimension",
            "mave",
            "layers",
            "mambaonly",
            "model",
            "encoder"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Configuration for our model, and the variants used in ablation studies are shown in Table &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.T6\" title=\"Table 6 &#8227; A.1 Implementation Details &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. All models were trained under consistent experimental settings, using Scaled Adam optimizer and Eden Secheduler proposed in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib46\" title=\"\">46</a>]</cite>, with a base learning rate of 0.01. Hyperparameters were carefully selected to ensure a fair comparison, with all models constrained to have approximately 830 million parameters.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">MAVE</span> (<span class=\"ltx_text ltx_font_bold\">M</span>amba with Cross-<span class=\"ltx_text ltx_font_bold\">A</span>ttention for <span class=\"ltx_text ltx_font_bold\">V</span>oice <span class=\"ltx_text ltx_font_bold\">E</span>diting and Synthesis), a novel autoregressive architecture for text-conditioned voice editing and high-fidelity text-to-speech (TTS) synthesis, built on a cross-attentive Mamba backbone. MAVE achieves state-of-the-art performance in speech editing and very competitive results in zero-shot TTS, while not being explicitly trained on the latter task, outperforming leading autoregressive and diffusion models on diverse, real-world audio. By integrating Mamba for efficient audio sequence modeling with cross-attention for precise text-acoustic alignment, MAVE enables context-aware voice editing with exceptional naturalness and speaker consistency. In pairwise human evaluations on a random 40-sample subset of the RealEdit benchmark (400 judgments), 57.2% of listeners rated MAVE-edited speech as perceptually equal to the original, while 24.8% prefered the original and 18.0% MAVE- demonstrating that in the majority of cases edits are indistinguishable from the source. MAVE compares favorably with VoiceCraft and FluentSpeech both on pairwise comparisons and standalone mean opinion score (MOS) evaluations. For zero-shot TTS, MAVE exceeds VoiceCraft in both speaker similarity and naturalness, without requiring multiple inference runs or post-processing. Remarkably, these quality gains come with a significantly lower memory cost and approximately the same latency: MAVE requires <math alttext=\"\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\sim$}}{\\scalebox{0.8}{$\\textstyle\\sim$}}{\\scalebox{0.8}{$\\scriptstyle\\sim$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\sim$}}6\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\times$}}{\\scalebox{0.8}{$\\textstyle\\times$}}{\\scalebox{0.8}{$\\scriptstyle\\times$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\times$}}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"m6\" intent=\":literal\"><semantics><mrow><mpadded depth=\"0.0pt\" height=\"2.9pt\" style=\"width:8.4pt;height:2.9pt;vertical-align:-0.0pt;\" width=\"8.4pt\"><mo>&#8764;</mo></mpadded><mn>6</mn><mpadded depth=\"0.7pt\" height=\"4.7pt\" style=\"width:8.0pt;height:5.4pt;vertical-align:-0.7pt;\" width=\"8.0pt\"><mo>&#215;</mo></mpadded></mrow><annotation encoding=\"application/x-tex\">\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\sim$}}{\\scalebox{0.8}{$\\textstyle\\sim$}}{\\scalebox{0.8}{$\\scriptstyle\\sim$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\sim$}}6\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\times$}}{\\scalebox{0.8}{$\\textstyle\\times$}}{\\scalebox{0.8}{$\\scriptstyle\\times$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\times$}}</annotation></semantics></math> less memory than VoiceCraft during inference on utterances from the RealEdit database (mean duration: 6.21s, A100, FP16, batch size 1). Our results demonstrate that MAVE establishes a new standard for flexible, high-fidelity voice editing and synthesis through the synergistic integration of structured state-space modeling and cross-modal attention.</p>\n\n",
                "matched_terms": [
                    "architecture",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these limitations, we propose <span class=\"ltx_text ltx_font_bold\">MAVE</span> (<span class=\"ltx_text ltx_font_bold\">M</span>amba with Cross-<span class=\"ltx_text ltx_font_bold\">A</span>ttention for <span class=\"ltx_text ltx_font_bold\">V</span>oice <span class=\"ltx_text ltx_font_bold\">E</span>diting and Synthesis), a novel <em class=\"ltx_emph ltx_font_italic\">autoregressive</em> architecture for text-conditioned voice editing and zero-shot TTS that synergizes Mamba state-space models with cross-attention mechanisms. Unlike Transformer-based decoders&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib42\" title=\"\">42</a>]</cite>, MAVE replaces self-attention with structured state-space sequences (SSMs), enabling <em class=\"ltx_emph ltx_font_italic\">linear-complexity modeling</em> of dependencies between acoustic tokens. Crucially, our cross-attention module dynamically aligns <em class=\"ltx_emph ltx_font_italic\">augmented text inputs</em> with acoustic tokens, allowing the model to &#8220;edit&#8221; speech by attending to the textual information.</p>\n\n",
                "matched_terms": [
                    "model",
                    "architecture",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To the best of our knowledge, <span class=\"ltx_text ltx_font_bold\">MAVE</span> &#8211; built on a cross-attentive Mamba backbone &#8211; is the first successful application of a structured state-space model to text-conditional speech generation, namely speech editing and zero-shot TTS. On the challenging RealEdit benchmark, MAVE achieves <span class=\"ltx_text ltx_font_bold\">human-parity naturalness</span> in speech editing and surpasses state-of-the-art models like VoiceCraft and FluentSpeech in both speaker similarity and naturalness (MOS) without requiring any post-processing. Moreover, MAVE offers significant efficiency gains, reducing memory usage by <span class=\"ltx_text ltx_font_bold\">six</span> times compared to Transformer-based VoiceCraft, while enabling single-pass generation without silent tokens handling or multiple runs. Our results demonstrate that Mamba-based models enhanced with cross-attention can outperform both autoregressive and diffusion-based approaches in fidelity, efficiency, and robustness, establishing a new direction for scalable and high-quality speech generation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The Evolution of Speech Editing</span>\nSpeech editing, defined as the precise modification of targeted speech segments while preserving unaltered regions, has evolved through three distinct generations of methodology. Early approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib21\" title=\"\">21</a>]</cite> relied on concatenating TTS-generated segments with original speech, inevitably introducing prosody mismatches and boundary artifacts due to the absence of contextual conditioning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib31\" title=\"\">31</a>]</cite>. Subsequent research introduced context-aware mechanisms through bidirectional fusion architectures&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib40\" title=\"\">40</a>]</cite> and masked reconstruction objectives&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib43\" title=\"\">43</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib5\" title=\"\">5</a>]</cite>, with Transformer-based systems demonstrating improved contextualization. Most recently, diffusion-based methods like FluentSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib20\" title=\"\">20</a>]</cite> achieved state-of-the-art performance on standard benchmarks through denoising frameworks. However, these approaches collectively suffer from three interrelated limitations. First, Transformer-based systems incur quadratic computational complexity relative to sequence length, restricting practical context windows. Second, evaluation protocols remain constrained to short editing spans&#8212;UniCATS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib11\" title=\"\">11</a>]</cite>, for instance, limits assessments to segments under two seconds, failing to address real-world editing scenarios involving multi-word phrases. Third and most critically, none incorporate mechanisms for leveraging linguistically augmented inputs (e.g., prosody-annotated text) to guide context-aware modifications. MAVE overcomes these constraints through its linear-complexity Mamba architecture and differentiable cross-attention fusion mechanism, enabling robust editing of spans up to 16 words while preserving natural prosody through explicit text augmentation.</p>\n\n",
                "matched_terms": [
                    "architecture",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MAVE in a Nutshell.</span>\nIn summary, MAVE establishes a novel paradigm in text-conditioned speech generation by bridging the efficiency of State Space Models with the flexibility of cross-modal attention. Unlike prior SSM-based approaches such as CM-Mamba&#160;&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib18\" title=\"\">18</a>]</cite>, which require strict alignment between text and audio sequences, our architecture introduces a length-agnostic cross-attention mechanism that enables rich, contextualized phoneme conditioning without artificial upsampling or architectural constraints. By replacing the quadratic self-attention of Transformer-based editors like Voicecraft with linear-time selective state updates, MAVE achieves scalable long-context modeling while preserving prosodic coherence and speaker identity through recurrence. Our framework further unifies three critical capabilities: (1) efficient bidirectional context access via causal token rearrangement, (2) precise linguistic control through differentiable cross-attention on phonemized input, and (3) reference-guided voice consistency without explicit speaker embeddings. As demonstrated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4\" title=\"4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, MAVE is the first model to simultaneously outperform both autoregressive (e.g., VoiceCraft) and non-autoregressive (e.g., FluentSpeech) baselines in human evaluations across speech editing and zero-shot TTS, setting a new standard for unified, high-fidelity, and scalable speech generation. To our knowledge, this is the first successful integration of cross-attention into a Mamba-based audio decoder for unrestricted, long-form speech editing and synthesis.</p>\n\n",
                "matched_terms": [
                    "model",
                    "architecture",
                    "decoder",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present <span class=\"ltx_text ltx_font_bold\">MAVE</span>, a novel architecture for text-conditioned speech editing and zero-shot text-to-speech (TTS). The core challenge lies in autoregressively generating long sequences of discrete audio tokens conditioned on a textual transcript, where the quadratic complexity of self-attention in Transformers becomes prohibitive. Audio signals are typically encoded at 50 Hz using residual vector quantization (RVQ), yielding 4 to 8 discrete tokens per frame, which amounts to 200&#8211;400 tokens per second of speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib47\" title=\"\">47</a>]</cite>. In contrast, the corresponding phoneme sequence contains approximately 12 units per second&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib36\" title=\"\">36</a>]</cite>, resulting in a significant sequence length mismatch between modalities. This disparity makes direct application of Transformer-based models inefficient for high-fidelity, long-form audio generation.</p>\n\n",
                "matched_terms": [
                    "architecture",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While State Space Models (SSMs) such as Mamba&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib13\" title=\"\">13</a>]</cite> offer linear-time sequence modeling, they face two key challenges in multimodal speech generation: (1) limited support for cross-modal conditioning (such as encoder-decoder architecture of transformers), and (2) potential loss of fine-grained acoustic details over long sequences. Prior work such as CM-Mamba&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib18\" title=\"\">18</a>]</cite> enables cross-attention but requires equal-length modalities, which is infeasible for text-conditioned speech processing. Unlike CM-Mamba, which requires equal-length modalities and thus forces phoneme padding or audio subsampling, our architecture supports arbitrary-length text-to-audio conditioning via explicit Cross-Attention module, attended access to text embeddings, decoupled from the autoregressive audio state evolution.</p>\n\n",
                "matched_terms": [
                    "architecture",
                    "encoderdecoder",
                    "details"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these issues, we propose MAVE, a hybrid decoder that combines the efficiency of Mamba for audio token modeling with a flexible cross-attention mechanism for text conditioning&#8212;without requiring aligned sequence lengths. Our model supports context-aware speech infilling and zero-shot TTS by leveraging both surrounding audio context and linguistic input.</p>\n\n",
                "matched_terms": [
                    "model",
                    "decoder",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ the Mamba architecture&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib13\" title=\"\">13</a>]</cite>, a selective State Space Model (SSM), to model long-range dependencies in the audio token sequence. Unlike Transformers, Mamba scales linearly with sequence length and maintains a compressed latent state that effectively captures speaker identity, prosody, and acoustic continuity&#8212;critical for coherent speech editing.</p>\n\n",
                "matched_terms": [
                    "architecture",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike transformers, which benefit from global self-attention, Mamba relies on selective state-space mechanisms that exhibit difficulty in retaining fine-grained information over long sequences. As a result, if we concatenate text and audio tokens and attempt to process them with the same decoder model, distant text tokens&#8212;critical for maintaining linguistic fidelity&#8212;tend to be encoded with degraded or &#8221;fuzzy&#8221; memory <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib41\" title=\"\">41</a>]</cite>, leading to a loss of semantic precision. This limitation is particularly detrimental in speech generation tasks, where high-fidelity retention of textual information is essential to ensure accurate alignment and high-quality output.</p>\n\n",
                "matched_terms": [
                    "model",
                    "decoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To incorporate linguistic information, we first convert the input transcript into a sequence of phonemes using the Montreal Forced Aligner (MFA) toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib30\" title=\"\">30</a>]</cite>. This ensures explicit modeling of pronunciation, especially for homographs. The phoneme sequence is then processed by a 4-layer Transformer encoder to produce contextualized embeddings <math alttext=\"\\mathbf{z}_{\\text{text}}=[\\mathbf{z}_{1},\\dots,\\mathbf{z}_{M}]\\in\\mathbb{R}^{M\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#119859;</mi><mtext>text</mtext></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119859;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119859;</mi><mi>M</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>M</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{z}_{\\text{text}}=[\\mathbf{z}_{1},\\dots,\\mathbf{z}_{M}]\\in\\mathbb{R}^{M\\times d}</annotation></semantics></math>, where <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> is the number of phonemes. As depicted in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S3.F1\" title=\"Figure 1 &#8227; 3 Proposed Method &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, a cross-attention module is inserted after each Mamba block. The query is derived from the Mamba block&#8217;s output, while keys and values come from <math alttext=\"\\mathbf{z}_{\\text{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119859;</mi><mtext>text</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{z}_{\\text{text}}</annotation></semantics></math>. This allows the audio decoder to attend to relevant phonetic content at each generation step, ensuring precise linguistic alignment.</p>\n\n",
                "matched_terms": [
                    "encoder",
                    "decoder",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MAVE does not rely on explicit speaker embeddings for voice identity preservation. Instead, it leverages <span class=\"ltx_text ltx_font_italic\">in-context learning</span> for TTS task by prepending a short reference utterance encoded into discrete audio tokens using <span class=\"ltx_text ltx_font_typewriter\">X-Codec</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib47\" title=\"\">47</a>]</cite> to the generation sequence. This reference context provides rich acoustic, prosodic, and timbral cues that guide the autoregressive decoder to synthesize speech in the target speaker&#8217;s voice, without requiring speaker labels or fine-tuning.</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During both training and inference, the reference tokens are placed at the beginning of the input sequence, followed by the masked audio context. The Mamba decoder attends to this context through its long-range recurrence, effectively conditioning the generated speech on the speaker&#8217;s voice characteristics. This strategy enables true zero-shot TTS: given only a few seconds of reference audio and a target transcript, the model generates high-fidelity, speaker-consistent speech.</p>\n\n",
                "matched_terms": [
                    "model",
                    "decoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The early RVQ codebooks in <span class=\"ltx_text ltx_font_typewriter\">X-Codec</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib47\" title=\"\">47</a>]</cite> encode semantic and linguistic content, while the later codebooks primarily model fine-grained acoustic details and high-frequency textures. To prioritize the accurate reconstruction of perceptually important features, we employ a weighted loss strategy:</p>\n\n",
                "matched_terms": [
                    "model",
                    "details"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, MAVE is a hybrid autoregressive decoder that integrates Mamba blocks for efficient long-sequence audio modeling with cross-attention for flexible text conditioning. By combining token rearrangement, delayed RVQ decoding, and explicit speaker conditioning, it supports high-fidelity speech editing and zero-shot TTS. The network efficiently handles a large disparity between text and audio sequence lengths, offering a scalable alternative to transformer-based methods.</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To train MAVE, we used the ScaledAdam optimizer and Eden Scheduler proposed in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib46\" title=\"\">46</a>]</cite> with a base learning rate of 0.01, batch size of 400k frames (i.e. 133.2 minutes), and total training step\nof 50k with gradient accumulation. The training of the 830M MAVE model took about 4 days on 4 NVIDIA A100 GPUs. For inference, we use Nucleus sampling <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib16\" title=\"\">16</a>]</cite> with p = 0.8 and a temperature of 1 for all experiments.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We computed the Word Error Rate (WER) between the generated audio and the target transcript using Whisper-large and Whisper-medium.en <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib34\" title=\"\">34</a>]</cite>. In addition, we conducted a human evaluation to assess naturalness and intelligibility, based on audio samples from 80 randomly selected test cases for each model. Specifically, 20 people of proven English fluency participated in the study, they were divided into 2 groups of 10 people, and each group was asked to evaluate 120 audios (40 from each method) on the naturalness and intelligibility on a 5-point Likert scale (poor=1, excellent=5). For further details we refer to&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS3.SSS1\" title=\"A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.3.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "details"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MAVE demonstrates superior performance compared to VoiceCraft, achieving smaller WER and higher mean opinion scores (MOS) in both naturalness (3.90 vs. 3.77) and intelligibility (4.25 vs. 4.20). Notably, the performance gap between our model and the ground truth recordings is minimal, with differences of less than 0.1 in naturalness MOS and less than 0.07 in intelligibility MOS. This is an indication that MAVE-edits feature strong perceptual correlation to human speech. It is worth noting that even for the original audio, the average naturalness ratings did not exceed 4.0 out of 5.0. This is mostly attributed to the fact that the audio data were collected in the wild and their quality is inferior to studio recordings, with the presence of background noise and other sources of recording artifacts. This also highlights the challenging conditions under which the evaluation was conducted.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable a meaningful comparison with diffusion-based speech generation models, we selected FluentSpeech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib20\" title=\"\">20</a>]</cite> as the most relevant open-source baseline. However, due to its significantly smaller model size, a direct comparison with our approach would not be fair. To address this, we leveraged the larger variant of FluentSpeech retrained and evaluated by the authors of VoiceCraft. The authors have not made this model publicly available but have provided 14 synthesized utterances, publishing for each sample: (1) VoiceCraft&#8217;s generation, (2) the enhanced FluentSpeech model&#8217;s generation, and (3) the original audio with original and target transcript.\nWe generated corresponding outputs using MAVE for the same 14 samples and conducted a perceptual listening study to compare the audio quality between the three models. The evaluation consisted of a pairwise, side-by-side comparison, where each participant (20 in total) was presented with two audio samples from different models and asked to judge which exhibited better naturalness and intelligibility. With 14 samples per system and three possible pairings between the models (VoiceCraft vs. FluentSpeech, VoiceCraft vs. Ours, and FluentSpeech vs. Ours), each participant evaluated a total of 42 audio pairs. Further details on the experimental setup, including participant instructions, interface design, and evaluation protocol, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS3.SSS2\" title=\"A.3.2 Comparative studies &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.3.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "details",
                    "model",
                    "mave",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.F2\" title=\"Figure 2 &#8227; 4.1 Results on Speech Editing &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> clearly indicate that MAVE accomplishes superior perceptual quality compared to both FluentSpeech and VoiceCraft. While VoiceCraft already holds an advantage over FluentSpeech as it was preferred in 47.1% vs. 13.2% for naturalness and 50.7% vs. 6.4% for intelligibility, our model surpasses FluentSpeech by an even larger margin, being favored in 62.9% of cases for naturalness and 59.3% for intelligibility. More importantly, our model also outperforms VoiceCraft: it is preferred over VoiceCraft in 33.6% of comparisons for naturalness and 21.4% for intelligibility, while VoiceCraft is preferred over ours in only 17.5% and 12.9%, respectively.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the subjective evaluation, we randomly selected 80 samples from our test set and generated corresponding speech outputs using VoiceCraft and our proposed model. To ensure a balanced assessment, we conducted a listening study involving 20 native or near-native English speakers (C1&#8211;C2 proficiency level). Participants were randomly assigned to two groups of 10, with each group evaluating a total of 120 audio clips: 40 generated by VoiceCraft, 40 produced by MAVE, and 40 ground-truth recordings. Each participant rated the naturalness and intelligibility of the audio samples on a 5-point Likert scale (1 = poor, 5 = excellent). Further details regarding the experimental setup, rating criteria, and instructions provided to participants are available in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS3.SSS1\" title=\"A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.3.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave",
                    "details"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MAVE outperformed VoiceCraft in MOS evaluations across both naturalness (3.48 vs. 3.22) and intelligibility (4.20 vs. 4.01). While the overall gap between our model and the ground truth remains notable, a closer analysis reveals a reduced performance disparity under certain conditions. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T3\" title=\"Table 3 &#8227; 4.2 Results on Zero-shot TTS &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, which breaks down results by the number of words to be generated, with split points selected to ensure approximately balanced sample sizes across ranges, specifically 17, 22, 21, 20 for the different ranges in ascending order. The performance gap narrows significantly in the 8&#8211;15 word range. In this interval, our model achieves a naturalness score of 3.71 compared to the ground truth&#8217;s 3.87, and an intelligibility score of 4.26 versus 4.36, indicating competitive quality for medium-length utterances, while the gap becomes more significant with the increasing length of the target transcript. This trend is consistent with what the model is trained on, as it was trained on the Gigaspeech dataset, which consists of speech with moderate utterance lengths.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we compare our model, with previous SOTA AR model for speech editing, namely VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite>, which uses a transformer backbone. All inference experiments were conducted on NVIDIA A100 GPU. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Efficiency Analysis &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> reports the inference time required to process the entire RealEdit dataset as well as the average and peak memory consumption. It is important to note that these reported numbers are based on a single generation per sample. The results clearly demonstrate the superior memory efficiency of MAVE compared to VoiceCraft with KV caching, requiring approximately six times less GPU memory&#8212;largely due to the selective state mechanism of the Mamba backbone. While VoiceCraft exhibits faster inference speed in this evaluation, this advantage is primarily attributed to the relatively short duration of the target audio segments. On average, our model generates about 85 tokens per sample, corresponding to approximately 1.7 seconds of speech, a regime in which autoregressive overheads are minimal.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, theoretical complexity analysis reveals that MAVE scales more favorably with sequence length. For longer generation tasks, our model not only maintains significant memory savings but also surpasses VoiceCraft in computational efficiency, offering better speed performance for extended audio synthesis (see &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS4\" title=\"A.4 Theoretical Analysis of Computational Complexity &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> for more details).</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave",
                    "details"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the ablation study, we considered the masked reconstruction task, in which we used a random subset of the Gigaspeech validation dataset, randomly masked a sequence of words and required the model to reconstruct them (details provided in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS2\" title=\"A.2 Ablation Study Setup &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Efficiency Analysis &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents a comprehensive ablation study comparing different architectural designs under similar conditions, including model size (approximately 830M parameters) and training setup. The results highlight that neither a pure transformer-based encoder-decoder architecture nor a standalone Mamba decoder achieves optimal performance, highlighting the importance of our hybrid design.</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "encoderdecoder",
                    "details",
                    "different",
                    "architecture",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The encoder-decoder transformer, with the encoder utilized to get contextualized text tokens, cross-attention to condition the audio decoder on text, and transformer decoder to model audio dependencies, achieves moderate performance but lags behind our model in all metrics, particularly in WER (10.8 vs. <span class=\"ltx_text ltx_font_bold\">7.8</span>). In contrast, the Mamba-only model, where text and audio tokens are concatenated and processed autoregressively, performs significantly worse with a higher WER (13.0).</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "encoderdecoder",
                    "transformer",
                    "mambaonly",
                    "model",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Crucially, our MAVE architecture integrates both components, it uses cross-attention to explicitly condition the generation process on textual input while leveraging the Mamba decoder to efficiently model acoustic dependencies. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Efficiency Analysis &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, this hybrid approach yields superior performance across all metrics, achieving the lowest WER and MCD, best fundamental frequency (F0) accuracy, and highest PESQ score. These improvements demonstrate that the strength of MAVE does not arise from either cross-attention or Mamba in isolation, but from the combination of both.</p>\n\n",
                "matched_terms": [
                    "model",
                    "architecture",
                    "decoder",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work we introduce MAVE, a novel hybrid architecture for high-fidelity speech synthesis that combines cross-attention and structured state-space modeling via Mamba within a single autoregressive framework. By leveraging Mamba for robust audio modeling and cross-attention for precise text&#8211;audio alignment, MAVE sets a new benchmark for speech editing and zero-shot text-to-speech in terms of both naturalness and efficiency. As future work, we plan to train MAVE on significantly longer audio sequences to further exploit Mamba&#8217;s ability to capture long-range dependencies.</p>\n\n",
                "matched_terms": [
                    "architecture",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development and deployment of speech editing models like MAVE raise important ethical concerns that must be carefully addressed. One primary issue is the potential for misuse in generating deceptive audio content, such as deepfakes, which could be exploited for misinformation, fraud, or impersonation. While our model enables beneficial applications, such as facilitating editing tasks by correcting errors in speech recordings, improving accessibility, or enhancing voice interfaces.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study was conducted to evaluate the quality of audio samples generated by our model in comparison to VoiceCraft and ground-truth recordings, to provide insights of how well our model performs against state-of-the-art models and to assess how closely our synthesized speech approaches the quality of natural, human speech.\nWe sampled 80 random examples from the RealEdit benchmark to evaluate the models on the speech editing task, and another 80 from the LibriTTS to evaluate on zero-shot TTS. Then, for each task we generated the synthesized audios using both our model MAVE and VoiceCraft. After that we divided these audios into two groups, each group was assigned 120 audios (40 from each category). We then asked 20 people to assess the quality of the generation, assigning each user to one group, ensuring that each group receives exactly 10 people. Audios are randomly shuffled for each new user. At the beginning of the study, the instructions are provided to the users as illustrated in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F3\" title=\"Figure 3 &#8227; A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Then for each audio sample, participants were first presented with the corresponding transcript and asked to rate the naturalness of the speech as depicted in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F4\" title=\"Figure 4 &#8227; A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. After submitting their naturalness rating, they were then prompted to evaluate the same audio in terms of intelligibility.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further assess the quality of the generated audios, and to enable direct comparison between the different methods, we have conducted 2 comparative user studies.\nThe first study evaluates our model against VoiceCraft and FluentSpeech. We selected the 14 publicly available audio samples generated by both VoiceCraft and FluentSpeech, which are provided on the VoiceCraft demo page&#160;<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://jasonppy.github.io/VoiceCraft_web/\" title=\"\">https://jasonppy.github.io/VoiceCraft_web/</a>. Using the same text prompts, we generated corresponding outputs using MAVE. This resulted in three pairwise comparisons: (1) VoiceCraft vs. FluentSpeech, (2) VoiceCraft vs. Ours, and (3) FluentSpeech vs. Ours), with 14 pairs per comparison, yielding a total of 42 unique audio pairs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mave",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">an <span class=\"ltx_text ltx_font_bold\">encoder&#8211;decoder</span> architecture in which a Transformer encoder\nprocesses the text and a stack of Mamba&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib13\" title=\"\">13</a>]</cite> layers autoregressively\ngenerates the audio. Each Mamba layer is followed by a cross&#8211;attention module\nconditioning on the encoder output.</p>\n\n",
                "matched_terms": [
                    "architecture",
                    "encoder",
                    "transformer",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"N_{d}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I2.i1.p1.m1\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>d</mi></msub><annotation encoding=\"application/x-tex\">N_{d}</annotation></semantics></math>: number of layers in the decoder&#8211;only Transformer, with hidden\ndimension <math alttext=\"H_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I2.i1.p1.m2\" intent=\":literal\"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">H_{0}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "dimension",
                    "transformer",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"N_{e}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I2.i2.p1.m1\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>e</mi></msub><annotation encoding=\"application/x-tex\">N_{e}</annotation></semantics></math>: number of encoder layers in the encoder&#8211;decoder model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "encoder",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"M_{d}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I2.i3.p1.m1\" intent=\":literal\"><semantics><msub><mi>M</mi><mi>d</mi></msub><annotation encoding=\"application/x-tex\">M_{d}</annotation></semantics></math>: number of Mamba layers in the encoder&#8211;decoder model, each with\nhidden dimension <math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I2.i3.p1.m2\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math> (assumed equal in encoder and decoder).</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "dimension",
                    "layers",
                    "model",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">dominated by the self&#8211;attention layers.\nDuring generation, each Mamba layer updates its hidden state in\n<math alttext=\"\\mathcal{O}(H)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}(H)</annotation></semantics></math> time per token and attends to the fixed encoder output of\nlength <math alttext=\"L_{x}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>x</mi></msub><annotation encoding=\"application/x-tex\">L_{x}</annotation></semantics></math>.\nThus the per&#8211;token decoder cost is\n<math alttext=\"\\mathcal{O}\\big(M_{d}H(L_{x}+1)\\big)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">(</mo><mrow><msub><mi>M</mi><mi>d</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>H</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>L</mi><mi>x</mi></msub><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}\\big(M_{d}H(L_{x}+1)\\big)</annotation></semantics></math>,\nand generating the entire audio sequence requires</p>\n\n",
                "matched_terms": [
                    "encoder",
                    "decoder",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Equations&#160;equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.E4\" title=\"In Decoder&#8211;only Transformer. &#8227; A.4 Theoretical Analysis of Computational Complexity &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>&#8211;equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.E6\" title=\"In Encoder&#8211;decoder with Mamba decoder. &#8227; A.4 Theoretical Analysis of Computational Complexity &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>\nhighlight a key difference:\nthe decoder&#8211;only Transformer grows <em class=\"ltx_emph ltx_font_italic\">quadratically</em> with the audio length\n<math alttext=\"L_{y}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>y</mi></msub><annotation encoding=\"application/x-tex\">L_{y}</annotation></semantics></math>,\nwhereas the proposed encoder&#8211;decoder with Mamba decoder grows only\n<em class=\"ltx_emph ltx_font_italic\">linearly</em> with <math alttext=\"L_{y}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>y</mi></msub><annotation encoding=\"application/x-tex\">L_{y}</annotation></semantics></math> (after a one&#8211;time\n<math alttext=\"\\mathcal{O}(N_{e}HL_{x}^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>N</mi><mi>e</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>H</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>L</mi><mi>x</mi><mn>2</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}(N_{e}HL_{x}^{2})</annotation></semantics></math> encoder cost).\nWhen the audio output is much longer than the text prompt\n(<math alttext=\"L_{y}\\gg L_{x}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mi>y</mi></msub><mo>&#8811;</mo><msub><mi>L</mi><mi>x</mi></msub></mrow><annotation encoding=\"application/x-tex\">L_{y}\\gg L_{x}</annotation></semantics></math>), the dominant terms simplify to</p>\n\n",
                "matched_terms": [
                    "encoder",
                    "decoder",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments the number of audio tokens is significantly larger than the\nnumber of text tokens (<math alttext=\"L_{y}\\gg L_{x}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mi>y</mi></msub><mo>&#8811;</mo><msub><mi>L</mi><mi>x</mi></msub></mrow><annotation encoding=\"application/x-tex\">L_{y}\\gg L_{x}</annotation></semantics></math>).\nUnder this regime the proposed encoder&#8211;decoder model with a Mamba decoder\noffers two clear advantages:</p>\n\n",
                "matched_terms": [
                    "model",
                    "decoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Memory efficiency:</span>\nalthough both models can use KV-cache, the decoder-only Transformer\nmust store keys and values for all past text and audio tokens\n(<math alttext=\"\\mathcal{O}(N_{d}H_{0}(L_{x}+L_{y}))\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I3.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>N</mi><mi>d</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>H</mi><mn>0</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>L</mi><mi>x</mi></msub><mo>+</mo><msub><mi>L</mi><mi>y</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}(N_{d}H_{0}(L_{x}+L_{y}))</annotation></semantics></math> per layer) which grows linearly as the generation goes, whereas the\nencoder&#8211;decoder model only stores a fixed encoder cache\n(<math alttext=\"\\mathcal{O}(N_{e}HL_{x})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I3.i2.p1.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>N</mi><mi>e</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>H</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>L</mi><mi>x</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}(N_{e}HL_{x})</annotation></semantics></math>) and the hidden state of Mamba. When <math alttext=\"L_{y}\\gg L_{x}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I3.i2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mi>y</mi></msub><mo>&#8811;</mo><msub><mi>L</mi><mi>x</mi></msub></mrow><annotation encoding=\"application/x-tex\">L_{y}\\gg L_{x}</annotation></semantics></math>, this results in\nsignificantly lower memory usage for the encoder&#8211;decoder model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "encoder",
                    "transformer"
                ]
            }
        ]
    },
    "A1.T7": {
        "source_file": "Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba",
        "caption": "Table 7: Training configuration parameters.",
        "body": "Parameter\nValue\n\n\nLearning rate (lr)\n0.01\n\n\nMax audio length\n20\n\n\nMin audio length\n2\n\n\nCodebook weight\n[0.25, 0.25, 0.25, 0.05, 0.05, 0.05, 0.05, 0.05]\n\n\nNumber of transformer heads\n16\n\n\nType\nbfloat16\n\n\nAMP (Automatic Mixed Precision)\ntrue",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Parameter</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Value</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Learning rate (lr)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">0.01</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1pt;padding-bottom:1pt;\">Max audio length</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1pt;padding-bottom:1pt;\">20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1pt;padding-bottom:1pt;\">Min audio length</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1pt;padding-bottom:1pt;\">2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1pt;padding-bottom:1pt;\">Codebook weight</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1pt;padding-bottom:1pt;\">[0.25, 0.25, 0.25, 0.05, 0.05, 0.05, 0.05, 0.05]</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1pt;padding-bottom:1pt;\">Number of transformer heads</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1pt;padding-bottom:1pt;\">16</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1pt;padding-bottom:1pt;\">Type</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1pt;padding-bottom:1pt;\">bfloat16</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">AMP (Automatic Mixed Precision)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">true</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "mixed",
            "true",
            "parameters",
            "transformer",
            "heads",
            "weight",
            "value",
            "rate",
            "min",
            "bfloat16",
            "max",
            "audio",
            "learning",
            "number",
            "amp",
            "configuration",
            "training",
            "parameter",
            "length",
            "precision",
            "codebook",
            "type",
            "automatic"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.T7\" title=\"Table 7 &#8227; A.1 Implementation Details &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows detailed configuration used to train the models.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present <span class=\"ltx_text ltx_font_bold\">MAVE</span>, a novel architecture for text-conditioned speech editing and zero-shot text-to-speech (TTS). The core challenge lies in autoregressively generating long sequences of discrete audio tokens conditioned on a textual transcript, where the quadratic complexity of self-attention in Transformers becomes prohibitive. Audio signals are typically encoded at 50 Hz using residual vector quantization (RVQ), yielding 4 to 8 discrete tokens per frame, which amounts to 200&#8211;400 tokens per second of speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib47\" title=\"\">47</a>]</cite>. In contrast, the corresponding phoneme sequence contains approximately 12 units per second&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib36\" title=\"\">36</a>]</cite>, resulting in a significant sequence length mismatch between modalities. This disparity makes direct application of Transformer-based models inefficient for high-fidelity, long-form audio generation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given an input audio waveform <math alttext=\"X\\in\\mathbb{R}^{T\\times f_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>f</mi><mi>s</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">X\\in\\mathbb{R}^{T\\times f_{s}}</annotation></semantics></math>, where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is duration in seconds and <math alttext=\"f_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">f_{s}</annotation></semantics></math> is the sampling rate (e.g., 16 kHz), we encode it using <span class=\"ltx_text ltx_font_typewriter\">X-Codec</span>, a residual vector quantization (RVQ)-based neural codec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib47\" title=\"\">47</a>]</cite>. This yields a sequence of <math alttext=\"K=8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">K=8</annotation></semantics></math> parallel streams of discrete tokens at a frame rate of <math alttext=\"f_{\\text{x}}=50\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mtext>x</mtext></msub><mo>=</mo><mn>50</mn></mrow><annotation encoding=\"application/x-tex\">f_{\\text{x}}=50</annotation></semantics></math> Hz. Let <math alttext=\"L=T\\cdot f_{\\text{x}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>=</mo><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><msub><mi>f</mi><mtext>x</mtext></msub></mrow></mrow><annotation encoding=\"application/x-tex\">L=T\\cdot f_{\\text{x}}</annotation></semantics></math> denote the number of frames. The encoded representation is:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "number",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech editing, we consider a non-autoregressive infilling task where one or more spans of audio tokens are masked and must be reconstructed. To enable bidirectional context access during autoregressive generation, we adopt the <span class=\"ltx_text ltx_font_bold\">causal masking</span> strategy from CM3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib1\" title=\"\">1</a>]</cite>. Masked spans are replaced with special mask tokens and moved to the end of the sequence, allowing the model to condition on both past and future context while preserving autoregressive training.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training, we sample <math alttext=\"m\\sim\\text{Poisson}(\\lambda=1)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo>&#8764;</mo><mrow><mtext>Poisson</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#955;</mi><mo>=</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">m\\sim\\text{Poisson}(\\lambda=1)</annotation></semantics></math> masked spans, capped at a maximum of <math alttext=\"N=3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">N=3</annotation></semantics></math>. Each span length is uniformly sampled from <math alttext=\"[1,600]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mn>600</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[1,600]</annotation></semantics></math> frames, and positions are selected to avoid overlap. If the <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math>-th span <math alttext=\"s_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m5\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">s_{j}</annotation></semantics></math> is masked, we:</p>\n\n",
                "matched_terms": [
                    "training",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ the Mamba architecture&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib13\" title=\"\">13</a>]</cite>, a selective State Space Model (SSM), to model long-range dependencies in the audio token sequence. Unlike Transformers, Mamba scales linearly with sequence length and maintains a compressed latent state that effectively captures speaker identity, prosody, and acoustic continuity&#8212;critical for coherent speech editing.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike transformers, which benefit from global self-attention, Mamba relies on selective state-space mechanisms that exhibit difficulty in retaining fine-grained information over long sequences. As a result, if we concatenate text and audio tokens and attempt to process them with the same decoder model, distant text tokens&#8212;critical for maintaining linguistic fidelity&#8212;tend to be encoded with degraded or &#8221;fuzzy&#8221; memory <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib41\" title=\"\">41</a>]</cite>, leading to a loss of semantic precision. This limitation is particularly detrimental in speech generation tasks, where high-fidelity retention of textual information is essential to ensure accurate alignment and high-quality output.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "precision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To incorporate linguistic information, we first convert the input transcript into a sequence of phonemes using the Montreal Forced Aligner (MFA) toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib30\" title=\"\">30</a>]</cite>. This ensures explicit modeling of pronunciation, especially for homographs. The phoneme sequence is then processed by a 4-layer Transformer encoder to produce contextualized embeddings <math alttext=\"\\mathbf{z}_{\\text{text}}=[\\mathbf{z}_{1},\\dots,\\mathbf{z}_{M}]\\in\\mathbb{R}^{M\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#119859;</mi><mtext>text</mtext></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119859;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119859;</mi><mi>M</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>M</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{z}_{\\text{text}}=[\\mathbf{z}_{1},\\dots,\\mathbf{z}_{M}]\\in\\mathbb{R}^{M\\times d}</annotation></semantics></math>, where <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> is the number of phonemes. As depicted in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S3.F1\" title=\"Figure 1 &#8227; 3 Proposed Method &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, a cross-attention module is inserted after each Mamba block. The query is derived from the Mamba block&#8217;s output, while keys and values come from <math alttext=\"\\mathbf{z}_{\\text{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119859;</mi><mtext>text</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{z}_{\\text{text}}</annotation></semantics></math>. This allows the audio decoder to attend to relevant phonetic content at each generation step, ensuring precise linguistic alignment.</p>\n\n",
                "matched_terms": [
                    "number",
                    "audio",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MAVE does not rely on explicit speaker embeddings for voice identity preservation. Instead, it leverages <span class=\"ltx_text ltx_font_italic\">in-context learning</span> for TTS task by prepending a short reference utterance encoded into discrete audio tokens using <span class=\"ltx_text ltx_font_typewriter\">X-Codec</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib47\" title=\"\">47</a>]</cite> to the generation sequence. This reference context provides rich acoustic, prosodic, and timbral cues that guide the autoregressive decoder to synthesize speech in the target speaker&#8217;s voice, without requiring speaker labels or fine-tuning.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "learning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During both training and inference, the reference tokens are placed at the beginning of the input sequence, followed by the masked audio context. The Mamba decoder attends to this context through its long-range recurrence, effectively conditioning the generated speech on the speaker&#8217;s voice characteristics. This strategy enables true zero-shot TTS: given only a few seconds of reference audio and a target transcript, the model generates high-fidelity, speaker-consistent speech.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "true",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\{\\alpha_{k}\\}_{k=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.Px1.p3.m1\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#945;</mi><mi>k</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><annotation encoding=\"application/x-tex\">\\{\\alpha_{k}\\}_{k=1}^{K}</annotation></semantics></math> are tunable hyperparameters with <math alttext=\"\\alpha_{1}\\geq\\alpha_{2}\\geq\\dots\\geq\\alpha_{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.Px1.p3.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#945;</mi><mn>1</mn></msub><mo>&#8805;</mo><msub><mi>&#945;</mi><mn>2</mn></msub><mo>&#8805;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>&#8805;</mo><msub><mi>&#945;</mi><mi>K</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\alpha_{1}\\geq\\alpha_{2}\\geq\\dots\\geq\\alpha_{K}</annotation></semantics></math>, typically set inversely proportional to codebook index. Following <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib28\" title=\"\">28</a>]</cite> we assign a weight of 0.25 to the first three levels and 0.05 to the last five. Additionally, following &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib1\" title=\"\">1</a>]</cite>, we compute the prediction loss over all valid audio tokens in the sequence, excluding special tokens such as mask tokens <math alttext=\"M_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.Px1.p3.m3\" intent=\":literal\"><semantics><msub><mi>M</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">M_{j}</annotation></semantics></math> regardless of whether they belong to masked or unmasked regions. This encourages the model to refine its internal representations throughout the sequence, improving contextual coherence and reconstruction fidelity.</p>\n\n",
                "matched_terms": [
                    "weight",
                    "audio",
                    "codebook"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Gigaspeech training set&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib6\" title=\"\">6</a>]</cite> is used as the training data, which contains 9k hours\nof audiobooks, podcasts, and YouTube videos at 16kHz audio sampling rate. For speech editing evaluation, we use the real-world benchmark called RealEdit from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite>. For TTS evaluation, we use a subset of libritts <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib51\" title=\"\">51</a>]</cite></p>\n\n",
                "matched_terms": [
                    "audio",
                    "rate",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To train MAVE, we used the ScaledAdam optimizer and Eden Scheduler proposed in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib46\" title=\"\">46</a>]</cite> with a base learning rate of 0.01, batch size of 400k frames (i.e. 133.2 minutes), and total training step\nof 50k with gradient accumulation. The training of the 830M MAVE model took about 4 days on 4 NVIDIA A100 GPUs. For inference, we use Nucleus sampling <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib16\" title=\"\">16</a>]</cite> with p = 0.8 and a temperature of 1 for all experiments.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "learning",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We computed the Word Error Rate (WER) between the generated audio and the target transcript using Whisper-large and Whisper-medium.en <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib34\" title=\"\">34</a>]</cite>. In addition, we conducted a human evaluation to assess naturalness and intelligibility, based on audio samples from 80 randomly selected test cases for each model. Specifically, 20 people of proven English fluency participated in the study, they were divided into 2 groups of 10 people, and each group was asked to evaluate 120 audios (40 from each method) on the naturalness and intelligibility on a 5-point Likert scale (poor=1, excellent=5). For further details we refer to&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS3.SSS1\" title=\"A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.3.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MAVE outperformed VoiceCraft in MOS evaluations across both naturalness (3.48 vs. 3.22) and intelligibility (4.20 vs. 4.01). While the overall gap between our model and the ground truth remains notable, a closer analysis reveals a reduced performance disparity under certain conditions. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T3\" title=\"Table 3 &#8227; 4.2 Results on Zero-shot TTS &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, which breaks down results by the number of words to be generated, with split points selected to ensure approximately balanced sample sizes across ranges, specifically 17, 22, 21, 20 for the different ranges in ascending order. The performance gap narrows significantly in the 8&#8211;15 word range. In this interval, our model achieves a naturalness score of 3.71 compared to the ground truth&#8217;s 3.87, and an intelligibility score of 4.26 versus 4.36, indicating competitive quality for medium-length utterances, while the gap becomes more significant with the increasing length of the target transcript. This trend is consistent with what the model is trained on, as it was trained on the Gigaspeech dataset, which consists of speech with moderate utterance lengths.</p>\n\n",
                "matched_terms": [
                    "number",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we compare our model, with previous SOTA AR model for speech editing, namely VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib33\" title=\"\">33</a>]</cite>, which uses a transformer backbone. All inference experiments were conducted on NVIDIA A100 GPU. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Efficiency Analysis &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> reports the inference time required to process the entire RealEdit dataset as well as the average and peak memory consumption. It is important to note that these reported numbers are based on a single generation per sample. The results clearly demonstrate the superior memory efficiency of MAVE compared to VoiceCraft with KV caching, requiring approximately six times less GPU memory&#8212;largely due to the selective state mechanism of the Mamba backbone. While VoiceCraft exhibits faster inference speed in this evaluation, this advantage is primarily attributed to the relatively short duration of the target audio segments. On average, our model generates about 85 tokens per sample, corresponding to approximately 1.7 seconds of speech, a regime in which autoregressive overheads are minimal.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, theoretical complexity analysis reveals that MAVE scales more favorably with sequence length. For longer generation tasks, our model not only maintains significant memory savings but also surpasses VoiceCraft in computational efficiency, offering better speed performance for extended audio synthesis (see &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS4\" title=\"A.4 Theoretical Analysis of Computational Complexity &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> for more details).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the ablation study, we considered the masked reconstruction task, in which we used a random subset of the Gigaspeech validation dataset, randomly masked a sequence of words and required the model to reconstruct them (details provided in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.SS2\" title=\"A.2 Ablation Study Setup &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>). Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#S4.T5\" title=\"Table 5 &#8227; 4.3 Efficiency Analysis &#8227; 4 Experiments &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents a comprehensive ablation study comparing different architectural designs under similar conditions, including model size (approximately 830M parameters) and training setup. The results highlight that neither a pure transformer-based encoder-decoder architecture nor a standalone Mamba decoder achieves optimal performance, highlighting the importance of our hybrid design.</p>\n\n",
                "matched_terms": [
                    "parameters",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The encoder-decoder transformer, with the encoder utilized to get contextualized text tokens, cross-attention to condition the audio decoder on text, and transformer decoder to model audio dependencies, achieves moderate performance but lags behind our model in all metrics, particularly in WER (10.8 vs. <span class=\"ltx_text ltx_font_bold\">7.8</span>). In contrast, the Mamba-only model, where text and audio tokens are concatenated and processed autoregressively, performs significantly worse with a higher WER (13.0).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Configuration for our model, and the variants used in ablation studies are shown in Table &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.T6\" title=\"Table 6 &#8227; A.1 Implementation Details &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. All models were trained under consistent experimental settings, using Scaled Adam optimizer and Eden Secheduler proposed in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib46\" title=\"\">46</a>]</cite>, with a base learning rate of 0.01. Hyperparameters were carefully selected to ensure a fair comparison, with all models constrained to have approximately 830 million parameters.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "parameters",
                    "configuration",
                    "learning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study was conducted to evaluate the quality of audio samples generated by our model in comparison to VoiceCraft and ground-truth recordings, to provide insights of how well our model performs against state-of-the-art models and to assess how closely our synthesized speech approaches the quality of natural, human speech.\nWe sampled 80 random examples from the RealEdit benchmark to evaluate the models on the speech editing task, and another 80 from the LibriTTS to evaluate on zero-shot TTS. Then, for each task we generated the synthesized audios using both our model MAVE and VoiceCraft. After that we divided these audios into two groups, each group was assigned 120 audios (40 from each category). We then asked 20 people to assess the quality of the generation, assigning each user to one group, ensuring that each group receives exactly 10 people. Audios are randomly shuffled for each new user. At the beginning of the study, the instructions are provided to the users as illustrated in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F3\" title=\"Figure 3 &#8227; A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Then for each audio sample, participants were first presented with the corresponding transcript and asked to rate the naturalness of the speech as depicted in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.F4\" title=\"Figure 4 &#8227; A.3.1 Speech editing and zero-shot TTS mean opinion score &#8227; A.3 Instructions and details for user study &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. After submitting their naturalness rating, they were then prompted to evaluate the same audio in terms of intelligibility.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide a theoretical comparison between two autoregressive\ngeneration paradigms for modeling paired text&#8211;audio data.\nLet <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m1\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> denote the input text sequence of length <math alttext=\"L_{x}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>x</mi></msub><annotation encoding=\"application/x-tex\">L_{x}</annotation></semantics></math>, and\n<math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m3\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math> the output audio sequence of length <math alttext=\"L_{y}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>y</mi></msub><annotation encoding=\"application/x-tex\">L_{y}</annotation></semantics></math>.\nWe study the computational complexity of:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">a <span class=\"ltx_text ltx_font_bold\">decoder&#8211;only Transformer</span> that consumes the concatenation of the text\nand the previously generated audio tokens, and</p>\n\n",
                "matched_terms": [
                    "audio",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">an <span class=\"ltx_text ltx_font_bold\">encoder&#8211;decoder</span> architecture in which a Transformer encoder\nprocesses the text and a stack of Mamba&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#bib.bib13\" title=\"\">13</a>]</cite> layers autoregressively\ngenerates the audio. Each Mamba layer is followed by a cross&#8211;attention module\nconditioning on the encoder output.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"N_{d}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I2.i1.p1.m1\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>d</mi></msub><annotation encoding=\"application/x-tex\">N_{d}</annotation></semantics></math>: number of layers in the decoder&#8211;only Transformer, with hidden\ndimension <math alttext=\"H_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I2.i1.p1.m2\" intent=\":literal\"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">H_{0}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "number",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">dominated by the self&#8211;attention layers.\nDuring generation, each Mamba layer updates its hidden state in\n<math alttext=\"\\mathcal{O}(H)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}(H)</annotation></semantics></math> time per token and attends to the fixed encoder output of\nlength <math alttext=\"L_{x}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>x</mi></msub><annotation encoding=\"application/x-tex\">L_{x}</annotation></semantics></math>.\nThus the per&#8211;token decoder cost is\n<math alttext=\"\\mathcal{O}\\big(M_{d}H(L_{x}+1)\\big)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">(</mo><mrow><msub><mi>M</mi><mi>d</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>H</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>L</mi><mi>x</mi></msub><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}\\big(M_{d}H(L_{x}+1)\\big)</annotation></semantics></math>,\nand generating the entire audio sequence requires</p>\n\n",
                "matched_terms": [
                    "audio",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Equations&#160;equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.E4\" title=\"In Decoder&#8211;only Transformer. &#8227; A.4 Theoretical Analysis of Computational Complexity &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>&#8211;equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04738v1#A1.E6\" title=\"In Encoder&#8211;decoder with Mamba decoder. &#8227; A.4 Theoretical Analysis of Computational Complexity &#8227; Appendix A Appendix &#8227; Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>\nhighlight a key difference:\nthe decoder&#8211;only Transformer grows <em class=\"ltx_emph ltx_font_italic\">quadratically</em> with the audio length\n<math alttext=\"L_{y}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>y</mi></msub><annotation encoding=\"application/x-tex\">L_{y}</annotation></semantics></math>,\nwhereas the proposed encoder&#8211;decoder with Mamba decoder grows only\n<em class=\"ltx_emph ltx_font_italic\">linearly</em> with <math alttext=\"L_{y}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>y</mi></msub><annotation encoding=\"application/x-tex\">L_{y}</annotation></semantics></math> (after a one&#8211;time\n<math alttext=\"\\mathcal{O}(N_{e}HL_{x}^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>N</mi><mi>e</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>H</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>L</mi><mi>x</mi><mn>2</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}(N_{e}HL_{x}^{2})</annotation></semantics></math> encoder cost).\nWhen the audio output is much longer than the text prompt\n(<math alttext=\"L_{y}\\gg L_{x}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mi>y</mi></msub><mo>&#8811;</mo><msub><mi>L</mi><mi>x</mi></msub></mrow><annotation encoding=\"application/x-tex\">L_{y}\\gg L_{x}</annotation></semantics></math>), the dominant terms simplify to</p>\n\n",
                "matched_terms": [
                    "audio",
                    "transformer",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments the number of audio tokens is significantly larger than the\nnumber of text tokens (<math alttext=\"L_{y}\\gg L_{x}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mi>y</mi></msub><mo>&#8811;</mo><msub><mi>L</mi><mi>x</mi></msub></mrow><annotation encoding=\"application/x-tex\">L_{y}\\gg L_{x}</annotation></semantics></math>).\nUnder this regime the proposed encoder&#8211;decoder model with a Mamba decoder\noffers two clear advantages:</p>\n\n",
                "matched_terms": [
                    "number",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Memory efficiency:</span>\nalthough both models can use KV-cache, the decoder-only Transformer\nmust store keys and values for all past text and audio tokens\n(<math alttext=\"\\mathcal{O}(N_{d}H_{0}(L_{x}+L_{y}))\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I3.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>N</mi><mi>d</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>H</mi><mn>0</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>L</mi><mi>x</mi></msub><mo>+</mo><msub><mi>L</mi><mi>y</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}(N_{d}H_{0}(L_{x}+L_{y}))</annotation></semantics></math> per layer) which grows linearly as the generation goes, whereas the\nencoder&#8211;decoder model only stores a fixed encoder cache\n(<math alttext=\"\\mathcal{O}(N_{e}HL_{x})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I3.i2.p1.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>N</mi><mi>e</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>H</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>L</mi><mi>x</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}(N_{e}HL_{x})</annotation></semantics></math>) and the hidden state of Mamba. When <math alttext=\"L_{y}\\gg L_{x}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.I3.i2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mi>y</mi></msub><mo>&#8811;</mo><msub><mi>L</mi><mi>x</mi></msub></mrow><annotation encoding=\"application/x-tex\">L_{y}\\gg L_{x}</annotation></semantics></math>, this results in\nsignificantly lower memory usage for the encoder&#8211;decoder model.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "transformer"
                ]
            }
        ]
    }
}