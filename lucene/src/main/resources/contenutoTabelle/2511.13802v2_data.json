{
    "S4.T1": {
        "source_file": "Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video",
        "caption": "Table 1: Test performance with validation-tuned thresholds.",
        "body": "Method\nAUROC\nAP\nF1\nAcc\nThr\n\n\nStats+PCA+RF (ours)\n0.953\n0.961\n0.851\n0.857\n0.636\n\n\nDL+INCEPTIONTIME\n0.938\n0.949\n0.824\n0.835\n0.794\n\n\nStats+PCA+ADA\n0.902\n0.919\n0.741\n0.769\n0.690\n\n\nStats+PCA+GBDT\n0.893\n0.925\n0.844\n0.846\n0.522\n\n\nDL+TSMIXER\n0.844\n0.865\n0.772\n0.747\n0.662",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">AUROC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">AP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">F1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Acc</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Thr</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Stats+PCA+RF (ours)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.953</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.961</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.851</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.857</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.636</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">DL+INCEPTIONTIME</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.938</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.949</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.824</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.835</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.794</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Stats+PCA+ADA</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.902</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.919</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.741</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.769</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.690</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Stats+PCA+GBDT</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.893</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.925</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.844</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.846</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.522</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">DL+TSMIXER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.844</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.865</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.772</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.747</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.95pt;padding-bottom:0.95pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.662</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "validationtuned",
            "acc",
            "test",
            "statspcagbdt",
            "thr",
            "statspcarf",
            "ours",
            "thresholds",
            "dlinceptiontime",
            "method",
            "auroc",
            "performance",
            "statspcaada",
            "dltsmixer"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13802v2#S4.T1\" title=\"Table 1 &#8227; 4.2 Results &#8227; 4 Experiments and Results &#8227; Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarizes the headline discrimination numbers for five heads, contrasting our Stats+PCA+RF default against a convolutional SOTA baseline (DL+INCEPTIONTIME), an MLP mixer (DL+TSMIXER), and two stronger tree ensembles (Stats+PCA+ADA/GBDT). The Stats+PCA+RF head attains the best AUROC, AP, F1 and Acc, so we keep it as our primary screening model for all subsequent analyses. The next figures unpack these results in a consistent order. First, Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13802v2#S4.F3\" title=\"Figure 3 &#8227; 4.2 Results &#8227; 4 Experiments and Results &#8227; Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13802v2#S4.F4\" title=\"Figure 4 &#8227; 4.2 Results &#8227; 4 Experiments and Results &#8227; Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> focus on ranking quality under different prevalence and threshold conditions. Second, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13802v2#S4.F5\" title=\"Figure 5 &#8227; 4.2 Results &#8227; 4 Experiments and Results &#8227; Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> connects the selected operating point to concrete error types and the distribution of posteriors. Third, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13802v2#S4.F6\" title=\"Figure 6 &#8227; 4.2 Results &#8227; 4 Experiments and Results &#8227; Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows how metrics change as the decision threshold moves, which clarifies the trade-off between precision and recall. Fourth, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13802v2#S4.F7\" title=\"Figure 7 &#8227; 4.2 Results &#8227; 4 Experiments and Results &#8227; Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> examines the marginal contribution of each microdynamic channel. Finally, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13802v2#S4.F8\" title=\"Figure 8 &#8227; 4.2 Results &#8227; 4 Experiments and Results &#8227; Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> provides qualitative examples that make quantitative patterns visible and interpretable.</p>\n\n",
            "<p class=\"ltx_p\">We benchmark our Stats+PCA+RF head against InceptionTime as a strong convolutional TSC reference&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fawaz2020inceptiontime</span>]</cite>, PatchTST and iTransformer as channel-aware Transformer forecasters&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nie2023patchtst</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">itransformer2024</span>]</cite>, TimesNet as a 2D-variation temporal model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2023timesnet</span>]</cite>, and TSMixer as an efficient all-MLP mixer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2023tsmixer</span>]</cite>, so that our calibrated tree ensemble is tested against diverse, state-of-the-art (SOTA) inductive biases. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13802v2#S4.F3\" title=\"Figure 3 &#8227; 4.2 Results &#8227; 4 Experiments and Results &#8227; Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> summarizes ranking performance across all possible thresholds. The curve for the Stats, PCA, then Random Forest head dominates the alternatives over a wide false positive range, which supports the aggregate AUROC reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13802v2#S4.T1\" title=\"Table 1 &#8227; 4.2 Results &#8227; 4 Experiments and Results &#8227; Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. This indicates that the chosen representation and head separate positive and negative clips consistently, even when the operating point is shifted. The shape near the low false positive region is especially relevant for screening, where the cost of false positives is high and precision must remain stable.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We target passive dementia screening from short camera-facing talking-head video, developing a facial temporal micro-dynamics analysis for language-free detection of early neuro-cognitive change. This enables unscripted, in-the-wild video analysis at scale to capture natural facial behaviors, transferrable across devices, topics, and cultures without active intervention by clinicians or researchers during recording. Most existing resources prioritize speech or scripted interviews, limiting use outside clinics and coupling predictions to language and transcription. In contrast, we identify and analyze whether temporal facial kinematics, including blink dynamics, small mouth&#8211;jaw motions, gaze variability, and subtle head adjustments, are sufficient for dementia screening without speech or text. By stabilizing facial signals, we convert these micro-movements into interpretable facial microdynamic time series, smooth them, and summarize short windows into compact clip-level statistics for screening. Each window is encoded by its activity mix (the relative share of motion across streams), thus the predictor analyzes the distribution of motion across streams rather than its magnitude, making per-channel effects transparent. We also introduce <a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/datasets/anonymous-multimodal-researcher/YT-DemTalk/tree/main\" title=\"\">YT-DemTalk</a>, a new dataset curated from publicly available, in-the-wild camera-facing videos. It contains 300 clips (150 with self-reported dementia, 150 controls) to test our model and offer a first benchmarking of the corpus. On YT-DemTalk, ablations identify gaze lability and mouth/jaw dynamics as the most informative cues, and light-weighted shallow classifiers could attain a dementia prediction performance of (AUROC) 0.953, 0.961 Average Precision (AP), 0.851 F1-score, and 0.857 accuracy.</p>\n\n",
                "matched_terms": [
                    "auroc",
                    "performance",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Passive screening from in-the-wild talking-head video requires signals that (i) are content-agnostic, so language, prompt, and topic do not confound predictions; (ii) remain stable under camera/viewpoint and subject identity; and (iii) are calibratable at inference to support risk thresholds in deployments. Content-dependent baselines (text/semantics, lexical prosody, audio spectra) excel on single streams but entangle diagnosis with &#8220;what is said,&#8221; microphone conditions, and dataset curation. Purely geometric streams (2D/3D landmarks) can be robust yet lose fine-scale dynamics needed for sensitive triage. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13802v2#S3.F2\" title=\"Figure 2 &#8227; 3.3 Design &#8227; 3 Method &#8227; Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the resulting system feedback loop. First, a lightweight landmark stabilizer decouples camera shake from facial micromotor activity, yielding per-stream traces that remain interpretable across content. Second, each stream produces a bounded indicator (e.g., blink regularity, eyelid &#8220;steadiness,&#8221; jaw activity, gaze lability, head micro-movement), which we aggregate into a conserved-sum simplex so no single cue dominates and trade-offs are explicit at thresholding. Third, we prioritize <em class=\"ltx_emph ltx_font_italic\">calibration</em>: alongside accuracy metrics, we target low Brier score and Expected Calibration Error (ECE) so badge summaries reflect reliable probabilities clinicians, or an automated triage system can act on. This combination, content-agnostic features, conserved compositions, and calibration-first reporting, guides the method described next and underpins our deployment-oriented evaluation.</p>\n\n",
                "matched_terms": [
                    "thresholds",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each clip yields <em class=\"ltx_emph ltx_font_italic\">six</em> micro-dynamics channels: (1) blink/eye aspect ratio (EAR), (2) eyelid stability, (3) mouth&#8211;jaw motion, (4) brow asymmetry, (5) gaze dispersion (iris trajectory), and (6) head micro-jitter (pose), computed on sliding windows of 6&#8201;s with a 2&#8201;s hop after a 15&#8201;s per-video baseline. We sweep window/hop, smoothing (Savitzky&#8211;Golay vs. exponential moving average (EMA)-only), PCA variance targets, and head family. Per-video features (mean and dispersion) are standardized, reduced by PCA (variance target 0.98), and scored by shallow heads (Random Forest (RF), Gradient-Boosted Decision Trees (GBDT), AdaBoost (ADA), Logistic Regression (LR), Support Vector Machine (SVM), Extra Trees (ET); RF is default). For SOTA sequence baselines we additionally train <em class=\"ltx_emph ltx_font_italic\">DL+INCEPTIONTIME</em> and <em class=\"ltx_emph ltx_font_italic\">DL+TSMIXER</em> on the same micro-dynamics streams following standard configurations&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fawaz2020inceptiontime</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2023tsmixer</span>]</cite>. Probabilities for all heads are calibrated on validation and frozen for test, following modern large-scale calibration practice in vision&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Minderer2021RevisitingCalibration</span>]</cite>. All experiments run on an NVIDIA RTX&#160;4090; full training and evaluation across baselines completes in <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>3 hours.</p>\n\n",
                "matched_terms": [
                    "dlinceptiontime",
                    "dltsmixer",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Head (Stats&#8211;PCA&#8211;RF).</span>\nHere, <em class=\"ltx_emph ltx_font_italic\">Stats</em> are per-window summary features, <em class=\"ltx_emph ltx_font_italic\">PCA</em> reduces their dimensionality, and <em class=\"ltx_emph ltx_font_italic\">RF</em> is a Random Forest trained on the PCA-reduced vectors. We report stratified bootstrap 95% CIs for AUROC/AP and use non-parametric permutation tests for between-head AUROC differences, which match the ordering in Figs.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13802v2#S4.F3\" title=\"Figure 3 &#8227; 4.2 Results &#8227; 4 Experiments and Results &#8227; Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13802v2#S4.F4\" title=\"Figure 4 &#8227; 4.2 Results &#8227; 4 Experiments and Results &#8227; Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. Because screening acts on thresholds, we calibrate probabilities by temperature scaling on the validation split and keep the temperature fixed at test time&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Minderer2021RevisitingCalibration</span>]</cite>. The chosen operating point (<math alttext=\"\\tau{=}0.636\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>=</mo><mn>0.636</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{=}0.636</annotation></semantics></math>; Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13802v2#S4.F6\" title=\"Figure 6 &#8227; 4.2 Results &#8227; 4 Experiments and Results &#8227; Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) lies on a broad Accuracy/F1 plateau and yields recall 0.771, specificity 0.953, accuracy 0.857, F1 0.851, and only two false positives (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13802v2#S4.F5\" title=\"Figure 5 &#8227; 4.2 Results &#8227; 4 Experiments and Results &#8227; Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). Single-channel ablations (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13802v2#S4.F7\" title=\"Figure 7 &#8227; 4.2 Results &#8227; 4 Experiments and Results &#8227; Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) show that <em class=\"ltx_emph ltx_font_italic\">gaze</em> lability and <em class=\"ltx_emph ltx_font_italic\">mouth/jaw</em> motion contribute most of the signal, with <em class=\"ltx_emph ltx_font_italic\">brow</em> asymmetry and <em class=\"ltx_emph ltx_font_italic\">head micro-jitter</em> providing smaller gains; this pattern is stable across the window/hop, smoothing, PCA-target, and head-family sweeps.</p>\n\n",
                "matched_terms": [
                    "thresholds",
                    "test",
                    "auroc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results</span>\nWe report AUROC and AP with stratified bootstrap 95% CIs, and calibration with Expected Calibration Error (ECE; 10 bins) and Brier score. Between-method AUROC differences are assessed via a non-parametric, label-preserving permutation test, following recent best practice for calibration/uncertainty evaluation and hypothesis testing in vision <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2022pairwise</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023pitfall</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mandel2024permutation</span>]</cite>. Our chosen head, <em class=\"ltx_emph ltx_font_italic\">Stats+PCA+RF (ours)</em>, attains\n<span class=\"ltx_text ltx_font_bold\">AUROC</span>&#160;0.953&#8201;<math alttext=\"[0.912,\\,0.984]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p6.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0.912</mn><mo>,</mo><mn>&#8201;0.984</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0.912,\\,0.984]</annotation></semantics></math> and\n<span class=\"ltx_text ltx_font_bold\">AP</span>&#160;0.961&#8201;<math alttext=\"[0.928,\\,0.986]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p6.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0.928</mn><mo>,</mo><mn>&#8201;0.986</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0.928,\\,0.986]</annotation></semantics></math>,\nwith <span class=\"ltx_text ltx_font_bold\">F1</span>&#160;0.851 and <span class=\"ltx_text ltx_font_bold\">ACC</span>&#160;0.857 at the tuned threshold&#160;0.636.\nCalibration is reasonable: <span class=\"ltx_text ltx_font_bold\">ECE</span>&#160;0.268 and <span class=\"ltx_text ltx_font_bold\">Brier</span>&#160;0.168.\nAmong the new baselines, the strongest competitor is the convolutional <em class=\"ltx_emph ltx_font_italic\">DL+INCEPTIONTIME</em> head, which reaches AUROC&#160;0.938 and AP&#160;0.949 (F1&#160;0.824, ACC&#160;0.835), while the best alternative tree model, <em class=\"ltx_emph ltx_font_italic\">Stats+PCA+GBDT</em>, attains AUROC&#160;0.893, AP&#160;0.925, F1&#160;0.844 and ACC&#160;0.846, and <em class=\"ltx_emph ltx_font_italic\">DL+TSMIXER</em> lags further behind (AUROC&#160;0.844, AP&#160;0.865). Permutation tests on AUROC differences find no significant gap between <em class=\"ltx_emph ltx_font_italic\">Stats+PCA+RF</em> and the next-best heads, but all alternatives show higher calibration error, so <em class=\"ltx_emph ltx_font_italic\">Stats+PCA+RF</em> remains the best compromise between discrimination and reliability under subject-safe splitting.</p>\n\n",
                "matched_terms": [
                    "acc",
                    "statspcagbdt",
                    "test",
                    "statspcarf",
                    "ours",
                    "dlinceptiontime",
                    "auroc",
                    "dltsmixer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The validation-tuned threshold <math alttext=\"\\tau{=}0.636\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>=</mo><mn>0.636</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{=}0.636</annotation></semantics></math> lies on a broad plateau for Accuracy/F1 (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13802v2#S4.F6\" title=\"Figure 6 &#8227; 4.2 Results &#8227; 4 Experiments and Results &#8227; Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>), indicating tolerance to mild distribution shift. At this point, the test confusion matrix shows few false positives and a clear separation band in the posterior histograms (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13802v2#S4.F5\" title=\"Figure 5 &#8227; 4.2 Results &#8227; 4 Experiments and Results &#8227; Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). Calibration is reasonable (ECE 0.268; Brier 0.168), so reported probabilities are interpretable for screening rather than only for ranking. Single-channel removals (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13802v2#S4.F7\" title=\"Figure 7 &#8227; 4.2 Results &#8227; 4 Experiments and Results &#8227; Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) identify <em class=\"ltx_emph ltx_font_italic\">gaze</em> variability and <em class=\"ltx_emph ltx_font_italic\">mouth&#8211;jaw</em> motion as the dominant contributors; ablating either causes the largest AUROC drops. <em class=\"ltx_emph ltx_font_italic\">Brow</em> asymmetry and <em class=\"ltx_emph ltx_font_italic\">head micro-jitter</em> provide secondary gains, suggesting that orofacial and oculomotor streams anchor performance in uncontrolled video. Common failures mirror these findings. False negatives concentrate in clips with (i) mouth occlusion or beard-induced tracking gaps, (ii) extreme head pose or framing that suppresses iris tracking, and (iii) compression artifacts that destabilize high-frequency micromovements. Simple capture guidance camera-facing framing, moderate lighting, and minimal mouth occlusion reduces these errors and preserves the same qualitative cues surfaced in the UI.</p>\n\n",
                "matched_terms": [
                    "validationtuned",
                    "performance",
                    "test",
                    "auroc"
                ]
            }
        ]
    }
}