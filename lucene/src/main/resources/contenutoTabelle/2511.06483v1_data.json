{
    "S4.T1": {
        "source_file": "SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS",
        "caption": "Table 1: Overall reasoning accuracy (%) across benchmarks using symbolic features.",
        "body": "Reasoner\nMMAU\nMMAR\nOmniBench\n\n\n\n\nQwen2.5-Omni\n64.6\n48.5\n30.7\n\n\nQwen3-Instruct\n67.2\n54.9\n35.8\n\n\nGemini 2.5 Pro\n73.5\n69.3\n38.7",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Reasoner</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MMAU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MMAR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">OmniBench</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2.5-Omni</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">64.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">48.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen3-Instruct</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">67.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">54.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">35.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">Gemini&#160;2.5&#160;Pro</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">73.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">69.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">38.7</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "symbolic",
            "overall",
            "reasoner",
            "benchmarks",
            "reasoning",
            "omnibench",
            "across",
            "features",
            "qwen3instruct",
            "gemini",
            "pro",
            "mmau",
            "qwen25omni",
            "accuracy",
            "mmar"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Overall Accuracy Across Datasets &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares three reasoning models on symbolic features across MMAU, MMAR, and OmniBench.\nGemini&#160;2.5&#160;Pro achieves the highest accuracy on all datasets, with a clear advantage on MMAR (69.3%).\nQwen3-Instruct improves stability over Qwen2.5-Omni but still lags behind Gemini.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Large language models (LLMs) have advanced in text and vision, but their reasoning on audio remains limited. Most existing methods rely on dense audio embeddings, which are difficult to interpret and often fail on structured reasoning tasks. Caption-based approaches, introduced in recent benchmarks such as MMAU, improve performance by translating audio into text, yet still depend on dense embeddings as input, offering little insight when models fail.</span>\n</p>\n\n",
                "matched_terms": [
                    "mmau",
                    "benchmarks",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We present SAR-LM, a symbolic audio reasoning pipeline that builds on this caption-based paradigm by converting audio into structured, human-readable features across speech, sound events, and music. These symbolic inputs support both reasoning and transparent error analysis, enabling us to trace failures to specific features. Across three benchmarks, MMAU, MMAR, and OmniBench, SAR-LM achieves competitive results, while prioritizing interpretability as its primary contribution.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "omnibench",
                    "benchmarks",
                    "reasoning",
                    "across",
                    "features",
                    "mmau",
                    "mmar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nAudio reasoning, Symbolic representations, Large language models, Interpretability, Audio question answering</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Reasoning about sound is essential for building AI systems that can understand the world. While humans easily infer who is speaking, what their intention is, or what caused a noise, large language models (LLMs) still lag behind in this ability. Most prior work has evaluated LLM reasoning on text or vision </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2023largereasoning</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yu2023sevila</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with audio reasoning only recently gaining traction. Current approaches typically rely on dense audio embeddings </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which achieve moderate accuracy but are difficult to interpret and provide little insight into why models fail.</span>\n</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we explore symbolic audio reasoning, where raw audio is translated into structured, human-readable features. We extract transcripts, speech emotion, sound events, musical notes, chords, and tags using pretrained and signal processing models. These features can be used directly or summarized into captions, and reasoning is then performed by an LLM. Because each symbolic layer is explicit, failure cases can be traced to specific components, making the pipeline more explainable than end-to-end captioning. We evaluate SAR-LM on three benchmarks: MMAU </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, MMAR </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025mmar</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and OmniBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">omnibench2024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with further details provided in Sections&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S2\" style=\"font-size:90%;\" title=\"2 Related Work &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.SS1\" style=\"font-size:90%;\" title=\"4.1 Setup &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">4.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nOur results show that symbolic reasoning enables detailed error analysis while achieving competitive accuracy, particularly in speech and environmental sound reasoning, with interpretability as the main contribution.\nTo the best of our knowledge, this is the first work to introduce symbolic audio reasoning with large language models. Our main contributions are:</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "omnibench",
                    "benchmarks",
                    "reasoning",
                    "features",
                    "mmau",
                    "accuracy",
                    "mmar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose SAR-LM, a modular pipeline that converts audio into symbolic, interpretable features for reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "reasoning",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate SAR-LM on three benchmarks (MMAU, MMAR, OmniBench), showing competitive performance while improving interpretability.</span>\n</p>\n\n",
                "matched_terms": [
                    "mmau",
                    "omnibench",
                    "benchmarks",
                    "mmar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We demonstrate detailed error analysis enabled by symbolic features, exposing model failures that are hidden in embedding-based systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Benchmarks such as MMAU </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, MMAR </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025mmar</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and OmniBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">omnibench2024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> define the task across speech, music, and environmental audio, often requiring temporal reasoning. Early methods include Audio-CoT </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025audiocot</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which applied chain-of-thought prompting </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wei2022chainofthought</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Audio-Reasoner </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">audio_reasoner2025</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which refined prompt structures with additional objectives.</span>\n</p>\n\n",
                "matched_terms": [
                    "omnibench",
                    "benchmarks",
                    "reasoning",
                    "across",
                    "mmau",
                    "mmar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Follow-up work explores alternatives: Acoustic Prompt Tuning </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">acoustic_prompt_tuning2025</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with learnable prompts, Omni-R1 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">omni_r1_2025</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> showing metadata-only inference, Joint Audio-Speech Co-Reasoning </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">joint_co_reasoning_2025</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Stepwise Audio Reasoning </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">stepwise_reasoning_2025</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These attempts highlight the need for interpretable, time-aligned inputs. Our work addresses this by replacing dense vectors with symbolic features and structured prompts, enabling clearer and more transparent reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "reasoning",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Dense audio embeddings are effective for captioning and reasoning but remain opaque and hard to align with language models. Discrete features such as transcripts, tags, or note sequences offer a more interpretable alternative.</span>\n</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This gap motivates our approach: using symbolic features directly or summarizing them into captions to support interpretability in audio reasoning, in contrast to the predominantly embedding-driven prior art.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "reasoning",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our goal is to help large language models (LLMs) understand and reason about audio. To achieve this, we design a modular pipeline that converts raw audio into structured, interpretable prompts for a language model. The pipeline consists of four main stages: symbolic feature extraction, prompt construction, LLM-based reasoning, and answer prediction, as shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2.2 Audio captioning &#8227; 2 Related Work &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given an input audio clip </span>\n  <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">x</mi>\n      <annotation encoding=\"application/x-tex\">x</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we extract symbolic features using pretrained and signal processing models:</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We support multiple prompt styles, including a flat format where all symbolic features are listed, and a caption-based format where the symbolic features are first summarized into a caption by the LLM before reasoning. The predicted answer is then evaluated against the ground-truth label from the benchmark.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "reasoning",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The pipeline is fully modular and text-based: feature extractors, prompt generators, and reasoning models can be swapped independently without retraining. This design emphasizes interpretability, as symbolic features allow tracing successes and failures to specific components. Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2.2 Audio captioning &#8227; 2 Related Work &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> illustrates the full SAR-LM architecture.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "reasoning",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Instead of relying on dense embeddings, which are hard for LLMs to interpret, we convert each audio clip into symbolic, time-aligned features represented in text. These cover multiple semantic layers, sound events, speech, emotion, and music, with content-aware filtering to include only relevant features.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">All symbolic features are encoded as plain text and aligned to the audio timeline. They can be passed directly to the LLM or summarized into captions, enabling transparent reasoning and fine-grained error analysis.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "reasoning",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Once symbolic features are extracted, we format them into a natural language prompt for the LLM. The goal is to present the audio content in a structured, interpretable way that supports reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "reasoning",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To make symbolic features more human-readable and easier for LLMs to reason over, we also generate natural language captions. Symbolic features are reformatted into structured text and passed to a captioning model, which produces a fluent paragraph summarizing the audio scene. These captions provide an alternative abstraction level for prompting, complementing direct symbolic inputs and supporting interpretability in reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "reasoning",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We use large language models (LLMs) to answer multiple-choice audio questions based on text-only inputs. Prompts consist of either flat symbolic features or symbolic captions, followed by the benchmark question and options.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluated several large language models for both caption generation and reasoning. Qwen3-32B&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen3technicalreport</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> often produced unstable outputs, sometimes overthinking even simple questions. Qwen3-30B-A3B-Instruct-2507&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen3technicalreport</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> gave more reliable predictions, while Qwen2.5-Omni-7B&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2.5-Omni</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> was tested as both captioner and reasoner but frequently hallucinated when generating captions from symbolic features. Based on these comparisons, we adopt Gemini&#160;2.5&#160;Pro&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25pro</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as our final choice for caption generation and reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "reasoner",
                    "gemini",
                    "reasoning",
                    "features",
                    "pro",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate on three benchmarks. MMAU&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a large-scale benchmark of 10k audio clips and 27 task types spanning speech, music, and environmental sounds, designed to test advanced perception and reasoning. We use the released mini-test set (1k samples) since its ground-truth labels are public. MMAR&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025mmar</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides 1k high-quality audio&#8211;QA pairs with multi-step chain-of-thought annotations, emphasizing deep reasoning across speech, music, and mixed audio. OmniBench&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">omnibench2024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> extends evaluation to tri-modal settings, requiring reasoning over acoustic, visual, and textual inputs. Each dataset provides audio clips paired with multiple-choice questions, and we report reasoning accuracy overall and by category (speech, music, sound).</span>\n</p>\n\n",
                "matched_terms": [
                    "overall",
                    "omnibench",
                    "benchmarks",
                    "reasoning",
                    "across",
                    "mmau",
                    "accuracy",
                    "mmar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The symbolic pipeline involves many potential features, such as transcripts, sound events, chords, and music tags. Some of these are highly informative, while others may add noise depending on the clip. Manually testing the contribution of each feature is impractical, as the search space grows quickly. To address this, we introduced a GPT-style agent that selects relevant features per sample. Qwen2.5-Omni proved unstable in this role, often hallucinating selections, whereas Gemini&#160;2.5&#160;Pro provided consistent and meaningful choices. This dynamic selection helped reduce irrelevant inputs and improved downstream reasoning, as we demonstrate in the MMAU results.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "gemini",
                    "reasoning",
                    "features",
                    "pro",
                    "qwen25omni",
                    "mmau"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.4 Detailed Results on MMAU &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports task-wise and overall accuracy for different input formats.\nCaptions are generated by Gemini&#160;2.5&#160;Pro, and the GPT-style agent for dynamic feature selection also uses Gemini&#160;2.5&#160;Pro.\nReasoning is performed with Qwen3-30B-A3B-Instruct-2507 for controlled symbolic comparisons.\nWe include both fixed symbolic inputs and dynamic agent-based selection.\nFlat symbolic features achieve strong performance in speech and sound reasoning, while symbolic captions perform slightly better on music.\nAgent-based selection provides consistent gains over non-agent variants, confirming that filtering out irrelevant features helps reduce noise.\nEnd-to-end captions are competitive for music tasks but remain less interpretable.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "overall",
                    "gemini",
                    "reasoning",
                    "features",
                    "pro",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We also evaluated Gemini&#160;2.5&#160;Pro directly as a reasoning model (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.4 Detailed Results on MMAU &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nWith raw audio as input, Gemini reaches the highest overall accuracy (78.6%), showing the advantage of end-to-end access to the waveform.\nWhen reasoning over symbolic features, it still achieves 73.5%, which is close in accuracy while providing far greater interpretability.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "overall",
                    "gemini",
                    "reasoning",
                    "features",
                    "pro",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We compare our approach against reported baselines on the MMAU, MMAR, and OmniBench benchmarks.\nIn all cases, we report our best-performing configuration: Gemini&#160;2.5&#160;Pro as reasoner with symbolic features as input.\nResults show consistent improvements over prior systems, particularly in speech reasoning and long-context MMAR tasks, while performance on OmniBench remains weaker, reflecting the added difficulty of tri-modal reasoning and highlighting an area for future improvement.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "reasoner",
                    "omnibench",
                    "benchmarks",
                    "reasoning",
                    "gemini",
                    "features",
                    "pro",
                    "mmar",
                    "mmau"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We illustrate a representative failure case in temporal reasoning.\n</span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Question:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> What was the order of the sounds in the sequence?\n</span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Correct Answer:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> light_switch_clicking </span>\n  <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS6.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\rightarrow</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> boiling_water </span>\n  <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS6.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\rightarrow</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> doorbell_ringing </span>\n  <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS6.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\rightarrow</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> clock_ticking.\nBoth flat symbolic features and symbolic captions predicted the wrong order, while the end-to-end captioner was correct.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "reasoning",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Closer inspection showed that PANNs failed to detect the early sounds (light switch, boiling water), leaving only later cues in the symbolic prompt.\nThis missing context explains the error and illustrates a broader limitation: symbolic reasoning pipelines are only as reliable as their feature extraction.\nUnlike symbolic inputs, end-to-end captioners had full access to the waveform and recovered the correct sequence.\nSuch interpretability is valuable, as it exposes where improvements are needed, in this case, more robust detection of short or low-energy sounds.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We presented SAR-LM, a modular pipeline for symbolic audio reasoning with large language models.\nOur experiments across MMAU, MMAR, and OmniBench showed that symbolic inputs enable competitive reasoning performance while retaining interpretability that helps diagnose errors such as missed temporal events.\nResults highlight the potential of combining symbolic structure with strong reasoning models to outperform existing baselines, particularly in speech and long-context reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "omnibench",
                    "reasoning",
                    "across",
                    "mmau",
                    "mmar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">A limitation of this approach is that running multiple specialized models for feature extraction can be computationally expensive compared to end-to-end pipelines.\nIn addition, performance is sensitive to the quality of symbolic features, as errors in speech recognition or music transcription can propagate to the reasoning stage.\nThese trade-offs underline the need to balance efficiency with interpretability.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "reasoning",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Future work will focus on strengthening the feature extraction stage, which remains the main bottleneck.\nOne direction is to incorporate a universal sound recognition backend to reduce missed events from domain-specific models (e.g., PANNs).\nAnother is to integrate stronger pretrained audio encoders such as MERT, fine-tuned per dataset, to capture richer and more generalisable features.\nUltimately, unifying feature extraction across all audio types could yield both interpretability and improved accuracy, pushing symbolic reasoning systems beyond current end-to-end baselines.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "reasoning",
                    "across",
                    "features",
                    "accuracy"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS",
        "caption": "Table 2: Task-wise and overall accuracy (%) on MMAU with Qwen3-Instruct as reasoner. Captions and agent-based selection use Gemini 2.5 Pro.",
        "body": "Input Type\nSound\nMusic\nSpeech\nOverall\n\n\n\n\nSymbolic Features\n69.37\n56.59\n73.87\n66.6\n\n\nSymbolic Features (agent)\n72.67\n57.78\n73.87\n68.1\n\n\nSymbolic Captions\n69.67\n58.38\n71.77\n66.6\n\n\nSymbolic Captions (agent)\n70.57\n58.08\n70.27\n66.3\n\n\nEnd-to-End Captions\n68.17\n62.28\n69.97\n66.8",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Input Type</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sound</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Music</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Overall</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Symbolic Features</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">69.37</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.59</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">73.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">66.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Symbolic Features (agent)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">72.67</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.78</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">73.87</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">68.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Symbolic Captions</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">69.67</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">58.38</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.77</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">66.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Symbolic Captions (agent)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.57</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">58.08</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.27</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">66.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">End-to-End Captions</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">68.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">62.28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">69.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">66.8</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "overall",
            "reasoner",
            "type",
            "agent",
            "features",
            "input",
            "agentbased",
            "captions",
            "mmau",
            "gemini",
            "speech",
            "symbolic",
            "qwen3instruct",
            "endtoend",
            "accuracy",
            "taskwise",
            "sound",
            "music",
            "selection",
            "use",
            "pro"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.4 Detailed Results on MMAU &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports task-wise and overall accuracy for different input formats.\nCaptions are generated by Gemini&#160;2.5&#160;Pro, and the GPT-style agent for dynamic feature selection also uses Gemini&#160;2.5&#160;Pro.\nReasoning is performed with Qwen3-30B-A3B-Instruct-2507 for controlled symbolic comparisons.\nWe include both fixed symbolic inputs and dynamic agent-based selection.\nFlat symbolic features achieve strong performance in speech and sound reasoning, while symbolic captions perform slightly better on music.\nAgent-based selection provides consistent gains over non-agent variants, confirming that filtering out irrelevant features helps reduce noise.\nEnd-to-end captions are competitive for music tasks but remain less interpretable.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Large language models (LLMs) have advanced in text and vision, but their reasoning on audio remains limited. Most existing methods rely on dense audio embeddings, which are difficult to interpret and often fail on structured reasoning tasks. Caption-based approaches, introduced in recent benchmarks such as MMAU, improve performance by translating audio into text, yet still depend on dense embeddings as input, offering little insight when models fail.</span>\n</p>\n\n",
                "matched_terms": [
                    "mmau",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We present SAR-LM, a symbolic audio reasoning pipeline that builds on this caption-based paradigm by converting audio into structured, human-readable features across speech, sound events, and music. These symbolic inputs support both reasoning and transparent error analysis, enabling us to trace failures to specific features. Across three benchmarks, MMAU, MMAR, and OmniBench, SAR-LM achieves competitive results, while prioritizing interpretability as its primary contribution.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "features",
                    "sound",
                    "music",
                    "mmau"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Reasoning about sound is essential for building AI systems that can understand the world. While humans easily infer who is speaking, what their intention is, or what caused a noise, large language models (LLMs) still lag behind in this ability. Most prior work has evaluated LLM reasoning on text or vision </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2023largereasoning</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yu2023sevila</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with audio reasoning only recently gaining traction. Current approaches typically rely on dense audio embeddings </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which achieve moderate accuracy but are difficult to interpret and provide little insight into why models fail.</span>\n</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we explore symbolic audio reasoning, where raw audio is translated into structured, human-readable features. We extract transcripts, speech emotion, sound events, musical notes, chords, and tags using pretrained and signal processing models. These features can be used directly or summarized into captions, and reasoning is then performed by an LLM. Because each symbolic layer is explicit, failure cases can be traced to specific components, making the pipeline more explainable than end-to-end captioning. We evaluate SAR-LM on three benchmarks: MMAU </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, MMAR </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025mmar</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and OmniBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">omnibench2024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with further details provided in Sections&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S2\" style=\"font-size:90%;\" title=\"2 Related Work &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.SS1\" style=\"font-size:90%;\" title=\"4.1 Setup &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">4.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nOur results show that symbolic reasoning enables detailed error analysis while achieving competitive accuracy, particularly in speech and environmental sound reasoning, with interpretability as the main contribution.\nTo the best of our knowledge, this is the first work to introduce symbolic audio reasoning with large language models. Our main contributions are:</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "endtoend",
                    "features",
                    "sound",
                    "captions",
                    "mmau",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose SAR-LM, a modular pipeline that converts audio into symbolic, interpretable features for reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We demonstrate detailed error analysis enabled by symbolic features, exposing model failures that are hidden in embedding-based systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Benchmarks such as MMAU </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, MMAR </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025mmar</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and OmniBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">omnibench2024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> define the task across speech, music, and environmental audio, often requiring temporal reasoning. Early methods include Audio-CoT </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025audiocot</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which applied chain-of-thought prompting </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wei2022chainofthought</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Audio-Reasoner </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">audio_reasoner2025</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which refined prompt structures with additional objectives.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mmau",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Follow-up work explores alternatives: Acoustic Prompt Tuning </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">acoustic_prompt_tuning2025</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with learnable prompts, Omni-R1 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">omni_r1_2025</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> showing metadata-only inference, Joint Audio-Speech Co-Reasoning </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">joint_co_reasoning_2025</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Stepwise Audio Reasoning </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">stepwise_reasoning_2025</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These attempts highlight the need for interpretable, time-aligned inputs. Our work addresses this by replacing dense vectors with symbolic features and structured prompts, enabling clearer and more transparent reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Audio captioning has been widely studied as a cross-modal task. Datasets like Clotho </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">clotho_2020</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and AudioCaps </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">audiocaps_2019</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> enable evaluation, and end-to-end captioning models </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mei2021audio</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023actual</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tang2024extending</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> achieve strong accuracy but rely heavily on dense embeddings. Modular and retrieval-based systems such as EnCLAP </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2024enclap</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, DRCap </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2025drcap</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and SLAM-AAC </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2025slam</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> increase flexibility, yet still depend on opaque feature spaces. Few works attempt to use such interpretable inputs, and even fewer explore structured prompting.</span>\n</p>\n\n",
                "matched_terms": [
                    "endtoend",
                    "use",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This gap motivates our approach: using symbolic features directly or summarizing them into captions to support interpretability in audio reasoning, in contrast to the predominantly embedding-driven prior art.</span>\n</p>\n\n",
                "matched_terms": [
                    "captions",
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given an input audio clip </span>\n  <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">x</mi>\n      <annotation encoding=\"application/x-tex\">x</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we extract symbolic features using pretrained and signal processing models:</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "input",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We support multiple prompt styles, including a flat format where all symbolic features are listed, and a caption-based format where the symbolic features are first summarized into a caption by the LLM before reasoning. The predicted answer is then evaluated against the ground-truth label from the benchmark.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The pipeline is fully modular and text-based: feature extractors, prompt generators, and reasoning models can be swapped independently without retraining. This design emphasizes interpretability, as symbolic features allow tracing successes and failures to specific components. Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2.2 Audio captioning &#8227; 2 Related Work &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> illustrates the full SAR-LM architecture.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Instead of relying on dense embeddings, which are hard for LLMs to interpret, we convert each audio clip into symbolic, time-aligned features represented in text. These cover multiple semantic layers, sound events, speech, emotion, and music, with content-aware filtering to include only relevant features.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "features",
                    "sound",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sound events.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We use PANNs </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">panns_2020</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to generate timestamped tags (e.g., footsteps, laughter, music), which both summarize the clip and guide which other modules are applied.</span>\n</p>\n\n",
                "matched_terms": [
                    "use",
                    "sound",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Whisper-large </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper_2023</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides full transcripts, while DAWN Transformer </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dawn_transformer_2023</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> predicts valence&#8211;arousal&#8211;dominance values that we discretize into symbolic emotion labels.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Music.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> MT3 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mt3_2021</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> outputs symbolic note sequences with pitch, timing, and instrument information. Chordino </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mauch2009chordino</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">holloway_chord_extractor_2021</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> extracts chord progressions, and Musicnn</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/jordipons/musicnn\" title=\"\">https://github.com/jordipons/musicnn</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides high-level tags describing genre or timbre.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">All symbolic features are encoded as plain text and aligned to the audio timeline. They can be passed directly to the LLM or summarized into captions, enabling transparent reasoning and fine-grained error analysis.</span>\n</p>\n\n",
                "matched_terms": [
                    "captions",
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Once symbolic features are extracted, we format them into a natural language prompt for the LLM. The goal is to present the audio content in a structured, interpretable way that supports reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We use PANNs as a reference to decide which features to include: if music is detected, we add notes, chords, and tags; if speech is detected, we add transcripts and emotion labels. This content-aware selection keeps prompts concise by including only relevant features.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "features",
                    "music",
                    "selection",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To make symbolic features more human-readable and easier for LLMs to reason over, we also generate natural language captions. Symbolic features are reformatted into structured text and passed to a captioning model, which produces a fluent paragraph summarizing the audio scene. These captions provide an alternative abstraction level for prompting, complementing direct symbolic inputs and supporting interpretability in reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "captions",
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We use large language models (LLMs) to answer multiple-choice audio questions based on text-only inputs. Prompts consist of either flat symbolic features or symbolic captions, followed by the benchmark question and options.</span>\n</p>\n\n",
                "matched_terms": [
                    "captions",
                    "symbolic",
                    "use",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluated several large language models for both caption generation and reasoning. Qwen3-32B&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen3technicalreport</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> often produced unstable outputs, sometimes overthinking even simple questions. Qwen3-30B-A3B-Instruct-2507&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen3technicalreport</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> gave more reliable predictions, while Qwen2.5-Omni-7B&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2.5-Omni</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> was tested as both captioner and reasoner but frequently hallucinated when generating captions from symbolic features. Based on these comparisons, we adopt Gemini&#160;2.5&#160;Pro&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25pro</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as our final choice for caption generation and reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "reasoner",
                    "symbolic",
                    "features",
                    "captions",
                    "pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate on three benchmarks. MMAU&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a large-scale benchmark of 10k audio clips and 27 task types spanning speech, music, and environmental sounds, designed to test advanced perception and reasoning. We use the released mini-test set (1k samples) since its ground-truth labels are public. MMAR&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025mmar</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides 1k high-quality audio&#8211;QA pairs with multi-step chain-of-thought annotations, emphasizing deep reasoning across speech, music, and mixed audio. OmniBench&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">omnibench2024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> extends evaluation to tri-modal settings, requiring reasoning over acoustic, visual, and textual inputs. Each dataset provides audio clips paired with multiple-choice questions, and we report reasoning accuracy overall and by category (speech, music, sound).</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "overall",
                    "sound",
                    "music",
                    "use",
                    "mmau",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Overall Accuracy Across Datasets &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares three reasoning models on symbolic features across MMAU, MMAR, and OmniBench.\nGemini&#160;2.5&#160;Pro achieves the highest accuracy on all datasets, with a clear advantage on MMAR (69.3%).\nQwen3-Instruct improves stability over Qwen2.5-Omni but still lags behind Gemini.</span>\n</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "symbolic",
                    "qwen3instruct",
                    "features",
                    "pro",
                    "mmau",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The symbolic pipeline involves many potential features, such as transcripts, sound events, chords, and music tags. Some of these are highly informative, while others may add noise depending on the clip. Manually testing the contribution of each feature is impractical, as the search space grows quickly. To address this, we introduced a GPT-style agent that selects relevant features per sample. Qwen2.5-Omni proved unstable in this role, often hallucinating selections, whereas Gemini&#160;2.5&#160;Pro provided consistent and meaningful choices. This dynamic selection helped reduce irrelevant inputs and improved downstream reasoning, as we demonstrate in the MMAU results.</span>\n</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "symbolic",
                    "agent",
                    "features",
                    "pro",
                    "sound",
                    "music",
                    "selection",
                    "mmau"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We also evaluated Gemini&#160;2.5&#160;Pro directly as a reasoning model (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.4 Detailed Results on MMAU &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nWith raw audio as input, Gemini reaches the highest overall accuracy (78.6%), showing the advantage of end-to-end access to the waveform.\nWhen reasoning over symbolic features, it still achieves 73.5%, which is close in accuracy while providing far greater interpretability.</span>\n</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "overall",
                    "symbolic",
                    "endtoend",
                    "features",
                    "input",
                    "pro",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We compare our approach against reported baselines on the MMAU, MMAR, and OmniBench benchmarks.\nIn all cases, we report our best-performing configuration: Gemini&#160;2.5&#160;Pro as reasoner with symbolic features as input.\nResults show consistent improvements over prior systems, particularly in speech reasoning and long-context MMAR tasks, while performance on OmniBench remains weaker, reflecting the added difficulty of tri-modal reasoning and highlighting an area for future improvement.</span>\n</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "reasoner",
                    "speech",
                    "symbolic",
                    "features",
                    "pro",
                    "input",
                    "mmau"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We illustrate a representative failure case in temporal reasoning.\n</span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Question:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> What was the order of the sounds in the sequence?\n</span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Correct Answer:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> light_switch_clicking </span>\n  <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS6.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\rightarrow</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> boiling_water </span>\n  <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS6.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\rightarrow</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> doorbell_ringing </span>\n  <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS6.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\rightarrow</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> clock_ticking.\nBoth flat symbolic features and symbolic captions predicted the wrong order, while the end-to-end captioner was correct.</span>\n</p>\n\n",
                "matched_terms": [
                    "captions",
                    "symbolic",
                    "endtoend",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Closer inspection showed that PANNs failed to detect the early sounds (light switch, boiling water), leaving only later cues in the symbolic prompt.\nThis missing context explains the error and illustrates a broader limitation: symbolic reasoning pipelines are only as reliable as their feature extraction.\nUnlike symbolic inputs, end-to-end captioners had full access to the waveform and recovered the correct sequence.\nSuch interpretability is valuable, as it exposes where improvements are needed, in this case, more robust detection of short or low-energy sounds.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We presented SAR-LM, a modular pipeline for symbolic audio reasoning with large language models.\nOur experiments across MMAU, MMAR, and OmniBench showed that symbolic inputs enable competitive reasoning performance while retaining interpretability that helps diagnose errors such as missed temporal events.\nResults highlight the potential of combining symbolic structure with strong reasoning models to outperform existing baselines, particularly in speech and long-context reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mmau",
                    "symbolic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">A limitation of this approach is that running multiple specialized models for feature extraction can be computationally expensive compared to end-to-end pipelines.\nIn addition, performance is sensitive to the quality of symbolic features, as errors in speech recognition or music transcription can propagate to the reasoning stage.\nThese trade-offs underline the need to balance efficiency with interpretability.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "endtoend",
                    "features",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Future work will focus on strengthening the feature extraction stage, which remains the main bottleneck.\nOne direction is to incorporate a universal sound recognition backend to reduce missed events from domain-specific models (e.g., PANNs).\nAnother is to integrate stronger pretrained audio encoders such as MERT, fine-tuned per dataset, to capture richer and more generalisable features.\nUltimately, unifying feature extraction across all audio types could yield both interpretability and improved accuracy, pushing symbolic reasoning systems beyond current end-to-end baselines.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "endtoend",
                    "features",
                    "sound",
                    "accuracy"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS",
        "caption": "Table 3: Task-wise and overall accuracy (%) on MMAU with Gemini 2.5 Pro as reasoner.",
        "body": "Input Type\nSound\nMusic\nSpeech\nOverall\n\n\n\n\nAudio-only\n80.18\n72.46\n83.18\n78.6\n\n\nSymbolic Features\n73.27\n64.97\n82.28\n73.5",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Input Type</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sound</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Music</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Overall</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Audio-only</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">80.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">72.46</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">83.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">78.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">Symbolic Features</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">73.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">64.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">82.28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">73.5</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "gemini",
            "overall",
            "reasoner",
            "type",
            "speech",
            "symbolic",
            "audioonly",
            "features",
            "pro",
            "input",
            "sound",
            "music",
            "mmau",
            "accuracy",
            "taskwise"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We also evaluated Gemini&#160;2.5&#160;Pro directly as a reasoning model (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.4 Detailed Results on MMAU &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nWith raw audio as input, Gemini reaches the highest overall accuracy (78.6%), showing the advantage of end-to-end access to the waveform.\nWhen reasoning over symbolic features, it still achieves 73.5%, which is close in accuracy while providing far greater interpretability.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Large language models (LLMs) have advanced in text and vision, but their reasoning on audio remains limited. Most existing methods rely on dense audio embeddings, which are difficult to interpret and often fail on structured reasoning tasks. Caption-based approaches, introduced in recent benchmarks such as MMAU, improve performance by translating audio into text, yet still depend on dense embeddings as input, offering little insight when models fail.</span>\n</p>\n\n",
                "matched_terms": [
                    "mmau",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We present SAR-LM, a symbolic audio reasoning pipeline that builds on this caption-based paradigm by converting audio into structured, human-readable features across speech, sound events, and music. These symbolic inputs support both reasoning and transparent error analysis, enabling us to trace failures to specific features. Across three benchmarks, MMAU, MMAR, and OmniBench, SAR-LM achieves competitive results, while prioritizing interpretability as its primary contribution.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "features",
                    "sound",
                    "music",
                    "mmau"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Reasoning about sound is essential for building AI systems that can understand the world. While humans easily infer who is speaking, what their intention is, or what caused a noise, large language models (LLMs) still lag behind in this ability. Most prior work has evaluated LLM reasoning on text or vision </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2023largereasoning</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yu2023sevila</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with audio reasoning only recently gaining traction. Current approaches typically rely on dense audio embeddings </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which achieve moderate accuracy but are difficult to interpret and provide little insight into why models fail.</span>\n</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we explore symbolic audio reasoning, where raw audio is translated into structured, human-readable features. We extract transcripts, speech emotion, sound events, musical notes, chords, and tags using pretrained and signal processing models. These features can be used directly or summarized into captions, and reasoning is then performed by an LLM. Because each symbolic layer is explicit, failure cases can be traced to specific components, making the pipeline more explainable than end-to-end captioning. We evaluate SAR-LM on three benchmarks: MMAU </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, MMAR </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025mmar</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and OmniBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">omnibench2024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with further details provided in Sections&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S2\" style=\"font-size:90%;\" title=\"2 Related Work &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.SS1\" style=\"font-size:90%;\" title=\"4.1 Setup &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">4.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nOur results show that symbolic reasoning enables detailed error analysis while achieving competitive accuracy, particularly in speech and environmental sound reasoning, with interpretability as the main contribution.\nTo the best of our knowledge, this is the first work to introduce symbolic audio reasoning with large language models. Our main contributions are:</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "features",
                    "sound",
                    "mmau",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose SAR-LM, a modular pipeline that converts audio into symbolic, interpretable features for reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We demonstrate detailed error analysis enabled by symbolic features, exposing model failures that are hidden in embedding-based systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Benchmarks such as MMAU </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, MMAR </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025mmar</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and OmniBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">omnibench2024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> define the task across speech, music, and environmental audio, often requiring temporal reasoning. Early methods include Audio-CoT </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025audiocot</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which applied chain-of-thought prompting </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wei2022chainofthought</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Audio-Reasoner </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">audio_reasoner2025</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which refined prompt structures with additional objectives.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mmau",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Follow-up work explores alternatives: Acoustic Prompt Tuning </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">acoustic_prompt_tuning2025</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with learnable prompts, Omni-R1 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">omni_r1_2025</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> showing metadata-only inference, Joint Audio-Speech Co-Reasoning </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">joint_co_reasoning_2025</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Stepwise Audio Reasoning </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">stepwise_reasoning_2025</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These attempts highlight the need for interpretable, time-aligned inputs. Our work addresses this by replacing dense vectors with symbolic features and structured prompts, enabling clearer and more transparent reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This gap motivates our approach: using symbolic features directly or summarizing them into captions to support interpretability in audio reasoning, in contrast to the predominantly embedding-driven prior art.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given an input audio clip </span>\n  <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">x</mi>\n      <annotation encoding=\"application/x-tex\">x</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we extract symbolic features using pretrained and signal processing models:</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "input",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We support multiple prompt styles, including a flat format where all symbolic features are listed, and a caption-based format where the symbolic features are first summarized into a caption by the LLM before reasoning. The predicted answer is then evaluated against the ground-truth label from the benchmark.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The pipeline is fully modular and text-based: feature extractors, prompt generators, and reasoning models can be swapped independently without retraining. This design emphasizes interpretability, as symbolic features allow tracing successes and failures to specific components. Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2.2 Audio captioning &#8227; 2 Related Work &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> illustrates the full SAR-LM architecture.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Instead of relying on dense embeddings, which are hard for LLMs to interpret, we convert each audio clip into symbolic, time-aligned features represented in text. These cover multiple semantic layers, sound events, speech, emotion, and music, with content-aware filtering to include only relevant features.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "features",
                    "sound",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sound events.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We use PANNs </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">panns_2020</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to generate timestamped tags (e.g., footsteps, laughter, music), which both summarize the clip and guide which other modules are applied.</span>\n</p>\n\n",
                "matched_terms": [
                    "sound",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Whisper-large </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper_2023</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides full transcripts, while DAWN Transformer </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dawn_transformer_2023</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> predicts valence&#8211;arousal&#8211;dominance values that we discretize into symbolic emotion labels.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Music.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> MT3 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mt3_2021</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> outputs symbolic note sequences with pitch, timing, and instrument information. Chordino </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mauch2009chordino</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">holloway_chord_extractor_2021</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> extracts chord progressions, and Musicnn</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/jordipons/musicnn\" title=\"\">https://github.com/jordipons/musicnn</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides high-level tags describing genre or timbre.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">All symbolic features are encoded as plain text and aligned to the audio timeline. They can be passed directly to the LLM or summarized into captions, enabling transparent reasoning and fine-grained error analysis.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Once symbolic features are extracted, we format them into a natural language prompt for the LLM. The goal is to present the audio content in a structured, interpretable way that supports reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We use PANNs as a reference to decide which features to include: if music is detected, we add notes, chords, and tags; if speech is detected, we add transcripts and emotion labels. This content-aware selection keeps prompts concise by including only relevant features.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "music",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To make symbolic features more human-readable and easier for LLMs to reason over, we also generate natural language captions. Symbolic features are reformatted into structured text and passed to a captioning model, which produces a fluent paragraph summarizing the audio scene. These captions provide an alternative abstraction level for prompting, complementing direct symbolic inputs and supporting interpretability in reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We use large language models (LLMs) to answer multiple-choice audio questions based on text-only inputs. Prompts consist of either flat symbolic features or symbolic captions, followed by the benchmark question and options.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluated several large language models for both caption generation and reasoning. Qwen3-32B&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen3technicalreport</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> often produced unstable outputs, sometimes overthinking even simple questions. Qwen3-30B-A3B-Instruct-2507&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen3technicalreport</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> gave more reliable predictions, while Qwen2.5-Omni-7B&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2.5-Omni</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> was tested as both captioner and reasoner but frequently hallucinated when generating captions from symbolic features. Based on these comparisons, we adopt Gemini&#160;2.5&#160;Pro&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25pro</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as our final choice for caption generation and reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "reasoner",
                    "symbolic",
                    "features",
                    "pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate on three benchmarks. MMAU&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a large-scale benchmark of 10k audio clips and 27 task types spanning speech, music, and environmental sounds, designed to test advanced perception and reasoning. We use the released mini-test set (1k samples) since its ground-truth labels are public. MMAR&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025mmar</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides 1k high-quality audio&#8211;QA pairs with multi-step chain-of-thought annotations, emphasizing deep reasoning across speech, music, and mixed audio. OmniBench&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">omnibench2024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> extends evaluation to tri-modal settings, requiring reasoning over acoustic, visual, and textual inputs. Each dataset provides audio clips paired with multiple-choice questions, and we report reasoning accuracy overall and by category (speech, music, sound).</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "overall",
                    "sound",
                    "music",
                    "mmau",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Overall Accuracy Across Datasets &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares three reasoning models on symbolic features across MMAU, MMAR, and OmniBench.\nGemini&#160;2.5&#160;Pro achieves the highest accuracy on all datasets, with a clear advantage on MMAR (69.3%).\nQwen3-Instruct improves stability over Qwen2.5-Omni but still lags behind Gemini.</span>\n</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "symbolic",
                    "features",
                    "pro",
                    "accuracy",
                    "mmau"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The symbolic pipeline involves many potential features, such as transcripts, sound events, chords, and music tags. Some of these are highly informative, while others may add noise depending on the clip. Manually testing the contribution of each feature is impractical, as the search space grows quickly. To address this, we introduced a GPT-style agent that selects relevant features per sample. Qwen2.5-Omni proved unstable in this role, often hallucinating selections, whereas Gemini&#160;2.5&#160;Pro provided consistent and meaningful choices. This dynamic selection helped reduce irrelevant inputs and improved downstream reasoning, as we demonstrate in the MMAU results.</span>\n</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "symbolic",
                    "features",
                    "sound",
                    "music",
                    "pro",
                    "mmau"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.4 Detailed Results on MMAU &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports task-wise and overall accuracy for different input formats.\nCaptions are generated by Gemini&#160;2.5&#160;Pro, and the GPT-style agent for dynamic feature selection also uses Gemini&#160;2.5&#160;Pro.\nReasoning is performed with Qwen3-30B-A3B-Instruct-2507 for controlled symbolic comparisons.\nWe include both fixed symbolic inputs and dynamic agent-based selection.\nFlat symbolic features achieve strong performance in speech and sound reasoning, while symbolic captions perform slightly better on music.\nAgent-based selection provides consistent gains over non-agent variants, confirming that filtering out irrelevant features helps reduce noise.\nEnd-to-end captions are competitive for music tasks but remain less interpretable.</span>\n</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "overall",
                    "speech",
                    "symbolic",
                    "features",
                    "input",
                    "sound",
                    "music",
                    "pro",
                    "accuracy",
                    "taskwise"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We compare our approach against reported baselines on the MMAU, MMAR, and OmniBench benchmarks.\nIn all cases, we report our best-performing configuration: Gemini&#160;2.5&#160;Pro as reasoner with symbolic features as input.\nResults show consistent improvements over prior systems, particularly in speech reasoning and long-context MMAR tasks, while performance on OmniBench remains weaker, reflecting the added difficulty of tri-modal reasoning and highlighting an area for future improvement.</span>\n</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "reasoner",
                    "speech",
                    "symbolic",
                    "features",
                    "input",
                    "pro",
                    "mmau"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We illustrate a representative failure case in temporal reasoning.\n</span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Question:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> What was the order of the sounds in the sequence?\n</span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Correct Answer:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> light_switch_clicking </span>\n  <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS6.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\rightarrow</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> boiling_water </span>\n  <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS6.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\rightarrow</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> doorbell_ringing </span>\n  <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS6.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\rightarrow</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> clock_ticking.\nBoth flat symbolic features and symbolic captions predicted the wrong order, while the end-to-end captioner was correct.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We presented SAR-LM, a modular pipeline for symbolic audio reasoning with large language models.\nOur experiments across MMAU, MMAR, and OmniBench showed that symbolic inputs enable competitive reasoning performance while retaining interpretability that helps diagnose errors such as missed temporal events.\nResults highlight the potential of combining symbolic structure with strong reasoning models to outperform existing baselines, particularly in speech and long-context reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mmau",
                    "symbolic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">A limitation of this approach is that running multiple specialized models for feature extraction can be computationally expensive compared to end-to-end pipelines.\nIn addition, performance is sensitive to the quality of symbolic features, as errors in speech recognition or music transcription can propagate to the reasoning stage.\nThese trade-offs underline the need to balance efficiency with interpretability.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "music",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Future work will focus on strengthening the feature extraction stage, which remains the main bottleneck.\nOne direction is to incorporate a universal sound recognition backend to reduce missed events from domain-specific models (e.g., PANNs).\nAnother is to integrate stronger pretrained audio encoders such as MERT, fine-tuned per dataset, to capture richer and more generalisable features.\nUltimately, unifying feature extraction across all audio types could yield both interpretability and improved accuracy, pushing symbolic reasoning systems beyond current end-to-end baselines.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "accuracy",
                    "sound",
                    "features"
                ]
            }
        ]
    },
    "S4.T4": {
        "source_file": "SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS",
        "caption": "Table 4: Comparison with baseline methods on MMAU (task-wise accuracy, %).",
        "body": "Method\nSound\nMusic\nSpeech\nOverall\n\n\n\n\nMMAU (Best)\n57.35\n49.70\n64.86\n57.30\n\n\nAudio-CoT\n62.16\n55.99\n56.16\n58.10\n\n\nAudio-Reasoner\n60.06\n64.30\n60.70\n61.71\n\n\nOurs (Gemini + Symbolic)\n73.27\n64.97\n82.28\n73.5",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sound</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Music</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Overall</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">MMAU (Best)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.35</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">49.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">64.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.30</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Audio-CoT</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">62.16</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">55.99</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.16</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">58.10</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Audio-Reasoner</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">60.06</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">64.30</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">60.70</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">61.71</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Ours (Gemini + Symbolic)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">73.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">64.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">82.28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">73.5</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "speech",
            "overall",
            "audiocot",
            "gemini",
            "symbolic",
            "accuracy",
            "sound",
            "ours",
            "music",
            "best",
            "baseline",
            "methods",
            "method",
            "mmau",
            "comparison",
            "audioreasoner",
            "taskwise"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Large language models (LLMs) have advanced in text and vision, but their reasoning on audio remains limited. Most existing methods rely on dense audio embeddings, which are difficult to interpret and often fail on structured reasoning tasks. Caption-based approaches, introduced in recent benchmarks such as MMAU, improve performance by translating audio into text, yet still depend on dense embeddings as input, offering little insight when models fail.</span>\n</p>\n\n",
                "matched_terms": [
                    "mmau",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We present SAR-LM, a symbolic audio reasoning pipeline that builds on this caption-based paradigm by converting audio into structured, human-readable features across speech, sound events, and music. These symbolic inputs support both reasoning and transparent error analysis, enabling us to trace failures to specific features. Across three benchmarks, MMAU, MMAR, and OmniBench, SAR-LM achieves competitive results, while prioritizing interpretability as its primary contribution.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "sound",
                    "music",
                    "mmau"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Reasoning about sound is essential for building AI systems that can understand the world. While humans easily infer who is speaking, what their intention is, or what caused a noise, large language models (LLMs) still lag behind in this ability. Most prior work has evaluated LLM reasoning on text or vision </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2023largereasoning</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yu2023sevila</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with audio reasoning only recently gaining traction. Current approaches typically rely on dense audio embeddings </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which achieve moderate accuracy but are difficult to interpret and provide little insight into why models fail.</span>\n</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we explore symbolic audio reasoning, where raw audio is translated into structured, human-readable features. We extract transcripts, speech emotion, sound events, musical notes, chords, and tags using pretrained and signal processing models. These features can be used directly or summarized into captions, and reasoning is then performed by an LLM. Because each symbolic layer is explicit, failure cases can be traced to specific components, making the pipeline more explainable than end-to-end captioning. We evaluate SAR-LM on three benchmarks: MMAU </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, MMAR </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025mmar</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and OmniBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">omnibench2024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with further details provided in Sections&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S2\" style=\"font-size:90%;\" title=\"2 Related Work &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.SS1\" style=\"font-size:90%;\" title=\"4.1 Setup &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">4.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nOur results show that symbolic reasoning enables detailed error analysis while achieving competitive accuracy, particularly in speech and environmental sound reasoning, with interpretability as the main contribution.\nTo the best of our knowledge, this is the first work to introduce symbolic audio reasoning with large language models. Our main contributions are:</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "sound",
                    "best",
                    "mmau",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Benchmarks such as MMAU </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, MMAR </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025mmar</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and OmniBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">omnibench2024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> define the task across speech, music, and environmental audio, often requiring temporal reasoning. Early methods include Audio-CoT </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025audiocot</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which applied chain-of-thought prompting </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wei2022chainofthought</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Audio-Reasoner </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">audio_reasoner2025</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which refined prompt structures with additional objectives.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audiocot",
                    "music",
                    "methods",
                    "mmau",
                    "audioreasoner"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Instead of relying on dense embeddings, which are hard for LLMs to interpret, we convert each audio clip into symbolic, time-aligned features represented in text. These cover multiple semantic layers, sound events, speech, emotion, and music, with content-aware filtering to include only relevant features.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "sound",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sound events.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We use PANNs </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">panns_2020</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to generate timestamped tags (e.g., footsteps, laughter, music), which both summarize the clip and guide which other modules are applied.</span>\n</p>\n\n",
                "matched_terms": [
                    "sound",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Whisper-large </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper_2023</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides full transcripts, while DAWN Transformer </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dawn_transformer_2023</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> predicts valence&#8211;arousal&#8211;dominance values that we discretize into symbolic emotion labels.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Music.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> MT3 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mt3_2021</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> outputs symbolic note sequences with pitch, timing, and instrument information. Chordino </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mauch2009chordino</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">holloway_chord_extractor_2021</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> extracts chord progressions, and Musicnn</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/jordipons/musicnn\" title=\"\">https://github.com/jordipons/musicnn</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides high-level tags describing genre or timbre.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We use PANNs as a reference to decide which features to include: if music is detected, we add notes, chords, and tags; if speech is detected, we add transcripts and emotion labels. This content-aware selection keeps prompts concise by including only relevant features.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluated several large language models for both caption generation and reasoning. Qwen3-32B&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen3technicalreport</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> often produced unstable outputs, sometimes overthinking even simple questions. Qwen3-30B-A3B-Instruct-2507&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen3technicalreport</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> gave more reliable predictions, while Qwen2.5-Omni-7B&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2.5-Omni</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> was tested as both captioner and reasoner but frequently hallucinated when generating captions from symbolic features. Based on these comparisons, we adopt Gemini&#160;2.5&#160;Pro&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25pro</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as our final choice for caption generation and reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "symbolic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate on three benchmarks. MMAU&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a large-scale benchmark of 10k audio clips and 27 task types spanning speech, music, and environmental sounds, designed to test advanced perception and reasoning. We use the released mini-test set (1k samples) since its ground-truth labels are public. MMAR&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025mmar</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides 1k high-quality audio&#8211;QA pairs with multi-step chain-of-thought annotations, emphasizing deep reasoning across speech, music, and mixed audio. OmniBench&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">omnibench2024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> extends evaluation to tri-modal settings, requiring reasoning over acoustic, visual, and textual inputs. Each dataset provides audio clips paired with multiple-choice questions, and we report reasoning accuracy overall and by category (speech, music, sound).</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "overall",
                    "sound",
                    "music",
                    "mmau",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Overall Accuracy Across Datasets &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares three reasoning models on symbolic features across MMAU, MMAR, and OmniBench.\nGemini&#160;2.5&#160;Pro achieves the highest accuracy on all datasets, with a clear advantage on MMAR (69.3%).\nQwen3-Instruct improves stability over Qwen2.5-Omni but still lags behind Gemini.</span>\n</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "mmau",
                    "symbolic",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The symbolic pipeline involves many potential features, such as transcripts, sound events, chords, and music tags. Some of these are highly informative, while others may add noise depending on the clip. Manually testing the contribution of each feature is impractical, as the search space grows quickly. To address this, we introduced a GPT-style agent that selects relevant features per sample. Qwen2.5-Omni proved unstable in this role, often hallucinating selections, whereas Gemini&#160;2.5&#160;Pro provided consistent and meaningful choices. This dynamic selection helped reduce irrelevant inputs and improved downstream reasoning, as we demonstrate in the MMAU results.</span>\n</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "symbolic",
                    "sound",
                    "music",
                    "mmau"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.4 Detailed Results on MMAU &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports task-wise and overall accuracy for different input formats.\nCaptions are generated by Gemini&#160;2.5&#160;Pro, and the GPT-style agent for dynamic feature selection also uses Gemini&#160;2.5&#160;Pro.\nReasoning is performed with Qwen3-30B-A3B-Instruct-2507 for controlled symbolic comparisons.\nWe include both fixed symbolic inputs and dynamic agent-based selection.\nFlat symbolic features achieve strong performance in speech and sound reasoning, while symbolic captions perform slightly better on music.\nAgent-based selection provides consistent gains over non-agent variants, confirming that filtering out irrelevant features helps reduce noise.\nEnd-to-end captions are competitive for music tasks but remain less interpretable.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "overall",
                    "gemini",
                    "symbolic",
                    "sound",
                    "music",
                    "accuracy",
                    "taskwise"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We also evaluated Gemini&#160;2.5&#160;Pro directly as a reasoning model (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.4 Detailed Results on MMAU &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nWith raw audio as input, Gemini reaches the highest overall accuracy (78.6%), showing the advantage of end-to-end access to the waveform.\nWhen reasoning over symbolic features, it still achieves 73.5%, which is close in accuracy while providing far greater interpretability.</span>\n</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "overall",
                    "symbolic",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We compare our approach against reported baselines on the MMAU, MMAR, and OmniBench benchmarks.\nIn all cases, we report our best-performing configuration: Gemini&#160;2.5&#160;Pro as reasoner with symbolic features as input.\nResults show consistent improvements over prior systems, particularly in speech reasoning and long-context MMAR tasks, while performance on OmniBench remains weaker, reflecting the added difficulty of tri-modal reasoning and highlighting an area for future improvement.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "gemini",
                    "mmau"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We presented SAR-LM, a modular pipeline for symbolic audio reasoning with large language models.\nOur experiments across MMAU, MMAR, and OmniBench showed that symbolic inputs enable competitive reasoning performance while retaining interpretability that helps diagnose errors such as missed temporal events.\nResults highlight the potential of combining symbolic structure with strong reasoning models to outperform existing baselines, particularly in speech and long-context reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mmau",
                    "symbolic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">A limitation of this approach is that running multiple specialized models for feature extraction can be computationally expensive compared to end-to-end pipelines.\nIn addition, performance is sensitive to the quality of symbolic features, as errors in speech recognition or music transcription can propagate to the reasoning stage.\nThese trade-offs underline the need to balance efficiency with interpretability.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Future work will focus on strengthening the feature extraction stage, which remains the main bottleneck.\nOne direction is to incorporate a universal sound recognition backend to reduce missed events from domain-specific models (e.g., PANNs).\nAnother is to integrate stronger pretrained audio encoders such as MERT, fine-tuned per dataset, to capture richer and more generalisable features.\nUltimately, unifying feature extraction across all audio types could yield both interpretability and improved accuracy, pushing symbolic reasoning systems beyond current end-to-end baselines.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "accuracy",
                    "sound"
                ]
            }
        ]
    },
    "S4.T5": {
        "source_file": "SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS",
        "caption": "Table 5: Comparison with baseline methods on MMAR (task-wise accuracy, %).",
        "body": "Method\nSound\nMusic\nSpeech\nOverall\n\n\n\n\nMMAR (Best)\n61.21\n50.97\n72.11\n65.6\n\n\nOurs (Gemini + Symbolic)\n52.73\n56.31\n80.95\n69.3",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sound</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Music</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Overall</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">MMAR (Best)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">61.21</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">50.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">72.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">65.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">Ours (Gemini + Symbolic)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">52.73</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">56.31</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">80.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">69.3</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "speech",
            "overall",
            "gemini",
            "symbolic",
            "accuracy",
            "sound",
            "ours",
            "music",
            "best",
            "baseline",
            "methods",
            "method",
            "comparison",
            "mmar",
            "taskwise"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We present SAR-LM, a symbolic audio reasoning pipeline that builds on this caption-based paradigm by converting audio into structured, human-readable features across speech, sound events, and music. These symbolic inputs support both reasoning and transparent error analysis, enabling us to trace failures to specific features. Across three benchmarks, MMAU, MMAR, and OmniBench, SAR-LM achieves competitive results, while prioritizing interpretability as its primary contribution.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "sound",
                    "music",
                    "mmar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Reasoning about sound is essential for building AI systems that can understand the world. While humans easily infer who is speaking, what their intention is, or what caused a noise, large language models (LLMs) still lag behind in this ability. Most prior work has evaluated LLM reasoning on text or vision </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2023largereasoning</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yu2023sevila</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with audio reasoning only recently gaining traction. Current approaches typically rely on dense audio embeddings </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which achieve moderate accuracy but are difficult to interpret and provide little insight into why models fail.</span>\n</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we explore symbolic audio reasoning, where raw audio is translated into structured, human-readable features. We extract transcripts, speech emotion, sound events, musical notes, chords, and tags using pretrained and signal processing models. These features can be used directly or summarized into captions, and reasoning is then performed by an LLM. Because each symbolic layer is explicit, failure cases can be traced to specific components, making the pipeline more explainable than end-to-end captioning. We evaluate SAR-LM on three benchmarks: MMAU </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, MMAR </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025mmar</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and OmniBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">omnibench2024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with further details provided in Sections&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S2\" style=\"font-size:90%;\" title=\"2 Related Work &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.SS1\" style=\"font-size:90%;\" title=\"4.1 Setup &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">4.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nOur results show that symbolic reasoning enables detailed error analysis while achieving competitive accuracy, particularly in speech and environmental sound reasoning, with interpretability as the main contribution.\nTo the best of our knowledge, this is the first work to introduce symbolic audio reasoning with large language models. Our main contributions are:</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "sound",
                    "best",
                    "accuracy",
                    "mmar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Benchmarks such as MMAU </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, MMAR </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025mmar</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and OmniBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">omnibench2024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> define the task across speech, music, and environmental audio, often requiring temporal reasoning. Early methods include Audio-CoT </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025audiocot</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which applied chain-of-thought prompting </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wei2022chainofthought</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Audio-Reasoner </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">audio_reasoner2025</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which refined prompt structures with additional objectives.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mmar",
                    "methods",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Instead of relying on dense embeddings, which are hard for LLMs to interpret, we convert each audio clip into symbolic, time-aligned features represented in text. These cover multiple semantic layers, sound events, speech, emotion, and music, with content-aware filtering to include only relevant features.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "sound",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sound events.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We use PANNs </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">panns_2020</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to generate timestamped tags (e.g., footsteps, laughter, music), which both summarize the clip and guide which other modules are applied.</span>\n</p>\n\n",
                "matched_terms": [
                    "sound",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Whisper-large </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper_2023</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides full transcripts, while DAWN Transformer </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dawn_transformer_2023</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> predicts valence&#8211;arousal&#8211;dominance values that we discretize into symbolic emotion labels.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Music.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> MT3 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mt3_2021</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> outputs symbolic note sequences with pitch, timing, and instrument information. Chordino </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mauch2009chordino</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">holloway_chord_extractor_2021</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> extracts chord progressions, and Musicnn</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/jordipons/musicnn\" title=\"\">https://github.com/jordipons/musicnn</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides high-level tags describing genre or timbre.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We use PANNs as a reference to decide which features to include: if music is detected, we add notes, chords, and tags; if speech is detected, we add transcripts and emotion labels. This content-aware selection keeps prompts concise by including only relevant features.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluated several large language models for both caption generation and reasoning. Qwen3-32B&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen3technicalreport</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> often produced unstable outputs, sometimes overthinking even simple questions. Qwen3-30B-A3B-Instruct-2507&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen3technicalreport</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> gave more reliable predictions, while Qwen2.5-Omni-7B&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2.5-Omni</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> was tested as both captioner and reasoner but frequently hallucinated when generating captions from symbolic features. Based on these comparisons, we adopt Gemini&#160;2.5&#160;Pro&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25pro</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as our final choice for caption generation and reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "symbolic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate on three benchmarks. MMAU&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a large-scale benchmark of 10k audio clips and 27 task types spanning speech, music, and environmental sounds, designed to test advanced perception and reasoning. We use the released mini-test set (1k samples) since its ground-truth labels are public. MMAR&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025mmar</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides 1k high-quality audio&#8211;QA pairs with multi-step chain-of-thought annotations, emphasizing deep reasoning across speech, music, and mixed audio. OmniBench&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">omnibench2024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> extends evaluation to tri-modal settings, requiring reasoning over acoustic, visual, and textual inputs. Each dataset provides audio clips paired with multiple-choice questions, and we report reasoning accuracy overall and by category (speech, music, sound).</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "overall",
                    "sound",
                    "music",
                    "accuracy",
                    "mmar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Overall Accuracy Across Datasets &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares three reasoning models on symbolic features across MMAU, MMAR, and OmniBench.\nGemini&#160;2.5&#160;Pro achieves the highest accuracy on all datasets, with a clear advantage on MMAR (69.3%).\nQwen3-Instruct improves stability over Qwen2.5-Omni but still lags behind Gemini.</span>\n</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "symbolic",
                    "accuracy",
                    "mmar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The symbolic pipeline involves many potential features, such as transcripts, sound events, chords, and music tags. Some of these are highly informative, while others may add noise depending on the clip. Manually testing the contribution of each feature is impractical, as the search space grows quickly. To address this, we introduced a GPT-style agent that selects relevant features per sample. Qwen2.5-Omni proved unstable in this role, often hallucinating selections, whereas Gemini&#160;2.5&#160;Pro provided consistent and meaningful choices. This dynamic selection helped reduce irrelevant inputs and improved downstream reasoning, as we demonstrate in the MMAU results.</span>\n</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "symbolic",
                    "sound",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.4 Detailed Results on MMAU &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports task-wise and overall accuracy for different input formats.\nCaptions are generated by Gemini&#160;2.5&#160;Pro, and the GPT-style agent for dynamic feature selection also uses Gemini&#160;2.5&#160;Pro.\nReasoning is performed with Qwen3-30B-A3B-Instruct-2507 for controlled symbolic comparisons.\nWe include both fixed symbolic inputs and dynamic agent-based selection.\nFlat symbolic features achieve strong performance in speech and sound reasoning, while symbolic captions perform slightly better on music.\nAgent-based selection provides consistent gains over non-agent variants, confirming that filtering out irrelevant features helps reduce noise.\nEnd-to-end captions are competitive for music tasks but remain less interpretable.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "overall",
                    "gemini",
                    "symbolic",
                    "sound",
                    "music",
                    "accuracy",
                    "taskwise"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We also evaluated Gemini&#160;2.5&#160;Pro directly as a reasoning model (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.4 Detailed Results on MMAU &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nWith raw audio as input, Gemini reaches the highest overall accuracy (78.6%), showing the advantage of end-to-end access to the waveform.\nWhen reasoning over symbolic features, it still achieves 73.5%, which is close in accuracy while providing far greater interpretability.</span>\n</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "overall",
                    "symbolic",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We compare our approach against reported baselines on the MMAU, MMAR, and OmniBench benchmarks.\nIn all cases, we report our best-performing configuration: Gemini&#160;2.5&#160;Pro as reasoner with symbolic features as input.\nResults show consistent improvements over prior systems, particularly in speech reasoning and long-context MMAR tasks, while performance on OmniBench remains weaker, reflecting the added difficulty of tri-modal reasoning and highlighting an area for future improvement.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "gemini",
                    "mmar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We presented SAR-LM, a modular pipeline for symbolic audio reasoning with large language models.\nOur experiments across MMAU, MMAR, and OmniBench showed that symbolic inputs enable competitive reasoning performance while retaining interpretability that helps diagnose errors such as missed temporal events.\nResults highlight the potential of combining symbolic structure with strong reasoning models to outperform existing baselines, particularly in speech and long-context reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "mmar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">A limitation of this approach is that running multiple specialized models for feature extraction can be computationally expensive compared to end-to-end pipelines.\nIn addition, performance is sensitive to the quality of symbolic features, as errors in speech recognition or music transcription can propagate to the reasoning stage.\nThese trade-offs underline the need to balance efficiency with interpretability.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Future work will focus on strengthening the feature extraction stage, which remains the main bottleneck.\nOne direction is to incorporate a universal sound recognition backend to reduce missed events from domain-specific models (e.g., PANNs).\nAnother is to integrate stronger pretrained audio encoders such as MERT, fine-tuned per dataset, to capture richer and more generalisable features.\nUltimately, unifying feature extraction across all audio types could yield both interpretability and improved accuracy, pushing symbolic reasoning systems beyond current end-to-end baselines.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "accuracy",
                    "sound"
                ]
            }
        ]
    },
    "S4.T6": {
        "source_file": "SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS",
        "caption": "Table 6: Comparison with baseline methods on OmniBench (task-wise accuracy, %).",
        "body": "Method\nSound\nMusic\nSpeech\n\n\n\n\nOmniBench (Best)\n60.00\n52.83\n55.25\n\n\nOurs (Gemini + Symbolic)\n29.43\n48.11\n40.60",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sound</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Music</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">OmniBench (Best)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">60.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">52.83</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">55.25</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">Ours (Gemini + Symbolic)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.43</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">48.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">40.60</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "speech",
            "gemini",
            "omnibench",
            "symbolic",
            "accuracy",
            "sound",
            "ours",
            "music",
            "best",
            "baseline",
            "methods",
            "method",
            "comparison",
            "taskwise"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We present SAR-LM, a symbolic audio reasoning pipeline that builds on this caption-based paradigm by converting audio into structured, human-readable features across speech, sound events, and music. These symbolic inputs support both reasoning and transparent error analysis, enabling us to trace failures to specific features. Across three benchmarks, MMAU, MMAR, and OmniBench, SAR-LM achieves competitive results, while prioritizing interpretability as its primary contribution.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "omnibench",
                    "sound",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Reasoning about sound is essential for building AI systems that can understand the world. While humans easily infer who is speaking, what their intention is, or what caused a noise, large language models (LLMs) still lag behind in this ability. Most prior work has evaluated LLM reasoning on text or vision </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2023largereasoning</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yu2023sevila</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with audio reasoning only recently gaining traction. Current approaches typically rely on dense audio embeddings </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which achieve moderate accuracy but are difficult to interpret and provide little insight into why models fail.</span>\n</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we explore symbolic audio reasoning, where raw audio is translated into structured, human-readable features. We extract transcripts, speech emotion, sound events, musical notes, chords, and tags using pretrained and signal processing models. These features can be used directly or summarized into captions, and reasoning is then performed by an LLM. Because each symbolic layer is explicit, failure cases can be traced to specific components, making the pipeline more explainable than end-to-end captioning. We evaluate SAR-LM on three benchmarks: MMAU </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, MMAR </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025mmar</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and OmniBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">omnibench2024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with further details provided in Sections&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S2\" style=\"font-size:90%;\" title=\"2 Related Work &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.SS1\" style=\"font-size:90%;\" title=\"4.1 Setup &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">4.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nOur results show that symbolic reasoning enables detailed error analysis while achieving competitive accuracy, particularly in speech and environmental sound reasoning, with interpretability as the main contribution.\nTo the best of our knowledge, this is the first work to introduce symbolic audio reasoning with large language models. Our main contributions are:</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "omnibench",
                    "sound",
                    "best",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Benchmarks such as MMAU </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, MMAR </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025mmar</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and OmniBench </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">omnibench2024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> define the task across speech, music, and environmental audio, often requiring temporal reasoning. Early methods include Audio-CoT </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025audiocot</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which applied chain-of-thought prompting </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wei2022chainofthought</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Audio-Reasoner </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">audio_reasoner2025</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which refined prompt structures with additional objectives.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "omnibench",
                    "methods",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Instead of relying on dense embeddings, which are hard for LLMs to interpret, we convert each audio clip into symbolic, time-aligned features represented in text. These cover multiple semantic layers, sound events, speech, emotion, and music, with content-aware filtering to include only relevant features.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "sound",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sound events.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We use PANNs </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">panns_2020</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to generate timestamped tags (e.g., footsteps, laughter, music), which both summarize the clip and guide which other modules are applied.</span>\n</p>\n\n",
                "matched_terms": [
                    "sound",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Whisper-large </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper_2023</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides full transcripts, while DAWN Transformer </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dawn_transformer_2023</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> predicts valence&#8211;arousal&#8211;dominance values that we discretize into symbolic emotion labels.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Music.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> MT3 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mt3_2021</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> outputs symbolic note sequences with pitch, timing, and instrument information. Chordino </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mauch2009chordino</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">holloway_chord_extractor_2021</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> extracts chord progressions, and Musicnn</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/jordipons/musicnn\" title=\"\">https://github.com/jordipons/musicnn</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides high-level tags describing genre or timbre.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We use PANNs as a reference to decide which features to include: if music is detected, we add notes, chords, and tags; if speech is detected, we add transcripts and emotion labels. This content-aware selection keeps prompts concise by including only relevant features.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluated several large language models for both caption generation and reasoning. Qwen3-32B&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen3technicalreport</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> often produced unstable outputs, sometimes overthinking even simple questions. Qwen3-30B-A3B-Instruct-2507&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen3technicalreport</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> gave more reliable predictions, while Qwen2.5-Omni-7B&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2.5-Omni</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> was tested as both captioner and reasoner but frequently hallucinated when generating captions from symbolic features. Based on these comparisons, we adopt Gemini&#160;2.5&#160;Pro&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25pro</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as our final choice for caption generation and reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "symbolic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate on three benchmarks. MMAU&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sakshi2024mmau</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a large-scale benchmark of 10k audio clips and 27 task types spanning speech, music, and environmental sounds, designed to test advanced perception and reasoning. We use the released mini-test set (1k samples) since its ground-truth labels are public. MMAR&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2025mmar</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides 1k high-quality audio&#8211;QA pairs with multi-step chain-of-thought annotations, emphasizing deep reasoning across speech, music, and mixed audio. OmniBench&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">omnibench2024</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> extends evaluation to tri-modal settings, requiring reasoning over acoustic, visual, and textual inputs. Each dataset provides audio clips paired with multiple-choice questions, and we report reasoning accuracy overall and by category (speech, music, sound).</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "omnibench",
                    "sound",
                    "music",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Overall Accuracy Across Datasets &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares three reasoning models on symbolic features across MMAU, MMAR, and OmniBench.\nGemini&#160;2.5&#160;Pro achieves the highest accuracy on all datasets, with a clear advantage on MMAR (69.3%).\nQwen3-Instruct improves stability over Qwen2.5-Omni but still lags behind Gemini.</span>\n</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "symbolic",
                    "omnibench",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The symbolic pipeline involves many potential features, such as transcripts, sound events, chords, and music tags. Some of these are highly informative, while others may add noise depending on the clip. Manually testing the contribution of each feature is impractical, as the search space grows quickly. To address this, we introduced a GPT-style agent that selects relevant features per sample. Qwen2.5-Omni proved unstable in this role, often hallucinating selections, whereas Gemini&#160;2.5&#160;Pro provided consistent and meaningful choices. This dynamic selection helped reduce irrelevant inputs and improved downstream reasoning, as we demonstrate in the MMAU results.</span>\n</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "symbolic",
                    "sound",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.4 Detailed Results on MMAU &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports task-wise and overall accuracy for different input formats.\nCaptions are generated by Gemini&#160;2.5&#160;Pro, and the GPT-style agent for dynamic feature selection also uses Gemini&#160;2.5&#160;Pro.\nReasoning is performed with Qwen3-30B-A3B-Instruct-2507 for controlled symbolic comparisons.\nWe include both fixed symbolic inputs and dynamic agent-based selection.\nFlat symbolic features achieve strong performance in speech and sound reasoning, while symbolic captions perform slightly better on music.\nAgent-based selection provides consistent gains over non-agent variants, confirming that filtering out irrelevant features helps reduce noise.\nEnd-to-end captions are competitive for music tasks but remain less interpretable.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "gemini",
                    "sound",
                    "music",
                    "accuracy",
                    "taskwise"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We also evaluated Gemini&#160;2.5&#160;Pro directly as a reasoning model (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06483v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.4 Detailed Results on MMAU &#8227; 4 Experiments and Results &#8227; SAR-LM: SYMBOLIC AUDIO REASONING WITH LARGE LANGUAGE MODELS\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nWith raw audio as input, Gemini reaches the highest overall accuracy (78.6%), showing the advantage of end-to-end access to the waveform.\nWhen reasoning over symbolic features, it still achieves 73.5%, which is close in accuracy while providing far greater interpretability.</span>\n</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "symbolic",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We compare our approach against reported baselines on the MMAU, MMAR, and OmniBench benchmarks.\nIn all cases, we report our best-performing configuration: Gemini&#160;2.5&#160;Pro as reasoner with symbolic features as input.\nResults show consistent improvements over prior systems, particularly in speech reasoning and long-context MMAR tasks, while performance on OmniBench remains weaker, reflecting the added difficulty of tri-modal reasoning and highlighting an area for future improvement.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "gemini",
                    "omnibench"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We presented SAR-LM, a modular pipeline for symbolic audio reasoning with large language models.\nOur experiments across MMAU, MMAR, and OmniBench showed that symbolic inputs enable competitive reasoning performance while retaining interpretability that helps diagnose errors such as missed temporal events.\nResults highlight the potential of combining symbolic structure with strong reasoning models to outperform existing baselines, particularly in speech and long-context reasoning.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "omnibench"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">A limitation of this approach is that running multiple specialized models for feature extraction can be computationally expensive compared to end-to-end pipelines.\nIn addition, performance is sensitive to the quality of symbolic features, as errors in speech recognition or music transcription can propagate to the reasoning stage.\nThese trade-offs underline the need to balance efficiency with interpretability.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "symbolic",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Future work will focus on strengthening the feature extraction stage, which remains the main bottleneck.\nOne direction is to incorporate a universal sound recognition backend to reduce missed events from domain-specific models (e.g., PANNs).\nAnother is to integrate stronger pretrained audio encoders such as MERT, fine-tuned per dataset, to capture richer and more generalisable features.\nUltimately, unifying feature extraction across all audio types could yield both interpretability and improved accuracy, pushing symbolic reasoning systems beyond current end-to-end baselines.</span>\n</p>\n\n",
                "matched_terms": [
                    "symbolic",
                    "accuracy",
                    "sound"
                ]
            }
        ]
    }
}